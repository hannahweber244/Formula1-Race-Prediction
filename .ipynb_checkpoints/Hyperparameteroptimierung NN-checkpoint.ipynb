{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Version ist die unaufbereitete Version, und verwendet noch Funktionen, welche vor Abgabe verändert werden müssen!\n",
    "\n",
    "Angepasst werden muss:\n",
    "<br>[x] nogo_columns\n",
    "<br>[x] rain id muss raus und an andere stelle\n",
    "<br>[x] Boxenstopberechnung muss raus\n",
    "<br>[x] Zielspalte muss umbenannt werden --> irgendwas mit Minuten\n",
    "<br>[ ] Hyperparameteroptimierung muss fertiggestellt werden\n",
    "<br>[ ] finales Testing muss rein oder in ein separates Notebook\n",
    "<br>[ ]  Optimierung des p von Dropout Layer?\n",
    "<br>[x] nicht nur Vorhersagen( Kommazahlen) vergleichen, sondern direkt sortieren und Podiums Positionen als klare Klassen übergeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import pandasql as sqldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel dieses Notebooks ist es Klassen bereit zu stellen, welche anhand einer Grid Search orientierten Methode die Hyperparameter eines Neuronalen Netzes optimieren. \n",
    "\n",
    "In der ersten Klasse werden die Hyperparameter Layeranzahl und Neuronenanzahl betrachtet, während die zweite Klasse die Anzahl von Trainingsepochen und die Lernrate für den Adam Optimizer optimiert.\n",
    "\n",
    "Für die Optimierung wird zusätzlich eine Klasse benötigt, welche dynamisch Neuronale Netze erzeugt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory = 'sliced_data'):\n",
    "    '''\n",
    "        Funktion, die die aufbereiteten/vorbereiteten Daten aus existierenden CSV Dateien \n",
    "        einliest und je nach Vollständigkeit in zwei Dictionaries abspeichert,\n",
    "        geschlüsselt nach der jeweiligen RaceId\n",
    "    '''\n",
    "    if os.path.exists(directory):\n",
    "        csv_filenames = []\n",
    "        #auslesen aller csv file dateinamen aus formula 1 datensatz und abspeichern in liste\n",
    "        for filename in os.listdir(os.getcwd()+'/'+directory):\n",
    "            typ = filename.split('.')[-1]\n",
    "            name = filename.split('.')[0]\n",
    "            if typ == 'csv':\n",
    "                csv_filenames.append(filename)\n",
    "        sliced_races = {}\n",
    "        #einlesen und abspeichern als dataframe aller dateien\n",
    "        for file in csv_filenames:\n",
    "            try:\n",
    "                df = pd.read_csv(directory+'/'+file, engine = 'python', sep = ';', decimal = '.')\n",
    "                del df['Unnamed: 0']\n",
    "            except Exception as e:\n",
    "                df = pd.read_csv(directory+'/'+file, engine = 'c', sep = ';', decimal = '.')\n",
    "                del df['Unnamed: 0']\n",
    "                print(e)\n",
    "            f = int(file.split('_')[-1].split('.')[0]) #raceid wird als key gesetzt\n",
    "            sliced_races[f] = df\n",
    "        print('Einlesen der '+directory+' Dateien erfolgreich')\n",
    "    else:\n",
    "        raise ('sliced Dateien können nicht eingelesen werden, da kein entsprechendes Verzeichnis existiert!')\n",
    "   \n",
    "    return sliced_races\n",
    "\n",
    "def train_dev_test(data_dict, train_p = 0.7, dev_p = 0.2, test_p = 0.1, nogo_columns = []):\n",
    "    \n",
    "    if round(train_p+dev_p+test_p,1) !=1.0:\n",
    "        raise ValueError ('No valid train/dev/test distribution')\n",
    "    \n",
    "    '''\n",
    "        Daten werde in einem Dictionary übergeben, Dataframes werden in dieser \n",
    "        Funktion geshuffled und dann in einen Traindatensatz, einen Development-\n",
    "        datensatz und in einen Testdatensatz aufgeteilt.\n",
    "    '''\n",
    "    #aufteilen in train, dev, test counter\n",
    "    train_count = round(len(data_dict.keys())*train_p, 0)\n",
    "    dev_count = train_count+round(len(data_dict.keys())*dev_p,0)\n",
    "    test_count = len(data_dict.keys())-(train_count+dev_count)\n",
    "    \n",
    "    #shufflen der übergebenen Daten\n",
    "    keys = list(data_dict.keys())\n",
    "    random.shuffle(keys)\n",
    "    data_shuffled = {}\n",
    "    for key in keys:\n",
    "        data_shuffled[key] = data_dict[key]\n",
    "        \n",
    "    #erzeugen separater train,dev,test dictionaries\n",
    "    train = {}\n",
    "    dev = {}\n",
    "    test = {}\n",
    "    c = 0\n",
    "    \n",
    "    #daten sollen nicht in tensoren umgewandelt werden\n",
    "    for id, df in data_shuffled.items():\n",
    "        #entfernen nicht gewollter spalten aus dataframe\n",
    "        cols = [col for col in df.columns if col not in nogo_columns]\n",
    "        df = df[cols]\n",
    "        if c < train_count:\n",
    "            train[id] = df\n",
    "        elif c >= train_count and c < dev_count:\n",
    "            dev[id] = df\n",
    "        else:\n",
    "            test[id] = df\n",
    "        c += 1\n",
    "                \n",
    "    return train, dev, test\n",
    "\n",
    "def to_tensor(train_data, dev_data, test_data, nogo_columns = []):\n",
    "    '''\n",
    "        Funktion erhält zuvor erzeugte train, dev und test Dictionarys mit Formel 1 Rennen\n",
    "        und wandelt die DateFrames in Tensoren um. Den Tensoren wird jeweils ihr Target Value\n",
    "        (podium_podition) zugeordnet. In Dictionary Form werden die Tensoren zurück gegeben.\n",
    "        Nicht gewollte Spalten (in nogo_columns) werden entfernt.\n",
    "    '''\n",
    "    train = []\n",
    "    train_ = {}\n",
    "    dev = []\n",
    "    dev_ = {}\n",
    "    test = []\n",
    "    test_ = {}\n",
    "    \n",
    "    for id, race in train_data.items():\n",
    "        for did in race.driverId.unique():\n",
    "            temp = race.where(race.driverId == did).dropna(how = \"all\")\n",
    "            temp_y = list(temp[\"podium_position\"])\n",
    "            #nicht gewollte Attribute werden aus Datensatz entfernt (bspw. podium_position)\n",
    "            cols = [col for col in temp.columns if col not in nogo_columns]\n",
    "            temp_x = temp[cols]\n",
    "            #die letze runde eines Fahrers wird betrachtet (enthält kumulierte Informationen über zuvor gefahrene Runden)\n",
    "            temp_x = temp_x.tail(1)\n",
    "            x_tensor = torch.tensor(temp_x[cols].values)\n",
    "            train.append((x_tensor, [temp_y[0]]))\n",
    "        train_[id] = train\n",
    "        train = []\n",
    "    for id, race in dev_data.items():\n",
    "        for did in race.driverId.unique():\n",
    "            temp = race.where(race.driverId == did).dropna(how = \"all\")\n",
    "            temp_y = list(temp[\"podium_position\"])\n",
    "            cols = [col for col in temp.columns if col not in nogo_columns]\n",
    "            temp_x = temp[cols]\n",
    "            temp_x = temp_x.tail(1)\n",
    "            x_tensor = torch.tensor(temp_x[temp_x.columns].values)\n",
    "            dev.append((x_tensor, [temp_y[0]]))\n",
    "        dev_[id] = dev\n",
    "        dev = []\n",
    "    for id, race in test_data.items():\n",
    "        for did in race.driverId.unique():\n",
    "            temp = race.where(race.driverId == did).dropna(how = \"all\")\n",
    "            temp_y = list(temp[\"podium_position\"])\n",
    "            cols = [col for col in temp.columns if col not in nogo_columns]\n",
    "            temp_x = temp[cols]\n",
    "            temp_x = temp_x.tail(1)\n",
    "            x_tensor = torch.tensor(temp_x[temp_x.columns].values)\n",
    "            test.append((x_tensor, [temp_y[0]]))\n",
    "        test_[id] = test\n",
    "        test = []\n",
    "        \n",
    "    return train_,dev_,test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen der Dateien als Dataframes, welche in zwei Dictionaries gespeichert werden:<br>\n",
    "- Attribut directory verweißt auf den Ordnernamen der sliced Dateien, Default ist 'sliced_data'\n",
    "- Rennen sind standardmäßig nach 50% gesliced (siehe hierzu Datenaufbereitung)\n",
    "\n",
    "<br> Einmal für das ganze Notebook wird festgelegt, ob Cuda beim Training der Neuronalen Netze verwendet werden soll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda? [y/n]: n\n",
      "Einlesen der sliced_data Dateien erfolgreich\n"
     ]
    }
   ],
   "source": [
    "cuda = input('Cuda? [y/n]: ')\n",
    "sliced_races = load_data(directory = 'sliced_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufteilen des Datensatzes in einen Trainings-, einen Development- und einen Testteil mit Hilfe der Funktion train_dev_test(). Standardmäßig wird der Datensatz zu 70% in Trainings-, zu 20% in Dev- und zu 10% in Testdaten aufgeteilt. Die Verteilung kann mit den Parametern train_p,dev_p,test_p die der Funktion train_dev_test() übergeben werden können, angepasst werden. Wichtig ist bloß, dass insgesamt train_p,dev_p und test_p 1 ergeben. Die Datenbeispiele aus sliced_races werden vor dem Aufteilen geshuffled. Die Funktion train_dev_test() gibt die Datenbeispiele dann wieder in Dictionary Form zurück, wobei jeder RaceId ein DataFrame mit Renndaten zugeordnet wird. Im nächsten Schritt werden die drei Datensammlungen der Funktion to_tensor() übergeben, die nicht gewünschte Spalten aus den Datensätzen entfernt (nogo_columns) und diese dann in Tensoren abspeichert, die wiederrum  ihrem Targetvalue (podiums_position) zugeordnet werden, um die Überprüfung der Vorhersagen später zu vereinfachen. Tupel mit Datensatz und Targetvalue werden dann wieder in dem jeweiligen Dictionary abgespeichert und ihrer RaceId zugeordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 96\n",
      "Dev: 27\n",
      "Test: 14\n",
      "Rennen insgesamt: 137\n",
      "=========================\n",
      "Tensoren:\n",
      "Train: 96\n",
      "Dev: 27\n",
      "Test: 14\n",
      "Rennen insgesamt: 137\n"
     ]
    }
   ],
   "source": [
    "#definieren nicht gewollter Attribute (Attribute die nicht dem Modell übergeben werden sollen):\n",
    "nogo_columns = ['year', \n",
    "                'podium_position', \n",
    "                'raceId',\n",
    "                'lap_number',\n",
    "                'driverId', \n",
    "                'driver_fullname',\n",
    "                'total_milliseconds',\n",
    "               'lap_in_milliseconds',\n",
    "               'total_minutes']\n",
    "#train dev test splitting\n",
    "train, dev, test = train_dev_test(sliced_races)\n",
    "#umwandeln in Tensoren und entfernen der nogo_columns\n",
    "train_T, dev_T, test_T = to_tensor(train, dev, test, nogo_columns)\n",
    "\n",
    "print('Train:',len(train))\n",
    "print('Dev:',len(dev))\n",
    "print('Test:',len(test))\n",
    "print('Rennen insgesamt:', len(train)+len(dev)+len(test))\n",
    "print(25*'=')\n",
    "print('Tensoren:')\n",
    "print('Train:',len(train_T))\n",
    "print('Dev:',len(dev_T))\n",
    "print('Test:',len(test_T))\n",
    "print('Rennen insgesamt:', len(train_T)+len(dev_T)+len(test_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definieren von notwendigen Klassen: \n",
    "    \n",
    "Zuerst werden zwei Klassen für Neuronale Netze definiert. Die erste Klasse definiert ein statisches Neuronales Netz, von dem wir wissen, dass es auf unseren Daten gut funktioniert. Die zweite Klasse definiert ein Dynamisches Neuronales Netz, welches in der Hyperparamteroptimierung verwendet wird, um Netze zu erzeugen, die gegen das statische Netz getestet werden.\n",
    "Der dynamischen Klasse werden Dictionarys übergeben, die spezifizieren wie das Neuronale Netz erzeugt werden soll. Ein Beispiel kann wie folgt aussehen:<br>\n",
    "\n",
    "{\n",
    "                 'first'    :    ['linear', 10,150],<br>\n",
    "                 'relu0'    :    ['linear',150,180],<br>\n",
    "                 'relu1'    :    ['dropout',0,0],<br>\n",
    "                 'no_activation0'    :    ['linear',180,190],<br>\n",
    "                 'relu2'    :    ['linear',190,120],<br>\n",
    "                 'relu3'    :    ['linear',120,100],<br>\n",
    "                 'relu4'    :    ['linear',100,70],<br>\n",
    "                 'relu5'    :    ['linear',70,30],<br>\n",
    "                 'relu6'    :    ['linear',30,1]\n",
    "}<br>\n",
    "         \n",
    "Dieses Dictionary würde ein Netz erzeugen, wie es in der statischen Klasse definiert wird. Die Keys des Dictionarys geben die dem Layer vorhergehende Activationfunction an (keine für das erste Layer, aus diesem Grund das Stichwort first), während der zugeordnete Value eine Liste mit Layerspezifikationen enthält. An erster Stelle wird der Typ des Layers definiert (dropout oder linear), an zweiter Stelle die Anzahl der Input Neuronen und an dritter Stelle die Anzahl der Output Neuronen für das Layer.\n",
    "\n",
    "Die nächsten Klassen werden für die Hyperparameteroptimierung definiert. In der ersten Klasse werden dynamisch Neuronale Netze erzeugt, die mit unterschiedlichen Layeranzahlen und Neuronenanzahlen erzeugt werden. Von jeder Layeranzahl wird eine Variation verschiedener Netze erzeugt, sodass eine feste Anzahl Netze mit einer festen Layeranzahl gegen Netz mit einer anderen Anzahl von Layern getestet werden kann. <br>\n",
    "In der zweiten Klasse werden die Parameter Trainigsepochen und Lernrate für den Adam OPtimizer optimiert. Die Klasse erhält das statische Neuronale Netz und das beste der dynamisch erzeugten Neuronalen Netze und optimiert diese Parameter für beide Netze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Netz(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Netz,self).__init__()\n",
    "        #self.fc1 = nn.Linear(53, 150)\n",
    "        self.fc1 = nn.Linear(52, 150)\n",
    "        self.fc2 = nn.Linear(150, 180)\n",
    "        self.fc3 = nn.Linear(180, 190)\n",
    "        self.fc4 = nn.Linear(190, 120)\n",
    "        self.fc5 = nn.Linear(120, 100)\n",
    "        self.fc6 = nn.Linear(100, 70)\n",
    "        self.fc7 = nn.Linear(70, 30)\n",
    "        self.fc8 = nn.Linear(30, 1)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc2(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc4(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc5(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc6(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc7(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc8(x.float())\n",
    "        return x\n",
    "\n",
    "class NetzDynamic(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_information):\n",
    "        super(NetzDynamic,self).__init__()\n",
    "        self.__layer_information = layer_information\n",
    "        #definieren von einer Moduleliste, die alle Layer des Netzes enthalten wird\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        #erzeugen der in layer_information übergebeben Layer und hinzufügen zu layers Moduleliste\n",
    "        for specs in self.__layer_information.values():\n",
    "            type_ = specs[0]\n",
    "            in_ = specs[1]\n",
    "            out_ = specs[2]\n",
    "            if type_ == 'linear':\n",
    "                self.layers.append(nn.Linear(in_,out_))\n",
    "            if type_ == 'dropout':\n",
    "                self.layers.append(nn.Dropout())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #definieren eines dynamischen forward pass, activation function richtet sich nach dictionary key\n",
    "        lay_idx = 0\n",
    "        for activation, specs in self.__layer_information.items():\n",
    "            if activation.startswith('relu'):\n",
    "                layer = self.layers[lay_idx] #auswählen des relevanten Layers aus layers ModuleList\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = F.relu(x.float())#anwenden der activation function\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = F.relu(x.float())\n",
    "                    x = layer(x.float()) \n",
    "            if activation.startswith('sigmoid'):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = torch.sigmoid(x.float())\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = torch.sigmoid(x.float())\n",
    "                    x = layer(x.float())\n",
    "                    \n",
    "            if activation.startswith('tanh'):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = torch.tanh(x.float())\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = torch.tanh(x.float())\n",
    "                    x = layer(x.float())\n",
    "            if activation.startswith(\"first\"):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float())\n",
    "            if activation.startswith(\"no_activation\"):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float()) \n",
    "            lay_idx += 1#updaten der layerid, um forward pass ins nächste layer zu machen\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HP_Layer_Optimizer(object):\n",
    "    \n",
    "    def __init__ (self, \n",
    "                  layer_range = (2,8), #range mindestanzahl layer, max anzahl layer\n",
    "                  input_start = 52, #anzahl inputneuronen für das Inputlayer\n",
    "                  output_last = 1, #anzahl outputneuronen für das letzte layer\n",
    "                  in_out_range = (30,200), #range in der sich die anzahl der Neuronen für die Hiddenlayer bewegen soll\n",
    "                  random_activation = False,#boolean, die angibt ob activation functions random gepicked werden sollen\n",
    "                  types = ['linear', 'dropout'],#layertypen, von denen gewählt wird\n",
    "                  activations = ['relu', 'sigmoid', 'tanh'],#activation functions\n",
    "                  activation_dist = [0.5,0.25,0.25],#wenn random_activation == False wird hier die Häufigkeit für jede Activation übergeben, nicht implementiert\n",
    "                  dropout_num = 1,#anzahl von dropoutlayern pro NN\n",
    "                  cuda = True,#soll mit cuda gearbeitet werden?\n",
    "                  lr = 0.0001,#lernrate für adam optimizer\n",
    "                  max_epochs = 5,#anzahl der Trainingsepochen\n",
    "                  create_combinations = True, #sollen von der Klasse Netze erzeugt werden, oder sollen diese von außen in die Klasse gegeben werden?\n",
    "                  create_variations = False,#soll von jeder Layeranzahl mehr als ein Netz erzeugt werden?\n",
    "                  num_variations = 4, #wenn Variationen erzeugt werden sollen wird eine festgelegte Zahl Netze mit 4 Layer, eine festgelegte Anzahl mit 5 Layern usw. erzeugt\n",
    "                  pure_activations = True #wenn random_activation auf True gesetzt ist kann hier festgelegt werden, dass zusätzlich zu absolut zufällig kombinierten activationfunctions\n",
    "                  #auch noch Netze erzeugt werden, die nur einheitliche activation functions haben (pure sind)\n",
    "                  ):\n",
    "        \n",
    "        self.__model = None #model, welches trainiert und getestet wird, wird hier zwischengespeichert\n",
    "        self.__layer_range = layer_range #min und max anzahl von layern in Tupelform\n",
    "        self.__input_start = input_start#Inputgröße für Startlayer/Inputlayer\n",
    "        self.__output_last = output_last#Outputgröße für letztes Layer / Outputlayer\n",
    "        self.__in_out_range = in_out_range#range in der sich die Anzahl der In- und Outputs für die Hiddenlayer bewegen soll (Tupelform)\n",
    "        self.__layer_types = types#liste welche Layertypen enthält (bspw. linear oder dropout)\n",
    "        self.__dropout_number = dropout_num#anzahl von layern, die pro Netz ein dropout Layer sein sollen\n",
    "        self.__random_activation = random_activation#Boolean, ob die activation zufällig gewählt werden soll\n",
    "        self.__create_variations = create_variations\n",
    "        self.__number_of_variations = num_variations\n",
    "        self.__pure = pure_activations\n",
    "        \n",
    "        self.__combination_results = {}#dictionary, die dem schlüssel zu einer NN Kombi einen MAE zuordnet\n",
    "        self.model_specs_combinations = {}#dictionary, welches die jeweiligen NN Kombinationen enthält (einem Schlüssel zugeordnet)\n",
    "        self.train_data = None#trainingsdaten, die dem Optimizer übergeben werden für die Modelle\n",
    "        self.test_data = None#developmentdaten, um die Modelle zu testen und Aussagen über die besten Kombinationen zu treffen\n",
    "        self.lr = lr#lernrate für Modell OPtimizer (default Adam)\n",
    "        self.max_epochs = max_epochs#Anzahl der Trainingsepochen\n",
    "        self.cuda = cuda#Boolean, ob mit cuda gearbeitet werden soll oder nicht\n",
    "        self.opt_combination = {}#dictionary enthält optimale kombination aus möglichen kombinationen\n",
    "        \n",
    "        self.__activations = None#entweder Liste (wenn activations random ausgewählt werden sollen), oder dictionary, \n",
    "        #wenn activation functions mit einer bestimmten Häufigkeit verwendet werden sollen\n",
    "        if self.__random_activation:#random pick von activation functions\n",
    "            self.__activations = activations\n",
    "        else:#activation function sollen mit einer gewissen häufigkeit ausgewählt werden\n",
    "            k = {}\n",
    "            for a in range(len(types)):#zuordnen einer häufihkeit aus activation_dist liste zu jeder activation function aus types\n",
    "                act = types[a]\n",
    "                dist = activation_dist[a]\n",
    "                k[act] = dist\n",
    "            self.__activations = k\n",
    "            \n",
    "        #aufrufen der Funktion, die anhand der übergebenen Parameter Modellkombinationen erzeugt\n",
    "        if create_combinations:\n",
    "            print(\"in constructor bevor die kombinationen erzeugt werden\")\n",
    "            self.__create_combinations()\n",
    "        else:\n",
    "            print('NN Kombinationen müssen in Dictionary Form selbst übergeben werden')\n",
    "        \n",
    "    def __train(self, epoch, optimizer):\n",
    "        '''\n",
    "            funktion übernimmt das Training von dem in self.__model\n",
    "            zwischengespeicherten NN. \n",
    "            Epoch: Jetzige Epoche in der trainiert wird (für coolen print Befehl wichtig)\n",
    "            Optimizer: Optimizer mit dem Parameter des NN aus self.__model optimiert werden\n",
    "        '''\n",
    "        if self.cuda:\n",
    "            self.__model.cuda()\n",
    "            self.__model.train()\n",
    "            batch_id = 0\n",
    "            for key in self.train_data.keys():\n",
    "                for data, target in self.train_data[key]:\n",
    "                    data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1).cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    out = self.__model(data)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss = criterion(out, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    ##rint(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    ##   epoch, batch_id *len(data), len(self.train_data),\n",
    "                    ##00. * batch_id / len(self.train_data), loss.item()))\n",
    "                    batch_id +=1\n",
    "        else:\n",
    "            self.__model.train()\n",
    "            batch_id = 0\n",
    "            for key in self.train_data.keys():\n",
    "                for data, target in self.train_data[key]:\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1)\n",
    "                    optimizer.zero_grad()\n",
    "                    out = self.__model(data)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss = criterion(out, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    #    epoch, batch_id *len(data), len(self.train_data),\n",
    "                    #100. * batch_id / len(self.train_data), loss.item()))\n",
    "                    batch_id +=1\n",
    "            \n",
    "    def __test(self):\n",
    "        '''\n",
    "            Funktion, die das Testen des Models aus self.__model auf den übergebenen Dev-Daten \n",
    "            (self.test_data) übernimmt und den gesamt MAE für das jeweilige Modell berechnet\n",
    "        '''\n",
    "        total = 0\n",
    "        count = 0\n",
    "        result_dict = {}\n",
    "        result = pd.DataFrame(columns = ['target','prediction'])\n",
    "        help_dict = {}\n",
    "        if self.cuda:\n",
    "            self.__model.cuda()\n",
    "            for key in self.test_data.keys():\n",
    "                help_dict = {}\n",
    "                for data, target in self.test_data[key]:\n",
    "                    self.__model.eval()\n",
    "                    data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1).cuda()\n",
    "                    out = self.__model(data).cpu()\n",
    "                    out = out.detach().numpy()\n",
    "                    target = target.cpu()\n",
    "                    target = target.detach().numpy()\n",
    "                    total += abs(out - target[0][0])\n",
    "                    help_dict[target[0][0]] = out\n",
    "                    count+=1\n",
    "                #Auslesen der predicteten Werte A und der zugehörigen targets y\n",
    "                A = [x[0][0] for x in list((help_dict.values()))]\n",
    "                y = list(help_dict.keys())\n",
    "                \n",
    "                #Anfügen der Werte an Result\n",
    "                t = pd.DataFrame()\n",
    "                t['target'] = y\n",
    "                t['prediction_value'] = A\n",
    "                t = sqldf.sqldf('''select * from t order by prediction_value ASC''')\n",
    "                t.reset_index(inplace = True)\n",
    "                t.rename(columns = {'index':'prediction'}, inplace = True)\n",
    "                t['prediction'] = t['prediction']+1\n",
    "                \n",
    "                \n",
    "                result = result.append(t, sort = True)\n",
    "                \n",
    "        else:\n",
    "            for raceId in self.test_data.keys():\n",
    "                help_dict = {}\n",
    "                for data, target in self.test_data[raceId]:\n",
    "                    self.__model.eval()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1)\n",
    "                    out = self.__model(data)\n",
    "                    out = out.detach().numpy()\n",
    "                    target = target.detach().numpy()\n",
    "                    total += abs(out - target[0][0])\n",
    "                    help_dict[target[0][0]] = out\n",
    "                    count+=1\n",
    "                #Auslesen der predicteten Werte A und der zugehörigen targets y\n",
    "                A = [x[0][0] for x in list((help_dict.values()))]\n",
    "                y = list(help_dict.keys())\n",
    "                \n",
    "                #Anfügen der Werte an Result\n",
    "                t = pd.DataFrame()\n",
    "                t['target'] = y\n",
    "                t['prediction_value'] = A\n",
    "                t = sqldf.sqldf('''select * from t order by prediction_value ASC''')\n",
    "                t.reset_index(inplace = True)\n",
    "                t.rename(columns = {'index':'prediction'}, inplace = True)\n",
    "                t['prediction'] = t['prediction']+1\n",
    "                \n",
    "                result = result.append(t, sort = True)\n",
    "        return result\n",
    "    \n",
    "    def get_all_information(self):\n",
    "        \n",
    "        print('All Model Combinations with encoding:\\n', self.model_specs_combinations)\n",
    "        print('Model Results:\\n', self.__combination_results)\n",
    "        print('Optimale Kombination:\\n', self.opt_combination)\n",
    "        \n",
    "    def validate_combinations(self):\n",
    "        \n",
    "        for key, combination in self.model_specs_combinations.items():\n",
    "            print(\"kombination, die jetzt trainiert wird\", combination)\n",
    "            self.__model = NetzDynamic(combination)\n",
    "            optimizer = optim.Adam(self.__model.parameters(), lr = self.lr)\n",
    "            #trainieren des modells\n",
    "            for epoch in range(1,self.max_epochs):\n",
    "                self.__train(epoch, optimizer)  \n",
    "            #Aufrufen der Testfunktion\n",
    "            result = self.__test()\n",
    "            A = result.prediction.tolist()\n",
    "            y = result.target.tolist()\n",
    "            mae = MAE(A,y)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.__combination_results[key] = mae\n",
    "            \n",
    "        #finden der besten kombination nach minimalstem Error (MAE)\n",
    "        key_min = min(self.__combination_results.keys(), key=(lambda k: self.__combination_results[k]))\n",
    "        best_combination = self.model_specs_combinations[key_min]\n",
    "        best_combination['mae'] = self.__combination_results[key_min]\n",
    "        self.opt_combination = best_combination\n",
    "        #self.__combination_overview[key] = specifics\n",
    "        \n",
    "        \n",
    "    def __create_combinations(self):\n",
    "        '''\n",
    "            Funktion erzeugt Dictionarys, mit NN Modellspezifikationen, die\n",
    "            später gegeneinander getestet werden sollen. Kombinationen werden\n",
    "            in dem Dictionary self.model_specs_combinations unter einem \n",
    "            Schlüssel abgespeichert\n",
    "        '''\n",
    "        \n",
    "        if self.__create_variations:\n",
    "            \n",
    "            if self.__pure:\n",
    "                '''\n",
    "                es werden mehr als ein Netz von einer bestimmten Layeranzahl erzeugt:\n",
    "                wenn die Layeranzahl Range zwischen 7 und 9 liegt, self.__create_variations = True ist und\n",
    "                die Variable self.__number_of_variations = 4 ist werden 4 Netze mit 7 Layern und 4 Netze\n",
    "                mit 8 Layern erzeugt. Diese können dann gegeneinander getestet werden und geben eine bessere\n",
    "                Übersicht über eine gute Layeranzahl. Wenn self.__pure auf True gesetzt wird, wird aus der \n",
    "                Liste mit activationfunctions [relu, sigmoid, tanh] als default 'reine' Netze erzeugt, die immmer\n",
    "                nur eine der Funktionen enthält (Relu als anfang). Danach werden diese netze kopiert, nur dass die \n",
    "                Aktivierungsfunktionen überschrieben werden (ReLu wird durch sigmoid und tanh ersetzt). Zuletzt werden \n",
    "                die Funktionen wild gemischt.\n",
    "                '''\n",
    "                min_layer = self.__layer_range[0]\n",
    "                max_layer = self.__layer_range[1]\n",
    "                if min_layer == max_layer:\n",
    "                    max_layer += 1\n",
    "                #pure ist true, deswegen wird zu anfang eine der aktivierungsfunktionen ausgewählt, die fürs ganze netz gewählt wird\n",
    "                act = random.choice(self.__activations)\n",
    "                for layer in range(min_layer, max_layer): \n",
    "                    variation_counter = 0\n",
    "                    while variation_counter < self.__number_of_variations:\n",
    "                        variation_counter +=1 \n",
    "                        dropout_counter = 0\n",
    "                        act_count = 0\n",
    "                        specs_dict = {}\n",
    "                        middle = layer//2\n",
    "                        for l in range(layer):\n",
    "                            layer_specs = []\n",
    "                            key = act+str(act_count)\n",
    "                            act_count += 1\n",
    "                            if dropout_counter == self.__dropout_number:\n",
    "                                #es wurden schon ausreichend dropout layer erzeugt\n",
    "                                l_ = random.choice([x for x in self.__layer_types if x not in ['dropout']])\n",
    "                            else:\n",
    "                                l_ = random.choice(self.__layer_types)\n",
    "                                if l_ == 'dropout':\n",
    "                                    dropout_counter +=1\n",
    "                                    \n",
    "                            if l == 0:\n",
    "                                in_ = self.__input_start\n",
    "                                if l_ == 'dropout':\n",
    "                                    dropout_counter = dropout_counter-1\n",
    "                                    l_ = 'linear'\n",
    "                                range_start = self.__in_out_range[0]\n",
    "                                range_end = self.__in_out_range[1]\n",
    "                                out_ = random.randint(range_start, range_end)\n",
    "                                layer_specs.append(l_)\n",
    "                                layer_specs.append(in_)\n",
    "                                layer_specs.append(out_)\n",
    "                            else:\n",
    "                                layer_before = specs_dict[list(specs_dict.keys())[-1]]\n",
    "                                out_alt = specs_dict[list(specs_dict.keys())[-1]][2]\n",
    "                                l_type = layer_before[0]\n",
    "                                i_ltype = -2\n",
    "                                while l_type == 'dropout':#überprüfen ob vorhergegangenes Layer ein dropout layer war\n",
    "                                    #sobald das vorhergegangene nicht-dropoutlayer gefunden wurde, wird output größe übernommen\n",
    "                                    layer_before = specs_dict[list(specs_dict.keys())[i_ltype]]\n",
    "                                    l_type = layer_before[0]\n",
    "                                    out_alt = specs_dict[list(specs_dict.keys())[i_ltype]][2]\n",
    "                                    i_ltype = i_ltype -1\n",
    "                                if l <= middle: #in der ertsen hälfte nimmt output zu\n",
    "                                    in_ = out_alt\n",
    "                                    range_start = out_alt\n",
    "                                    range_end = self.__in_out_range[1]\n",
    "                                    out_ = random.randint(range_start, range_end)\n",
    "                                else:#in zweiter hälfte der layer nimmt output wieder ab\n",
    "                                    in_ = out_alt\n",
    "                                    range_start = self.__in_out_range[0]\n",
    "                                    range_end = out_alt\n",
    "                                    out_ = random.randint(range_start, range_end)\n",
    "                                layer_specs.append(l_)\n",
    "                                layer_specs.append(in_)\n",
    "                                layer_specs.append(out_)\n",
    "                            #if l == layer-1:\n",
    "                            #    l_ = 'linear'\n",
    "                            #    layer_specs = [l_,in_,self.__output_last]\n",
    "                                #specs_dict['last'] = layer_specs#layer wird ohne activation gespeichert und als letztes Layer des NN gekennzeichnet\n",
    "                                \n",
    "                            if l == 0:\n",
    "                                specs_dict['first'] = layer_specs\n",
    "                            else:\n",
    "                                if specs_dict[list(specs_dict.keys())[-1]][0] == 'dropout':# keine activation function bei layern direkt nach einem dropout layer\n",
    "                                    key = 'no_activation'+str(act_count)\n",
    "                                if l == layer -1:\n",
    "                                    l_ = 'linear'\n",
    "                                    layer_specs = [l_, in_, self.__output_last]\n",
    "                                specs_dict[key] = layer_specs\n",
    "                        key = random.randint(0,10000)\n",
    "                        while key in list(self.model_specs_combinations.keys()):\n",
    "                             key = random.randint(0,10000)\n",
    "                        self.model_specs_combinations[key]= specs_dict   \n",
    "                        print(specs_dict,'\\n')\n",
    "                        \n",
    "                used_acts = [act]\n",
    "                act_count = 0\n",
    "                netze = list(self.model_specs_combinations.values())\n",
    "                mixed = []\n",
    "                if self.__random_activation:\n",
    "                    p_ = 0\n",
    "                    for netz in netze:\n",
    "                        netz_neu = {}\n",
    "                        for act_alt, layer_spec in netz.items():\n",
    "                            if act_alt.startswith('no_activation') or act_alt.startswith('first'):\n",
    "                                netz_neu[act_alt] = layer_spec\n",
    "                            else:\n",
    "                                act_neu = random.choice(self.__activations)\n",
    "                                pp = act_neu+str(p_)\n",
    "                                p_ += 1\n",
    "                                netz_neu[pp]= layer_spec\n",
    "                        mixed.append(netz_neu)\n",
    "                for activation in self.__activations:\n",
    "                    if activation in used_acts:\n",
    "                        continue\n",
    "                    else:\n",
    "                        for netz in netze:#alle bisher erzeugten netze werden betrachtet\n",
    "                            key = random.randint(0,10000)\n",
    "                            while key in list(self.model_specs_combinations.keys()):\n",
    "                                key = random.randint(0,10000)\n",
    "                            netz_neu = {}\n",
    "                            for act_alt, layer in netz.items():\n",
    "                                if act_alt.startswith('no_activation') or act_alt.startswith('first'):\n",
    "                                    netz_neu[act_alt] = layer\n",
    "                                else:\n",
    "                                    act_ = activation+str(act_count)\n",
    "                                    act_count += 1\n",
    "                                    netz_neu[act_] = layer\n",
    "                            print(netz_neu,'\\n')\n",
    "                            self.model_specs_combinations[key] = netz_neu\n",
    "                if len(mixed) != 0:\n",
    "                    for netz in mixed:\n",
    "                        print(\"mixed netz\",netz)\n",
    "                        key = random.randint(0,10000)\n",
    "                        while key in list(self.model_specs_combinations.keys()):\n",
    "                            key = random.randint(0,10000)\n",
    "                        self.model_specs_combinations[key] = netz\n",
    "                print(\"alle kombinationen nach create combination:\", self.model_specs_combinations)\n",
    "            else:\n",
    "                raise Exception('please et pure_activations to True, False not implemented')\n",
    "                    \n",
    "        else:\n",
    "            min_layer = self.__layer_range[0]\n",
    "            max_layer = self.__layer_range[1]\n",
    "            if min_layer == max_layer:\n",
    "                max_layer += 1\n",
    "            for layer in range(min_layer, max_layer): \n",
    "                \n",
    "                dropout_counter = 0\n",
    "                act_count = 0\n",
    "                specs_dict = {}\n",
    "                middle = layer//2\n",
    "                for l in range(layer):\n",
    "                    layer_specs = []\n",
    "                    if self.__random_activation:#random activation pick ist aktiviert\n",
    "                        act = random.choice(self.__activations)\n",
    "                        key = act+str(act_count)\n",
    "                        act_count += 1\n",
    "                        if dropout_counter == self.__dropout_number:\n",
    "                            #es wurden schon ausreichend dropout layer erzeugt\n",
    "                            l_ = random.choice([x for x in self.__layer_types if x not in ['dropout']])\n",
    "                        else:\n",
    "                            l_ = random.choice(self.__layer_types)\n",
    "                            if l_ == 'dropout':\n",
    "                                dropout_counter +=1\n",
    "                                \n",
    "                        if l == 0:\n",
    "                            in_ = self.__input_start\n",
    "                            if l_ == 'dropout':\n",
    "                                dropout_counter = dropout_counter-1\n",
    "                                l_ = 'linear'\n",
    "                            range_start = self.__in_out_range[0]\n",
    "                            range_end = self.__in_out_range[1]\n",
    "                            out_ = random.randint(range_start, range_end)\n",
    "                            layer_specs.append(l_)\n",
    "                            layer_specs.append(in_)\n",
    "                            layer_specs.append(out_)\n",
    "                        else:\n",
    "                            layer_before = specs_dict[list(specs_dict.keys())[-1]]\n",
    "                            out_alt = specs_dict[list(specs_dict.keys())[-1]][2]\n",
    "                            l_type = layer_before[0]\n",
    "                            i_ltype = -2\n",
    "                            while l_type == 'dropout':#überprüfen ob vorhergegangenes Layer ein dropout layer war\n",
    "                                #sobald das vorhergegangene nicht-dropoutlayer gefunden wurde, wird output größe übernommen\n",
    "                                layer_before = specs_dict[list(specs_dict.keys())[i_ltype]]\n",
    "                                l_type = layer_before[0]\n",
    "                                out_alt = specs_dict[list(specs_dict.keys())[i_ltype]][2]\n",
    "                                i_ltype = i_ltype -1\n",
    "                            if l <= middle: #in der ertsen hälfte nimmt output zu\n",
    "                                in_ = out_alt\n",
    "                                range_start = out_alt\n",
    "                                range_end = self.__in_out_range[1]\n",
    "                                out_ = random.randint(range_start, range_end)\n",
    "                            else:#in zweiter hälfte der layer nimmt output wieder ab\n",
    "                                in_ = out_alt\n",
    "                                range_start = self.__in_out_range[0]\n",
    "                                range_end = out_alt\n",
    "                                out_ = random.randint(range_start, range_end)\n",
    "                            layer_specs.append(l_)\n",
    "                            layer_specs.append(in_)\n",
    "                            layer_specs.append(out_)\n",
    "                        #if l == layer-1:\n",
    "                        #    l_ = 'linear'\n",
    "                        #    layer_specs = [l_,in_,self.__output_last]\n",
    "                        #    specs_dict['last'] = layer_specs#layer wird ohne activation gespeichert und als letztes Layer des NN gekennzeichnet\n",
    "                            \n",
    "                        #elif l == 0:\n",
    "                        if l == 0:\n",
    "                            specs_dict['first'] = layer_specs\n",
    "                        else:\n",
    "                            #if l_ == 'dropout':#keine activation function bei dropout layern\n",
    "                            #    key = 'no_activation'+str(act_count)\n",
    "                            if specs_dict[list(specs_dict.keys())[-1]][0] == 'dropout':# keine activation function bei layern direkt nach einem dropout layer\n",
    "                                key = 'no_activation'+str(act_count)\n",
    "                            if l == layer -1:\n",
    "                                l_ = 'linear'\n",
    "                                layer_specs = [l_, in_, self.__output_last]\n",
    "                            specs_dict[key] = layer_specs\n",
    "                key = random.randint(0,10000)\n",
    "                while key in list(self.model_specs_combinations.keys()):\n",
    "                     key = random.randint(0,10000)\n",
    "                self.model_specs_combinations[key]= specs_dict   \n",
    "                print(specs_dict)\n",
    "\n",
    "                \n",
    "class HP_Optimizer(object):\n",
    "    \n",
    "    def __init__(self, lr_range = (0.0001,0.0001), step_size = 0.0001, max_epochs = (2,2), opt = 'Adam', cuda = True, dynamic = False, dyn_combination = {}):\n",
    "        \n",
    "        self.__model = Netz()\n",
    "        self.__lr = lr_range\n",
    "        self.__epochs = max_epochs\n",
    "        self.__optimizer = opt\n",
    "        self.__steps = step_size\n",
    "        self.__combination_results = {}\n",
    "        self.__combination_overview = {}\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.cuda = cuda\n",
    "        self.opt_combination = {}\n",
    "        self.__dynamic = dynamic\n",
    "        self.__dyn_combination = dyn_combination\n",
    "        \n",
    "        \n",
    "    def validate_combinations(self):\n",
    "        \n",
    "        specifics = {}\n",
    "        if self.__optimizer == 'Adam':\n",
    "            \n",
    "            #definieren der range für die lernratenoptimierung\n",
    "            '''\n",
    "            \n",
    "            IST HIER WAS KAPUTT?\n",
    "            \n",
    "            '''\n",
    "            if isinstance(self.__lr, tuple):\n",
    "                lr_s = self.__lr[0]\n",
    "                lr_e = self.__lr[1]\n",
    "                #wurde eine range für die anzahl der epochen übergeben?\n",
    "                if self.__epochs[0] == self.__epochs[1]:\n",
    "                    #hyperparamter epochenanzahl wird nicht optimiert\n",
    "                    max_epoch = self.__epochs[0]\n",
    "                    if lr_s == lr_e:\n",
    "                        #es ist keine range für die lernrate gegeben, in der diese optimiert werden soll\n",
    "                        print('Parameter Epochen und Lernrate können nicht optimiert werden, da kein Intervall übergeben wurde')\n",
    "                        #setzen des optimizers als Adam und erzeugen des Modells\n",
    "                        if self.__dynamic:\n",
    "                            self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                        else:\n",
    "                            self.__model = Netz()\n",
    "                        optimizer = optim.Adam(self.__model.parameters(), lr = lr_s)\n",
    "                        for epoch in range(1,max_epoch):\n",
    "                            self.__train(epoch, optimizer)     \n",
    "                        result = self.__test()\n",
    "                        A = result.prediction.tolist()\n",
    "                        y = result.target.tolist()\n",
    "                        mae = MAE(A,y)\n",
    "                        specifics = {}\n",
    "                        specifics['lr'] = lr_s\n",
    "                        specifics['epochen'] = max_epoch\n",
    "                        key = random.randint(0,10000)\n",
    "                        while key in list(self.__combination_results.keys()):\n",
    "                             key = random.randint(0,10000)\n",
    "                        self.__combination_results[key] = mae\n",
    "                        self.__combination_overview[key] = specifics \n",
    "                    else:\n",
    "                        for l in np.arange(lr_s,lr_e, self.__steps):\n",
    "                            #setzen des optimizers als Adam und erzeugen des Modells\n",
    "                            if self.__dynamic:\n",
    "                                self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                            else:\n",
    "                                self.__model = Netz()\n",
    "                            optimizer = optim.Adam(self.__model.parameters(), lr = l)\n",
    "                            #trainieren des modells\n",
    "                            for epoch in range(1,max_epoch):\n",
    "                                self.__train(epoch, optimizer)  \n",
    "                            result = self.__test()\n",
    "                            A = result.prediction.tolist()\n",
    "                            y = result.target.tolist()\n",
    "                            mae = MAE(A,y)\n",
    "                            specifics = {}\n",
    "                            specifics['lr'] = l\n",
    "                            specifics['epochen'] = max_epoch\n",
    "                            #abspeichern der gewonnenen informationen (MAE nach lr und anzahl durchgeführter epochen)\n",
    "                            key = random.randint(0,10000)\n",
    "                            while key in list(self.__combination_results.keys()):\n",
    "                                 key = random.randint(0,10000)\n",
    "                            self.__combination_results[key] = mae\n",
    "                            self.__combination_overview[key] = specifics\n",
    "                else:\n",
    "                    for max_epoch in range(self.__epochs[0], self.__epochs[1]):\n",
    "                        #definieren der range für die lernratenoptimierung\n",
    "                        lr_s = self.__lr[0]\n",
    "                        lr_e = self.__lr[1]\n",
    "                        #hyperparamter epochenanzahl wird nicht optimiert\n",
    "                        if lr_s == lr_e:\n",
    "                            #es ist keine range für die lernrate gegeben, in der diese optimiert werden soll\n",
    "                            #print('Parameter Epochen und Lernrate können nicht optimiert werden, da kein Intervall übergeben wurde')\n",
    "                            if self.__dynamic:\n",
    "                                self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                            else:\n",
    "                                self.__model = Netz()\n",
    "                            optimizer = optim.Adam(self.__model.parameters(), lr = lr_s)\n",
    "                            for epoch in range(1,max_epoch):\n",
    "                                self.__train(epoch, optimizer)       \n",
    "                            result = self.__test()\n",
    "                            A = result.prediction.tolist()\n",
    "                            y = result.target.tolist()\n",
    "                            mae = MAE(A,y)\n",
    "                            specifics = {}\n",
    "                            specifics['lr'] = lr_s\n",
    "                            specifics['epochen'] = max_epoch\n",
    "                            key = random.randint(0,10000)\n",
    "                            while key in list(self.__combination_results.keys()):\n",
    "                                 key = random.randint(0,10000)  \n",
    "                            self.__combination_results[key] = mae\n",
    "                            self.__combination_overview[key] = specifics          \n",
    "                        else:\n",
    "                            for l in np.arange(lr_s,lr_e, self.__steps):\n",
    "                                if self.__dynamic:\n",
    "                                    self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                                else:\n",
    "                                    self.__model = Netz()\n",
    "                                optimizer = optim.Adam(self.__model.parameters(), lr = l)\n",
    "                                #trainieren des modells\n",
    "                                for epoch in range(1,max_epoch):\n",
    "                                    self.__train(epoch, optimizer)  \n",
    "                                result = self.__test()\n",
    "                                A = result.prediction.tolist()\n",
    "                                y = result.target.tolist()\n",
    "                                mae = MAE(A,y)\n",
    "                                specifics = {}\n",
    "                                specifics['lr'] = l\n",
    "                                specifics['epochen'] = max_epoch\n",
    "                                #abspeichern der gewonnenen informationen (MAE nach lr und anzahl durchgeführter epochen)\n",
    "                                key = random.randint(0,10000)\n",
    "                                while key in list(self.__combination_results.keys()):\n",
    "                                     key = random.randint(0,10000)\n",
    "                                self.__combination_results[key] = mae\n",
    "                                self.__combination_overview[key] = specifics\n",
    "                #finden der besten kombination nach minimalstem Error (MAE)\n",
    "                key_min = min(self.__combination_results.keys(), key=(lambda k: self.__combination_results[k]))\n",
    "                best_combination = self.__combination_overview[key_min]\n",
    "                best_combination['mae'] = self.__combination_results[key_min]\n",
    "                self.opt_combination = best_combination\n",
    "            elif isinstance(self.__lr, list):\n",
    "                #wurde eine range für die anzahl der epochen übergeben?\n",
    "                if self.__epochs[0] == self.__epochs[1]:\n",
    "                    #hyperparamter epochenanzahl wird nicht optimiert\n",
    "                    max_epoch = self.__epochs[0]\n",
    "                    if len(self.__lr) ==1:\n",
    "                        #es ist keine range für die lernrate gegeben, in der diese optimiert werden soll\n",
    "                        print('Parameter Epochen und Lernrate können nicht optimiert werden, da kein Intervall übergeben wurde')\n",
    "                        #setzen des optimizers als Adam und erzeugen des Modells\n",
    "                        if self.__dynamic:\n",
    "                            self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                        else:\n",
    "                            self.__model = Netz()\n",
    "                        optimizer = optim.Adam(self.__model.parameters(), lr = self.__lr[0])\n",
    "                        for epoch in range(1,max_epoch):\n",
    "                            self.__train(epoch, optimizer)     \n",
    "                        result = self.__test()\n",
    "                        A = result.prediction.tolist()\n",
    "                        y = result.target.tolist()\n",
    "                        mae = MAE(A,y)\n",
    "                        specifics = {}\n",
    "                        specifics['lr'] = self.__lr[0]\n",
    "                        specifics['epochen'] = max_epoch\n",
    "                        key = random.randint(0,10000)\n",
    "                        while key in list(self.__combination_results.keys()):\n",
    "                             key = random.randint(0,10000)\n",
    "                        self.__combination_results[key] = mae\n",
    "                        self.__combination_overview[key] = specifics \n",
    "                    else:\n",
    "                        for l in self.__lr:\n",
    "                            #setzen des optimizers als Adam und erzeugen des Modells\n",
    "                            if self.__dynamic:\n",
    "                                self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                            else:\n",
    "                                self.__model = Netz()\n",
    "                            optimizer = optim.Adam(self.__model.parameters(), lr = l)\n",
    "                            #trainieren des modells\n",
    "                            for epoch in range(1,max_epoch):\n",
    "                                self.__train(epoch, optimizer)  \n",
    "                            result = self.__test()\n",
    "                            A = result.prediction.tolist()\n",
    "                            y = result.target.tolist()\n",
    "                            mae = MAE(A,y)\n",
    "                            specifics = {}\n",
    "                            specifics['lr'] = l\n",
    "                            specifics['epochen'] = max_epoch\n",
    "                            #abspeichern der gewonnenen informationen (MAE nach lr und anzahl durchgeführter epochen)\n",
    "                            key = random.randint(0,10000)\n",
    "                            while key in list(self.__combination_results.keys()):\n",
    "                                 key = random.randint(0,10000)\n",
    "                            self.__combination_results[key] = mae\n",
    "                            self.__combination_overview[key] = specifics\n",
    "                else:\n",
    "                    for max_epoch in range(self.__epochs[0], self.__epochs[1]):\n",
    "                        #definieren der range für die lernratenoptimierung\n",
    "                        lr_s = self.__lr[0]\n",
    "                        lr_e = self.__lr[1]\n",
    "                        #hyperparamter epochenanzahl wird nicht optimiert\n",
    "                        if len(self.__lr) == 1:\n",
    "                            #es ist keine range für die lernrate gegeben, in der diese optimiert werden soll\n",
    "                            #print('Parameter Epochen und Lernrate können nicht optimiert werden, da kein Intervall übergeben wurde')\n",
    "                            if self.__dynamic:\n",
    "                                self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                            else:\n",
    "                                self.__model = Netz()\n",
    "                            optimizer = optim.Adam(self.__model.parameters(), lr = self.__lr[0])\n",
    "                            for epoch in range(1,max_epoch):\n",
    "                                self.__train(epoch, optimizer)       \n",
    "                            result = self.__test()\n",
    "                            A = result.prediction.tolist()\n",
    "                            y = result.target.tolist()\n",
    "                            mae = MAE(A,y)\n",
    "                            specifics = {}\n",
    "                            specifics['lr'] = self.__lr[0]\n",
    "                            specifics['epochen'] = max_epoch\n",
    "                            key = random.randint(0,10000)\n",
    "                            while key in list(self.__combination_results.keys()):\n",
    "                                 key = random.randint(0,10000)  \n",
    "                            self.__combination_results[key] = mae\n",
    "                            self.__combination_overview[key] = specifics          \n",
    "                        else:\n",
    "                            for l in self.__lr:\n",
    "                                if self.__dynamic:\n",
    "                                    self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                                else:\n",
    "                                    self.__model = Netz()\n",
    "                                optimizer = optim.Adam(self.__model.parameters(), lr = l)\n",
    "                                #trainieren des modells\n",
    "                                for epoch in range(1,max_epoch):\n",
    "                                    self.__train(epoch, optimizer)  \n",
    "                                result = self.__test()\n",
    "                                A = result.prediction.tolist()\n",
    "                                y = result.target.tolist()\n",
    "                                mae = MAE(A,y)\n",
    "                                specifics = {}\n",
    "                                specifics['lr'] = l\n",
    "                                specifics['epochen'] = max_epoch\n",
    "                                #abspeichern der gewonnenen informationen (MAE nach lr und anzahl durchgeführter epochen)\n",
    "                                key = random.randint(0,10000)\n",
    "                                while key in list(self.__combination_results.keys()):\n",
    "                                     key = random.randint(0,10000)\n",
    "                                self.__combination_results[key] = mae\n",
    "                                self.__combination_overview[key] = specifics\n",
    "                #finden der besten kombination nach minimalstem Error (MAE)\n",
    "                key_min = min(self.__combination_results.keys(), key=(lambda k: self.__combination_results[k]))\n",
    "                best_combination = self.__combination_overview[key_min]\n",
    "                best_combination['mae'] = self.__combination_results[key_min]\n",
    "                self.opt_combination = best_combination\n",
    "                \n",
    "        else:\n",
    "            raise ('No valid optimizer given! Try Adam for example!')\n",
    "            \n",
    "    def __train(self, epoch, optimizer):\n",
    "        if self.cuda:\n",
    "            self.__model.cuda()\n",
    "            self.__model.train()\n",
    "            batch_id = 0\n",
    "            for key in self.train_data.keys():\n",
    "                for data, target in self.train_data[key]:\n",
    "                    data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1).cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    out = self.__model(data)\n",
    "                    #print(\"Out: \", out, out.size())\n",
    "                    #print(\"Target: \", target, target.size())\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss = criterion(out, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #rint(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    #   epoch, batch_id *len(data), len(self.train_data),\n",
    "                    #00. * batch_id / len(self.train_data), loss.item()))\n",
    "                    batch_id +=1\n",
    "        else:\n",
    "            self.__model.train()\n",
    "            batch_id = 0\n",
    "            for key in self.train_data.keys():\n",
    "                for data, target in self.train_data[key]:\n",
    "                    #data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)#.cuda()\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1)#.cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    out = self.__model(data)\n",
    "                    #print(\"Out: \", out, out.size())\n",
    "                    #print(\"Target: \", target, target.size())\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss = criterion(out, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    #    epoch, batch_id *len(data), len(train_data),\n",
    "                    #100. * batch_id / len(train_data), loss.item()))\n",
    "                    batch_id +=1\n",
    "                    \n",
    "            \n",
    "    def __test(self):\n",
    "        total = 0\n",
    "        count = 0\n",
    "        result_dict = {}\n",
    "        result = pd.DataFrame(columns = ['target','prediction'])\n",
    "        help_dict = {}\n",
    "        if self.cuda:\n",
    "            self.__model.cuda()\n",
    "            for key in self.test_data.keys():\n",
    "                #print(key)\n",
    "                help_dict = {}\n",
    "                for data, target in self.test_data[key]:\n",
    "                    self.__model.eval()\n",
    "                    #files.listdir(path)\n",
    "                    data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1).cuda()\n",
    "                    out = self.__model(data).cpu()\n",
    "                    #print(out)\n",
    "                    out = out.detach().numpy()\n",
    "                    #out = np.round(out)\n",
    "                    target = target.cpu()\n",
    "                    target = target.detach().numpy()\n",
    "                    #print(data)\n",
    "                    #print(data[\"driverId\"])\n",
    "                    total += abs(out - target[0][0])\n",
    "                    #print(\"current_position: \", data[0][0].item())\n",
    "                    #print(\"Output: \", out)\n",
    "                    #print(\"Target: \", target)\n",
    "                    help_dict[target[0][0]] = out\n",
    "                    #print(\"Difference: \", out - target)\n",
    "                    count+=1\n",
    "                #Auslesen der predicteten Werte A und der zugehörigen targets y\n",
    "                A = [x[0][0] for x in list((help_dict.values()))]\n",
    "                y = list(help_dict.keys())\n",
    "                \n",
    "                #Anfügen der Werte an Result\n",
    "                t = pd.DataFrame()\n",
    "                t['target'] = y\n",
    "                t['prediction_value'] = A\n",
    "                t = sqldf.sqldf('''select * from t order by prediction_value ASC''')\n",
    "                t.reset_index(inplace = True)\n",
    "                t.rename(columns = {'index':'prediction'}, inplace = True)\n",
    "                t['prediction'] = t['prediction']+1\n",
    "                \n",
    "                \n",
    "                result = result.append(t, sort = True)\n",
    "                \n",
    "        else:\n",
    "            for raceId in self.test_data.keys():\n",
    "                #print(key)\n",
    "                help_dict = {}\n",
    "                for data, target in self.test_data[raceId]:\n",
    "                    self.__model.eval()\n",
    "                    #files.listdir(path)\n",
    "                    #data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1)#.cuda()\n",
    "                    out = self.__model(data)#.cpu()\n",
    "                    #print(out)\n",
    "                    out = out.detach().numpy()\n",
    "                    #out = np.round(out)\n",
    "                    #target = target.cpu()\n",
    "                    target = target.detach().numpy()\n",
    "                    #print(data)\n",
    "                    #print(data[\"driverId\"])\n",
    "                    total += abs(out - target[0][0])\n",
    "                    #print(\"current_position: \", data[0][0].item())\n",
    "                    #print(\"Output: \", out)\n",
    "                    #print(\"Target: \", target)\n",
    "                    help_dict[target[0][0]] = out\n",
    "                    #print(\"Difference: \", out - target)\n",
    "                    count+=1\n",
    "                #Auslesen der predicteten Werte A und der zugehörigen targets y\n",
    "                A = [x[0][0] for x in list((help_dict.values()))]\n",
    "                y = list(help_dict.keys())\n",
    "                \n",
    "                #Anfügen der Werte an Result\n",
    "                t = pd.DataFrame()\n",
    "                t['target'] = y\n",
    "                t['prediction_value'] = A\n",
    "                t = sqldf.sqldf('''select * from t order by prediction_value ASC''')\n",
    "                t.reset_index(inplace = True)\n",
    "                t.rename(columns = {'index':'prediction'}, inplace = True)\n",
    "                t['prediction'] = t['prediction']+1\n",
    "                \n",
    "                result = result.append(t, sort = True)\n",
    "        return result\n",
    "         \n",
    "            \n",
    "    def get_all_information(self):\n",
    "        \n",
    "        print('Chosen Model:',self.__model)\n",
    "        print('Learningrate Range:',self.__lr)\n",
    "        print('Maximum Epochs:', self.__epochs)\n",
    "        print('Chosen Optimizer:', self.__optimizer)\n",
    "        print('Result Encoding:', self.__combination_overview)\n",
    "        print('Results:', self.__combination_results)\n",
    "        print('Optimale Kombination:', self.opt_combination)\n",
    "        \n",
    "    def help(self):\n",
    "        print('Parameters with defaults:\\nlr_range --> (0.0001,0.0001),\\nstep_size--> 0.0001,\\nmax_epochs-->(2,2),\\nopt-->\"Adam\",\\ncuda=True')\n",
    "        print('lr_range: Tupel with learnrate range')\n",
    "        print('step_size: float/int for step_size of learnrate')\n",
    "        print('max_epochs: Tupel with number of epochs range')\n",
    "        print('opt: Optimizer (by default Adam)')\n",
    "        print('cuda: True/False if cuda should be used, default = True\\n')\n",
    "        print('Attributes:')\n",
    "        print('set self.train_data as dictionary with races (form: {raceId: race(dataframe)})')\n",
    "        print('set self.test_data as dictionary with races (form: {raceId: race(dataframe)})')\n",
    "        print('self.opt_combination: Dictionary which contains the best combination of the given parameters\\n')\n",
    "        print('Methods:')\n",
    "        print('call self.validate_combination() to compare all combinations')\n",
    "        print('get all information/results with self.get_all_information()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finden des \"optimalsten\" Netzes  \n",
    "Aufrufen des Layeroptimizers, um verschiedene Neuronale Netze gegeneinander zu testen, mit verschiedenen Layeranzahlen, und Input und Output Neuronen. Gemeinsam haben die Netze, dass bis zur Mitte die Anzahl Neuronen zunimmt, und dann zum letzten Layer hin wieder abnimmt.\n",
    "<br>Es wird das Netz ausgewählt, welches den niedrigsten MAE auf dem Dev Datensatz hat. Ziel ist es durch Aufrufen dieser Klasse die beste Layeranzahl-Aktivierungsfunktionen-Kombination für den Use Case zu finden (anhand vorgegebener einschränkender Paramter). Da die Anzahl der Neuronen jedes Mal zufällig implementiert wird, und dieser Faktor nur wenig eingeschränkt ist, kann hier mit jedem neuen Aufruf der Klasse ein anderes Netz als optimal ausgewählt werden. Das optimalste Netz richtet sich auch stark nach dem train-dev-test splitting des Datensatzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in constructor bevor die kombinationen erzeugt werden\n",
      "{'first': ['linear', 52, 51], 'relu1': ['dropout', 51, 59], 'no_activation3': ['linear', 51, 86], 'relu3': ['linear', 86, 95], 'relu4': ['linear', 95, 55], 'relu5': ['linear', 55, 36], 'relu6': ['linear', 36, 1]} \n",
      "\n",
      "{'first': ['linear', 52, 51], 'sigmoid0': ['dropout', 51, 59], 'sigmoid1': ['linear', 51, 86], 'sigmoid2': ['linear', 86, 95], 'sigmoid3': ['linear', 95, 55], 'sigmoid4': ['linear', 55, 36], 'sigmoid5': ['linear', 36, 1]} \n",
      "\n",
      "{'first': ['linear', 52, 51], 'tanh6': ['dropout', 51, 59], 'tanh7': ['linear', 51, 86], 'tanh8': ['linear', 86, 95], 'tanh9': ['linear', 95, 55], 'tanh10': ['linear', 55, 36], 'tanh11': ['linear', 36, 1]} \n",
      "\n",
      "mixed netz {'first': ['linear', 52, 51], 'tanh0': ['dropout', 51, 59], 'relu1': ['linear', 51, 86], 'relu2': ['linear', 86, 95], 'sigmoid3': ['linear', 95, 55], 'relu4': ['linear', 55, 36], 'tanh5': ['linear', 36, 1]}\n",
      "alle kombinationen nach create combination: {9208: {'first': ['linear', 52, 51], 'relu1': ['dropout', 51, 59], 'no_activation3': ['linear', 51, 86], 'relu3': ['linear', 86, 95], 'relu4': ['linear', 95, 55], 'relu5': ['linear', 55, 36], 'relu6': ['linear', 36, 1]}, 7402: {'first': ['linear', 52, 51], 'sigmoid0': ['dropout', 51, 59], 'sigmoid1': ['linear', 51, 86], 'sigmoid2': ['linear', 86, 95], 'sigmoid3': ['linear', 95, 55], 'sigmoid4': ['linear', 55, 36], 'sigmoid5': ['linear', 36, 1]}, 2793: {'first': ['linear', 52, 51], 'tanh6': ['dropout', 51, 59], 'tanh7': ['linear', 51, 86], 'tanh8': ['linear', 86, 95], 'tanh9': ['linear', 95, 55], 'tanh10': ['linear', 55, 36], 'tanh11': ['linear', 36, 1]}, 395: {'first': ['linear', 52, 51], 'tanh0': ['dropout', 51, 59], 'relu1': ['linear', 51, 86], 'relu2': ['linear', 86, 95], 'sigmoid3': ['linear', 95, 55], 'relu4': ['linear', 55, 36], 'tanh5': ['linear', 36, 1]}}\n",
      "kombination, die jetzt trainiert wird {'first': ['linear', 52, 51], 'relu1': ['dropout', 51, 59], 'no_activation3': ['linear', 51, 86], 'relu3': ['linear', 86, 95], 'relu4': ['linear', 95, 55], 'relu5': ['linear', 55, 36], 'relu6': ['linear', 36, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kombination, die jetzt trainiert wird {'first': ['linear', 52, 51], 'sigmoid0': ['dropout', 51, 59], 'sigmoid1': ['linear', 51, 86], 'sigmoid2': ['linear', 86, 95], 'sigmoid3': ['linear', 95, 55], 'sigmoid4': ['linear', 55, 36], 'sigmoid5': ['linear', 36, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kombination, die jetzt trainiert wird {'first': ['linear', 52, 51], 'tanh6': ['dropout', 51, 59], 'tanh7': ['linear', 51, 86], 'tanh8': ['linear', 86, 95], 'tanh9': ['linear', 95, 55], 'tanh10': ['linear', 55, 36], 'tanh11': ['linear', 36, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kombination, die jetzt trainiert wird {'first': ['linear', 52, 51], 'tanh0': ['dropout', 51, 59], 'relu1': ['linear', 51, 86], 'relu2': ['linear', 86, 95], 'sigmoid3': ['linear', 95, 55], 'relu4': ['linear', 55, 36], 'tanh5': ['linear', 36, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Model Combinations with encoding:\n",
      " {9208: {'first': ['linear', 52, 51], 'relu1': ['dropout', 51, 59], 'no_activation3': ['linear', 51, 86], 'relu3': ['linear', 86, 95], 'relu4': ['linear', 95, 55], 'relu5': ['linear', 55, 36], 'relu6': ['linear', 36, 1], 'mae': 2.118320610687023}, 7402: {'first': ['linear', 52, 51], 'sigmoid0': ['dropout', 51, 59], 'sigmoid1': ['linear', 51, 86], 'sigmoid2': ['linear', 86, 95], 'sigmoid3': ['linear', 95, 55], 'sigmoid4': ['linear', 55, 36], 'sigmoid5': ['linear', 36, 1]}, 2793: {'first': ['linear', 52, 51], 'tanh6': ['dropout', 51, 59], 'tanh7': ['linear', 51, 86], 'tanh8': ['linear', 86, 95], 'tanh9': ['linear', 95, 55], 'tanh10': ['linear', 55, 36], 'tanh11': ['linear', 36, 1]}, 395: {'first': ['linear', 52, 51], 'tanh0': ['dropout', 51, 59], 'relu1': ['linear', 51, 86], 'relu2': ['linear', 86, 95], 'sigmoid3': ['linear', 95, 55], 'relu4': ['linear', 55, 36], 'tanh5': ['linear', 36, 1]}}\n",
      "Model Results:\n",
      " {9208: 2.118320610687023, 7402: 2.6717557251908395, 2793: 3.683206106870229, 395: 2.6106870229007635}\n",
      "Optimale Kombination:\n",
      " {'first': ['linear', 52, 51], 'relu1': ['dropout', 51, 59], 'no_activation3': ['linear', 51, 86], 'relu3': ['linear', 86, 95], 'relu4': ['linear', 95, 55], 'relu5': ['linear', 55, 36], 'relu6': ['linear', 36, 1], 'mae': 2.118320610687023}\n",
      "{'first': ['linear', 52, 51], 'relu1': ['dropout', 51, 59], 'no_activation3': ['linear', 51, 86], 'relu3': ['linear', 86, 95], 'relu4': ['linear', 95, 55], 'relu5': ['linear', 55, 36], 'relu6': ['linear', 36, 1], 'mae': 2.118320610687023}\n",
      "einmal optimieren fertig\n",
      "in constructor bevor die kombinationen erzeugt werden\n",
      "{'first': ['linear', 52, 51], 'relu1': ['linear', 51, 195], 'relu2': ['linear', 195, 197], 'relu3': ['dropout', 197, 197], 'no_activation5': ['linear', 197, 50], 'relu5': ['linear', 50, 33], 'relu6': ['linear', 33, 1]} \n",
      "\n",
      "{'first': ['linear', 52, 51], 'sigmoid0': ['linear', 51, 195], 'sigmoid1': ['linear', 195, 197], 'sigmoid2': ['dropout', 197, 197], 'sigmoid3': ['linear', 197, 50], 'sigmoid4': ['linear', 50, 33], 'sigmoid5': ['linear', 33, 1]} \n",
      "\n",
      "{'first': ['linear', 52, 51], 'tanh6': ['linear', 51, 195], 'tanh7': ['linear', 195, 197], 'tanh8': ['dropout', 197, 197], 'tanh9': ['linear', 197, 50], 'tanh10': ['linear', 50, 33], 'tanh11': ['linear', 33, 1]} \n",
      "\n",
      "mixed netz {'first': ['linear', 52, 51], 'relu0': ['linear', 51, 195], 'relu1': ['linear', 195, 197], 'sigmoid2': ['dropout', 197, 197], 'relu3': ['linear', 197, 50], 'sigmoid4': ['linear', 50, 33], 'relu5': ['linear', 33, 1]}\n",
      "alle kombinationen nach create combination: {5219: {'first': ['linear', 52, 51], 'relu1': ['linear', 51, 195], 'relu2': ['linear', 195, 197], 'relu3': ['dropout', 197, 197], 'no_activation5': ['linear', 197, 50], 'relu5': ['linear', 50, 33], 'relu6': ['linear', 33, 1]}, 6917: {'first': ['linear', 52, 51], 'sigmoid0': ['linear', 51, 195], 'sigmoid1': ['linear', 195, 197], 'sigmoid2': ['dropout', 197, 197], 'sigmoid3': ['linear', 197, 50], 'sigmoid4': ['linear', 50, 33], 'sigmoid5': ['linear', 33, 1]}, 1745: {'first': ['linear', 52, 51], 'tanh6': ['linear', 51, 195], 'tanh7': ['linear', 195, 197], 'tanh8': ['dropout', 197, 197], 'tanh9': ['linear', 197, 50], 'tanh10': ['linear', 50, 33], 'tanh11': ['linear', 33, 1]}, 3811: {'first': ['linear', 52, 51], 'relu0': ['linear', 51, 195], 'relu1': ['linear', 195, 197], 'sigmoid2': ['dropout', 197, 197], 'relu3': ['linear', 197, 50], 'sigmoid4': ['linear', 50, 33], 'relu5': ['linear', 33, 1]}}\n",
      "kombination, die jetzt trainiert wird {'first': ['linear', 52, 51], 'relu1': ['linear', 51, 195], 'relu2': ['linear', 195, 197], 'relu3': ['dropout', 197, 197], 'no_activation5': ['linear', 197, 50], 'relu5': ['linear', 50, 33], 'relu6': ['linear', 33, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kombination, die jetzt trainiert wird {'first': ['linear', 52, 51], 'sigmoid0': ['linear', 51, 195], 'sigmoid1': ['linear', 195, 197], 'sigmoid2': ['dropout', 197, 197], 'sigmoid3': ['linear', 197, 50], 'sigmoid4': ['linear', 50, 33], 'sigmoid5': ['linear', 33, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kombination, die jetzt trainiert wird {'first': ['linear', 52, 51], 'tanh6': ['linear', 51, 195], 'tanh7': ['linear', 195, 197], 'tanh8': ['dropout', 197, 197], 'tanh9': ['linear', 197, 50], 'tanh10': ['linear', 50, 33], 'tanh11': ['linear', 33, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kombination, die jetzt trainiert wird {'first': ['linear', 52, 51], 'relu0': ['linear', 51, 195], 'relu1': ['linear', 195, 197], 'sigmoid2': ['dropout', 197, 197], 'relu3': ['linear', 197, 50], 'sigmoid4': ['linear', 50, 33], 'relu5': ['linear', 33, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Model Combinations with encoding:\n",
      " {5219: {'first': ['linear', 52, 51], 'relu1': ['linear', 51, 195], 'relu2': ['linear', 195, 197], 'relu3': ['dropout', 197, 197], 'no_activation5': ['linear', 197, 50], 'relu5': ['linear', 50, 33], 'relu6': ['linear', 33, 1], 'mae': 2.0610687022900764}, 6917: {'first': ['linear', 52, 51], 'sigmoid0': ['linear', 51, 195], 'sigmoid1': ['linear', 195, 197], 'sigmoid2': ['dropout', 197, 197], 'sigmoid3': ['linear', 197, 50], 'sigmoid4': ['linear', 50, 33], 'sigmoid5': ['linear', 33, 1]}, 1745: {'first': ['linear', 52, 51], 'tanh6': ['linear', 51, 195], 'tanh7': ['linear', 195, 197], 'tanh8': ['dropout', 197, 197], 'tanh9': ['linear', 197, 50], 'tanh10': ['linear', 50, 33], 'tanh11': ['linear', 33, 1]}, 3811: {'first': ['linear', 52, 51], 'relu0': ['linear', 51, 195], 'relu1': ['linear', 195, 197], 'sigmoid2': ['dropout', 197, 197], 'relu3': ['linear', 197, 50], 'sigmoid4': ['linear', 50, 33], 'relu5': ['linear', 33, 1]}}\n",
      "Model Results:\n",
      " {5219: 2.0610687022900764, 6917: 3.0610687022900764, 1745: 2.8244274809160306, 3811: 2.4618320610687023}\n",
      "Optimale Kombination:\n",
      " {'first': ['linear', 52, 51], 'relu1': ['linear', 51, 195], 'relu2': ['linear', 195, 197], 'relu3': ['dropout', 197, 197], 'no_activation5': ['linear', 197, 50], 'relu5': ['linear', 50, 33], 'relu6': ['linear', 33, 1], 'mae': 2.0610687022900764}\n",
      "{'first': ['linear', 52, 51], 'relu1': ['linear', 51, 195], 'relu2': ['linear', 195, 197], 'relu3': ['dropout', 197, 197], 'no_activation5': ['linear', 197, 50], 'relu5': ['linear', 50, 33], 'relu6': ['linear', 33, 1], 'mae': 2.0610687022900764}\n",
      "einmal optimieren fertig\n"
     ]
    }
   ],
   "source": [
    "opt_combination = []\n",
    "for k in range(2): #30\n",
    "    if cuda.lower() == 'y':\n",
    "        l = HP_Layer_Optimizer(layer_range=(8,11),#5,15\n",
    "                           max_epochs = 4,#10\n",
    "                           input_start = 52,\n",
    "                           #activations = ['relu'], \n",
    "                           random_activation=True,\n",
    "                           create_combinations=True, \n",
    "                           create_variations = True, \n",
    "                           num_variations = 2,#5\n",
    "                           pure_activations = True,\n",
    "                           cuda = True)\n",
    "    else:\n",
    "        l = HP_Layer_Optimizer(layer_range=(7,8),\n",
    "                           max_epochs = 2,\n",
    "                           input_start = 52,\n",
    "                           #activations = ['relu'], \n",
    "                           random_activation=True,\n",
    "                           create_combinations=True, \n",
    "                           create_variations = True, \n",
    "                           num_variations = 1,\n",
    "                           pure_activations = True,\n",
    "                           cuda = False)\n",
    "    #l.model_specs_combinations = netze\n",
    "    l.train_data = train_T\n",
    "    l.test_data = dev_T\n",
    "    l.validate_combinations()\n",
    "    l.get_all_information()\n",
    "    print(l.opt_combination)\n",
    "    opt_combination.append(l.opt_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'first': ['linear', 52, 51],\n",
       "  'relu1': ['dropout', 51, 59],\n",
       "  'no_activation3': ['linear', 51, 86],\n",
       "  'relu3': ['linear', 86, 95],\n",
       "  'relu4': ['linear', 95, 55],\n",
       "  'relu5': ['linear', 55, 36],\n",
       "  'relu6': ['linear', 36, 1],\n",
       "  'mae': 2.118320610687023},\n",
       " {'first': ['linear', 52, 51],\n",
       "  'relu1': ['linear', 51, 195],\n",
       "  'relu2': ['linear', 195, 197],\n",
       "  'relu3': ['dropout', 197, 197],\n",
       "  'no_activation5': ['linear', 197, 50],\n",
       "  'relu5': ['linear', 50, 33],\n",
       "  'relu6': ['linear', 33, 1],\n",
       "  'mae': 2.0610687022900764}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relu</th>\n",
       "      <th>sigmoid</th>\n",
       "      <th>tanh</th>\n",
       "      <th>layer_sum</th>\n",
       "      <th>dropout_loc</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2.11832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2.06107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relu sigmoid tanh layer_sum dropout_loc      mae\n",
       "0    5       0    0         7           2  2.11832\n",
       "1    5       0    0         7           4  2.06107"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auswertung = pd.DataFrame(columns = ['relu', 'sigmoid', 'tanh', 'layer_sum', 'dropout_loc','mae'])\n",
    "idx = 0\n",
    "for combination in opt_combination:\n",
    "    relu = 0\n",
    "    sigmoid = 0\n",
    "    tanh = 0\n",
    "    layer_sum = 0\n",
    "    dropout_loc = 0\n",
    "    mae = 0\n",
    "    layer_count = 1\n",
    "    for act, layer in combination.items():\n",
    "        if act == \"mae\":\n",
    "            mae = layer\n",
    "        else:\n",
    "            if act.startswith('relu'):\n",
    "                relu += 1\n",
    "            elif act.startswith('sigmoid'):\n",
    "                sigmoid += 1\n",
    "            elif act.startswith('tanh'):\n",
    "                tanh += 1\n",
    "            else:\n",
    "                pass\n",
    "            if layer[0] == \"dropout\":\n",
    "                dropout_loc = layer_count\n",
    "        layer_count += 1\n",
    "    layer_sum = len(list(combination.keys()))-1\n",
    "    row = [relu, sigmoid, tanh, layer_sum, dropout_loc, mae]\n",
    "    auswertung.loc[idx, :] = row\n",
    "    idx += 1\n",
    "auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "auswertung.to_csv(\"auswertung_netze.csv\",sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimieren der Lernrate und Trainingsepochenanzahl\n",
    "Paralleles Aufrufen des Optimierers, übergeben der zuvor erzeugten Dev- und Trainingsdatensätze, angeben der Intervalle für die die Hyperparameter getestet werden sollen (Lernrate und Epochenanzahl). Für das dynamische Modell wird das Dictionary übergeben, welches von dem vorhergegangenen Layer Optimizer am besten bewertet wurde und die Variable dynamic auf True gesetzt (default False). Der \"normale\" HP_Optimizer basiert auf der Klasse Netz()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'first': ['linear', 52, 51],\n",
       "  'relu1': ['dropout', 51, 59],\n",
       "  'no_activation3': ['linear', 51, 86],\n",
       "  'relu3': ['linear', 86, 95],\n",
       "  'relu4': ['linear', 95, 55],\n",
       "  'relu5': ['linear', 55, 36],\n",
       "  'relu6': ['linear', 36, 1],\n",
       "  'mae': 2.118320610687023},\n",
       " {'first': ['linear', 52, 51],\n",
       "  'relu1': ['linear', 51, 195],\n",
       "  'relu2': ['linear', 195, 197],\n",
       "  'relu3': ['dropout', 197, 197],\n",
       "  'no_activation5': ['linear', 197, 50],\n",
       "  'relu5': ['linear', 50, 33],\n",
       "  'relu6': ['linear', 33, 1],\n",
       "  'mae': 2.0610687022900764}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters with defaults:\n",
      "lr_range --> (0.0001,0.0001),\n",
      "step_size--> 0.0001,\n",
      "max_epochs-->(2,2),\n",
      "opt-->\"Adam\",\n",
      "cuda=True\n",
      "lr_range: Tupel with learnrate range\n",
      "step_size: float/int for step_size of learnrate\n",
      "max_epochs: Tupel with number of epochs range\n",
      "opt: Optimizer (by default Adam)\n",
      "cuda: True/False if cuda should be used, default = True\n",
      "\n",
      "Attributes:\n",
      "set self.train_data as dictionary with races (form: {raceId: race(dataframe)})\n",
      "set self.test_data as dictionary with races (form: {raceId: race(dataframe)})\n",
      "self.opt_combination: Dictionary which contains the best combination of the given parameters\n",
      "\n",
      "Methods:\n",
      "call self.validate_combination() to compare all combinations\n",
      "get all information/results with self.get_all_information()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statisches Netz():\n",
      "\n",
      "Chosen Model: Netz(\n",
      "  (fc1): Linear(in_features=52, out_features=150, bias=True)\n",
      "  (fc2): Linear(in_features=150, out_features=180, bias=True)\n",
      "  (fc3): Linear(in_features=180, out_features=190, bias=True)\n",
      "  (fc4): Linear(in_features=190, out_features=120, bias=True)\n",
      "  (fc5): Linear(in_features=120, out_features=100, bias=True)\n",
      "  (fc6): Linear(in_features=100, out_features=70, bias=True)\n",
      "  (fc7): Linear(in_features=70, out_features=30, bias=True)\n",
      "  (fc8): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Learningrate Range: [0.0001, 0.00045]\n",
      "Maximum Epochs: (3, 5)\n",
      "Chosen Optimizer: Adam\n",
      "Result Encoding: {2026: {'lr': 0.0001, 'epochen': 3}, 4195: {'lr': 0.00045, 'epochen': 3}, 3314: {'lr': 0.0001, 'epochen': 4}, 6943: {'lr': 0.00045, 'epochen': 4, 'mae': 1.633587786259542}}\n",
      "Results: {2026: 1.9198473282442747, 4195: 1.7442748091603053, 3314: 1.9122137404580153, 6943: 1.633587786259542}\n",
      "Optimale Kombination: {'lr': 0.00045, 'epochen': 4, 'mae': 1.633587786259542}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========================= \n",
      "\n",
      "Dynamisches Netz:\n",
      "\n",
      "Chosen Model: NetzDynamic(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=52, out_features=51, bias=True)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): Linear(in_features=51, out_features=86, bias=True)\n",
      "    (3): Linear(in_features=86, out_features=95, bias=True)\n",
      "    (4): Linear(in_features=95, out_features=55, bias=True)\n",
      "    (5): Linear(in_features=55, out_features=36, bias=True)\n",
      "    (6): Linear(in_features=36, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Learningrate Range: [0.0001, 0.0006]\n",
      "Maximum Epochs: (3, 5)\n",
      "Chosen Optimizer: Adam\n",
      "Result Encoding: {9952: {'lr': 0.0001, 'epochen': 3}, 4866: {'lr': 0.0006, 'epochen': 3}, 392: {'lr': 0.0001, 'epochen': 4}, 8550: {'lr': 0.0006, 'epochen': 4, 'mae': 1.751908396946565}}\n",
      "Results: {9952: 2.1297709923664123, 4866: 1.8549618320610688, 392: 2.0229007633587788, 8550: 1.751908396946565}\n",
      "Optimale Kombination: {'lr': 0.0006, 'epochen': 4, 'mae': 1.751908396946565}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========================= \n",
      "\n",
      "Dynamisches Netz:\n",
      "\n",
      "Chosen Model: NetzDynamic(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=52, out_features=51, bias=True)\n",
      "    (1): Linear(in_features=51, out_features=195, bias=True)\n",
      "    (2): Linear(in_features=195, out_features=197, bias=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=197, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=33, bias=True)\n",
      "    (6): Linear(in_features=33, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Learningrate Range: [0.0001, 0.0006]\n",
      "Maximum Epochs: (3, 5)\n",
      "Chosen Optimizer: Adam\n",
      "Result Encoding: {391: {'lr': 0.0001, 'epochen': 3}, 1416: {'lr': 0.0006, 'epochen': 3}, 7550: {'lr': 0.0001, 'epochen': 4}, 3690: {'lr': 0.0006, 'epochen': 4, 'mae': 1.5076335877862594}}\n",
      "Results: {391: 1.9389312977099236, 1416: 1.6908396946564885, 7550: 1.9045801526717556, 3690: 1.5076335877862594}\n",
      "Optimale Kombination: {'lr': 0.0006, 'epochen': 4, 'mae': 1.5076335877862594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relu</th>\n",
       "      <th>sigmoid</th>\n",
       "      <th>tanh</th>\n",
       "      <th>layer_sum</th>\n",
       "      <th>dropout_loc</th>\n",
       "      <th>mae</th>\n",
       "      <th>mae_l</th>\n",
       "      <th>lr</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2.11832</td>\n",
       "      <td>1.751908</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2.06107</td>\n",
       "      <td>1.507634</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relu sigmoid tanh layer_sum dropout_loc      mae     mae_l      lr  epochs\n",
       "0    5       0    0         7           2  2.11832  1.751908  0.0006       4\n",
       "1    5       0    0         7           4  2.06107  1.507634  0.0006       4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamische_netze = []\n",
    "#\"korrigieren\" der netz dictionarys --> mae aus den keys entfernen \n",
    "for netz in opt_combination:\n",
    "    opt_={}\n",
    "    for key in netz.keys():\n",
    "        if key != 'mae':#mae wird noch in dictionary von HP_Optimizer hinzugefügt und muss entfernt werden\n",
    "            opt_[key] = netz[key]\n",
    "    dynamische_netze.append(opt_)\n",
    "    \n",
    "#statisches netz auf lernrate und trainingsepochen optimieren\n",
    "if cuda.lower() == 'y':\n",
    "    h = HP_Optimizer(lr_range = [0.0001,0.00045],step_size = 0.0001, max_epochs=(3,5),cuda = True)\n",
    "else:\n",
    "    h = HP_Optimizer(lr_range = [0.0001,0.00045],step_size = 0.0001, max_epochs=(3,5),cuda = False)\n",
    "    \n",
    "h.help()\n",
    "#Zuweisen der Trainingsdaten\n",
    "h.train_data = train_T\n",
    "#Zuweisen der Dev Daten als Testdaten\n",
    "h.test_data = dev_T\n",
    "#Aufruf der Funktion die verschiedene Kombinationen der Epochen und Lernraten gegeneinander vergleicht\n",
    "h.validate_combinations()\n",
    "#Ausgabe der Ergebnisse des Vergleichs aus self.validate_combinations()\n",
    "print('Statisches Netz():\\n')\n",
    "h.get_all_information()\n",
    "\n",
    "    \n",
    "#alle dynamisch erzeugten netze werden auf lernrate und trainingsepocen optimiert\n",
    "idx = 0\n",
    "auswertung['mae_l'] = 0\n",
    "auswertung['lr'] = 0\n",
    "auswertung['epochs'] = 0\n",
    "for netz in dynamische_netze:\n",
    "    #überprüfen ob cuda verwendet werden soll, dann Aufruf des Optimizers für beide Netze\n",
    "    if cuda.lower() == 'y':\n",
    "        h_dynamic = HP_Optimizer(lr_range = [0.0001,0.0006],step_size = 0.0001, max_epochs=(3,5),cuda = True, dynamic = True, dyn_combination = netz)\n",
    "    else:\n",
    "        h_dynamic = HP_Optimizer(lr_range = [0.0001,0.0006],step_size = 0.0001, max_epochs=(3,5),cuda = False, dynamic = True, dyn_combination = netz)\n",
    "    h_dynamic.train_data = train_T\n",
    "    h_dynamic.test_data = dev_T\n",
    "    h_dynamic.validate_combinations()\n",
    "    print('\\n',25*'=','\\n')\n",
    "    print('Dynamisches Netz:\\n')\n",
    "    h_dynamic.get_all_information()\n",
    "    \n",
    "    max_epochs_dyn = h_dynamic.opt_combination['epochen']\n",
    "    lr_dynamic = h_dynamic.opt_combination['lr']\n",
    "    mae_dynamic = h_dynamic.opt_combination[\"mae\"]\n",
    "    auswertung.loc[idx, 'mae_l'] = mae_dynamic\n",
    "    auswertung.loc[idx, 'lr'] = lr_dynamic\n",
    "    auswertung.loc[idx, 'epochs'] = max_epochs_dyn\n",
    "    netz[\"epochen\"] = max_epochs_dyn\n",
    "    netz[\"lr\"] = lr_dynamic\n",
    "    netz[\"mae\"] = mae_dynamic\n",
    "    dynamische_netze[idx] = netz\n",
    "    idx += 1\n",
    "auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'first': ['linear', 52, 51],\n",
       "  'relu1': ['dropout', 51, 59],\n",
       "  'no_activation3': ['linear', 51, 86],\n",
       "  'relu3': ['linear', 86, 95],\n",
       "  'relu4': ['linear', 95, 55],\n",
       "  'relu5': ['linear', 55, 36],\n",
       "  'relu6': ['linear', 36, 1],\n",
       "  'epochen': 4,\n",
       "  'lr': 0.0006,\n",
       "  'mae': 1.751908396946565},\n",
       " {'first': ['linear', 52, 51],\n",
       "  'relu1': ['linear', 51, 195],\n",
       "  'relu2': ['linear', 195, 197],\n",
       "  'relu3': ['dropout', 197, 197],\n",
       "  'no_activation5': ['linear', 197, 50],\n",
       "  'relu5': ['linear', 50, 33],\n",
       "  'relu6': ['linear', 33, 1],\n",
       "  'epochen': 4,\n",
       "  'lr': 0.0006,\n",
       "  'mae': 1.5076335877862594}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamische_netze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testen der erzeugten Netze\n",
    "Es werden die als optimal in den gegebenen Intervallen bestimmten Werte aus dem Optimierer genommen und zwei neue Netze mit ihren jeweiligen Parametern für Trainingsepochen und Lernrate trainiert (dies geschieht für das statische und das dynamische Netz). Zum Schluss werden beide Netze auf den Testdaten getestet und die Ergebnisse des Testlaufes ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [0/96 (0%)]\tLoss: 10.252098\n",
      "Train Epoche: 1 [1/96 (1%)]\tLoss: 1.439008\n",
      "Train Epoche: 1 [2/96 (2%)]\tLoss: 84.408844\n",
      "Train Epoche: 1 [3/96 (3%)]\tLoss: 537.373962\n",
      "Train Epoche: 1 [4/96 (4%)]\tLoss: 201.078766\n",
      "Train Epoche: 1 [5/96 (5%)]\tLoss: 367.384338\n",
      "Train Epoche: 1 [6/96 (6%)]\tLoss: 173.193649\n",
      "Train Epoche: 1 [7/96 (7%)]\tLoss: 17.213982\n",
      "Train Epoche: 1 [8/96 (8%)]\tLoss: 4.584175\n",
      "Train Epoche: 1 [9/96 (9%)]\tLoss: 26.345398\n",
      "Train Epoche: 1 [10/96 (10%)]\tLoss: 583.650940\n",
      "Train Epoche: 1 [11/96 (11%)]\tLoss: 446.076904\n",
      "Train Epoche: 1 [12/96 (12%)]\tLoss: 488.412964\n",
      "Train Epoche: 1 [13/96 (14%)]\tLoss: 37.397865\n",
      "Train Epoche: 1 [14/96 (15%)]\tLoss: 101.726212\n",
      "Train Epoche: 1 [15/96 (16%)]\tLoss: 227.288681\n",
      "Train Epoche: 1 [16/96 (17%)]\tLoss: 326.252655\n",
      "Train Epoche: 1 [17/96 (18%)]\tLoss: 49.735176\n",
      "Train Epoche: 1 [18/96 (19%)]\tLoss: 64.300903\n",
      "Train Epoche: 1 [19/96 (20%)]\tLoss: 144.750015\n",
      "Train Epoche: 1 [20/96 (21%)]\tLoss: 121.257484\n",
      "Train Epoche: 1 [21/96 (22%)]\tLoss: 288.765564\n",
      "Train Epoche: 1 [22/96 (23%)]\tLoss: 253.103394\n",
      "Train Epoche: 1 [23/96 (24%)]\tLoss: 394.962341\n",
      "Train Epoche: 1 [24/96 (25%)]\tLoss: 389.451874\n",
      "Train Epoche: 1 [25/96 (26%)]\tLoss: 13.777069\n",
      "Train Epoche: 1 [26/96 (27%)]\tLoss: 389.132446\n",
      "Train Epoche: 1 [27/96 (28%)]\tLoss: 6.966790\n",
      "Train Epoche: 1 [28/96 (29%)]\tLoss: 368.126648\n",
      "Train Epoche: 1 [29/96 (30%)]\tLoss: 380.697632\n",
      "Train Epoche: 1 [30/96 (31%)]\tLoss: 0.045290\n",
      "Train Epoche: 1 [31/96 (32%)]\tLoss: 134.222458\n",
      "Train Epoche: 1 [32/96 (33%)]\tLoss: 370.076080\n",
      "Train Epoche: 1 [33/96 (34%)]\tLoss: 105.220528\n",
      "Train Epoche: 1 [34/96 (35%)]\tLoss: 25.509829\n",
      "Train Epoche: 1 [35/96 (36%)]\tLoss: 0.026991\n",
      "Train Epoche: 1 [36/96 (38%)]\tLoss: 5.932454\n",
      "Train Epoche: 1 [37/96 (39%)]\tLoss: 7.628795\n",
      "Train Epoche: 1 [38/96 (40%)]\tLoss: 71.984245\n",
      "Train Epoche: 1 [39/96 (41%)]\tLoss: 131.479004\n",
      "Train Epoche: 1 [40/96 (42%)]\tLoss: 13.595441\n",
      "Train Epoche: 1 [41/96 (43%)]\tLoss: 16.708733\n",
      "Train Epoche: 1 [42/96 (44%)]\tLoss: 6.316286\n",
      "Train Epoche: 1 [43/96 (45%)]\tLoss: 38.680279\n",
      "Train Epoche: 1 [44/96 (46%)]\tLoss: 256.136444\n",
      "Train Epoche: 1 [45/96 (47%)]\tLoss: 28.306080\n",
      "Train Epoche: 1 [46/96 (48%)]\tLoss: 14.104352\n",
      "Train Epoche: 1 [47/96 (49%)]\tLoss: 16.988995\n",
      "Train Epoche: 1 [48/96 (50%)]\tLoss: 11.059118\n",
      "Train Epoche: 1 [49/96 (51%)]\tLoss: 0.002842\n",
      "Train Epoche: 1 [50/96 (52%)]\tLoss: 277.369537\n",
      "Train Epoche: 1 [51/96 (53%)]\tLoss: 48.298893\n",
      "Train Epoche: 1 [52/96 (54%)]\tLoss: 3.659664\n",
      "Train Epoche: 1 [53/96 (55%)]\tLoss: 149.233978\n",
      "Train Epoche: 1 [54/96 (56%)]\tLoss: 45.777527\n",
      "Train Epoche: 1 [55/96 (57%)]\tLoss: 31.492270\n",
      "Train Epoche: 1 [56/96 (58%)]\tLoss: 136.051849\n",
      "Train Epoche: 1 [57/96 (59%)]\tLoss: 0.181566\n",
      "Train Epoche: 1 [58/96 (60%)]\tLoss: 8.006779\n",
      "Train Epoche: 1 [59/96 (61%)]\tLoss: 68.642105\n",
      "Train Epoche: 1 [60/96 (62%)]\tLoss: 36.702919\n",
      "Train Epoche: 1 [61/96 (64%)]\tLoss: 12.534658\n",
      "Train Epoche: 1 [62/96 (65%)]\tLoss: 6.612827\n",
      "Train Epoche: 1 [63/96 (66%)]\tLoss: 0.050410\n",
      "Train Epoche: 1 [64/96 (67%)]\tLoss: 4.454207\n",
      "Train Epoche: 1 [65/96 (68%)]\tLoss: 148.350693\n",
      "Train Epoche: 1 [66/96 (69%)]\tLoss: 67.021057\n",
      "Train Epoche: 1 [67/96 (70%)]\tLoss: 63.977253\n",
      "Train Epoche: 1 [68/96 (71%)]\tLoss: 76.398102\n",
      "Train Epoche: 1 [69/96 (72%)]\tLoss: 17.095608\n",
      "Train Epoche: 1 [70/96 (73%)]\tLoss: 183.565704\n",
      "Train Epoche: 1 [71/96 (74%)]\tLoss: 34.335293\n",
      "Train Epoche: 1 [72/96 (75%)]\tLoss: 16.151970\n",
      "Train Epoche: 1 [73/96 (76%)]\tLoss: 29.752478\n",
      "Train Epoche: 1 [74/96 (77%)]\tLoss: 9.688910\n",
      "Train Epoche: 1 [75/96 (78%)]\tLoss: 2.251359\n",
      "Train Epoche: 1 [76/96 (79%)]\tLoss: 1.068691\n",
      "Train Epoche: 1 [77/96 (80%)]\tLoss: 76.506165\n",
      "Train Epoche: 1 [78/96 (81%)]\tLoss: 0.035080\n",
      "Train Epoche: 1 [79/96 (82%)]\tLoss: 38.733070\n",
      "Train Epoche: 1 [80/96 (83%)]\tLoss: 2.094150\n",
      "Train Epoche: 1 [81/96 (84%)]\tLoss: 40.235336\n",
      "Train Epoche: 1 [82/96 (85%)]\tLoss: 49.803417\n",
      "Train Epoche: 1 [83/96 (86%)]\tLoss: 204.386078\n",
      "Train Epoche: 1 [84/96 (88%)]\tLoss: 142.555557\n",
      "Train Epoche: 1 [85/96 (89%)]\tLoss: 3.109906\n",
      "Train Epoche: 1 [86/96 (90%)]\tLoss: 14.459026\n",
      "Train Epoche: 1 [87/96 (91%)]\tLoss: 97.794151\n",
      "Train Epoche: 1 [88/96 (92%)]\tLoss: 61.676750\n",
      "Train Epoche: 1 [89/96 (93%)]\tLoss: 76.198990\n",
      "Train Epoche: 1 [90/96 (94%)]\tLoss: 81.269615\n",
      "Train Epoche: 1 [91/96 (95%)]\tLoss: 75.796234\n",
      "Train Epoche: 1 [92/96 (96%)]\tLoss: 0.279835\n",
      "Train Epoche: 1 [93/96 (97%)]\tLoss: 0.143720\n",
      "Train Epoche: 1 [94/96 (98%)]\tLoss: 20.065115\n",
      "Train Epoche: 1 [95/96 (99%)]\tLoss: 211.397369\n",
      "Train Epoche: 1 [96/96 (100%)]\tLoss: 187.984680\n",
      "Train Epoche: 1 [97/96 (101%)]\tLoss: 267.380707\n",
      "Train Epoche: 1 [98/96 (102%)]\tLoss: 0.390995\n",
      "Train Epoche: 1 [99/96 (103%)]\tLoss: 211.204651\n",
      "Train Epoche: 1 [100/96 (104%)]\tLoss: 74.399628\n",
      "Train Epoche: 1 [101/96 (105%)]\tLoss: 8.998547\n",
      "Train Epoche: 1 [102/96 (106%)]\tLoss: 0.872260\n",
      "Train Epoche: 1 [103/96 (107%)]\tLoss: 67.611435\n",
      "Train Epoche: 1 [104/96 (108%)]\tLoss: 17.032120\n",
      "Train Epoche: 1 [105/96 (109%)]\tLoss: 5.252962\n",
      "Train Epoche: 1 [106/96 (110%)]\tLoss: 1.167636\n",
      "Train Epoche: 1 [107/96 (111%)]\tLoss: 6.070299\n",
      "Train Epoche: 1 [108/96 (112%)]\tLoss: 167.008209\n",
      "Train Epoche: 1 [109/96 (114%)]\tLoss: 115.069778\n",
      "Train Epoche: 1 [110/96 (115%)]\tLoss: 34.300594\n",
      "Train Epoche: 1 [111/96 (116%)]\tLoss: 2.543539\n",
      "Train Epoche: 1 [112/96 (117%)]\tLoss: 2.011707\n",
      "Train Epoche: 1 [113/96 (118%)]\tLoss: 25.614925\n",
      "Train Epoche: 1 [114/96 (119%)]\tLoss: 44.423000\n",
      "Train Epoche: 1 [115/96 (120%)]\tLoss: 17.197628\n",
      "Train Epoche: 1 [116/96 (121%)]\tLoss: 43.443977\n",
      "Train Epoche: 1 [117/96 (122%)]\tLoss: 4.402198\n",
      "Train Epoche: 1 [118/96 (123%)]\tLoss: 0.478107\n",
      "Train Epoche: 1 [119/96 (124%)]\tLoss: 61.288471\n",
      "Train Epoche: 1 [120/96 (125%)]\tLoss: 15.922625\n",
      "Train Epoche: 1 [121/96 (126%)]\tLoss: 40.145760\n",
      "Train Epoche: 1 [122/96 (127%)]\tLoss: 38.874889\n",
      "Train Epoche: 1 [123/96 (128%)]\tLoss: 183.593170\n",
      "Train Epoche: 1 [124/96 (129%)]\tLoss: 4.916349\n",
      "Train Epoche: 1 [125/96 (130%)]\tLoss: 157.920944\n",
      "Train Epoche: 1 [126/96 (131%)]\tLoss: 0.657664\n",
      "Train Epoche: 1 [127/96 (132%)]\tLoss: 2.902399\n",
      "Train Epoche: 1 [128/96 (133%)]\tLoss: 151.097504\n",
      "Train Epoche: 1 [129/96 (134%)]\tLoss: 0.248475\n",
      "Train Epoche: 1 [130/96 (135%)]\tLoss: 110.318062\n",
      "Train Epoche: 1 [131/96 (136%)]\tLoss: 84.774620\n",
      "Train Epoche: 1 [132/96 (138%)]\tLoss: 131.993362\n",
      "Train Epoche: 1 [133/96 (139%)]\tLoss: 52.311043\n",
      "Train Epoche: 1 [134/96 (140%)]\tLoss: 22.271805\n",
      "Train Epoche: 1 [135/96 (141%)]\tLoss: 56.167046\n",
      "Train Epoche: 1 [136/96 (142%)]\tLoss: 111.801682\n",
      "Train Epoche: 1 [137/96 (143%)]\tLoss: 43.528740\n",
      "Train Epoche: 1 [138/96 (144%)]\tLoss: 63.031162\n",
      "Train Epoche: 1 [139/96 (145%)]\tLoss: 75.575294\n",
      "Train Epoche: 1 [140/96 (146%)]\tLoss: 122.736687\n",
      "Train Epoche: 1 [141/96 (147%)]\tLoss: 37.933674\n",
      "Train Epoche: 1 [142/96 (148%)]\tLoss: 55.218452\n",
      "Train Epoche: 1 [143/96 (149%)]\tLoss: 10.435553\n",
      "Train Epoche: 1 [144/96 (150%)]\tLoss: 29.223043\n",
      "Train Epoche: 1 [145/96 (151%)]\tLoss: 23.051502\n",
      "Train Epoche: 1 [146/96 (152%)]\tLoss: 12.305451\n",
      "Train Epoche: 1 [147/96 (153%)]\tLoss: 10.971500\n",
      "Train Epoche: 1 [148/96 (154%)]\tLoss: 283.378540\n",
      "Train Epoche: 1 [149/96 (155%)]\tLoss: 13.178562\n",
      "Train Epoche: 1 [150/96 (156%)]\tLoss: 0.204909\n",
      "Train Epoche: 1 [151/96 (157%)]\tLoss: 1.706154\n",
      "Train Epoche: 1 [152/96 (158%)]\tLoss: 29.295956\n",
      "Train Epoche: 1 [153/96 (159%)]\tLoss: 6.006561\n",
      "Train Epoche: 1 [154/96 (160%)]\tLoss: 28.109779\n",
      "Train Epoche: 1 [155/96 (161%)]\tLoss: 8.982388\n",
      "Train Epoche: 1 [156/96 (162%)]\tLoss: 194.187851\n",
      "Train Epoche: 1 [157/96 (164%)]\tLoss: 131.147461\n",
      "Train Epoche: 1 [158/96 (165%)]\tLoss: 6.595091\n",
      "Train Epoche: 1 [159/96 (166%)]\tLoss: 163.046707\n",
      "Train Epoche: 1 [160/96 (167%)]\tLoss: 2.812473\n",
      "Train Epoche: 1 [161/96 (168%)]\tLoss: 37.712902\n",
      "Train Epoche: 1 [162/96 (169%)]\tLoss: 3.828146\n",
      "Train Epoche: 1 [163/96 (170%)]\tLoss: 2.454907\n",
      "Train Epoche: 1 [164/96 (171%)]\tLoss: 1.768086\n",
      "Train Epoche: 1 [165/96 (172%)]\tLoss: 9.282570\n",
      "Train Epoche: 1 [166/96 (173%)]\tLoss: 130.150360\n",
      "Train Epoche: 1 [167/96 (174%)]\tLoss: 135.460571\n",
      "Train Epoche: 1 [168/96 (175%)]\tLoss: 63.801422\n",
      "Train Epoche: 1 [169/96 (176%)]\tLoss: 56.428326\n",
      "Train Epoche: 1 [170/96 (177%)]\tLoss: 25.837530\n",
      "Train Epoche: 1 [171/96 (178%)]\tLoss: 27.767719\n",
      "Train Epoche: 1 [172/96 (179%)]\tLoss: 1.861938\n",
      "Train Epoche: 1 [173/96 (180%)]\tLoss: 54.288269\n",
      "Train Epoche: 1 [174/96 (181%)]\tLoss: 20.045820\n",
      "Train Epoche: 1 [175/96 (182%)]\tLoss: 0.152722\n",
      "Train Epoche: 1 [176/96 (183%)]\tLoss: 0.335841\n",
      "Train Epoche: 1 [177/96 (184%)]\tLoss: 1.790480\n",
      "Train Epoche: 1 [178/96 (185%)]\tLoss: 23.336679\n",
      "Train Epoche: 1 [179/96 (186%)]\tLoss: 6.538728\n",
      "Train Epoche: 1 [180/96 (188%)]\tLoss: 12.200123\n",
      "Train Epoche: 1 [181/96 (189%)]\tLoss: 1.929019\n",
      "Train Epoche: 1 [182/96 (190%)]\tLoss: 2.602067\n",
      "Train Epoche: 1 [183/96 (191%)]\tLoss: 1.229875\n",
      "Train Epoche: 1 [184/96 (192%)]\tLoss: 9.191767\n",
      "Train Epoche: 1 [185/96 (193%)]\tLoss: 1.300934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [186/96 (194%)]\tLoss: 1.359141\n",
      "Train Epoche: 1 [187/96 (195%)]\tLoss: 5.089943\n",
      "Train Epoche: 1 [188/96 (196%)]\tLoss: 3.947318\n",
      "Train Epoche: 1 [189/96 (197%)]\tLoss: 39.755493\n",
      "Train Epoche: 1 [190/96 (198%)]\tLoss: 0.086951\n",
      "Train Epoche: 1 [191/96 (199%)]\tLoss: 23.468927\n",
      "Train Epoche: 1 [192/96 (200%)]\tLoss: 16.265173\n",
      "Train Epoche: 1 [193/96 (201%)]\tLoss: 4.582908\n",
      "Train Epoche: 1 [194/96 (202%)]\tLoss: 5.289869\n",
      "Train Epoche: 1 [195/96 (203%)]\tLoss: 1.375330\n",
      "Train Epoche: 1 [196/96 (204%)]\tLoss: 0.848787\n",
      "Train Epoche: 1 [197/96 (205%)]\tLoss: 34.962055\n",
      "Train Epoche: 1 [198/96 (206%)]\tLoss: 7.461388\n",
      "Train Epoche: 1 [199/96 (207%)]\tLoss: 18.010136\n",
      "Train Epoche: 1 [200/96 (208%)]\tLoss: 1.744517\n",
      "Train Epoche: 1 [201/96 (209%)]\tLoss: 0.380191\n",
      "Train Epoche: 1 [202/96 (210%)]\tLoss: 13.367286\n",
      "Train Epoche: 1 [203/96 (211%)]\tLoss: 40.102264\n",
      "Train Epoche: 1 [204/96 (212%)]\tLoss: 1.812668\n",
      "Train Epoche: 1 [205/96 (214%)]\tLoss: 0.530340\n",
      "Train Epoche: 1 [206/96 (215%)]\tLoss: 1.287451\n",
      "Train Epoche: 1 [207/96 (216%)]\tLoss: 16.553293\n",
      "Train Epoche: 1 [208/96 (217%)]\tLoss: 203.590073\n",
      "Train Epoche: 1 [209/96 (218%)]\tLoss: 231.672394\n",
      "Train Epoche: 1 [210/96 (219%)]\tLoss: 2.892766\n",
      "Train Epoche: 1 [211/96 (220%)]\tLoss: 2.242350\n",
      "Train Epoche: 1 [212/96 (221%)]\tLoss: 0.442546\n",
      "Train Epoche: 1 [213/96 (222%)]\tLoss: 7.007429\n",
      "Train Epoche: 1 [214/96 (223%)]\tLoss: 6.418230\n",
      "Train Epoche: 1 [215/96 (224%)]\tLoss: 0.780175\n",
      "Train Epoche: 1 [216/96 (225%)]\tLoss: 2.453168\n",
      "Train Epoche: 1 [217/96 (226%)]\tLoss: 67.566666\n",
      "Train Epoche: 1 [218/96 (227%)]\tLoss: 1.844169\n",
      "Train Epoche: 1 [219/96 (228%)]\tLoss: 20.787403\n",
      "Train Epoche: 1 [220/96 (229%)]\tLoss: 33.030605\n",
      "Train Epoche: 1 [221/96 (230%)]\tLoss: 12.944736\n",
      "Train Epoche: 1 [222/96 (231%)]\tLoss: 0.242035\n",
      "Train Epoche: 1 [223/96 (232%)]\tLoss: 0.956220\n",
      "Train Epoche: 1 [224/96 (233%)]\tLoss: 60.421688\n",
      "Train Epoche: 1 [225/96 (234%)]\tLoss: 31.029242\n",
      "Train Epoche: 1 [226/96 (235%)]\tLoss: 23.414709\n",
      "Train Epoche: 1 [227/96 (236%)]\tLoss: 11.499533\n",
      "Train Epoche: 1 [228/96 (238%)]\tLoss: 0.829445\n",
      "Train Epoche: 1 [229/96 (239%)]\tLoss: 0.107833\n",
      "Train Epoche: 1 [230/96 (240%)]\tLoss: 5.682441\n",
      "Train Epoche: 1 [231/96 (241%)]\tLoss: 249.307388\n",
      "Train Epoche: 1 [232/96 (242%)]\tLoss: 0.457636\n",
      "Train Epoche: 1 [233/96 (243%)]\tLoss: 15.948360\n",
      "Train Epoche: 1 [234/96 (244%)]\tLoss: 69.229713\n",
      "Train Epoche: 1 [235/96 (245%)]\tLoss: 93.588211\n",
      "Train Epoche: 1 [236/96 (246%)]\tLoss: 222.279282\n",
      "Train Epoche: 1 [237/96 (247%)]\tLoss: 86.546211\n",
      "Train Epoche: 1 [238/96 (248%)]\tLoss: 0.009387\n",
      "Train Epoche: 1 [239/96 (249%)]\tLoss: 70.034691\n",
      "Train Epoche: 1 [240/96 (250%)]\tLoss: 32.255989\n",
      "Train Epoche: 1 [241/96 (251%)]\tLoss: 2.294640\n",
      "Train Epoche: 1 [242/96 (252%)]\tLoss: 21.841068\n",
      "Train Epoche: 1 [243/96 (253%)]\tLoss: 7.503931\n",
      "Train Epoche: 1 [244/96 (254%)]\tLoss: 0.012267\n",
      "Train Epoche: 1 [245/96 (255%)]\tLoss: 3.406712\n",
      "Train Epoche: 1 [246/96 (256%)]\tLoss: 21.942268\n",
      "Train Epoche: 1 [247/96 (257%)]\tLoss: 0.212913\n",
      "Train Epoche: 1 [248/96 (258%)]\tLoss: 5.958378\n",
      "Train Epoche: 1 [249/96 (259%)]\tLoss: 145.858688\n",
      "Train Epoche: 1 [250/96 (260%)]\tLoss: 26.166092\n",
      "Train Epoche: 1 [251/96 (261%)]\tLoss: 14.174847\n",
      "Train Epoche: 1 [252/96 (262%)]\tLoss: 0.804285\n",
      "Train Epoche: 1 [253/96 (264%)]\tLoss: 14.687446\n",
      "Train Epoche: 1 [254/96 (265%)]\tLoss: 172.601120\n",
      "Train Epoche: 1 [255/96 (266%)]\tLoss: 0.046557\n",
      "Train Epoche: 1 [256/96 (267%)]\tLoss: 5.107015\n",
      "Train Epoche: 1 [257/96 (268%)]\tLoss: 4.432820\n",
      "Train Epoche: 1 [258/96 (269%)]\tLoss: 6.629246\n",
      "Train Epoche: 1 [259/96 (270%)]\tLoss: 4.432894\n",
      "Train Epoche: 1 [260/96 (271%)]\tLoss: 61.469234\n",
      "Train Epoche: 1 [261/96 (272%)]\tLoss: 0.643271\n",
      "Train Epoche: 1 [262/96 (273%)]\tLoss: 0.305352\n",
      "Train Epoche: 1 [263/96 (274%)]\tLoss: 2.939391\n",
      "Train Epoche: 1 [264/96 (275%)]\tLoss: 3.958915\n",
      "Train Epoche: 1 [265/96 (276%)]\tLoss: 0.576858\n",
      "Train Epoche: 1 [266/96 (277%)]\tLoss: 22.142126\n",
      "Train Epoche: 1 [267/96 (278%)]\tLoss: 158.375824\n",
      "Train Epoche: 1 [268/96 (279%)]\tLoss: 24.189493\n",
      "Train Epoche: 1 [269/96 (280%)]\tLoss: 10.530328\n",
      "Train Epoche: 1 [270/96 (281%)]\tLoss: 14.423307\n",
      "Train Epoche: 1 [271/96 (282%)]\tLoss: 2.244010\n",
      "Train Epoche: 1 [272/96 (283%)]\tLoss: 0.756986\n",
      "Train Epoche: 1 [273/96 (284%)]\tLoss: 13.846584\n",
      "Train Epoche: 1 [274/96 (285%)]\tLoss: 9.900270\n",
      "Train Epoche: 1 [275/96 (286%)]\tLoss: 10.318649\n",
      "Train Epoche: 1 [276/96 (288%)]\tLoss: 79.944244\n",
      "Train Epoche: 1 [277/96 (289%)]\tLoss: 1.457387\n",
      "Train Epoche: 1 [278/96 (290%)]\tLoss: 14.014240\n",
      "Train Epoche: 1 [279/96 (291%)]\tLoss: 2.063112\n",
      "Train Epoche: 1 [280/96 (292%)]\tLoss: 61.239159\n",
      "Train Epoche: 1 [281/96 (293%)]\tLoss: 0.005051\n",
      "Train Epoche: 1 [282/96 (294%)]\tLoss: 7.200140\n",
      "Train Epoche: 1 [283/96 (295%)]\tLoss: 38.613083\n",
      "Train Epoche: 1 [284/96 (296%)]\tLoss: 0.255684\n",
      "Train Epoche: 1 [285/96 (297%)]\tLoss: 7.521051\n",
      "Train Epoche: 1 [286/96 (298%)]\tLoss: 15.470144\n",
      "Train Epoche: 1 [287/96 (299%)]\tLoss: 0.939456\n",
      "Train Epoche: 1 [288/96 (300%)]\tLoss: 3.582769\n",
      "Train Epoche: 1 [289/96 (301%)]\tLoss: 7.668285\n",
      "Train Epoche: 1 [290/96 (302%)]\tLoss: 0.274967\n",
      "Train Epoche: 1 [291/96 (303%)]\tLoss: 9.608879\n",
      "Train Epoche: 1 [292/96 (304%)]\tLoss: 0.630964\n",
      "Train Epoche: 1 [293/96 (305%)]\tLoss: 3.825299\n",
      "Train Epoche: 1 [294/96 (306%)]\tLoss: 14.690501\n",
      "Train Epoche: 1 [295/96 (307%)]\tLoss: 9.943194\n",
      "Train Epoche: 1 [296/96 (308%)]\tLoss: 0.278619\n",
      "Train Epoche: 1 [297/96 (309%)]\tLoss: 0.644130\n",
      "Train Epoche: 1 [298/96 (310%)]\tLoss: 2.151222\n",
      "Train Epoche: 1 [299/96 (311%)]\tLoss: 2.062663\n",
      "Train Epoche: 1 [300/96 (312%)]\tLoss: 2.503088\n",
      "Train Epoche: 1 [301/96 (314%)]\tLoss: 95.018097\n",
      "Train Epoche: 1 [302/96 (315%)]\tLoss: 8.342717\n",
      "Train Epoche: 1 [303/96 (316%)]\tLoss: 7.122974\n",
      "Train Epoche: 1 [304/96 (317%)]\tLoss: 3.937631\n",
      "Train Epoche: 1 [305/96 (318%)]\tLoss: 2.546317\n",
      "Train Epoche: 1 [306/96 (319%)]\tLoss: 18.170685\n",
      "Train Epoche: 1 [307/96 (320%)]\tLoss: 0.418741\n",
      "Train Epoche: 1 [308/96 (321%)]\tLoss: 32.127728\n",
      "Train Epoche: 1 [309/96 (322%)]\tLoss: 18.753443\n",
      "Train Epoche: 1 [310/96 (323%)]\tLoss: 0.078374\n",
      "Train Epoche: 1 [311/96 (324%)]\tLoss: 0.705595\n",
      "Train Epoche: 1 [312/96 (325%)]\tLoss: 1.880092\n",
      "Train Epoche: 1 [313/96 (326%)]\tLoss: 0.383820\n",
      "Train Epoche: 1 [314/96 (327%)]\tLoss: 122.789162\n",
      "Train Epoche: 1 [315/96 (328%)]\tLoss: 7.562988\n",
      "Train Epoche: 1 [316/96 (329%)]\tLoss: 6.267145\n",
      "Train Epoche: 1 [317/96 (330%)]\tLoss: 4.855238\n",
      "Train Epoche: 1 [318/96 (331%)]\tLoss: 0.383919\n",
      "Train Epoche: 1 [319/96 (332%)]\tLoss: 6.141773\n",
      "Train Epoche: 1 [320/96 (333%)]\tLoss: 12.652712\n",
      "Train Epoche: 1 [321/96 (334%)]\tLoss: 34.427673\n",
      "Train Epoche: 1 [322/96 (335%)]\tLoss: 6.622638\n",
      "Train Epoche: 1 [323/96 (336%)]\tLoss: 29.356308\n",
      "Train Epoche: 1 [324/96 (338%)]\tLoss: 0.166731\n",
      "Train Epoche: 1 [325/96 (339%)]\tLoss: 0.033220\n",
      "Train Epoche: 1 [326/96 (340%)]\tLoss: 0.932607\n",
      "Train Epoche: 1 [327/96 (341%)]\tLoss: 0.021552\n",
      "Train Epoche: 1 [328/96 (342%)]\tLoss: 6.685416\n",
      "Train Epoche: 1 [329/96 (343%)]\tLoss: 7.496675\n",
      "Train Epoche: 1 [330/96 (344%)]\tLoss: 130.484177\n",
      "Train Epoche: 1 [331/96 (345%)]\tLoss: 1.433281\n",
      "Train Epoche: 1 [332/96 (346%)]\tLoss: 0.119480\n",
      "Train Epoche: 1 [333/96 (347%)]\tLoss: 0.105206\n",
      "Train Epoche: 1 [334/96 (348%)]\tLoss: 3.174207\n",
      "Train Epoche: 1 [335/96 (349%)]\tLoss: 10.080560\n",
      "Train Epoche: 1 [336/96 (350%)]\tLoss: 31.598969\n",
      "Train Epoche: 1 [337/96 (351%)]\tLoss: 0.469154\n",
      "Train Epoche: 1 [338/96 (352%)]\tLoss: 0.952694\n",
      "Train Epoche: 1 [339/96 (353%)]\tLoss: 0.329299\n",
      "Train Epoche: 1 [340/96 (354%)]\tLoss: 27.095062\n",
      "Train Epoche: 1 [341/96 (355%)]\tLoss: 0.214872\n",
      "Train Epoche: 1 [342/96 (356%)]\tLoss: 0.004016\n",
      "Train Epoche: 1 [343/96 (357%)]\tLoss: 134.667953\n",
      "Train Epoche: 1 [344/96 (358%)]\tLoss: 185.416779\n",
      "Train Epoche: 1 [345/96 (359%)]\tLoss: 410.519470\n",
      "Train Epoche: 1 [346/96 (360%)]\tLoss: 1.399220\n",
      "Train Epoche: 1 [347/96 (361%)]\tLoss: 9.962580\n",
      "Train Epoche: 1 [348/96 (362%)]\tLoss: 14.264846\n",
      "Train Epoche: 1 [349/96 (364%)]\tLoss: 0.022169\n",
      "Train Epoche: 1 [350/96 (365%)]\tLoss: 11.620185\n",
      "Train Epoche: 1 [351/96 (366%)]\tLoss: 54.754181\n",
      "Train Epoche: 1 [352/96 (367%)]\tLoss: 161.220993\n",
      "Train Epoche: 1 [353/96 (368%)]\tLoss: 2.892905\n",
      "Train Epoche: 1 [354/96 (369%)]\tLoss: 2.395118\n",
      "Train Epoche: 1 [355/96 (370%)]\tLoss: 0.198605\n",
      "Train Epoche: 1 [356/96 (371%)]\tLoss: 232.107773\n",
      "Train Epoche: 1 [357/96 (372%)]\tLoss: 2.786225\n",
      "Train Epoche: 1 [358/96 (373%)]\tLoss: 16.864397\n",
      "Train Epoche: 1 [359/96 (374%)]\tLoss: 1.969453\n",
      "Train Epoche: 1 [360/96 (375%)]\tLoss: 33.032162\n",
      "Train Epoche: 1 [361/96 (376%)]\tLoss: 0.000463\n",
      "Train Epoche: 1 [362/96 (377%)]\tLoss: 5.333027\n",
      "Train Epoche: 1 [363/96 (378%)]\tLoss: 0.226040\n",
      "Train Epoche: 1 [364/96 (379%)]\tLoss: 2.157952\n",
      "Train Epoche: 1 [365/96 (380%)]\tLoss: 3.331488\n",
      "Train Epoche: 1 [366/96 (381%)]\tLoss: 0.336348\n",
      "Train Epoche: 1 [367/96 (382%)]\tLoss: 16.509695\n",
      "Train Epoche: 1 [368/96 (383%)]\tLoss: 0.761419\n",
      "Train Epoche: 1 [369/96 (384%)]\tLoss: 14.198698\n",
      "Train Epoche: 1 [370/96 (385%)]\tLoss: 19.221285\n",
      "Train Epoche: 1 [371/96 (386%)]\tLoss: 0.108629\n",
      "Train Epoche: 1 [372/96 (388%)]\tLoss: 70.725182\n",
      "Train Epoche: 1 [373/96 (389%)]\tLoss: 46.326385\n",
      "Train Epoche: 1 [374/96 (390%)]\tLoss: 1.509494\n",
      "Train Epoche: 1 [375/96 (391%)]\tLoss: 3.162681\n",
      "Train Epoche: 1 [376/96 (392%)]\tLoss: 80.025406\n",
      "Train Epoche: 1 [377/96 (393%)]\tLoss: 0.198598\n",
      "Train Epoche: 1 [378/96 (394%)]\tLoss: 0.651710\n",
      "Train Epoche: 1 [379/96 (395%)]\tLoss: 6.156568\n",
      "Train Epoche: 1 [380/96 (396%)]\tLoss: 1.907114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [381/96 (397%)]\tLoss: 20.628696\n",
      "Train Epoche: 1 [382/96 (398%)]\tLoss: 0.352881\n",
      "Train Epoche: 1 [383/96 (399%)]\tLoss: 2.028916\n",
      "Train Epoche: 1 [384/96 (400%)]\tLoss: 0.215312\n",
      "Train Epoche: 1 [385/96 (401%)]\tLoss: 0.744429\n",
      "Train Epoche: 1 [386/96 (402%)]\tLoss: 17.945356\n",
      "Train Epoche: 1 [387/96 (403%)]\tLoss: 44.178043\n",
      "Train Epoche: 1 [388/96 (404%)]\tLoss: 34.517250\n",
      "Train Epoche: 1 [389/96 (405%)]\tLoss: 1.105668\n",
      "Train Epoche: 1 [390/96 (406%)]\tLoss: 5.210042\n",
      "Train Epoche: 1 [391/96 (407%)]\tLoss: 67.673317\n",
      "Train Epoche: 1 [392/96 (408%)]\tLoss: 14.909184\n",
      "Train Epoche: 1 [393/96 (409%)]\tLoss: 298.092468\n",
      "Train Epoche: 1 [394/96 (410%)]\tLoss: 43.786793\n",
      "Train Epoche: 1 [395/96 (411%)]\tLoss: 14.607330\n",
      "Train Epoche: 1 [396/96 (412%)]\tLoss: 0.075472\n",
      "Train Epoche: 1 [397/96 (414%)]\tLoss: 10.168317\n",
      "Train Epoche: 1 [398/96 (415%)]\tLoss: 4.115166\n",
      "Train Epoche: 1 [399/96 (416%)]\tLoss: 0.326573\n",
      "Train Epoche: 1 [400/96 (417%)]\tLoss: 9.674073\n",
      "Train Epoche: 1 [401/96 (418%)]\tLoss: 12.751132\n",
      "Train Epoche: 1 [402/96 (419%)]\tLoss: 7.815339\n",
      "Train Epoche: 1 [403/96 (420%)]\tLoss: 0.066144\n",
      "Train Epoche: 1 [404/96 (421%)]\tLoss: 22.035080\n",
      "Train Epoche: 1 [405/96 (422%)]\tLoss: 0.181495\n",
      "Train Epoche: 1 [406/96 (423%)]\tLoss: 0.580406\n",
      "Train Epoche: 1 [407/96 (424%)]\tLoss: 9.155158\n",
      "Train Epoche: 1 [408/96 (425%)]\tLoss: 7.304657\n",
      "Train Epoche: 1 [409/96 (426%)]\tLoss: 3.858857\n",
      "Train Epoche: 1 [410/96 (427%)]\tLoss: 2.902311\n",
      "Train Epoche: 1 [411/96 (428%)]\tLoss: 0.021463\n",
      "Train Epoche: 1 [412/96 (429%)]\tLoss: 32.615265\n",
      "Train Epoche: 1 [413/96 (430%)]\tLoss: 8.481514\n",
      "Train Epoche: 1 [414/96 (431%)]\tLoss: 8.166130\n",
      "Train Epoche: 1 [415/96 (432%)]\tLoss: 26.404537\n",
      "Train Epoche: 1 [416/96 (433%)]\tLoss: 13.775248\n",
      "Train Epoche: 1 [417/96 (434%)]\tLoss: 0.205668\n",
      "Train Epoche: 1 [418/96 (435%)]\tLoss: 0.025357\n",
      "Train Epoche: 1 [419/96 (436%)]\tLoss: 27.428391\n",
      "Train Epoche: 1 [420/96 (438%)]\tLoss: 6.492913\n",
      "Train Epoche: 1 [421/96 (439%)]\tLoss: 7.456032\n",
      "Train Epoche: 1 [422/96 (440%)]\tLoss: 0.626374\n",
      "Train Epoche: 1 [423/96 (441%)]\tLoss: 145.541321\n",
      "Train Epoche: 1 [424/96 (442%)]\tLoss: 0.106292\n",
      "Train Epoche: 1 [425/96 (443%)]\tLoss: 75.493523\n",
      "Train Epoche: 1 [426/96 (444%)]\tLoss: 4.290116\n",
      "Train Epoche: 1 [427/96 (445%)]\tLoss: 16.941120\n",
      "Train Epoche: 1 [428/96 (446%)]\tLoss: 16.065878\n",
      "Train Epoche: 1 [429/96 (447%)]\tLoss: 6.693573\n",
      "Train Epoche: 1 [430/96 (448%)]\tLoss: 39.897747\n",
      "Train Epoche: 1 [431/96 (449%)]\tLoss: 3.751918\n",
      "Train Epoche: 1 [432/96 (450%)]\tLoss: 0.272737\n",
      "Train Epoche: 1 [433/96 (451%)]\tLoss: 1.257095\n",
      "Train Epoche: 1 [434/96 (452%)]\tLoss: 13.867664\n",
      "Train Epoche: 1 [435/96 (453%)]\tLoss: 13.925036\n",
      "Train Epoche: 1 [436/96 (454%)]\tLoss: 10.592400\n",
      "Train Epoche: 1 [437/96 (455%)]\tLoss: 11.590578\n",
      "Train Epoche: 1 [438/96 (456%)]\tLoss: 1.657437\n",
      "Train Epoche: 1 [439/96 (457%)]\tLoss: 8.514041\n",
      "Train Epoche: 1 [440/96 (458%)]\tLoss: 0.230918\n",
      "Train Epoche: 1 [441/96 (459%)]\tLoss: 0.866109\n",
      "Train Epoche: 1 [442/96 (460%)]\tLoss: 2.539024\n",
      "Train Epoche: 1 [443/96 (461%)]\tLoss: 10.687390\n",
      "Train Epoche: 1 [444/96 (462%)]\tLoss: 43.575527\n",
      "Train Epoche: 1 [445/96 (464%)]\tLoss: 3.748710\n",
      "Train Epoche: 1 [446/96 (465%)]\tLoss: 208.619110\n",
      "Train Epoche: 1 [447/96 (466%)]\tLoss: 0.504143\n",
      "Train Epoche: 1 [448/96 (467%)]\tLoss: 2.513558\n",
      "Train Epoche: 1 [449/96 (468%)]\tLoss: 0.006156\n",
      "Train Epoche: 1 [450/96 (469%)]\tLoss: 36.703659\n",
      "Train Epoche: 1 [451/96 (470%)]\tLoss: 0.258936\n",
      "Train Epoche: 1 [452/96 (471%)]\tLoss: 13.490084\n",
      "Train Epoche: 1 [453/96 (472%)]\tLoss: 50.601620\n",
      "Train Epoche: 1 [454/96 (473%)]\tLoss: 17.809311\n",
      "Train Epoche: 1 [455/96 (474%)]\tLoss: 44.253143\n",
      "Train Epoche: 1 [456/96 (475%)]\tLoss: 50.151978\n",
      "Train Epoche: 1 [457/96 (476%)]\tLoss: 13.993020\n",
      "Train Epoche: 1 [458/96 (477%)]\tLoss: 2.023508\n",
      "Train Epoche: 1 [459/96 (478%)]\tLoss: 0.841693\n",
      "Train Epoche: 1 [460/96 (479%)]\tLoss: 115.829018\n",
      "Train Epoche: 1 [461/96 (480%)]\tLoss: 2.753932\n",
      "Train Epoche: 1 [462/96 (481%)]\tLoss: 0.498977\n",
      "Train Epoche: 1 [463/96 (482%)]\tLoss: 1.890196\n",
      "Train Epoche: 1 [464/96 (483%)]\tLoss: 219.556366\n",
      "Train Epoche: 1 [465/96 (484%)]\tLoss: 20.017273\n",
      "Train Epoche: 1 [466/96 (485%)]\tLoss: 0.132782\n",
      "Train Epoche: 1 [467/96 (486%)]\tLoss: 116.529991\n",
      "Train Epoche: 1 [468/96 (488%)]\tLoss: 0.708986\n",
      "Train Epoche: 1 [469/96 (489%)]\tLoss: 36.185242\n",
      "Train Epoche: 1 [470/96 (490%)]\tLoss: 0.243539\n",
      "Train Epoche: 1 [471/96 (491%)]\tLoss: 33.809818\n",
      "Train Epoche: 1 [472/96 (492%)]\tLoss: 3.189684\n",
      "Train Epoche: 1 [473/96 (493%)]\tLoss: 4.855117\n",
      "Train Epoche: 1 [474/96 (494%)]\tLoss: 7.719551\n",
      "Train Epoche: 1 [475/96 (495%)]\tLoss: 77.783478\n",
      "Train Epoche: 1 [476/96 (496%)]\tLoss: 26.737747\n",
      "Train Epoche: 1 [477/96 (497%)]\tLoss: 0.671824\n",
      "Train Epoche: 1 [478/96 (498%)]\tLoss: 29.682741\n",
      "Train Epoche: 1 [479/96 (499%)]\tLoss: 28.568935\n",
      "Train Epoche: 1 [480/96 (500%)]\tLoss: 58.860622\n",
      "Train Epoche: 1 [481/96 (501%)]\tLoss: 7.576998\n",
      "Train Epoche: 1 [482/96 (502%)]\tLoss: 0.033722\n",
      "Train Epoche: 1 [483/96 (503%)]\tLoss: 35.320652\n",
      "Train Epoche: 1 [484/96 (504%)]\tLoss: 0.595132\n",
      "Train Epoche: 1 [485/96 (505%)]\tLoss: 4.689484\n",
      "Train Epoche: 1 [486/96 (506%)]\tLoss: 1.324460\n",
      "Train Epoche: 1 [487/96 (507%)]\tLoss: 32.426521\n",
      "Train Epoche: 1 [488/96 (508%)]\tLoss: 1.919383\n",
      "Train Epoche: 1 [489/96 (509%)]\tLoss: 6.827031\n",
      "Train Epoche: 1 [490/96 (510%)]\tLoss: 16.223953\n",
      "Train Epoche: 1 [491/96 (511%)]\tLoss: 8.597604\n",
      "Train Epoche: 1 [492/96 (512%)]\tLoss: 16.789223\n",
      "Train Epoche: 1 [493/96 (514%)]\tLoss: 11.374151\n",
      "Train Epoche: 1 [494/96 (515%)]\tLoss: 8.069455\n",
      "Train Epoche: 1 [495/96 (516%)]\tLoss: 12.709385\n",
      "Train Epoche: 1 [496/96 (517%)]\tLoss: 47.913330\n",
      "Train Epoche: 1 [497/96 (518%)]\tLoss: 75.451485\n",
      "Train Epoche: 1 [498/96 (519%)]\tLoss: 17.331539\n",
      "Train Epoche: 1 [499/96 (520%)]\tLoss: 1.133188\n",
      "Train Epoche: 1 [500/96 (521%)]\tLoss: 35.020790\n",
      "Train Epoche: 1 [501/96 (522%)]\tLoss: 2.295033\n",
      "Train Epoche: 1 [502/96 (523%)]\tLoss: 10.787790\n",
      "Train Epoche: 1 [503/96 (524%)]\tLoss: 1.475447\n",
      "Train Epoche: 1 [504/96 (525%)]\tLoss: 2.987756\n",
      "Train Epoche: 1 [505/96 (526%)]\tLoss: 93.789200\n",
      "Train Epoche: 1 [506/96 (527%)]\tLoss: 0.043795\n",
      "Train Epoche: 1 [507/96 (528%)]\tLoss: 35.262600\n",
      "Train Epoche: 1 [508/96 (529%)]\tLoss: 1.019224\n",
      "Train Epoche: 1 [509/96 (530%)]\tLoss: 8.262469\n",
      "Train Epoche: 1 [510/96 (531%)]\tLoss: 16.089718\n",
      "Train Epoche: 1 [511/96 (532%)]\tLoss: 6.961699\n",
      "Train Epoche: 1 [512/96 (533%)]\tLoss: 11.875981\n",
      "Train Epoche: 1 [513/96 (534%)]\tLoss: 7.830829\n",
      "Train Epoche: 1 [514/96 (535%)]\tLoss: 1.877822\n",
      "Train Epoche: 1 [515/96 (536%)]\tLoss: 2.154876\n",
      "Train Epoche: 1 [516/96 (538%)]\tLoss: 5.087249\n",
      "Train Epoche: 1 [517/96 (539%)]\tLoss: 24.540760\n",
      "Train Epoche: 1 [518/96 (540%)]\tLoss: 2.703112\n",
      "Train Epoche: 1 [519/96 (541%)]\tLoss: 10.335319\n",
      "Train Epoche: 1 [520/96 (542%)]\tLoss: 5.207879\n",
      "Train Epoche: 1 [521/96 (543%)]\tLoss: 85.779930\n",
      "Train Epoche: 1 [522/96 (544%)]\tLoss: 4.760505\n",
      "Train Epoche: 1 [523/96 (545%)]\tLoss: 12.471101\n",
      "Train Epoche: 1 [524/96 (546%)]\tLoss: 30.930246\n",
      "Train Epoche: 1 [525/96 (547%)]\tLoss: 7.150910\n",
      "Train Epoche: 1 [526/96 (548%)]\tLoss: 3.411467\n",
      "Train Epoche: 1 [527/96 (549%)]\tLoss: 8.883592\n",
      "Train Epoche: 1 [528/96 (550%)]\tLoss: 5.986500\n",
      "Train Epoche: 1 [529/96 (551%)]\tLoss: 0.666131\n",
      "Train Epoche: 1 [530/96 (552%)]\tLoss: 1.369056\n",
      "Train Epoche: 1 [531/96 (553%)]\tLoss: 7.142795\n",
      "Train Epoche: 1 [532/96 (554%)]\tLoss: 0.339703\n",
      "Train Epoche: 1 [533/96 (555%)]\tLoss: 126.690636\n",
      "Train Epoche: 1 [534/96 (556%)]\tLoss: 2.359675\n",
      "Train Epoche: 1 [535/96 (557%)]\tLoss: 2.520552\n",
      "Train Epoche: 1 [536/96 (558%)]\tLoss: 13.265903\n",
      "Train Epoche: 1 [537/96 (559%)]\tLoss: 9.587677\n",
      "Train Epoche: 1 [538/96 (560%)]\tLoss: 2.232490\n",
      "Train Epoche: 1 [539/96 (561%)]\tLoss: 7.333854\n",
      "Train Epoche: 1 [540/96 (562%)]\tLoss: 7.143656\n",
      "Train Epoche: 1 [541/96 (564%)]\tLoss: 0.363605\n",
      "Train Epoche: 1 [542/96 (565%)]\tLoss: 3.296916\n",
      "Train Epoche: 1 [543/96 (566%)]\tLoss: 0.348814\n",
      "Train Epoche: 1 [544/96 (567%)]\tLoss: 12.968505\n",
      "Train Epoche: 1 [545/96 (568%)]\tLoss: 0.000009\n",
      "Train Epoche: 1 [546/96 (569%)]\tLoss: 16.123131\n",
      "Train Epoche: 1 [547/96 (570%)]\tLoss: 1.282272\n",
      "Train Epoche: 1 [548/96 (571%)]\tLoss: 17.237856\n",
      "Train Epoche: 1 [549/96 (572%)]\tLoss: 0.066620\n",
      "Train Epoche: 1 [550/96 (573%)]\tLoss: 2.617213\n",
      "Train Epoche: 1 [551/96 (574%)]\tLoss: 27.547361\n",
      "Train Epoche: 1 [552/96 (575%)]\tLoss: 7.700771\n",
      "Train Epoche: 1 [553/96 (576%)]\tLoss: 337.836365\n",
      "Train Epoche: 1 [554/96 (577%)]\tLoss: 0.927673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [555/96 (578%)]\tLoss: 2.115099\n",
      "Train Epoche: 1 [556/96 (579%)]\tLoss: 0.018118\n",
      "Train Epoche: 1 [557/96 (580%)]\tLoss: 1.890874\n",
      "Train Epoche: 1 [558/96 (581%)]\tLoss: 3.066392\n",
      "Train Epoche: 1 [559/96 (582%)]\tLoss: 0.001293\n",
      "Train Epoche: 1 [560/96 (583%)]\tLoss: 6.254531\n",
      "Train Epoche: 1 [561/96 (584%)]\tLoss: 2.281894\n",
      "Train Epoche: 1 [562/96 (585%)]\tLoss: 5.961154\n",
      "Train Epoche: 1 [563/96 (586%)]\tLoss: 7.304420\n",
      "Train Epoche: 1 [564/96 (588%)]\tLoss: 4.838174\n",
      "Train Epoche: 1 [565/96 (589%)]\tLoss: 7.853054\n",
      "Train Epoche: 1 [566/96 (590%)]\tLoss: 9.345592\n",
      "Train Epoche: 1 [567/96 (591%)]\tLoss: 4.930578\n",
      "Train Epoche: 1 [568/96 (592%)]\tLoss: 0.420362\n",
      "Train Epoche: 1 [569/96 (593%)]\tLoss: 17.796637\n",
      "Train Epoche: 1 [570/96 (594%)]\tLoss: 0.001820\n",
      "Train Epoche: 1 [571/96 (595%)]\tLoss: 8.691522\n",
      "Train Epoche: 1 [572/96 (596%)]\tLoss: 0.053996\n",
      "Train Epoche: 1 [573/96 (597%)]\tLoss: 1.228545\n",
      "Train Epoche: 1 [574/96 (598%)]\tLoss: 0.034055\n",
      "Train Epoche: 1 [575/96 (599%)]\tLoss: 4.073642\n",
      "Train Epoche: 1 [576/96 (600%)]\tLoss: 16.004784\n",
      "Train Epoche: 1 [577/96 (601%)]\tLoss: 0.006175\n",
      "Train Epoche: 1 [578/96 (602%)]\tLoss: 8.093966\n",
      "Train Epoche: 1 [579/96 (603%)]\tLoss: 0.160395\n",
      "Train Epoche: 1 [580/96 (604%)]\tLoss: 1.224960\n",
      "Train Epoche: 1 [581/96 (605%)]\tLoss: 0.000150\n",
      "Train Epoche: 1 [582/96 (606%)]\tLoss: 0.129093\n",
      "Train Epoche: 1 [583/96 (607%)]\tLoss: 1.290201\n",
      "Train Epoche: 1 [584/96 (608%)]\tLoss: 16.188148\n",
      "Train Epoche: 1 [585/96 (609%)]\tLoss: 9.992435\n",
      "Train Epoche: 1 [586/96 (610%)]\tLoss: 5.051262\n",
      "Train Epoche: 1 [587/96 (611%)]\tLoss: 6.774054\n",
      "Train Epoche: 1 [588/96 (612%)]\tLoss: 3.729744\n",
      "Train Epoche: 1 [589/96 (614%)]\tLoss: 22.783466\n",
      "Train Epoche: 1 [590/96 (615%)]\tLoss: 4.760450\n",
      "Train Epoche: 1 [591/96 (616%)]\tLoss: 5.939554\n",
      "Train Epoche: 1 [592/96 (617%)]\tLoss: 86.853882\n",
      "Train Epoche: 1 [593/96 (618%)]\tLoss: 11.122746\n",
      "Train Epoche: 1 [594/96 (619%)]\tLoss: 120.743500\n",
      "Train Epoche: 1 [595/96 (620%)]\tLoss: 4.448143\n",
      "Train Epoche: 1 [596/96 (621%)]\tLoss: 3.144487\n",
      "Train Epoche: 1 [597/96 (622%)]\tLoss: 0.061647\n",
      "Train Epoche: 1 [598/96 (623%)]\tLoss: 27.421968\n",
      "Train Epoche: 1 [599/96 (624%)]\tLoss: 14.353583\n",
      "Train Epoche: 1 [600/96 (625%)]\tLoss: 2.794075\n",
      "Train Epoche: 1 [601/96 (626%)]\tLoss: 2.650571\n",
      "Train Epoche: 1 [602/96 (627%)]\tLoss: 4.292230\n",
      "Train Epoche: 1 [603/96 (628%)]\tLoss: 0.792821\n",
      "Train Epoche: 1 [604/96 (629%)]\tLoss: 2.314555\n",
      "Train Epoche: 1 [605/96 (630%)]\tLoss: 32.279133\n",
      "Train Epoche: 1 [606/96 (631%)]\tLoss: 38.372845\n",
      "Train Epoche: 1 [607/96 (632%)]\tLoss: 355.219269\n",
      "Train Epoche: 1 [608/96 (633%)]\tLoss: 36.492256\n",
      "Train Epoche: 1 [609/96 (634%)]\tLoss: 24.812469\n",
      "Train Epoche: 1 [610/96 (635%)]\tLoss: 65.500946\n",
      "Train Epoche: 1 [611/96 (636%)]\tLoss: 19.679001\n",
      "Train Epoche: 1 [612/96 (638%)]\tLoss: 144.746109\n",
      "Train Epoche: 1 [613/96 (639%)]\tLoss: 26.514963\n",
      "Train Epoche: 1 [614/96 (640%)]\tLoss: 2.741923\n",
      "Train Epoche: 1 [615/96 (641%)]\tLoss: 0.759882\n",
      "Train Epoche: 1 [616/96 (642%)]\tLoss: 1.746595\n",
      "Train Epoche: 1 [617/96 (643%)]\tLoss: 358.538544\n",
      "Train Epoche: 1 [618/96 (644%)]\tLoss: 26.435575\n",
      "Train Epoche: 1 [619/96 (645%)]\tLoss: 97.328773\n",
      "Train Epoche: 1 [620/96 (646%)]\tLoss: 0.132188\n",
      "Train Epoche: 1 [621/96 (647%)]\tLoss: 16.673382\n",
      "Train Epoche: 1 [622/96 (648%)]\tLoss: 49.823334\n",
      "Train Epoche: 1 [623/96 (649%)]\tLoss: 27.657671\n",
      "Train Epoche: 1 [624/96 (650%)]\tLoss: 75.473335\n",
      "Train Epoche: 1 [625/96 (651%)]\tLoss: 393.425751\n",
      "Train Epoche: 1 [626/96 (652%)]\tLoss: 0.271584\n",
      "Train Epoche: 1 [627/96 (653%)]\tLoss: 1.186683\n",
      "Train Epoche: 1 [628/96 (654%)]\tLoss: 0.432491\n",
      "Train Epoche: 1 [629/96 (655%)]\tLoss: 80.605713\n",
      "Train Epoche: 1 [630/96 (656%)]\tLoss: 19.065763\n",
      "Train Epoche: 1 [631/96 (657%)]\tLoss: 0.357506\n",
      "Train Epoche: 1 [632/96 (658%)]\tLoss: 1.644496\n",
      "Train Epoche: 1 [633/96 (659%)]\tLoss: 18.667923\n",
      "Train Epoche: 1 [634/96 (660%)]\tLoss: 1.365412\n",
      "Train Epoche: 1 [635/96 (661%)]\tLoss: 7.181737\n",
      "Train Epoche: 1 [636/96 (662%)]\tLoss: 10.299417\n",
      "Train Epoche: 1 [637/96 (664%)]\tLoss: 0.385148\n",
      "Train Epoche: 1 [638/96 (665%)]\tLoss: 8.528514\n",
      "Train Epoche: 1 [639/96 (666%)]\tLoss: 5.752308\n",
      "Train Epoche: 1 [640/96 (667%)]\tLoss: 8.165628\n",
      "Train Epoche: 1 [641/96 (668%)]\tLoss: 5.568991\n",
      "Train Epoche: 1 [642/96 (669%)]\tLoss: 12.979786\n",
      "Train Epoche: 1 [643/96 (670%)]\tLoss: 5.469845\n",
      "Train Epoche: 1 [644/96 (671%)]\tLoss: 2.492929\n",
      "Train Epoche: 1 [645/96 (672%)]\tLoss: 24.900841\n",
      "Train Epoche: 1 [646/96 (673%)]\tLoss: 24.566080\n",
      "Train Epoche: 1 [647/96 (674%)]\tLoss: 58.074005\n",
      "Train Epoche: 1 [648/96 (675%)]\tLoss: 109.711449\n",
      "Train Epoche: 1 [649/96 (676%)]\tLoss: 30.929747\n",
      "Train Epoche: 1 [650/96 (677%)]\tLoss: 0.841860\n",
      "Train Epoche: 1 [651/96 (678%)]\tLoss: 119.697617\n",
      "Train Epoche: 1 [652/96 (679%)]\tLoss: 2.650462\n",
      "Train Epoche: 1 [653/96 (680%)]\tLoss: 0.504645\n",
      "Train Epoche: 1 [654/96 (681%)]\tLoss: 4.049768\n",
      "Train Epoche: 1 [655/96 (682%)]\tLoss: 9.703838\n",
      "Train Epoche: 1 [656/96 (683%)]\tLoss: 8.947228\n",
      "Train Epoche: 1 [657/96 (684%)]\tLoss: 310.311798\n",
      "Train Epoche: 1 [658/96 (685%)]\tLoss: 16.339956\n",
      "Train Epoche: 1 [659/96 (686%)]\tLoss: 1.166118\n",
      "Train Epoche: 1 [660/96 (688%)]\tLoss: 129.186325\n",
      "Train Epoche: 1 [661/96 (689%)]\tLoss: 2.712345\n",
      "Train Epoche: 1 [662/96 (690%)]\tLoss: 10.053327\n",
      "Train Epoche: 1 [663/96 (691%)]\tLoss: 1.721019\n",
      "Train Epoche: 1 [664/96 (692%)]\tLoss: 29.643753\n",
      "Train Epoche: 1 [665/96 (693%)]\tLoss: 0.155247\n",
      "Train Epoche: 1 [666/96 (694%)]\tLoss: 15.492921\n",
      "Train Epoche: 1 [667/96 (695%)]\tLoss: 0.658700\n",
      "Train Epoche: 1 [668/96 (696%)]\tLoss: 3.760602\n",
      "Train Epoche: 1 [669/96 (697%)]\tLoss: 13.009041\n",
      "Train Epoche: 1 [670/96 (698%)]\tLoss: 5.674514\n",
      "Train Epoche: 1 [671/96 (699%)]\tLoss: 0.933174\n",
      "Train Epoche: 1 [672/96 (700%)]\tLoss: 0.396715\n",
      "Train Epoche: 1 [673/96 (701%)]\tLoss: 7.109450\n",
      "Train Epoche: 1 [674/96 (702%)]\tLoss: 0.127926\n",
      "Train Epoche: 1 [675/96 (703%)]\tLoss: 1.852498\n",
      "Train Epoche: 1 [676/96 (704%)]\tLoss: 11.433641\n",
      "Train Epoche: 1 [677/96 (705%)]\tLoss: 2.988081\n",
      "Train Epoche: 1 [678/96 (706%)]\tLoss: 7.478476\n",
      "Train Epoche: 1 [679/96 (707%)]\tLoss: 2.634303\n",
      "Train Epoche: 1 [680/96 (708%)]\tLoss: 8.048644\n",
      "Train Epoche: 1 [681/96 (709%)]\tLoss: 8.650092\n",
      "Train Epoche: 1 [682/96 (710%)]\tLoss: 8.796667\n",
      "Train Epoche: 1 [683/96 (711%)]\tLoss: 3.834235\n",
      "Train Epoche: 1 [684/96 (712%)]\tLoss: 2.301347\n",
      "Train Epoche: 1 [685/96 (714%)]\tLoss: 28.038095\n",
      "Train Epoche: 1 [686/96 (715%)]\tLoss: 2.002767\n",
      "Train Epoche: 1 [687/96 (716%)]\tLoss: 2.893921\n",
      "Train Epoche: 1 [688/96 (717%)]\tLoss: 1.200214\n",
      "Train Epoche: 1 [689/96 (718%)]\tLoss: 1.447556\n",
      "Train Epoche: 1 [690/96 (719%)]\tLoss: 0.846263\n",
      "Train Epoche: 1 [691/96 (720%)]\tLoss: 13.184033\n",
      "Train Epoche: 1 [692/96 (721%)]\tLoss: 6.185536\n",
      "Train Epoche: 1 [693/96 (722%)]\tLoss: 13.829286\n",
      "Train Epoche: 1 [694/96 (723%)]\tLoss: 0.758362\n",
      "Train Epoche: 1 [695/96 (724%)]\tLoss: 8.059933\n",
      "Train Epoche: 1 [696/96 (725%)]\tLoss: 8.761267\n",
      "Train Epoche: 1 [697/96 (726%)]\tLoss: 17.740767\n",
      "Train Epoche: 1 [698/96 (727%)]\tLoss: 12.983449\n",
      "Train Epoche: 1 [699/96 (728%)]\tLoss: 3.992381\n",
      "Train Epoche: 1 [700/96 (729%)]\tLoss: 9.952835\n",
      "Train Epoche: 1 [701/96 (730%)]\tLoss: 3.831461\n",
      "Train Epoche: 1 [702/96 (731%)]\tLoss: 4.598016\n",
      "Train Epoche: 1 [703/96 (732%)]\tLoss: 5.036133\n",
      "Train Epoche: 1 [704/96 (733%)]\tLoss: 1.309384\n",
      "Train Epoche: 1 [705/96 (734%)]\tLoss: 3.155101\n",
      "Train Epoche: 1 [706/96 (735%)]\tLoss: 27.718943\n",
      "Train Epoche: 1 [707/96 (736%)]\tLoss: 21.724300\n",
      "Train Epoche: 1 [708/96 (738%)]\tLoss: 0.047456\n",
      "Train Epoche: 1 [709/96 (739%)]\tLoss: 2.000997\n",
      "Train Epoche: 1 [710/96 (740%)]\tLoss: 3.566975\n",
      "Train Epoche: 1 [711/96 (741%)]\tLoss: 9.497623\n",
      "Train Epoche: 1 [712/96 (742%)]\tLoss: 0.642426\n",
      "Train Epoche: 1 [713/96 (743%)]\tLoss: 0.199009\n",
      "Train Epoche: 1 [714/96 (744%)]\tLoss: 9.204805\n",
      "Train Epoche: 1 [715/96 (745%)]\tLoss: 5.711997\n",
      "Train Epoche: 1 [716/96 (746%)]\tLoss: 1.757814\n",
      "Train Epoche: 1 [717/96 (747%)]\tLoss: 0.802399\n",
      "Train Epoche: 1 [718/96 (748%)]\tLoss: 4.713247\n",
      "Train Epoche: 1 [719/96 (749%)]\tLoss: 1.004849\n",
      "Train Epoche: 1 [720/96 (750%)]\tLoss: 1.388850\n",
      "Train Epoche: 1 [721/96 (751%)]\tLoss: 0.013858\n",
      "Train Epoche: 1 [722/96 (752%)]\tLoss: 4.473525\n",
      "Train Epoche: 1 [723/96 (753%)]\tLoss: 0.085898\n",
      "Train Epoche: 1 [724/96 (754%)]\tLoss: 7.914615\n",
      "Train Epoche: 1 [725/96 (755%)]\tLoss: 13.950051\n",
      "Train Epoche: 1 [726/96 (756%)]\tLoss: 0.098926\n",
      "Train Epoche: 1 [727/96 (757%)]\tLoss: 0.613186\n",
      "Train Epoche: 1 [728/96 (758%)]\tLoss: 13.896524\n",
      "Train Epoche: 1 [729/96 (759%)]\tLoss: 7.922102\n",
      "Train Epoche: 1 [730/96 (760%)]\tLoss: 25.506624\n",
      "Train Epoche: 1 [731/96 (761%)]\tLoss: 231.108917\n",
      "Train Epoche: 1 [732/96 (762%)]\tLoss: 61.466232\n",
      "Train Epoche: 1 [733/96 (764%)]\tLoss: 0.207304\n",
      "Train Epoche: 1 [734/96 (765%)]\tLoss: 7.207256\n",
      "Train Epoche: 1 [735/96 (766%)]\tLoss: 0.443189\n",
      "Train Epoche: 1 [736/96 (767%)]\tLoss: 3.087458\n",
      "Train Epoche: 1 [737/96 (768%)]\tLoss: 8.568291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [738/96 (769%)]\tLoss: 2.418453\n",
      "Train Epoche: 1 [739/96 (770%)]\tLoss: 0.171357\n",
      "Train Epoche: 1 [740/96 (771%)]\tLoss: 7.842426\n",
      "Train Epoche: 1 [741/96 (772%)]\tLoss: 13.485860\n",
      "Train Epoche: 1 [742/96 (773%)]\tLoss: 9.475664\n",
      "Train Epoche: 1 [743/96 (774%)]\tLoss: 1.098639\n",
      "Train Epoche: 1 [744/96 (775%)]\tLoss: 8.590561\n",
      "Train Epoche: 1 [745/96 (776%)]\tLoss: 7.064062\n",
      "Train Epoche: 1 [746/96 (777%)]\tLoss: 69.497810\n",
      "Train Epoche: 1 [747/96 (778%)]\tLoss: 59.244270\n",
      "Train Epoche: 1 [748/96 (779%)]\tLoss: 79.259331\n",
      "Train Epoche: 1 [749/96 (780%)]\tLoss: 246.701248\n",
      "Train Epoche: 1 [750/96 (781%)]\tLoss: 27.238596\n",
      "Train Epoche: 1 [751/96 (782%)]\tLoss: 6.118718\n",
      "Train Epoche: 1 [752/96 (783%)]\tLoss: 24.094858\n",
      "Train Epoche: 1 [753/96 (784%)]\tLoss: 2.645104\n",
      "Train Epoche: 1 [754/96 (785%)]\tLoss: 30.606649\n",
      "Train Epoche: 1 [755/96 (786%)]\tLoss: 4.075379\n",
      "Train Epoche: 1 [756/96 (788%)]\tLoss: 25.089884\n",
      "Train Epoche: 1 [757/96 (789%)]\tLoss: 10.389211\n",
      "Train Epoche: 1 [758/96 (790%)]\tLoss: 12.182648\n",
      "Train Epoche: 1 [759/96 (791%)]\tLoss: 0.001158\n",
      "Train Epoche: 1 [760/96 (792%)]\tLoss: 29.647959\n",
      "Train Epoche: 1 [761/96 (793%)]\tLoss: 81.832382\n",
      "Train Epoche: 1 [762/96 (794%)]\tLoss: 0.027211\n",
      "Train Epoche: 1 [763/96 (795%)]\tLoss: 49.946102\n",
      "Train Epoche: 1 [764/96 (796%)]\tLoss: 34.466759\n",
      "Train Epoche: 1 [765/96 (797%)]\tLoss: 11.676565\n",
      "Train Epoche: 1 [766/96 (798%)]\tLoss: 109.339653\n",
      "Train Epoche: 1 [767/96 (799%)]\tLoss: 76.599953\n",
      "Train Epoche: 1 [768/96 (800%)]\tLoss: 169.088455\n",
      "Train Epoche: 1 [769/96 (801%)]\tLoss: 100.619545\n",
      "Train Epoche: 1 [770/96 (802%)]\tLoss: 0.137509\n",
      "Train Epoche: 1 [771/96 (803%)]\tLoss: 33.087444\n",
      "Train Epoche: 1 [772/96 (804%)]\tLoss: 95.626045\n",
      "Train Epoche: 1 [773/96 (805%)]\tLoss: 5.841473\n",
      "Train Epoche: 1 [774/96 (806%)]\tLoss: 43.441578\n",
      "Train Epoche: 1 [775/96 (807%)]\tLoss: 141.021088\n",
      "Train Epoche: 1 [776/96 (808%)]\tLoss: 81.697098\n",
      "Train Epoche: 1 [777/96 (809%)]\tLoss: 50.522144\n",
      "Train Epoche: 1 [778/96 (810%)]\tLoss: 19.899912\n",
      "Train Epoche: 1 [779/96 (811%)]\tLoss: 13.208644\n",
      "Train Epoche: 1 [780/96 (812%)]\tLoss: 0.487185\n",
      "Train Epoche: 1 [781/96 (814%)]\tLoss: 0.531245\n",
      "Train Epoche: 1 [782/96 (815%)]\tLoss: 211.927383\n",
      "Train Epoche: 1 [783/96 (816%)]\tLoss: 12.303040\n",
      "Train Epoche: 1 [784/96 (817%)]\tLoss: 11.078803\n",
      "Train Epoche: 1 [785/96 (818%)]\tLoss: 3.166075\n",
      "Train Epoche: 1 [786/96 (819%)]\tLoss: 10.838022\n",
      "Train Epoche: 1 [787/96 (820%)]\tLoss: 1.631523\n",
      "Train Epoche: 1 [788/96 (821%)]\tLoss: 0.426960\n",
      "Train Epoche: 1 [789/96 (822%)]\tLoss: 2.504849\n",
      "Train Epoche: 1 [790/96 (823%)]\tLoss: 22.356140\n",
      "Train Epoche: 1 [791/96 (824%)]\tLoss: 31.018135\n",
      "Train Epoche: 1 [792/96 (825%)]\tLoss: 84.461929\n",
      "Train Epoche: 1 [793/96 (826%)]\tLoss: 83.624512\n",
      "Train Epoche: 1 [794/96 (827%)]\tLoss: 149.100266\n",
      "Train Epoche: 1 [795/96 (828%)]\tLoss: 313.415955\n",
      "Train Epoche: 1 [796/96 (829%)]\tLoss: 131.042358\n",
      "Train Epoche: 1 [797/96 (830%)]\tLoss: 0.309928\n",
      "Train Epoche: 1 [798/96 (831%)]\tLoss: 0.115220\n",
      "Train Epoche: 1 [799/96 (832%)]\tLoss: 1.325541\n",
      "Train Epoche: 1 [800/96 (833%)]\tLoss: 17.441769\n",
      "Train Epoche: 1 [801/96 (834%)]\tLoss: 69.978218\n",
      "Train Epoche: 1 [802/96 (835%)]\tLoss: 9.869954\n",
      "Train Epoche: 1 [803/96 (836%)]\tLoss: 32.679237\n",
      "Train Epoche: 1 [804/96 (838%)]\tLoss: 2.840400\n",
      "Train Epoche: 1 [805/96 (839%)]\tLoss: 81.409462\n",
      "Train Epoche: 1 [806/96 (840%)]\tLoss: 1.597067\n",
      "Train Epoche: 1 [807/96 (841%)]\tLoss: 31.679728\n",
      "Train Epoche: 1 [808/96 (842%)]\tLoss: 7.118937\n",
      "Train Epoche: 1 [809/96 (843%)]\tLoss: 356.831787\n",
      "Train Epoche: 1 [810/96 (844%)]\tLoss: 54.828331\n",
      "Train Epoche: 1 [811/96 (845%)]\tLoss: 2.310915\n",
      "Train Epoche: 1 [812/96 (846%)]\tLoss: 43.496582\n",
      "Train Epoche: 1 [813/96 (847%)]\tLoss: 44.305828\n",
      "Train Epoche: 1 [814/96 (848%)]\tLoss: 42.921196\n",
      "Train Epoche: 1 [815/96 (849%)]\tLoss: 1.165860\n",
      "Train Epoche: 1 [816/96 (850%)]\tLoss: 2.877765\n",
      "Train Epoche: 1 [817/96 (851%)]\tLoss: 76.473808\n",
      "Train Epoche: 1 [818/96 (852%)]\tLoss: 11.001954\n",
      "Train Epoche: 1 [819/96 (853%)]\tLoss: 5.310560\n",
      "Train Epoche: 1 [820/96 (854%)]\tLoss: 16.403343\n",
      "Train Epoche: 1 [821/96 (855%)]\tLoss: 0.757725\n",
      "Train Epoche: 1 [822/96 (856%)]\tLoss: 14.112354\n",
      "Train Epoche: 1 [823/96 (857%)]\tLoss: 4.521685\n",
      "Train Epoche: 1 [824/96 (858%)]\tLoss: 5.372288\n",
      "Train Epoche: 1 [825/96 (859%)]\tLoss: 7.996956\n",
      "Train Epoche: 1 [826/96 (860%)]\tLoss: 8.355688\n",
      "Train Epoche: 1 [827/96 (861%)]\tLoss: 11.469519\n",
      "Train Epoche: 1 [828/96 (862%)]\tLoss: 121.847366\n",
      "Train Epoche: 1 [829/96 (864%)]\tLoss: 343.093506\n",
      "Train Epoche: 1 [830/96 (865%)]\tLoss: 6.105340\n",
      "Train Epoche: 1 [831/96 (866%)]\tLoss: 3.955986\n",
      "Train Epoche: 1 [832/96 (867%)]\tLoss: 0.341605\n",
      "Train Epoche: 1 [833/96 (868%)]\tLoss: 11.012604\n",
      "Train Epoche: 1 [834/96 (869%)]\tLoss: 18.696880\n",
      "Train Epoche: 1 [835/96 (870%)]\tLoss: 98.135284\n",
      "Train Epoche: 1 [836/96 (871%)]\tLoss: 14.287991\n",
      "Train Epoche: 1 [837/96 (872%)]\tLoss: 123.381729\n",
      "Train Epoche: 1 [838/96 (873%)]\tLoss: 7.744319\n",
      "Train Epoche: 1 [839/96 (874%)]\tLoss: 4.451016\n",
      "Train Epoche: 1 [840/96 (875%)]\tLoss: 0.107202\n",
      "Train Epoche: 1 [841/96 (876%)]\tLoss: 1.377555\n",
      "Train Epoche: 1 [842/96 (877%)]\tLoss: 0.022374\n",
      "Train Epoche: 1 [843/96 (878%)]\tLoss: 2.367222\n",
      "Train Epoche: 1 [844/96 (879%)]\tLoss: 13.264888\n",
      "Train Epoche: 1 [845/96 (880%)]\tLoss: 6.698024\n",
      "Train Epoche: 1 [846/96 (881%)]\tLoss: 7.282595\n",
      "Train Epoche: 1 [847/96 (882%)]\tLoss: 0.748903\n",
      "Train Epoche: 1 [848/96 (883%)]\tLoss: 65.257133\n",
      "Train Epoche: 1 [849/96 (884%)]\tLoss: 10.094380\n",
      "Train Epoche: 1 [850/96 (885%)]\tLoss: 0.934742\n",
      "Train Epoche: 1 [851/96 (886%)]\tLoss: 81.041412\n",
      "Train Epoche: 1 [852/96 (888%)]\tLoss: 27.495480\n",
      "Train Epoche: 1 [853/96 (889%)]\tLoss: 57.159737\n",
      "Train Epoche: 1 [854/96 (890%)]\tLoss: 120.573860\n",
      "Train Epoche: 1 [855/96 (891%)]\tLoss: 8.096604\n",
      "Train Epoche: 1 [856/96 (892%)]\tLoss: 39.920582\n",
      "Train Epoche: 1 [857/96 (893%)]\tLoss: 1.924296\n",
      "Train Epoche: 1 [858/96 (894%)]\tLoss: 170.927216\n",
      "Train Epoche: 1 [859/96 (895%)]\tLoss: 8.398220\n",
      "Train Epoche: 1 [860/96 (896%)]\tLoss: 0.248174\n",
      "Train Epoche: 1 [861/96 (897%)]\tLoss: 0.312382\n",
      "Train Epoche: 1 [862/96 (898%)]\tLoss: 0.250212\n",
      "Train Epoche: 1 [863/96 (899%)]\tLoss: 72.345909\n",
      "Train Epoche: 1 [864/96 (900%)]\tLoss: 31.469893\n",
      "Train Epoche: 1 [865/96 (901%)]\tLoss: 86.663872\n",
      "Train Epoche: 1 [866/96 (902%)]\tLoss: 19.612974\n",
      "Train Epoche: 1 [867/96 (903%)]\tLoss: 0.316288\n",
      "Train Epoche: 1 [868/96 (904%)]\tLoss: 19.016800\n",
      "Train Epoche: 1 [869/96 (905%)]\tLoss: 2.198400\n",
      "Train Epoche: 1 [870/96 (906%)]\tLoss: 0.002345\n",
      "Train Epoche: 1 [871/96 (907%)]\tLoss: 5.577474\n",
      "Train Epoche: 1 [872/96 (908%)]\tLoss: 1.248852\n",
      "Train Epoche: 1 [873/96 (909%)]\tLoss: 2.649528\n",
      "Train Epoche: 1 [874/96 (910%)]\tLoss: 3.152506\n",
      "Train Epoche: 1 [875/96 (911%)]\tLoss: 0.415140\n",
      "Train Epoche: 1 [876/96 (912%)]\tLoss: 26.157526\n",
      "Train Epoche: 1 [877/96 (914%)]\tLoss: 3.422070\n",
      "Train Epoche: 1 [878/96 (915%)]\tLoss: 0.011641\n",
      "Train Epoche: 1 [879/96 (916%)]\tLoss: 18.858257\n",
      "Train Epoche: 1 [880/96 (917%)]\tLoss: 1.460956\n",
      "Train Epoche: 1 [881/96 (918%)]\tLoss: 1.698486\n",
      "Train Epoche: 1 [882/96 (919%)]\tLoss: 12.943968\n",
      "Train Epoche: 1 [883/96 (920%)]\tLoss: 47.187752\n",
      "Train Epoche: 1 [884/96 (921%)]\tLoss: 1.698964\n",
      "Train Epoche: 1 [885/96 (922%)]\tLoss: 84.518906\n",
      "Train Epoche: 1 [886/96 (923%)]\tLoss: 0.328742\n",
      "Train Epoche: 1 [887/96 (924%)]\tLoss: 15.517100\n",
      "Train Epoche: 1 [888/96 (925%)]\tLoss: 12.566103\n",
      "Train Epoche: 1 [889/96 (926%)]\tLoss: 7.782833\n",
      "Train Epoche: 1 [890/96 (927%)]\tLoss: 5.603238\n",
      "Train Epoche: 1 [891/96 (928%)]\tLoss: 0.085499\n",
      "Train Epoche: 1 [892/96 (929%)]\tLoss: 103.409294\n",
      "Train Epoche: 1 [893/96 (930%)]\tLoss: 0.158531\n",
      "Train Epoche: 1 [894/96 (931%)]\tLoss: 2.019120\n",
      "Train Epoche: 1 [895/96 (932%)]\tLoss: 1.551336\n",
      "Train Epoche: 1 [896/96 (933%)]\tLoss: 0.720071\n",
      "Train Epoche: 1 [897/96 (934%)]\tLoss: 5.374558\n",
      "Train Epoche: 1 [898/96 (935%)]\tLoss: 2.445127\n",
      "Train Epoche: 1 [899/96 (936%)]\tLoss: 0.946741\n",
      "Train Epoche: 1 [900/96 (938%)]\tLoss: 97.123573\n",
      "Train Epoche: 1 [901/96 (939%)]\tLoss: 10.782141\n",
      "Train Epoche: 1 [902/96 (940%)]\tLoss: 130.064514\n",
      "Train Epoche: 1 [903/96 (941%)]\tLoss: 11.694988\n",
      "Train Epoche: 1 [904/96 (942%)]\tLoss: 18.050035\n",
      "Train Epoche: 1 [905/96 (943%)]\tLoss: 71.536430\n",
      "Train Epoche: 1 [906/96 (944%)]\tLoss: 12.040445\n",
      "Train Epoche: 1 [907/96 (945%)]\tLoss: 10.330641\n",
      "Train Epoche: 1 [908/96 (946%)]\tLoss: 2.917271\n",
      "Train Epoche: 1 [909/96 (947%)]\tLoss: 48.904690\n",
      "Train Epoche: 1 [910/96 (948%)]\tLoss: 2.502330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [911/96 (949%)]\tLoss: 3.337016\n",
      "Train Epoche: 1 [912/96 (950%)]\tLoss: 0.000334\n",
      "Train Epoche: 1 [913/96 (951%)]\tLoss: 0.161676\n",
      "Train Epoche: 1 [914/96 (952%)]\tLoss: 81.311249\n",
      "Train Epoche: 1 [915/96 (953%)]\tLoss: 0.026280\n",
      "Train Epoche: 1 [916/96 (954%)]\tLoss: 0.099436\n",
      "Train Epoche: 1 [917/96 (955%)]\tLoss: 13.746104\n",
      "Train Epoche: 1 [918/96 (956%)]\tLoss: 7.960202\n",
      "Train Epoche: 1 [919/96 (957%)]\tLoss: 111.069748\n",
      "Train Epoche: 1 [920/96 (958%)]\tLoss: 26.664724\n",
      "Train Epoche: 1 [921/96 (959%)]\tLoss: 13.472723\n",
      "Train Epoche: 1 [922/96 (960%)]\tLoss: 64.983437\n",
      "Train Epoche: 1 [923/96 (961%)]\tLoss: 1.306906\n",
      "Train Epoche: 1 [924/96 (962%)]\tLoss: 1.032250\n",
      "Train Epoche: 1 [925/96 (964%)]\tLoss: 0.268000\n",
      "Train Epoche: 1 [926/96 (965%)]\tLoss: 31.512995\n",
      "Train Epoche: 1 [927/96 (966%)]\tLoss: 4.033371\n",
      "Train Epoche: 1 [928/96 (967%)]\tLoss: 45.631058\n",
      "Train Epoche: 1 [929/96 (968%)]\tLoss: 6.721322\n",
      "Train Epoche: 1 [930/96 (969%)]\tLoss: 1.917965\n",
      "Train Epoche: 1 [931/96 (970%)]\tLoss: 2.429995\n",
      "Train Epoche: 1 [932/96 (971%)]\tLoss: 179.008911\n",
      "Train Epoche: 1 [933/96 (972%)]\tLoss: 370.231476\n",
      "Train Epoche: 1 [934/96 (973%)]\tLoss: 0.910677\n",
      "Train Epoche: 1 [935/96 (974%)]\tLoss: 38.340160\n",
      "Train Epoche: 1 [936/96 (975%)]\tLoss: 0.112614\n",
      "Train Epoche: 1 [937/96 (976%)]\tLoss: 1.352405\n",
      "Train Epoche: 1 [938/96 (977%)]\tLoss: 1.690178\n",
      "Train Epoche: 1 [939/96 (978%)]\tLoss: 19.063324\n",
      "Train Epoche: 1 [940/96 (979%)]\tLoss: 1.930032\n",
      "Train Epoche: 1 [941/96 (980%)]\tLoss: 30.180887\n",
      "Train Epoche: 1 [942/96 (981%)]\tLoss: 10.659985\n",
      "Train Epoche: 1 [943/96 (982%)]\tLoss: 9.602838\n",
      "Train Epoche: 1 [944/96 (983%)]\tLoss: 28.161448\n",
      "Train Epoche: 1 [945/96 (984%)]\tLoss: 17.636261\n",
      "Train Epoche: 1 [946/96 (985%)]\tLoss: 4.913427\n",
      "Train Epoche: 1 [947/96 (986%)]\tLoss: 24.524515\n",
      "Train Epoche: 1 [948/96 (988%)]\tLoss: 7.857325\n",
      "Train Epoche: 1 [949/96 (989%)]\tLoss: 10.355870\n",
      "Train Epoche: 1 [950/96 (990%)]\tLoss: 22.369305\n",
      "Train Epoche: 1 [951/96 (991%)]\tLoss: 1.718706\n",
      "Train Epoche: 1 [952/96 (992%)]\tLoss: 0.689447\n",
      "Train Epoche: 1 [953/96 (993%)]\tLoss: 83.407913\n",
      "Train Epoche: 1 [954/96 (994%)]\tLoss: 189.017365\n",
      "Train Epoche: 1 [955/96 (995%)]\tLoss: 28.696655\n",
      "Train Epoche: 1 [956/96 (996%)]\tLoss: 120.825882\n",
      "Train Epoche: 1 [957/96 (997%)]\tLoss: 12.262119\n",
      "Train Epoche: 1 [958/96 (998%)]\tLoss: 25.509697\n",
      "Train Epoche: 1 [959/96 (999%)]\tLoss: 206.165527\n",
      "Train Epoche: 1 [960/96 (1000%)]\tLoss: 1.092446\n",
      "Train Epoche: 1 [961/96 (1001%)]\tLoss: 31.021826\n",
      "Train Epoche: 1 [962/96 (1002%)]\tLoss: 10.365454\n",
      "Train Epoche: 1 [963/96 (1003%)]\tLoss: 72.476891\n",
      "Train Epoche: 1 [964/96 (1004%)]\tLoss: 82.749443\n",
      "Train Epoche: 1 [965/96 (1005%)]\tLoss: 9.414952\n",
      "Train Epoche: 1 [966/96 (1006%)]\tLoss: 5.220235\n",
      "Train Epoche: 1 [967/96 (1007%)]\tLoss: 5.287505\n",
      "Train Epoche: 1 [968/96 (1008%)]\tLoss: 6.458195\n",
      "Train Epoche: 1 [969/96 (1009%)]\tLoss: 87.024361\n",
      "Train Epoche: 1 [970/96 (1010%)]\tLoss: 0.898100\n",
      "Train Epoche: 1 [971/96 (1011%)]\tLoss: 1.918027\n",
      "Train Epoche: 1 [972/96 (1012%)]\tLoss: 0.050883\n",
      "Train Epoche: 1 [973/96 (1014%)]\tLoss: 8.840133\n",
      "Train Epoche: 1 [974/96 (1015%)]\tLoss: 0.127649\n",
      "Train Epoche: 1 [975/96 (1016%)]\tLoss: 3.289989\n",
      "Train Epoche: 1 [976/96 (1017%)]\tLoss: 0.791754\n",
      "Train Epoche: 1 [977/96 (1018%)]\tLoss: 54.905434\n",
      "Train Epoche: 1 [978/96 (1019%)]\tLoss: 4.002915\n",
      "Train Epoche: 1 [979/96 (1020%)]\tLoss: 28.481899\n",
      "Train Epoche: 1 [980/96 (1021%)]\tLoss: 0.479638\n",
      "Train Epoche: 1 [981/96 (1022%)]\tLoss: 183.194031\n",
      "Train Epoche: 1 [982/96 (1023%)]\tLoss: 1.559017\n",
      "Train Epoche: 1 [983/96 (1024%)]\tLoss: 3.083456\n",
      "Train Epoche: 1 [984/96 (1025%)]\tLoss: 0.017648\n",
      "Train Epoche: 1 [985/96 (1026%)]\tLoss: 0.483005\n",
      "Train Epoche: 1 [986/96 (1027%)]\tLoss: 7.276312\n",
      "Train Epoche: 1 [987/96 (1028%)]\tLoss: 13.764150\n",
      "Train Epoche: 1 [988/96 (1029%)]\tLoss: 15.327770\n",
      "Train Epoche: 1 [989/96 (1030%)]\tLoss: 9.621766\n",
      "Train Epoche: 1 [990/96 (1031%)]\tLoss: 2.506614\n",
      "Train Epoche: 1 [991/96 (1032%)]\tLoss: 4.741784\n",
      "Train Epoche: 1 [992/96 (1033%)]\tLoss: 1.850245\n",
      "Train Epoche: 1 [993/96 (1034%)]\tLoss: 5.535279\n",
      "Train Epoche: 1 [994/96 (1035%)]\tLoss: 143.037247\n",
      "Train Epoche: 1 [995/96 (1036%)]\tLoss: 0.056758\n",
      "Train Epoche: 1 [996/96 (1038%)]\tLoss: 13.952523\n",
      "Train Epoche: 1 [997/96 (1039%)]\tLoss: 1.175325\n",
      "Train Epoche: 1 [998/96 (1040%)]\tLoss: 8.638594\n",
      "Train Epoche: 1 [999/96 (1041%)]\tLoss: 6.229420\n",
      "Train Epoche: 1 [1000/96 (1042%)]\tLoss: 1.204297\n",
      "Train Epoche: 1 [1001/96 (1043%)]\tLoss: 36.082443\n",
      "Train Epoche: 1 [1002/96 (1044%)]\tLoss: 4.497028\n",
      "Train Epoche: 1 [1003/96 (1045%)]\tLoss: 37.410557\n",
      "Train Epoche: 1 [1004/96 (1046%)]\tLoss: 0.072751\n",
      "Train Epoche: 1 [1005/96 (1047%)]\tLoss: 15.423547\n",
      "Train Epoche: 1 [1006/96 (1048%)]\tLoss: 20.046665\n",
      "Train Epoche: 1 [1007/96 (1049%)]\tLoss: 45.952400\n",
      "Train Epoche: 1 [1008/96 (1050%)]\tLoss: 1.457905\n",
      "Train Epoche: 1 [1009/96 (1051%)]\tLoss: 3.174054\n",
      "Train Epoche: 1 [1010/96 (1052%)]\tLoss: 4.952172\n",
      "Train Epoche: 1 [1011/96 (1053%)]\tLoss: 0.096042\n",
      "Train Epoche: 1 [1012/96 (1054%)]\tLoss: 276.408569\n",
      "Train Epoche: 1 [1013/96 (1055%)]\tLoss: 146.349777\n",
      "Train Epoche: 1 [1014/96 (1056%)]\tLoss: 7.427805\n",
      "Train Epoche: 1 [1015/96 (1057%)]\tLoss: 111.808334\n",
      "Train Epoche: 1 [1016/96 (1058%)]\tLoss: 10.826867\n",
      "Train Epoche: 1 [1017/96 (1059%)]\tLoss: 19.774134\n",
      "Train Epoche: 1 [1018/96 (1060%)]\tLoss: 204.457687\n",
      "Train Epoche: 1 [1019/96 (1061%)]\tLoss: 5.330019\n",
      "Train Epoche: 1 [1020/96 (1062%)]\tLoss: 0.006019\n",
      "Train Epoche: 1 [1021/96 (1064%)]\tLoss: 110.795090\n",
      "Train Epoche: 1 [1022/96 (1065%)]\tLoss: 114.791061\n",
      "Train Epoche: 1 [1023/96 (1066%)]\tLoss: 45.853298\n",
      "Train Epoche: 1 [1024/96 (1067%)]\tLoss: 112.659111\n",
      "Train Epoche: 1 [1025/96 (1068%)]\tLoss: 17.462534\n",
      "Train Epoche: 1 [1026/96 (1069%)]\tLoss: 57.028572\n",
      "Train Epoche: 1 [1027/96 (1070%)]\tLoss: 25.772106\n",
      "Train Epoche: 1 [1028/96 (1071%)]\tLoss: 35.971840\n",
      "Train Epoche: 1 [1029/96 (1072%)]\tLoss: 17.346855\n",
      "Train Epoche: 1 [1030/96 (1073%)]\tLoss: 17.312986\n",
      "Train Epoche: 1 [1031/96 (1074%)]\tLoss: 11.800675\n",
      "Train Epoche: 1 [1032/96 (1075%)]\tLoss: 1.108504\n",
      "Train Epoche: 1 [1033/96 (1076%)]\tLoss: 372.030273\n",
      "Train Epoche: 1 [1034/96 (1077%)]\tLoss: 236.700104\n",
      "Train Epoche: 1 [1035/96 (1078%)]\tLoss: 80.732468\n",
      "Train Epoche: 1 [1036/96 (1079%)]\tLoss: 0.159935\n",
      "Train Epoche: 1 [1037/96 (1080%)]\tLoss: 0.792406\n",
      "Train Epoche: 1 [1038/96 (1081%)]\tLoss: 47.468079\n",
      "Train Epoche: 1 [1039/96 (1082%)]\tLoss: 209.481628\n",
      "Train Epoche: 1 [1040/96 (1083%)]\tLoss: 235.745209\n",
      "Train Epoche: 1 [1041/96 (1084%)]\tLoss: 0.963913\n",
      "Train Epoche: 1 [1042/96 (1085%)]\tLoss: 0.068139\n",
      "Train Epoche: 1 [1043/96 (1086%)]\tLoss: 79.048759\n",
      "Train Epoche: 1 [1044/96 (1088%)]\tLoss: 284.087402\n",
      "Train Epoche: 1 [1045/96 (1089%)]\tLoss: 3.032634\n",
      "Train Epoche: 1 [1046/96 (1090%)]\tLoss: 0.579112\n",
      "Train Epoche: 1 [1047/96 (1091%)]\tLoss: 30.861742\n",
      "Train Epoche: 1 [1048/96 (1092%)]\tLoss: 0.504676\n",
      "Train Epoche: 1 [1049/96 (1093%)]\tLoss: 224.787476\n",
      "Train Epoche: 1 [1050/96 (1094%)]\tLoss: 40.773537\n",
      "Train Epoche: 1 [1051/96 (1095%)]\tLoss: 32.289341\n",
      "Train Epoche: 1 [1052/96 (1096%)]\tLoss: 5.728022\n",
      "Train Epoche: 1 [1053/96 (1097%)]\tLoss: 5.188866\n",
      "Train Epoche: 1 [1054/96 (1098%)]\tLoss: 0.054245\n",
      "Train Epoche: 1 [1055/96 (1099%)]\tLoss: 0.385862\n",
      "Train Epoche: 1 [1056/96 (1100%)]\tLoss: 6.000182\n",
      "Train Epoche: 1 [1057/96 (1101%)]\tLoss: 0.756317\n",
      "Train Epoche: 1 [1058/96 (1102%)]\tLoss: 1.060519\n",
      "Train Epoche: 1 [1059/96 (1103%)]\tLoss: 0.827458\n",
      "Train Epoche: 1 [1060/96 (1104%)]\tLoss: 0.094054\n",
      "Train Epoche: 1 [1061/96 (1105%)]\tLoss: 0.213914\n",
      "Train Epoche: 1 [1062/96 (1106%)]\tLoss: 0.801080\n",
      "Train Epoche: 1 [1063/96 (1107%)]\tLoss: 0.330558\n",
      "Train Epoche: 1 [1064/96 (1108%)]\tLoss: 0.518712\n",
      "Train Epoche: 1 [1065/96 (1109%)]\tLoss: 0.503572\n",
      "Train Epoche: 1 [1066/96 (1110%)]\tLoss: 10.215374\n",
      "Train Epoche: 1 [1067/96 (1111%)]\tLoss: 42.496555\n",
      "Train Epoche: 1 [1068/96 (1112%)]\tLoss: 100.483635\n",
      "Train Epoche: 1 [1069/96 (1114%)]\tLoss: 0.446547\n",
      "Train Epoche: 1 [1070/96 (1115%)]\tLoss: 93.426773\n",
      "Train Epoche: 1 [1071/96 (1116%)]\tLoss: 22.717344\n",
      "Train Epoche: 1 [1072/96 (1117%)]\tLoss: 8.646059\n",
      "Train Epoche: 1 [1073/96 (1118%)]\tLoss: 17.331125\n",
      "Train Epoche: 1 [1074/96 (1119%)]\tLoss: 7.812068\n",
      "Train Epoche: 1 [1075/96 (1120%)]\tLoss: 0.028550\n",
      "Train Epoche: 1 [1076/96 (1121%)]\tLoss: 0.000610\n",
      "Train Epoche: 1 [1077/96 (1122%)]\tLoss: 1.638885\n",
      "Train Epoche: 1 [1078/96 (1123%)]\tLoss: 7.562594\n",
      "Train Epoche: 1 [1079/96 (1124%)]\tLoss: 18.615641\n",
      "Train Epoche: 1 [1080/96 (1125%)]\tLoss: 0.045015\n",
      "Train Epoche: 1 [1081/96 (1126%)]\tLoss: 0.777017\n",
      "Train Epoche: 1 [1082/96 (1127%)]\tLoss: 0.474672\n",
      "Train Epoche: 1 [1083/96 (1128%)]\tLoss: 1.351192\n",
      "Train Epoche: 1 [1084/96 (1129%)]\tLoss: 68.054771\n",
      "Train Epoche: 1 [1085/96 (1130%)]\tLoss: 20.704584\n",
      "Train Epoche: 1 [1086/96 (1131%)]\tLoss: 1.988998\n",
      "Train Epoche: 1 [1087/96 (1132%)]\tLoss: 0.013114\n",
      "Train Epoche: 1 [1088/96 (1133%)]\tLoss: 5.912503\n",
      "Train Epoche: 1 [1089/96 (1134%)]\tLoss: 0.002567\n",
      "Train Epoche: 1 [1090/96 (1135%)]\tLoss: 11.751092\n",
      "Train Epoche: 1 [1091/96 (1136%)]\tLoss: 38.486416\n",
      "Train Epoche: 1 [1092/96 (1138%)]\tLoss: 30.807531\n",
      "Train Epoche: 1 [1093/96 (1139%)]\tLoss: 0.170955\n",
      "Train Epoche: 1 [1094/96 (1140%)]\tLoss: 83.844353\n",
      "Train Epoche: 1 [1095/96 (1141%)]\tLoss: 11.216012\n",
      "Train Epoche: 1 [1096/96 (1142%)]\tLoss: 12.770070\n",
      "Train Epoche: 1 [1097/96 (1143%)]\tLoss: 13.362829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1098/96 (1144%)]\tLoss: 4.152620\n",
      "Train Epoche: 1 [1099/96 (1145%)]\tLoss: 6.066761\n",
      "Train Epoche: 1 [1100/96 (1146%)]\tLoss: 77.557404\n",
      "Train Epoche: 1 [1101/96 (1147%)]\tLoss: 2.426593\n",
      "Train Epoche: 1 [1102/96 (1148%)]\tLoss: 8.636920\n",
      "Train Epoche: 1 [1103/96 (1149%)]\tLoss: 4.106372\n",
      "Train Epoche: 1 [1104/96 (1150%)]\tLoss: 31.470577\n",
      "Train Epoche: 1 [1105/96 (1151%)]\tLoss: 11.034451\n",
      "Train Epoche: 1 [1106/96 (1152%)]\tLoss: 3.291873\n",
      "Train Epoche: 1 [1107/96 (1153%)]\tLoss: 0.950775\n",
      "Train Epoche: 1 [1108/96 (1154%)]\tLoss: 12.558298\n",
      "Train Epoche: 1 [1109/96 (1155%)]\tLoss: 5.895587\n",
      "Train Epoche: 1 [1110/96 (1156%)]\tLoss: 440.035065\n",
      "Train Epoche: 1 [1111/96 (1157%)]\tLoss: 0.067285\n",
      "Train Epoche: 1 [1112/96 (1158%)]\tLoss: 4.112563\n",
      "Train Epoche: 1 [1113/96 (1159%)]\tLoss: 33.895458\n",
      "Train Epoche: 1 [1114/96 (1160%)]\tLoss: 0.383342\n",
      "Train Epoche: 1 [1115/96 (1161%)]\tLoss: 29.757690\n",
      "Train Epoche: 1 [1116/96 (1162%)]\tLoss: 9.586802\n",
      "Train Epoche: 1 [1117/96 (1164%)]\tLoss: 1.600180\n",
      "Train Epoche: 1 [1118/96 (1165%)]\tLoss: 1.614956\n",
      "Train Epoche: 1 [1119/96 (1166%)]\tLoss: 1.192487\n",
      "Train Epoche: 1 [1120/96 (1167%)]\tLoss: 0.004969\n",
      "Train Epoche: 1 [1121/96 (1168%)]\tLoss: 0.553518\n",
      "Train Epoche: 1 [1122/96 (1169%)]\tLoss: 4.449419\n",
      "Train Epoche: 1 [1123/96 (1170%)]\tLoss: 0.501786\n",
      "Train Epoche: 1 [1124/96 (1171%)]\tLoss: 28.939394\n",
      "Train Epoche: 1 [1125/96 (1172%)]\tLoss: 55.023430\n",
      "Train Epoche: 1 [1126/96 (1173%)]\tLoss: 0.653324\n",
      "Train Epoche: 1 [1127/96 (1174%)]\tLoss: 16.494087\n",
      "Train Epoche: 1 [1128/96 (1175%)]\tLoss: 3.484092\n",
      "Train Epoche: 1 [1129/96 (1176%)]\tLoss: 19.714977\n",
      "Train Epoche: 1 [1130/96 (1177%)]\tLoss: 27.839407\n",
      "Train Epoche: 1 [1131/96 (1178%)]\tLoss: 17.563137\n",
      "Train Epoche: 1 [1132/96 (1179%)]\tLoss: 0.606980\n",
      "Train Epoche: 1 [1133/96 (1180%)]\tLoss: 1.081149\n",
      "Train Epoche: 1 [1134/96 (1181%)]\tLoss: 116.174721\n",
      "Train Epoche: 1 [1135/96 (1182%)]\tLoss: 404.947601\n",
      "Train Epoche: 1 [1136/96 (1183%)]\tLoss: 270.937653\n",
      "Train Epoche: 1 [1137/96 (1184%)]\tLoss: 0.004824\n",
      "Train Epoche: 1 [1138/96 (1185%)]\tLoss: 33.248108\n",
      "Train Epoche: 1 [1139/96 (1186%)]\tLoss: 37.087730\n",
      "Train Epoche: 1 [1140/96 (1188%)]\tLoss: 17.042229\n",
      "Train Epoche: 1 [1141/96 (1189%)]\tLoss: 59.920986\n",
      "Train Epoche: 1 [1142/96 (1190%)]\tLoss: 14.706164\n",
      "Train Epoche: 1 [1143/96 (1191%)]\tLoss: 39.709290\n",
      "Train Epoche: 1 [1144/96 (1192%)]\tLoss: 12.605501\n",
      "Train Epoche: 1 [1145/96 (1193%)]\tLoss: 14.367455\n",
      "Train Epoche: 1 [1146/96 (1194%)]\tLoss: 30.762304\n",
      "Train Epoche: 1 [1147/96 (1195%)]\tLoss: 51.407581\n",
      "Train Epoche: 1 [1148/96 (1196%)]\tLoss: 4.586040\n",
      "Train Epoche: 1 [1149/96 (1197%)]\tLoss: 103.621727\n",
      "Train Epoche: 1 [1150/96 (1198%)]\tLoss: 10.148851\n",
      "Train Epoche: 1 [1151/96 (1199%)]\tLoss: 1.346488\n",
      "Train Epoche: 1 [1152/96 (1200%)]\tLoss: 0.261212\n",
      "Train Epoche: 1 [1153/96 (1201%)]\tLoss: 0.047495\n",
      "Train Epoche: 1 [1154/96 (1202%)]\tLoss: 76.876114\n",
      "Train Epoche: 1 [1155/96 (1203%)]\tLoss: 6.347562\n",
      "Train Epoche: 1 [1156/96 (1204%)]\tLoss: 2.980058\n",
      "Train Epoche: 1 [1157/96 (1205%)]\tLoss: 0.001592\n",
      "Train Epoche: 1 [1158/96 (1206%)]\tLoss: 7.530663\n",
      "Train Epoche: 1 [1159/96 (1207%)]\tLoss: 8.112172\n",
      "Train Epoche: 1 [1160/96 (1208%)]\tLoss: 0.108379\n",
      "Train Epoche: 1 [1161/96 (1209%)]\tLoss: 10.731458\n",
      "Train Epoche: 1 [1162/96 (1210%)]\tLoss: 67.721230\n",
      "Train Epoche: 1 [1163/96 (1211%)]\tLoss: 1.159279\n",
      "Train Epoche: 1 [1164/96 (1212%)]\tLoss: 1.794504\n",
      "Train Epoche: 1 [1165/96 (1214%)]\tLoss: 2.203332\n",
      "Train Epoche: 1 [1166/96 (1215%)]\tLoss: 18.663807\n",
      "Train Epoche: 1 [1167/96 (1216%)]\tLoss: 3.935905\n",
      "Train Epoche: 1 [1168/96 (1217%)]\tLoss: 21.885410\n",
      "Train Epoche: 1 [1169/96 (1218%)]\tLoss: 5.289128\n",
      "Train Epoche: 1 [1170/96 (1219%)]\tLoss: 4.867359\n",
      "Train Epoche: 1 [1171/96 (1220%)]\tLoss: 0.234612\n",
      "Train Epoche: 1 [1172/96 (1221%)]\tLoss: 10.151237\n",
      "Train Epoche: 1 [1173/96 (1222%)]\tLoss: 23.780552\n",
      "Train Epoche: 1 [1174/96 (1223%)]\tLoss: 3.157853\n",
      "Train Epoche: 1 [1175/96 (1224%)]\tLoss: 6.601899\n",
      "Train Epoche: 1 [1176/96 (1225%)]\tLoss: 24.389860\n",
      "Train Epoche: 1 [1177/96 (1226%)]\tLoss: 10.529214\n",
      "Train Epoche: 1 [1178/96 (1227%)]\tLoss: 24.119623\n",
      "Train Epoche: 1 [1179/96 (1228%)]\tLoss: 8.163497\n",
      "Train Epoche: 1 [1180/96 (1229%)]\tLoss: 25.606741\n",
      "Train Epoche: 1 [1181/96 (1230%)]\tLoss: 62.018913\n",
      "Train Epoche: 1 [1182/96 (1231%)]\tLoss: 13.094175\n",
      "Train Epoche: 1 [1183/96 (1232%)]\tLoss: 4.284090\n",
      "Train Epoche: 1 [1184/96 (1233%)]\tLoss: 11.610564\n",
      "Train Epoche: 1 [1185/96 (1234%)]\tLoss: 50.019234\n",
      "Train Epoche: 1 [1186/96 (1235%)]\tLoss: 68.504662\n",
      "Train Epoche: 1 [1187/96 (1236%)]\tLoss: 11.269982\n",
      "Train Epoche: 1 [1188/96 (1238%)]\tLoss: 24.370760\n",
      "Train Epoche: 1 [1189/96 (1239%)]\tLoss: 0.094277\n",
      "Train Epoche: 1 [1190/96 (1240%)]\tLoss: 0.458661\n",
      "Train Epoche: 1 [1191/96 (1241%)]\tLoss: 1.071124\n",
      "Train Epoche: 1 [1192/96 (1242%)]\tLoss: 0.019073\n",
      "Train Epoche: 1 [1193/96 (1243%)]\tLoss: 0.423765\n",
      "Train Epoche: 1 [1194/96 (1244%)]\tLoss: 0.362226\n",
      "Train Epoche: 1 [1195/96 (1245%)]\tLoss: 22.585722\n",
      "Train Epoche: 1 [1196/96 (1246%)]\tLoss: 1.758873\n",
      "Train Epoche: 1 [1197/96 (1247%)]\tLoss: 7.502653\n",
      "Train Epoche: 1 [1198/96 (1248%)]\tLoss: 251.575012\n",
      "Train Epoche: 1 [1199/96 (1249%)]\tLoss: 2.862261\n",
      "Train Epoche: 1 [1200/96 (1250%)]\tLoss: 0.152730\n",
      "Train Epoche: 1 [1201/96 (1251%)]\tLoss: 49.681797\n",
      "Train Epoche: 1 [1202/96 (1252%)]\tLoss: 0.124310\n",
      "Train Epoche: 1 [1203/96 (1253%)]\tLoss: 160.388306\n",
      "Train Epoche: 1 [1204/96 (1254%)]\tLoss: 4.451933\n",
      "Train Epoche: 1 [1205/96 (1255%)]\tLoss: 0.002054\n",
      "Train Epoche: 1 [1206/96 (1256%)]\tLoss: 0.279239\n",
      "Train Epoche: 1 [1207/96 (1257%)]\tLoss: 18.197119\n",
      "Train Epoche: 1 [1208/96 (1258%)]\tLoss: 13.794835\n",
      "Train Epoche: 1 [1209/96 (1259%)]\tLoss: 37.373280\n",
      "Train Epoche: 1 [1210/96 (1260%)]\tLoss: 11.063169\n",
      "Train Epoche: 1 [1211/96 (1261%)]\tLoss: 1.198261\n",
      "Train Epoche: 1 [1212/96 (1262%)]\tLoss: 1.040033\n",
      "Train Epoche: 1 [1213/96 (1264%)]\tLoss: 0.898126\n",
      "Train Epoche: 1 [1214/96 (1265%)]\tLoss: 0.019374\n",
      "Train Epoche: 1 [1215/96 (1266%)]\tLoss: 12.036362\n",
      "Train Epoche: 1 [1216/96 (1267%)]\tLoss: 0.551164\n",
      "Train Epoche: 1 [1217/96 (1268%)]\tLoss: 16.770893\n",
      "Train Epoche: 1 [1218/96 (1269%)]\tLoss: 0.003046\n",
      "Train Epoche: 1 [1219/96 (1270%)]\tLoss: 0.010066\n",
      "Train Epoche: 1 [1220/96 (1271%)]\tLoss: 1.413888\n",
      "Train Epoche: 1 [1221/96 (1272%)]\tLoss: 0.913255\n",
      "Train Epoche: 1 [1222/96 (1273%)]\tLoss: 0.989256\n",
      "Train Epoche: 1 [1223/96 (1274%)]\tLoss: 12.056732\n",
      "Train Epoche: 1 [1224/96 (1275%)]\tLoss: 1.893157\n",
      "Train Epoche: 1 [1225/96 (1276%)]\tLoss: 57.096821\n",
      "Train Epoche: 1 [1226/96 (1277%)]\tLoss: 8.311831\n",
      "Train Epoche: 1 [1227/96 (1278%)]\tLoss: 0.049203\n",
      "Train Epoche: 1 [1228/96 (1279%)]\tLoss: 1.166153\n",
      "Train Epoche: 1 [1229/96 (1280%)]\tLoss: 59.280144\n",
      "Train Epoche: 1 [1230/96 (1281%)]\tLoss: 0.077078\n",
      "Train Epoche: 1 [1231/96 (1282%)]\tLoss: 28.538727\n",
      "Train Epoche: 1 [1232/96 (1283%)]\tLoss: 4.719788\n",
      "Train Epoche: 1 [1233/96 (1284%)]\tLoss: 3.948338\n",
      "Train Epoche: 1 [1234/96 (1285%)]\tLoss: 3.712157\n",
      "Train Epoche: 1 [1235/96 (1286%)]\tLoss: 2.930012\n",
      "Train Epoche: 1 [1236/96 (1288%)]\tLoss: 0.020001\n",
      "Train Epoche: 1 [1237/96 (1289%)]\tLoss: 5.670553\n",
      "Train Epoche: 1 [1238/96 (1290%)]\tLoss: 0.043803\n",
      "Train Epoche: 1 [1239/96 (1291%)]\tLoss: 26.742758\n",
      "Train Epoche: 1 [1240/96 (1292%)]\tLoss: 0.065808\n",
      "Train Epoche: 1 [1241/96 (1293%)]\tLoss: 2.845955\n",
      "Train Epoche: 1 [1242/96 (1294%)]\tLoss: 0.585050\n",
      "Train Epoche: 1 [1243/96 (1295%)]\tLoss: 3.705497\n",
      "Train Epoche: 1 [1244/96 (1296%)]\tLoss: 0.125865\n",
      "Train Epoche: 1 [1245/96 (1297%)]\tLoss: 0.704137\n",
      "Train Epoche: 1 [1246/96 (1298%)]\tLoss: 3.924327\n",
      "Train Epoche: 1 [1247/96 (1299%)]\tLoss: 3.677989\n",
      "Train Epoche: 1 [1248/96 (1300%)]\tLoss: 7.936931\n",
      "Train Epoche: 1 [1249/96 (1301%)]\tLoss: 33.945545\n",
      "Train Epoche: 1 [1250/96 (1302%)]\tLoss: 18.515764\n",
      "Train Epoche: 1 [1251/96 (1303%)]\tLoss: 0.031376\n",
      "Train Epoche: 1 [1252/96 (1304%)]\tLoss: 19.915104\n",
      "Train Epoche: 1 [1253/96 (1305%)]\tLoss: 32.609577\n",
      "Train Epoche: 1 [1254/96 (1306%)]\tLoss: 116.714233\n",
      "Train Epoche: 1 [1255/96 (1307%)]\tLoss: 3.977137\n",
      "Train Epoche: 1 [1256/96 (1308%)]\tLoss: 3.135952\n",
      "Train Epoche: 1 [1257/96 (1309%)]\tLoss: 19.299028\n",
      "Train Epoche: 1 [1258/96 (1310%)]\tLoss: 14.162136\n",
      "Train Epoche: 1 [1259/96 (1311%)]\tLoss: 0.043316\n",
      "Train Epoche: 1 [1260/96 (1312%)]\tLoss: 2.398854\n",
      "Train Epoche: 1 [1261/96 (1314%)]\tLoss: 18.598381\n",
      "Train Epoche: 1 [1262/96 (1315%)]\tLoss: 1.916686\n",
      "Train Epoche: 1 [1263/96 (1316%)]\tLoss: 4.754719\n",
      "Train Epoche: 1 [1264/96 (1317%)]\tLoss: 35.283810\n",
      "Train Epoche: 1 [1265/96 (1318%)]\tLoss: 0.808993\n",
      "Train Epoche: 1 [1266/96 (1319%)]\tLoss: 317.674561\n",
      "Train Epoche: 1 [1267/96 (1320%)]\tLoss: 7.737654\n",
      "Train Epoche: 1 [1268/96 (1321%)]\tLoss: 78.173912\n",
      "Train Epoche: 1 [1269/96 (1322%)]\tLoss: 0.219740\n",
      "Train Epoche: 1 [1270/96 (1323%)]\tLoss: 48.412308\n",
      "Train Epoche: 1 [1271/96 (1324%)]\tLoss: 0.057889\n",
      "Train Epoche: 1 [1272/96 (1325%)]\tLoss: 12.429058\n",
      "Train Epoche: 1 [1273/96 (1326%)]\tLoss: 13.197382\n",
      "Train Epoche: 1 [1274/96 (1327%)]\tLoss: 267.753601\n",
      "Train Epoche: 1 [1275/96 (1328%)]\tLoss: 2.632291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1276/96 (1329%)]\tLoss: 37.352386\n",
      "Train Epoche: 1 [1277/96 (1330%)]\tLoss: 0.238166\n",
      "Train Epoche: 1 [1278/96 (1331%)]\tLoss: 11.811267\n",
      "Train Epoche: 1 [1279/96 (1332%)]\tLoss: 12.904955\n",
      "Train Epoche: 1 [1280/96 (1333%)]\tLoss: 3.018426\n",
      "Train Epoche: 1 [1281/96 (1334%)]\tLoss: 28.640957\n",
      "Train Epoche: 1 [1282/96 (1335%)]\tLoss: 17.593878\n",
      "Train Epoche: 1 [1283/96 (1336%)]\tLoss: 4.948921\n",
      "Train Epoche: 1 [1284/96 (1338%)]\tLoss: 6.666610\n",
      "Train Epoche: 1 [1285/96 (1339%)]\tLoss: 15.715286\n",
      "Train Epoche: 1 [1286/96 (1340%)]\tLoss: 8.242325\n",
      "Train Epoche: 1 [1287/96 (1341%)]\tLoss: 3.385997\n",
      "Train Epoche: 1 [1288/96 (1342%)]\tLoss: 119.536049\n",
      "Train Epoche: 1 [1289/96 (1343%)]\tLoss: 3.417366\n",
      "Train Epoche: 1 [1290/96 (1344%)]\tLoss: 57.115906\n",
      "Train Epoche: 1 [1291/96 (1345%)]\tLoss: 32.705933\n",
      "Train Epoche: 1 [1292/96 (1346%)]\tLoss: 21.039448\n",
      "Train Epoche: 1 [1293/96 (1347%)]\tLoss: 14.043695\n",
      "Train Epoche: 1 [1294/96 (1348%)]\tLoss: 2.402326\n",
      "Train Epoche: 1 [1295/96 (1349%)]\tLoss: 6.539923\n",
      "Train Epoche: 1 [1296/96 (1350%)]\tLoss: 0.155233\n",
      "Train Epoche: 1 [1297/96 (1351%)]\tLoss: 6.503717\n",
      "Train Epoche: 1 [1298/96 (1352%)]\tLoss: 9.828218\n",
      "Train Epoche: 1 [1299/96 (1353%)]\tLoss: 0.129849\n",
      "Train Epoche: 1 [1300/96 (1354%)]\tLoss: 21.901403\n",
      "Train Epoche: 1 [1301/96 (1355%)]\tLoss: 0.671596\n",
      "Train Epoche: 1 [1302/96 (1356%)]\tLoss: 43.272495\n",
      "Train Epoche: 1 [1303/96 (1357%)]\tLoss: 1.211746\n",
      "Train Epoche: 1 [1304/96 (1358%)]\tLoss: 9.145664\n",
      "Train Epoche: 1 [1305/96 (1359%)]\tLoss: 4.100009\n",
      "Train Epoche: 1 [1306/96 (1360%)]\tLoss: 0.186606\n",
      "Train Epoche: 1 [1307/96 (1361%)]\tLoss: 0.420064\n",
      "Train Epoche: 1 [1308/96 (1362%)]\tLoss: 2.724888\n",
      "Train Epoche: 1 [1309/96 (1364%)]\tLoss: 20.381292\n",
      "Train Epoche: 1 [1310/96 (1365%)]\tLoss: 1.211082\n",
      "Train Epoche: 1 [1311/96 (1366%)]\tLoss: 184.719437\n",
      "Train Epoche: 1 [1312/96 (1367%)]\tLoss: 28.395237\n",
      "Train Epoche: 1 [1313/96 (1368%)]\tLoss: 4.603466\n",
      "Train Epoche: 1 [1314/96 (1369%)]\tLoss: 22.188862\n",
      "Train Epoche: 1 [1315/96 (1370%)]\tLoss: 10.173664\n",
      "Train Epoche: 1 [1316/96 (1371%)]\tLoss: 23.254200\n",
      "Train Epoche: 1 [1317/96 (1372%)]\tLoss: 2.610948\n",
      "Train Epoche: 1 [1318/96 (1373%)]\tLoss: 324.449280\n",
      "Train Epoche: 1 [1319/96 (1374%)]\tLoss: 3.888380\n",
      "Train Epoche: 1 [1320/96 (1375%)]\tLoss: 1.171678\n",
      "Train Epoche: 1 [1321/96 (1376%)]\tLoss: 1.788618\n",
      "Train Epoche: 1 [1322/96 (1377%)]\tLoss: 8.180247\n",
      "Train Epoche: 1 [1323/96 (1378%)]\tLoss: 9.701162\n",
      "Train Epoche: 1 [1324/96 (1379%)]\tLoss: 12.981778\n",
      "Train Epoche: 1 [1325/96 (1380%)]\tLoss: 49.217548\n",
      "Train Epoche: 1 [1326/96 (1381%)]\tLoss: 0.001294\n",
      "Train Epoche: 1 [1327/96 (1382%)]\tLoss: 10.736369\n",
      "Train Epoche: 1 [1328/96 (1383%)]\tLoss: 4.317696\n",
      "Train Epoche: 1 [1329/96 (1384%)]\tLoss: 297.786865\n",
      "Train Epoche: 1 [1330/96 (1385%)]\tLoss: 8.147026\n",
      "Train Epoche: 1 [1331/96 (1386%)]\tLoss: 3.133536\n",
      "Train Epoche: 1 [1332/96 (1388%)]\tLoss: 40.210358\n",
      "Train Epoche: 1 [1333/96 (1389%)]\tLoss: 49.205601\n",
      "Train Epoche: 1 [1334/96 (1390%)]\tLoss: 7.689681\n",
      "Train Epoche: 1 [1335/96 (1391%)]\tLoss: 13.311280\n",
      "Train Epoche: 1 [1336/96 (1392%)]\tLoss: 3.211192\n",
      "Train Epoche: 1 [1337/96 (1393%)]\tLoss: 25.486044\n",
      "Train Epoche: 1 [1338/96 (1394%)]\tLoss: 19.102716\n",
      "Train Epoche: 1 [1339/96 (1395%)]\tLoss: 15.489287\n",
      "Train Epoche: 1 [1340/96 (1396%)]\tLoss: 693.941895\n",
      "Train Epoche: 1 [1341/96 (1397%)]\tLoss: 78.147758\n",
      "Train Epoche: 1 [1342/96 (1398%)]\tLoss: 0.490499\n",
      "Train Epoche: 1 [1343/96 (1399%)]\tLoss: 0.063720\n",
      "Train Epoche: 1 [1344/96 (1400%)]\tLoss: 85.709267\n",
      "Train Epoche: 1 [1345/96 (1401%)]\tLoss: 5.829217\n",
      "Train Epoche: 1 [1346/96 (1402%)]\tLoss: 1.002197\n",
      "Train Epoche: 1 [1347/96 (1403%)]\tLoss: 14.059177\n",
      "Train Epoche: 1 [1348/96 (1404%)]\tLoss: 1.956116\n",
      "Train Epoche: 1 [1349/96 (1405%)]\tLoss: 0.105849\n",
      "Train Epoche: 1 [1350/96 (1406%)]\tLoss: 0.117190\n",
      "Train Epoche: 1 [1351/96 (1407%)]\tLoss: 0.003220\n",
      "Train Epoche: 1 [1352/96 (1408%)]\tLoss: 4.931908\n",
      "Train Epoche: 1 [1353/96 (1409%)]\tLoss: 43.974091\n",
      "Train Epoche: 1 [1354/96 (1410%)]\tLoss: 24.363831\n",
      "Train Epoche: 1 [1355/96 (1411%)]\tLoss: 133.733627\n",
      "Train Epoche: 1 [1356/96 (1412%)]\tLoss: 36.914246\n",
      "Train Epoche: 1 [1357/96 (1414%)]\tLoss: 23.121712\n",
      "Train Epoche: 1 [1358/96 (1415%)]\tLoss: 1.075405\n",
      "Train Epoche: 1 [1359/96 (1416%)]\tLoss: 89.753601\n",
      "Train Epoche: 1 [1360/96 (1417%)]\tLoss: 0.547179\n",
      "Train Epoche: 1 [1361/96 (1418%)]\tLoss: 4.472425\n",
      "Train Epoche: 1 [1362/96 (1419%)]\tLoss: 0.052051\n",
      "Train Epoche: 1 [1363/96 (1420%)]\tLoss: 79.823532\n",
      "Train Epoche: 1 [1364/96 (1421%)]\tLoss: 220.648514\n",
      "Train Epoche: 1 [1365/96 (1422%)]\tLoss: 6.540493\n",
      "Train Epoche: 1 [1366/96 (1423%)]\tLoss: 0.000063\n",
      "Train Epoche: 1 [1367/96 (1424%)]\tLoss: 17.401302\n",
      "Train Epoche: 1 [1368/96 (1425%)]\tLoss: 18.455498\n",
      "Train Epoche: 1 [1369/96 (1426%)]\tLoss: 82.152107\n",
      "Train Epoche: 1 [1370/96 (1427%)]\tLoss: 5.917357\n",
      "Train Epoche: 1 [1371/96 (1428%)]\tLoss: 19.631046\n",
      "Train Epoche: 1 [1372/96 (1429%)]\tLoss: 13.193239\n",
      "Train Epoche: 1 [1373/96 (1430%)]\tLoss: 4.672375\n",
      "Train Epoche: 1 [1374/96 (1431%)]\tLoss: 3.878801\n",
      "Train Epoche: 1 [1375/96 (1432%)]\tLoss: 2.131776\n",
      "Train Epoche: 1 [1376/96 (1433%)]\tLoss: 4.662210\n",
      "Train Epoche: 1 [1377/96 (1434%)]\tLoss: 5.948568\n",
      "Train Epoche: 1 [1378/96 (1435%)]\tLoss: 0.833527\n",
      "Train Epoche: 1 [1379/96 (1436%)]\tLoss: 6.944337\n",
      "Train Epoche: 1 [1380/96 (1438%)]\tLoss: 1.080811\n",
      "Train Epoche: 1 [1381/96 (1439%)]\tLoss: 0.153166\n",
      "Train Epoche: 1 [1382/96 (1440%)]\tLoss: 4.398749\n",
      "Train Epoche: 1 [1383/96 (1441%)]\tLoss: 1.059119\n",
      "Train Epoche: 1 [1384/96 (1442%)]\tLoss: 0.040316\n",
      "Train Epoche: 1 [1385/96 (1443%)]\tLoss: 4.800086\n",
      "Train Epoche: 1 [1386/96 (1444%)]\tLoss: 12.252477\n",
      "Train Epoche: 1 [1387/96 (1445%)]\tLoss: 3.271349\n",
      "Train Epoche: 1 [1388/96 (1446%)]\tLoss: 12.059162\n",
      "Train Epoche: 1 [1389/96 (1447%)]\tLoss: 25.023428\n",
      "Train Epoche: 1 [1390/96 (1448%)]\tLoss: 23.388266\n",
      "Train Epoche: 1 [1391/96 (1449%)]\tLoss: 3.959192\n",
      "Train Epoche: 1 [1392/96 (1450%)]\tLoss: 9.285876\n",
      "Train Epoche: 1 [1393/96 (1451%)]\tLoss: 0.042499\n",
      "Train Epoche: 1 [1394/96 (1452%)]\tLoss: 58.962662\n",
      "Train Epoche: 1 [1395/96 (1453%)]\tLoss: 0.069916\n",
      "Train Epoche: 1 [1396/96 (1454%)]\tLoss: 3.238842\n",
      "Train Epoche: 1 [1397/96 (1455%)]\tLoss: 66.709732\n",
      "Train Epoche: 1 [1398/96 (1456%)]\tLoss: 0.794029\n",
      "Train Epoche: 1 [1399/96 (1457%)]\tLoss: 1.662716\n",
      "Train Epoche: 1 [1400/96 (1458%)]\tLoss: 3.744317\n",
      "Train Epoche: 1 [1401/96 (1459%)]\tLoss: 0.285577\n",
      "Train Epoche: 1 [1402/96 (1460%)]\tLoss: 8.036144\n",
      "Train Epoche: 1 [1403/96 (1461%)]\tLoss: 7.073055\n",
      "Train Epoche: 1 [1404/96 (1462%)]\tLoss: 0.935329\n",
      "Train Epoche: 1 [1405/96 (1464%)]\tLoss: 1.700291\n",
      "Train Epoche: 1 [1406/96 (1465%)]\tLoss: 0.442553\n",
      "Train Epoche: 1 [1407/96 (1466%)]\tLoss: 21.774488\n",
      "Train Epoche: 1 [1408/96 (1467%)]\tLoss: 0.036903\n",
      "Train Epoche: 1 [1409/96 (1468%)]\tLoss: 42.314419\n",
      "Train Epoche: 1 [1410/96 (1469%)]\tLoss: 0.206712\n",
      "Train Epoche: 1 [1411/96 (1470%)]\tLoss: 157.467407\n",
      "Train Epoche: 1 [1412/96 (1471%)]\tLoss: 20.751989\n",
      "Train Epoche: 1 [1413/96 (1472%)]\tLoss: 6.845408\n",
      "Train Epoche: 1 [1414/96 (1473%)]\tLoss: 2.575659\n",
      "Train Epoche: 1 [1415/96 (1474%)]\tLoss: 0.077298\n",
      "Train Epoche: 1 [1416/96 (1475%)]\tLoss: 3.156273\n",
      "Train Epoche: 1 [1417/96 (1476%)]\tLoss: 0.253344\n",
      "Train Epoche: 1 [1418/96 (1477%)]\tLoss: 86.317017\n",
      "Train Epoche: 1 [1419/96 (1478%)]\tLoss: 1.058033\n",
      "Train Epoche: 1 [1420/96 (1479%)]\tLoss: 11.748013\n",
      "Train Epoche: 1 [1421/96 (1480%)]\tLoss: 6.835182\n",
      "Train Epoche: 1 [1422/96 (1481%)]\tLoss: 0.340183\n",
      "Train Epoche: 1 [1423/96 (1482%)]\tLoss: 0.001256\n",
      "Train Epoche: 1 [1424/96 (1483%)]\tLoss: 7.205873\n",
      "Train Epoche: 1 [1425/96 (1484%)]\tLoss: 17.831116\n",
      "Train Epoche: 1 [1426/96 (1485%)]\tLoss: 10.972815\n",
      "Train Epoche: 1 [1427/96 (1486%)]\tLoss: 0.001466\n",
      "Train Epoche: 1 [1428/96 (1488%)]\tLoss: 9.359039\n",
      "Train Epoche: 1 [1429/96 (1489%)]\tLoss: 0.051479\n",
      "Train Epoche: 1 [1430/96 (1490%)]\tLoss: 5.526389\n",
      "Train Epoche: 1 [1431/96 (1491%)]\tLoss: 6.046586\n",
      "Train Epoche: 1 [1432/96 (1492%)]\tLoss: 71.257080\n",
      "Train Epoche: 1 [1433/96 (1493%)]\tLoss: 1.175326\n",
      "Train Epoche: 1 [1434/96 (1494%)]\tLoss: 0.858463\n",
      "Train Epoche: 1 [1435/96 (1495%)]\tLoss: 16.367130\n",
      "Train Epoche: 1 [1436/96 (1496%)]\tLoss: 3.926813\n",
      "Train Epoche: 1 [1437/96 (1497%)]\tLoss: 1.498678\n",
      "Train Epoche: 1 [1438/96 (1498%)]\tLoss: 3.253377\n",
      "Train Epoche: 1 [1439/96 (1499%)]\tLoss: 0.182530\n",
      "Train Epoche: 1 [1440/96 (1500%)]\tLoss: 0.000089\n",
      "Train Epoche: 1 [1441/96 (1501%)]\tLoss: 1.055221\n",
      "Train Epoche: 1 [1442/96 (1502%)]\tLoss: 252.246277\n",
      "Train Epoche: 1 [1443/96 (1503%)]\tLoss: 99.279770\n",
      "Train Epoche: 1 [1444/96 (1504%)]\tLoss: 4.090863\n",
      "Train Epoche: 1 [1445/96 (1505%)]\tLoss: 0.177542\n",
      "Train Epoche: 1 [1446/96 (1506%)]\tLoss: 18.611067\n",
      "Train Epoche: 1 [1447/96 (1507%)]\tLoss: 10.082946\n",
      "Train Epoche: 1 [1448/96 (1508%)]\tLoss: 0.013616\n",
      "Train Epoche: 1 [1449/96 (1509%)]\tLoss: 12.485910\n",
      "Train Epoche: 1 [1450/96 (1510%)]\tLoss: 23.343130\n",
      "Train Epoche: 1 [1451/96 (1511%)]\tLoss: 41.918144\n",
      "Train Epoche: 1 [1452/96 (1512%)]\tLoss: 5.704651\n",
      "Train Epoche: 1 [1453/96 (1514%)]\tLoss: 25.966871\n",
      "Train Epoche: 1 [1454/96 (1515%)]\tLoss: 0.375746\n",
      "Train Epoche: 1 [1455/96 (1516%)]\tLoss: 12.048203\n",
      "Train Epoche: 1 [1456/96 (1517%)]\tLoss: 2.297058\n",
      "Train Epoche: 1 [1457/96 (1518%)]\tLoss: 88.940071\n",
      "Train Epoche: 1 [1458/96 (1519%)]\tLoss: 18.918777\n",
      "Train Epoche: 1 [1459/96 (1520%)]\tLoss: 3.467277\n",
      "Train Epoche: 1 [1460/96 (1521%)]\tLoss: 0.045704\n",
      "Train Epoche: 1 [1461/96 (1522%)]\tLoss: 0.562682\n",
      "Train Epoche: 1 [1462/96 (1523%)]\tLoss: 0.963759\n",
      "Train Epoche: 1 [1463/96 (1524%)]\tLoss: 8.104095\n",
      "Train Epoche: 1 [1464/96 (1525%)]\tLoss: 9.798390\n",
      "Train Epoche: 1 [1465/96 (1526%)]\tLoss: 0.192208\n",
      "Train Epoche: 1 [1466/96 (1527%)]\tLoss: 1.252303\n",
      "Train Epoche: 1 [1467/96 (1528%)]\tLoss: 0.381366\n",
      "Train Epoche: 1 [1468/96 (1529%)]\tLoss: 125.787735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1469/96 (1530%)]\tLoss: 1.730801\n",
      "Train Epoche: 1 [1470/96 (1531%)]\tLoss: 11.971845\n",
      "Train Epoche: 1 [1471/96 (1532%)]\tLoss: 9.872309\n",
      "Train Epoche: 1 [1472/96 (1533%)]\tLoss: 13.185439\n",
      "Train Epoche: 1 [1473/96 (1534%)]\tLoss: 4.324051\n",
      "Train Epoche: 1 [1474/96 (1535%)]\tLoss: 18.101252\n",
      "Train Epoche: 1 [1475/96 (1536%)]\tLoss: 3.467784\n",
      "Train Epoche: 1 [1476/96 (1538%)]\tLoss: 0.004825\n",
      "Train Epoche: 1 [1477/96 (1539%)]\tLoss: 95.934296\n",
      "Train Epoche: 1 [1478/96 (1540%)]\tLoss: 35.449303\n",
      "Train Epoche: 1 [1479/96 (1541%)]\tLoss: 0.007050\n",
      "Train Epoche: 1 [1480/96 (1542%)]\tLoss: 7.148087\n",
      "Train Epoche: 1 [1481/96 (1543%)]\tLoss: 0.001684\n",
      "Train Epoche: 1 [1482/96 (1544%)]\tLoss: 10.634642\n",
      "Train Epoche: 1 [1483/96 (1545%)]\tLoss: 4.641965\n",
      "Train Epoche: 1 [1484/96 (1546%)]\tLoss: 5.273760\n",
      "Train Epoche: 1 [1485/96 (1547%)]\tLoss: 2.482795\n",
      "Train Epoche: 1 [1486/96 (1548%)]\tLoss: 207.482391\n",
      "Train Epoche: 1 [1487/96 (1549%)]\tLoss: 0.322035\n",
      "Train Epoche: 1 [1488/96 (1550%)]\tLoss: 2.981081\n",
      "Train Epoche: 1 [1489/96 (1551%)]\tLoss: 0.644333\n",
      "Train Epoche: 1 [1490/96 (1552%)]\tLoss: 22.378670\n",
      "Train Epoche: 1 [1491/96 (1553%)]\tLoss: 9.463038\n",
      "Train Epoche: 1 [1492/96 (1554%)]\tLoss: 11.082891\n",
      "Train Epoche: 1 [1493/96 (1555%)]\tLoss: 26.015471\n",
      "Train Epoche: 1 [1494/96 (1556%)]\tLoss: 125.807571\n",
      "Train Epoche: 1 [1495/96 (1557%)]\tLoss: 4.389310\n",
      "Train Epoche: 1 [1496/96 (1558%)]\tLoss: 0.141322\n",
      "Train Epoche: 1 [1497/96 (1559%)]\tLoss: 6.975830\n",
      "Train Epoche: 1 [1498/96 (1560%)]\tLoss: 7.771727\n",
      "Train Epoche: 1 [1499/96 (1561%)]\tLoss: 18.246346\n",
      "Train Epoche: 1 [1500/96 (1562%)]\tLoss: 4.114493\n",
      "Train Epoche: 1 [1501/96 (1564%)]\tLoss: 9.235640\n",
      "Train Epoche: 1 [1502/96 (1565%)]\tLoss: 9.997277\n",
      "Train Epoche: 1 [1503/96 (1566%)]\tLoss: 11.329251\n",
      "Train Epoche: 1 [1504/96 (1567%)]\tLoss: 11.759708\n",
      "Train Epoche: 1 [1505/96 (1568%)]\tLoss: 1.335873\n",
      "Train Epoche: 1 [1506/96 (1569%)]\tLoss: 0.001513\n",
      "Train Epoche: 1 [1507/96 (1570%)]\tLoss: 1.193466\n",
      "Train Epoche: 1 [1508/96 (1571%)]\tLoss: 6.614598\n",
      "Train Epoche: 1 [1509/96 (1572%)]\tLoss: 0.096578\n",
      "Train Epoche: 1 [1510/96 (1573%)]\tLoss: 0.295211\n",
      "Train Epoche: 1 [1511/96 (1574%)]\tLoss: 13.125798\n",
      "Train Epoche: 1 [1512/96 (1575%)]\tLoss: 3.049679\n",
      "Train Epoche: 1 [1513/96 (1576%)]\tLoss: 0.592593\n",
      "Train Epoche: 1 [1514/96 (1577%)]\tLoss: 5.964973\n",
      "Train Epoche: 1 [1515/96 (1578%)]\tLoss: 32.929054\n",
      "Train Epoche: 1 [1516/96 (1579%)]\tLoss: 1.950137\n",
      "Train Epoche: 1 [1517/96 (1580%)]\tLoss: 4.648295\n",
      "Train Epoche: 1 [1518/96 (1581%)]\tLoss: 3.617592\n",
      "Train Epoche: 1 [1519/96 (1582%)]\tLoss: 0.823749\n",
      "Train Epoche: 1 [1520/96 (1583%)]\tLoss: 0.002847\n",
      "Train Epoche: 1 [1521/96 (1584%)]\tLoss: 0.341239\n",
      "Train Epoche: 1 [1522/96 (1585%)]\tLoss: 18.090746\n",
      "Train Epoche: 1 [1523/96 (1586%)]\tLoss: 2.077646\n",
      "Train Epoche: 1 [1524/96 (1588%)]\tLoss: 15.952435\n",
      "Train Epoche: 1 [1525/96 (1589%)]\tLoss: 6.285436\n",
      "Train Epoche: 1 [1526/96 (1590%)]\tLoss: 11.694072\n",
      "Train Epoche: 1 [1527/96 (1591%)]\tLoss: 9.005531\n",
      "Train Epoche: 1 [1528/96 (1592%)]\tLoss: 0.863909\n",
      "Train Epoche: 1 [1529/96 (1593%)]\tLoss: 0.481360\n",
      "Train Epoche: 1 [1530/96 (1594%)]\tLoss: 2.320496\n",
      "Train Epoche: 1 [1531/96 (1595%)]\tLoss: 1.153120\n",
      "Train Epoche: 1 [1532/96 (1596%)]\tLoss: 12.552763\n",
      "Train Epoche: 1 [1533/96 (1597%)]\tLoss: 6.369378\n",
      "Train Epoche: 1 [1534/96 (1598%)]\tLoss: 0.530878\n",
      "Train Epoche: 1 [1535/96 (1599%)]\tLoss: 34.397594\n",
      "Train Epoche: 1 [1536/96 (1600%)]\tLoss: 0.861208\n",
      "Train Epoche: 1 [1537/96 (1601%)]\tLoss: 0.747176\n",
      "Train Epoche: 1 [1538/96 (1602%)]\tLoss: 8.713914\n",
      "Train Epoche: 1 [1539/96 (1603%)]\tLoss: 19.518515\n",
      "Train Epoche: 1 [1540/96 (1604%)]\tLoss: 7.328454\n",
      "Train Epoche: 1 [1541/96 (1605%)]\tLoss: 2.322286\n",
      "Train Epoche: 1 [1542/96 (1606%)]\tLoss: 96.479172\n",
      "Train Epoche: 1 [1543/96 (1607%)]\tLoss: 7.127775\n",
      "Train Epoche: 1 [1544/96 (1608%)]\tLoss: 3.246065\n",
      "Train Epoche: 1 [1545/96 (1609%)]\tLoss: 1.133974\n",
      "Train Epoche: 1 [1546/96 (1610%)]\tLoss: 5.150859\n",
      "Train Epoche: 1 [1547/96 (1611%)]\tLoss: 1.980700\n",
      "Train Epoche: 1 [1548/96 (1612%)]\tLoss: 1.022672\n",
      "Train Epoche: 1 [1549/96 (1614%)]\tLoss: 1.865532\n",
      "Train Epoche: 1 [1550/96 (1615%)]\tLoss: 38.682636\n",
      "Train Epoche: 1 [1551/96 (1616%)]\tLoss: 43.408798\n",
      "Train Epoche: 1 [1552/96 (1617%)]\tLoss: 22.293953\n",
      "Train Epoche: 1 [1553/96 (1618%)]\tLoss: 8.427874\n",
      "Train Epoche: 1 [1554/96 (1619%)]\tLoss: 4.710937\n",
      "Train Epoche: 1 [1555/96 (1620%)]\tLoss: 1.431651\n",
      "Train Epoche: 1 [1556/96 (1621%)]\tLoss: 0.137775\n",
      "Train Epoche: 1 [1557/96 (1622%)]\tLoss: 0.808451\n",
      "Train Epoche: 1 [1558/96 (1623%)]\tLoss: 34.950195\n",
      "Train Epoche: 1 [1559/96 (1624%)]\tLoss: 0.758319\n",
      "Train Epoche: 1 [1560/96 (1625%)]\tLoss: 2.372325\n",
      "Train Epoche: 1 [1561/96 (1626%)]\tLoss: 10.468061\n",
      "Train Epoche: 1 [1562/96 (1627%)]\tLoss: 0.044857\n",
      "Train Epoche: 1 [1563/96 (1628%)]\tLoss: 4.830801\n",
      "Train Epoche: 1 [1564/96 (1629%)]\tLoss: 0.403945\n",
      "Train Epoche: 1 [1565/96 (1630%)]\tLoss: 0.006650\n",
      "Train Epoche: 1 [1566/96 (1631%)]\tLoss: 4.185272\n",
      "Train Epoche: 1 [1567/96 (1632%)]\tLoss: 34.268906\n",
      "Train Epoche: 1 [1568/96 (1633%)]\tLoss: 1.828387\n",
      "Train Epoche: 1 [1569/96 (1634%)]\tLoss: 2.444680\n",
      "Train Epoche: 1 [1570/96 (1635%)]\tLoss: 19.280569\n",
      "Train Epoche: 1 [1571/96 (1636%)]\tLoss: 24.502310\n",
      "Train Epoche: 1 [1572/96 (1638%)]\tLoss: 3.832335\n",
      "Train Epoche: 1 [1573/96 (1639%)]\tLoss: 3.921576\n",
      "Train Epoche: 1 [1574/96 (1640%)]\tLoss: 5.118262\n",
      "Train Epoche: 1 [1575/96 (1641%)]\tLoss: 53.099876\n",
      "Train Epoche: 1 [1576/96 (1642%)]\tLoss: 0.210926\n",
      "Train Epoche: 1 [1577/96 (1643%)]\tLoss: 12.372483\n",
      "Train Epoche: 1 [1578/96 (1644%)]\tLoss: 0.571694\n",
      "Train Epoche: 1 [1579/96 (1645%)]\tLoss: 2.753033\n",
      "Train Epoche: 1 [1580/96 (1646%)]\tLoss: 18.099833\n",
      "Train Epoche: 1 [1581/96 (1647%)]\tLoss: 0.394011\n",
      "Train Epoche: 1 [1582/96 (1648%)]\tLoss: 1.077449\n",
      "Train Epoche: 1 [1583/96 (1649%)]\tLoss: 22.607967\n",
      "Train Epoche: 1 [1584/96 (1650%)]\tLoss: 0.443300\n",
      "Train Epoche: 1 [1585/96 (1651%)]\tLoss: 6.371265\n",
      "Train Epoche: 1 [1586/96 (1652%)]\tLoss: 1.714335\n",
      "Train Epoche: 1 [1587/96 (1653%)]\tLoss: 22.596125\n",
      "Train Epoche: 1 [1588/96 (1654%)]\tLoss: 4.563929\n",
      "Train Epoche: 1 [1589/96 (1655%)]\tLoss: 3.844337\n",
      "Train Epoche: 1 [1590/96 (1656%)]\tLoss: 0.000215\n",
      "Train Epoche: 1 [1591/96 (1657%)]\tLoss: 0.146289\n",
      "Train Epoche: 1 [1592/96 (1658%)]\tLoss: 0.030493\n",
      "Train Epoche: 1 [1593/96 (1659%)]\tLoss: 0.230149\n",
      "Train Epoche: 1 [1594/96 (1660%)]\tLoss: 0.508617\n",
      "Train Epoche: 1 [1595/96 (1661%)]\tLoss: 3.575794\n",
      "Train Epoche: 1 [1596/96 (1662%)]\tLoss: 2.456560\n",
      "Train Epoche: 1 [1597/96 (1664%)]\tLoss: 3.781918\n",
      "Train Epoche: 1 [1598/96 (1665%)]\tLoss: 0.215543\n",
      "Train Epoche: 1 [1599/96 (1666%)]\tLoss: 2.788130\n",
      "Train Epoche: 1 [1600/96 (1667%)]\tLoss: 308.082153\n",
      "Train Epoche: 1 [1601/96 (1668%)]\tLoss: 0.089612\n",
      "Train Epoche: 1 [1602/96 (1669%)]\tLoss: 55.620087\n",
      "Train Epoche: 1 [1603/96 (1670%)]\tLoss: 91.045609\n",
      "Train Epoche: 1 [1604/96 (1671%)]\tLoss: 4.905024\n",
      "Train Epoche: 1 [1605/96 (1672%)]\tLoss: 7.398427\n",
      "Train Epoche: 1 [1606/96 (1673%)]\tLoss: 0.000141\n",
      "Train Epoche: 1 [1607/96 (1674%)]\tLoss: 0.379831\n",
      "Train Epoche: 1 [1608/96 (1675%)]\tLoss: 0.991115\n",
      "Train Epoche: 1 [1609/96 (1676%)]\tLoss: 5.873414\n",
      "Train Epoche: 1 [1610/96 (1677%)]\tLoss: 37.669407\n",
      "Train Epoche: 1 [1611/96 (1678%)]\tLoss: 4.222211\n",
      "Train Epoche: 1 [1612/96 (1679%)]\tLoss: 0.002588\n",
      "Train Epoche: 1 [1613/96 (1680%)]\tLoss: 65.803047\n",
      "Train Epoche: 1 [1614/96 (1681%)]\tLoss: 178.890442\n",
      "Train Epoche: 1 [1615/96 (1682%)]\tLoss: 4.149185\n",
      "Train Epoche: 1 [1616/96 (1683%)]\tLoss: 0.358206\n",
      "Train Epoche: 1 [1617/96 (1684%)]\tLoss: 11.176869\n",
      "Train Epoche: 1 [1618/96 (1685%)]\tLoss: 102.940788\n",
      "Train Epoche: 1 [1619/96 (1686%)]\tLoss: 40.122597\n",
      "Train Epoche: 1 [1620/96 (1688%)]\tLoss: 66.718048\n",
      "Train Epoche: 1 [1621/96 (1689%)]\tLoss: 66.995140\n",
      "Train Epoche: 1 [1622/96 (1690%)]\tLoss: 6.903609\n",
      "Train Epoche: 1 [1623/96 (1691%)]\tLoss: 249.672699\n",
      "Train Epoche: 1 [1624/96 (1692%)]\tLoss: 0.000667\n",
      "Train Epoche: 1 [1625/96 (1693%)]\tLoss: 56.122356\n",
      "Train Epoche: 1 [1626/96 (1694%)]\tLoss: 42.557003\n",
      "Train Epoche: 1 [1627/96 (1695%)]\tLoss: 120.661102\n",
      "Train Epoche: 1 [1628/96 (1696%)]\tLoss: 4.609637\n",
      "Train Epoche: 1 [1629/96 (1697%)]\tLoss: 3.443946\n",
      "Train Epoche: 1 [1630/96 (1698%)]\tLoss: 31.571354\n",
      "Train Epoche: 1 [1631/96 (1699%)]\tLoss: 8.513022\n",
      "Train Epoche: 1 [1632/96 (1700%)]\tLoss: 6.496865\n",
      "Train Epoche: 1 [1633/96 (1701%)]\tLoss: 0.006815\n",
      "Train Epoche: 1 [1634/96 (1702%)]\tLoss: 0.329355\n",
      "Train Epoche: 1 [1635/96 (1703%)]\tLoss: 2.863257\n",
      "Train Epoche: 1 [1636/96 (1704%)]\tLoss: 3.478317\n",
      "Train Epoche: 1 [1637/96 (1705%)]\tLoss: 2.886325\n",
      "Train Epoche: 1 [1638/96 (1706%)]\tLoss: 4.058532\n",
      "Train Epoche: 1 [1639/96 (1707%)]\tLoss: 26.369061\n",
      "Train Epoche: 1 [1640/96 (1708%)]\tLoss: 13.518786\n",
      "Train Epoche: 1 [1641/96 (1709%)]\tLoss: 0.053292\n",
      "Train Epoche: 1 [1642/96 (1710%)]\tLoss: 0.203891\n",
      "Train Epoche: 1 [1643/96 (1711%)]\tLoss: 1.181183\n",
      "Train Epoche: 1 [1644/96 (1712%)]\tLoss: 185.872345\n",
      "Train Epoche: 1 [1645/96 (1714%)]\tLoss: 0.633447\n",
      "Train Epoche: 1 [1646/96 (1715%)]\tLoss: 11.225244\n",
      "Train Epoche: 1 [1647/96 (1716%)]\tLoss: 0.008226\n",
      "Train Epoche: 1 [1648/96 (1717%)]\tLoss: 0.300807\n",
      "Train Epoche: 1 [1649/96 (1718%)]\tLoss: 2.198867\n",
      "Train Epoche: 1 [1650/96 (1719%)]\tLoss: 2.508485\n",
      "Train Epoche: 1 [1651/96 (1720%)]\tLoss: 1.365256\n",
      "Train Epoche: 1 [1652/96 (1721%)]\tLoss: 2.423145\n",
      "Train Epoche: 1 [1653/96 (1722%)]\tLoss: 154.436371\n",
      "Train Epoche: 1 [1654/96 (1723%)]\tLoss: 55.761372\n",
      "Train Epoche: 1 [1655/96 (1724%)]\tLoss: 92.634056\n",
      "Train Epoche: 1 [1656/96 (1725%)]\tLoss: 31.042620\n",
      "Train Epoche: 1 [1657/96 (1726%)]\tLoss: 60.940689\n",
      "Train Epoche: 1 [1658/96 (1727%)]\tLoss: 10.228631\n",
      "Train Epoche: 1 [1659/96 (1728%)]\tLoss: 0.534838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1660/96 (1729%)]\tLoss: 3.918903\n",
      "Train Epoche: 1 [1661/96 (1730%)]\tLoss: 0.011570\n",
      "Train Epoche: 1 [1662/96 (1731%)]\tLoss: 17.033796\n",
      "Train Epoche: 1 [1663/96 (1732%)]\tLoss: 12.035370\n",
      "Train Epoche: 1 [1664/96 (1733%)]\tLoss: 8.550048\n",
      "Train Epoche: 1 [1665/96 (1734%)]\tLoss: 0.847995\n",
      "Train Epoche: 1 [1666/96 (1735%)]\tLoss: 0.153015\n",
      "Train Epoche: 1 [1667/96 (1736%)]\tLoss: 0.049619\n",
      "Train Epoche: 1 [1668/96 (1738%)]\tLoss: 16.751263\n",
      "Train Epoche: 1 [1669/96 (1739%)]\tLoss: 1.138801\n",
      "Train Epoche: 1 [1670/96 (1740%)]\tLoss: 1.652252\n",
      "Train Epoche: 1 [1671/96 (1741%)]\tLoss: 7.606684\n",
      "Train Epoche: 1 [1672/96 (1742%)]\tLoss: 88.198845\n",
      "Train Epoche: 1 [1673/96 (1743%)]\tLoss: 100.683388\n",
      "Train Epoche: 1 [1674/96 (1744%)]\tLoss: 36.526871\n",
      "Train Epoche: 1 [1675/96 (1745%)]\tLoss: 8.757628\n",
      "Train Epoche: 1 [1676/96 (1746%)]\tLoss: 26.194218\n",
      "Train Epoche: 1 [1677/96 (1747%)]\tLoss: 16.113539\n",
      "Train Epoche: 1 [1678/96 (1748%)]\tLoss: 133.729385\n",
      "Train Epoche: 1 [1679/96 (1749%)]\tLoss: 9.394521\n",
      "Train Epoche: 1 [1680/96 (1750%)]\tLoss: 6.121445\n",
      "Train Epoche: 1 [1681/96 (1751%)]\tLoss: 0.033985\n",
      "Train Epoche: 1 [1682/96 (1752%)]\tLoss: 2.776206\n",
      "Train Epoche: 1 [1683/96 (1753%)]\tLoss: 30.172525\n",
      "Train Epoche: 1 [1684/96 (1754%)]\tLoss: 0.430862\n",
      "Train Epoche: 1 [1685/96 (1755%)]\tLoss: 357.940430\n",
      "Train Epoche: 1 [1686/96 (1756%)]\tLoss: 0.599691\n",
      "Train Epoche: 1 [1687/96 (1757%)]\tLoss: 1.569718\n",
      "Train Epoche: 1 [1688/96 (1758%)]\tLoss: 12.319982\n",
      "Train Epoche: 1 [1689/96 (1759%)]\tLoss: 4.450493\n",
      "Train Epoche: 1 [1690/96 (1760%)]\tLoss: 9.867314\n",
      "Train Epoche: 1 [1691/96 (1761%)]\tLoss: 10.053678\n",
      "Train Epoche: 1 [1692/96 (1762%)]\tLoss: 19.577656\n",
      "Train Epoche: 1 [1693/96 (1764%)]\tLoss: 3.982628\n",
      "Train Epoche: 1 [1694/96 (1765%)]\tLoss: 45.695129\n",
      "Train Epoche: 1 [1695/96 (1766%)]\tLoss: 1.279973\n",
      "Train Epoche: 1 [1696/96 (1767%)]\tLoss: 0.250624\n",
      "Train Epoche: 1 [1697/96 (1768%)]\tLoss: 160.099472\n",
      "Train Epoche: 1 [1698/96 (1769%)]\tLoss: 94.346825\n",
      "Train Epoche: 1 [1699/96 (1770%)]\tLoss: 98.472557\n",
      "Train Epoche: 1 [1700/96 (1771%)]\tLoss: 16.087116\n",
      "Train Epoche: 1 [1701/96 (1772%)]\tLoss: 0.535425\n",
      "Train Epoche: 1 [1702/96 (1773%)]\tLoss: 0.758442\n",
      "Train Epoche: 1 [1703/96 (1774%)]\tLoss: 3.410765\n",
      "Train Epoche: 1 [1704/96 (1775%)]\tLoss: 1.829775\n",
      "Train Epoche: 1 [1705/96 (1776%)]\tLoss: 206.027649\n",
      "Train Epoche: 1 [1706/96 (1777%)]\tLoss: 3.303456\n",
      "Train Epoche: 1 [1707/96 (1778%)]\tLoss: 29.934895\n",
      "Train Epoche: 1 [1708/96 (1779%)]\tLoss: 33.658436\n",
      "Train Epoche: 1 [1709/96 (1780%)]\tLoss: 8.523507\n",
      "Train Epoche: 1 [1710/96 (1781%)]\tLoss: 13.553818\n",
      "Train Epoche: 1 [1711/96 (1782%)]\tLoss: 0.873060\n",
      "Train Epoche: 1 [1712/96 (1783%)]\tLoss: 14.498704\n",
      "Train Epoche: 1 [1713/96 (1784%)]\tLoss: 0.966872\n",
      "Train Epoche: 1 [1714/96 (1785%)]\tLoss: 3.397948\n",
      "Train Epoche: 1 [1715/96 (1786%)]\tLoss: 80.335083\n",
      "Train Epoche: 1 [1716/96 (1788%)]\tLoss: 1.895320\n",
      "Train Epoche: 1 [1717/96 (1789%)]\tLoss: 0.325075\n",
      "Train Epoche: 1 [1718/96 (1790%)]\tLoss: 2.436808\n",
      "Train Epoche: 1 [1719/96 (1791%)]\tLoss: 1.558016\n",
      "Train Epoche: 1 [1720/96 (1792%)]\tLoss: 37.327190\n",
      "Train Epoche: 1 [1721/96 (1793%)]\tLoss: 28.850985\n",
      "Train Epoche: 1 [1722/96 (1794%)]\tLoss: 0.413804\n",
      "Train Epoche: 1 [1723/96 (1795%)]\tLoss: 2.747620\n",
      "Train Epoche: 1 [1724/96 (1796%)]\tLoss: 0.007268\n",
      "Train Epoche: 1 [1725/96 (1797%)]\tLoss: 2.414874\n",
      "Train Epoche: 1 [1726/96 (1798%)]\tLoss: 14.130116\n",
      "Train Epoche: 1 [1727/96 (1799%)]\tLoss: 85.281540\n",
      "Train Epoche: 1 [1728/96 (1800%)]\tLoss: 16.147003\n",
      "Train Epoche: 1 [1729/96 (1801%)]\tLoss: 4.813997\n",
      "Train Epoche: 1 [1730/96 (1802%)]\tLoss: 82.768272\n",
      "Train Epoche: 1 [1731/96 (1803%)]\tLoss: 41.613903\n",
      "Train Epoche: 1 [1732/96 (1804%)]\tLoss: 0.041509\n",
      "Train Epoche: 1 [1733/96 (1805%)]\tLoss: 2.807381\n",
      "Train Epoche: 1 [1734/96 (1806%)]\tLoss: 15.296788\n",
      "Train Epoche: 1 [1735/96 (1807%)]\tLoss: 0.315900\n",
      "Train Epoche: 1 [1736/96 (1808%)]\tLoss: 0.000066\n",
      "Train Epoche: 1 [1737/96 (1809%)]\tLoss: 6.150327\n",
      "Train Epoche: 1 [1738/96 (1810%)]\tLoss: 80.674652\n",
      "Train Epoche: 1 [1739/96 (1811%)]\tLoss: 3.320899\n",
      "Train Epoche: 1 [1740/96 (1812%)]\tLoss: 0.047519\n",
      "Train Epoche: 1 [1741/96 (1814%)]\tLoss: 33.040646\n",
      "Train Epoche: 1 [1742/96 (1815%)]\tLoss: 19.225492\n",
      "Train Epoche: 1 [1743/96 (1816%)]\tLoss: 53.655090\n",
      "Train Epoche: 1 [1744/96 (1817%)]\tLoss: 0.091017\n",
      "Train Epoche: 1 [1745/96 (1818%)]\tLoss: 2.898277\n",
      "Train Epoche: 1 [1746/96 (1819%)]\tLoss: 32.036774\n",
      "Train Epoche: 1 [1747/96 (1820%)]\tLoss: 0.159311\n",
      "Train Epoche: 1 [1748/96 (1821%)]\tLoss: 2.361070\n",
      "Train Epoche: 1 [1749/96 (1822%)]\tLoss: 10.556364\n",
      "Train Epoche: 1 [1750/96 (1823%)]\tLoss: 11.753253\n",
      "Train Epoche: 1 [1751/96 (1824%)]\tLoss: 0.001384\n",
      "Train Epoche: 1 [1752/96 (1825%)]\tLoss: 1.167896\n",
      "Train Epoche: 1 [1753/96 (1826%)]\tLoss: 0.063775\n",
      "Train Epoche: 1 [1754/96 (1827%)]\tLoss: 76.458298\n",
      "Train Epoche: 1 [1755/96 (1828%)]\tLoss: 0.171596\n",
      "Train Epoche: 1 [1756/96 (1829%)]\tLoss: 113.385971\n",
      "Train Epoche: 1 [1757/96 (1830%)]\tLoss: 7.589710\n",
      "Train Epoche: 1 [1758/96 (1831%)]\tLoss: 2.908983\n",
      "Train Epoche: 1 [1759/96 (1832%)]\tLoss: 68.826950\n",
      "Train Epoche: 1 [1760/96 (1833%)]\tLoss: 5.521346\n",
      "Train Epoche: 1 [1761/96 (1834%)]\tLoss: 3.034511\n",
      "Train Epoche: 1 [1762/96 (1835%)]\tLoss: 5.548565\n",
      "Train Epoche: 1 [1763/96 (1836%)]\tLoss: 364.490448\n",
      "Train Epoche: 1 [1764/96 (1838%)]\tLoss: 256.147614\n",
      "Train Epoche: 1 [1765/96 (1839%)]\tLoss: 35.351460\n",
      "Train Epoche: 1 [1766/96 (1840%)]\tLoss: 115.815308\n",
      "Train Epoche: 1 [1767/96 (1841%)]\tLoss: 14.779078\n",
      "Train Epoche: 1 [1768/96 (1842%)]\tLoss: 3.098846\n",
      "Train Epoche: 1 [1769/96 (1843%)]\tLoss: 17.391422\n",
      "Train Epoche: 1 [1770/96 (1844%)]\tLoss: 132.410690\n",
      "Train Epoche: 1 [1771/96 (1845%)]\tLoss: 152.898407\n",
      "Train Epoche: 1 [1772/96 (1846%)]\tLoss: 2.511058\n",
      "Train Epoche: 1 [1773/96 (1847%)]\tLoss: 60.019550\n",
      "Train Epoche: 1 [1774/96 (1848%)]\tLoss: 7.213359\n",
      "Train Epoche: 1 [1775/96 (1849%)]\tLoss: 1.739802\n",
      "Train Epoche: 1 [1776/96 (1850%)]\tLoss: 2.837732\n",
      "Train Epoche: 1 [1777/96 (1851%)]\tLoss: 0.103738\n",
      "Train Epoche: 1 [1778/96 (1852%)]\tLoss: 19.229631\n",
      "Train Epoche: 1 [1779/96 (1853%)]\tLoss: 23.296740\n",
      "Train Epoche: 1 [1780/96 (1854%)]\tLoss: 1.894459\n",
      "Train Epoche: 1 [1781/96 (1855%)]\tLoss: 9.216434\n",
      "Train Epoche: 1 [1782/96 (1856%)]\tLoss: 0.000149\n",
      "Train Epoche: 1 [1783/96 (1857%)]\tLoss: 41.353428\n",
      "Train Epoche: 1 [1784/96 (1858%)]\tLoss: 17.617872\n",
      "Train Epoche: 1 [1785/96 (1859%)]\tLoss: 0.504063\n",
      "Train Epoche: 1 [1786/96 (1860%)]\tLoss: 4.080658\n",
      "Train Epoche: 1 [1787/96 (1861%)]\tLoss: 0.002720\n",
      "Train Epoche: 1 [1788/96 (1862%)]\tLoss: 117.840065\n",
      "Train Epoche: 1 [1789/96 (1864%)]\tLoss: 8.460475\n",
      "Train Epoche: 1 [1790/96 (1865%)]\tLoss: 67.264442\n",
      "Train Epoche: 1 [1791/96 (1866%)]\tLoss: 0.532414\n",
      "Train Epoche: 1 [1792/96 (1867%)]\tLoss: 42.161404\n",
      "Train Epoche: 1 [1793/96 (1868%)]\tLoss: 11.811502\n",
      "Train Epoche: 1 [1794/96 (1869%)]\tLoss: 24.895311\n",
      "Train Epoche: 1 [1795/96 (1870%)]\tLoss: 11.245926\n",
      "Train Epoche: 1 [1796/96 (1871%)]\tLoss: 14.208094\n",
      "Train Epoche: 1 [1797/96 (1872%)]\tLoss: 0.868120\n",
      "Train Epoche: 1 [1798/96 (1873%)]\tLoss: 0.355054\n",
      "Train Epoche: 1 [1799/96 (1874%)]\tLoss: 5.693110\n",
      "Train Epoche: 1 [1800/96 (1875%)]\tLoss: 9.762812\n",
      "Train Epoche: 1 [1801/96 (1876%)]\tLoss: 5.516721\n",
      "Train Epoche: 1 [1802/96 (1877%)]\tLoss: 6.438579\n",
      "Train Epoche: 1 [1803/96 (1878%)]\tLoss: 5.384372\n",
      "Train Epoche: 1 [1804/96 (1879%)]\tLoss: 17.541403\n",
      "Train Epoche: 1 [1805/96 (1880%)]\tLoss: 158.587219\n",
      "Train Epoche: 1 [1806/96 (1881%)]\tLoss: 0.012436\n",
      "Train Epoche: 1 [1807/96 (1882%)]\tLoss: 0.093169\n",
      "Train Epoche: 1 [1808/96 (1883%)]\tLoss: 2.087448\n",
      "Train Epoche: 1 [1809/96 (1884%)]\tLoss: 30.572533\n",
      "Train Epoche: 1 [1810/96 (1885%)]\tLoss: 15.530000\n",
      "Train Epoche: 1 [1811/96 (1886%)]\tLoss: 8.701019\n",
      "Train Epoche: 1 [1812/96 (1888%)]\tLoss: 1.381710\n",
      "Train Epoche: 1 [1813/96 (1889%)]\tLoss: 48.438957\n",
      "Train Epoche: 1 [1814/96 (1890%)]\tLoss: 9.751998\n",
      "Train Epoche: 1 [1815/96 (1891%)]\tLoss: 1.035205\n",
      "Train Epoche: 1 [1816/96 (1892%)]\tLoss: 14.667263\n",
      "Train Epoche: 1 [1817/96 (1893%)]\tLoss: 23.489222\n",
      "Train Epoche: 1 [1818/96 (1894%)]\tLoss: 37.525696\n",
      "Train Epoche: 1 [1819/96 (1895%)]\tLoss: 3.660452\n",
      "Train Epoche: 1 [1820/96 (1896%)]\tLoss: 235.411819\n",
      "Train Epoche: 1 [1821/96 (1897%)]\tLoss: 1.420026\n",
      "Train Epoche: 1 [1822/96 (1898%)]\tLoss: 3.105646\n",
      "Train Epoche: 1 [1823/96 (1899%)]\tLoss: 0.680493\n",
      "Train Epoche: 1 [1824/96 (1900%)]\tLoss: 9.499475\n",
      "Train Epoche: 1 [1825/96 (1901%)]\tLoss: 49.494335\n",
      "Train Epoche: 1 [1826/96 (1902%)]\tLoss: 61.689857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1827/96 (1903%)]\tLoss: 4.295791\n",
      "Train Epoche: 1 [1828/96 (1904%)]\tLoss: 0.367076\n",
      "Train Epoche: 1 [1829/96 (1905%)]\tLoss: 41.260715\n",
      "Train Epoche: 1 [1830/96 (1906%)]\tLoss: 4.708048\n",
      "Train Epoche: 1 [1831/96 (1907%)]\tLoss: 25.391537\n",
      "Train Epoche: 1 [1832/96 (1908%)]\tLoss: 28.245075\n",
      "Train Epoche: 1 [1833/96 (1909%)]\tLoss: 140.664642\n",
      "Train Epoche: 1 [1834/96 (1910%)]\tLoss: 6.764924\n",
      "Train Epoche: 1 [1835/96 (1911%)]\tLoss: 2.898835\n",
      "Train Epoche: 1 [1836/96 (1912%)]\tLoss: 4.276182\n",
      "Train Epoche: 1 [1837/96 (1914%)]\tLoss: 6.939367\n",
      "Train Epoche: 1 [1838/96 (1915%)]\tLoss: 0.781481\n",
      "Train Epoche: 1 [1839/96 (1916%)]\tLoss: 1.084226\n",
      "Train Epoche: 1 [1840/96 (1917%)]\tLoss: 2.586357\n",
      "Train Epoche: 1 [1841/96 (1918%)]\tLoss: 64.906601\n",
      "Train Epoche: 1 [1842/96 (1919%)]\tLoss: 13.541184\n",
      "Train Epoche: 1 [1843/96 (1920%)]\tLoss: 0.074324\n",
      "Train Epoche: 1 [1844/96 (1921%)]\tLoss: 44.052441\n",
      "Train Epoche: 1 [1845/96 (1922%)]\tLoss: 0.062655\n",
      "Train Epoche: 1 [1846/96 (1923%)]\tLoss: 8.735472\n",
      "Train Epoche: 1 [1847/96 (1924%)]\tLoss: 4.732779\n",
      "Train Epoche: 1 [1848/96 (1925%)]\tLoss: 1.994818\n",
      "Train Epoche: 1 [1849/96 (1926%)]\tLoss: 3.079052\n",
      "Train Epoche: 1 [1850/96 (1927%)]\tLoss: 5.813170\n",
      "Train Epoche: 1 [1851/96 (1928%)]\tLoss: 0.166028\n",
      "Train Epoche: 1 [1852/96 (1929%)]\tLoss: 5.392479\n",
      "Train Epoche: 1 [1853/96 (1930%)]\tLoss: 3.866040\n",
      "Train Epoche: 1 [1854/96 (1931%)]\tLoss: 248.361008\n",
      "Train Epoche: 1 [1855/96 (1932%)]\tLoss: 22.434593\n",
      "Train Epoche: 1 [1856/96 (1933%)]\tLoss: 1.802032\n",
      "Train Epoche: 1 [1857/96 (1934%)]\tLoss: 1.253439\n",
      "Train Epoche: 1 [1858/96 (1935%)]\tLoss: 0.295478\n",
      "Train Epoche: 1 [1859/96 (1936%)]\tLoss: 0.382213\n",
      "Train Epoche: 1 [1860/96 (1938%)]\tLoss: 1.029614\n",
      "Train Epoche: 1 [1861/96 (1939%)]\tLoss: 4.606339\n",
      "Train Epoche: 1 [1862/96 (1940%)]\tLoss: 10.598397\n",
      "Train Epoche: 1 [1863/96 (1941%)]\tLoss: 4.226947\n",
      "Train Epoche: 1 [1864/96 (1942%)]\tLoss: 0.144100\n",
      "Train Epoche: 1 [1865/96 (1943%)]\tLoss: 2.067215\n",
      "Train Epoche: 1 [1866/96 (1944%)]\tLoss: 5.752887\n",
      "Train Epoche: 1 [1867/96 (1945%)]\tLoss: 9.385613\n",
      "Train Epoche: 1 [1868/96 (1946%)]\tLoss: 5.113797\n",
      "Train Epoche: 1 [1869/96 (1947%)]\tLoss: 2.962690\n",
      "Train Epoche: 1 [1870/96 (1948%)]\tLoss: 1.893300\n",
      "Train Epoche: 1 [1871/96 (1949%)]\tLoss: 13.442251\n",
      "Train Epoche: 1 [1872/96 (1950%)]\tLoss: 0.120602\n",
      "Train Epoche: 1 [1873/96 (1951%)]\tLoss: 0.947166\n",
      "Train Epoche: 1 [1874/96 (1952%)]\tLoss: 0.047901\n",
      "Train Epoche: 1 [1875/96 (1953%)]\tLoss: 2.854726\n",
      "Train Epoche: 1 [1876/96 (1954%)]\tLoss: 9.105945\n",
      "Train Epoche: 1 [1877/96 (1955%)]\tLoss: 9.115795\n",
      "Train Epoche: 1 [1878/96 (1956%)]\tLoss: 1.026962\n",
      "Train Epoche: 1 [1879/96 (1957%)]\tLoss: 36.363289\n",
      "Train Epoche: 1 [1880/96 (1958%)]\tLoss: 4.140254\n",
      "Train Epoche: 1 [1881/96 (1959%)]\tLoss: 12.282857\n",
      "Train Epoche: 1 [1882/96 (1960%)]\tLoss: 0.251014\n",
      "Train Epoche: 1 [1883/96 (1961%)]\tLoss: 4.066759\n",
      "Train Epoche: 1 [1884/96 (1962%)]\tLoss: 320.315979\n",
      "Train Epoche: 1 [1885/96 (1964%)]\tLoss: 0.675222\n",
      "Train Epoche: 1 [1886/96 (1965%)]\tLoss: 2.756258\n",
      "Train Epoche: 1 [1887/96 (1966%)]\tLoss: 0.029727\n",
      "Train Epoche: 1 [1888/96 (1967%)]\tLoss: 0.782332\n",
      "Train Epoche: 1 [1889/96 (1968%)]\tLoss: 0.038838\n",
      "Train Epoche: 1 [1890/96 (1969%)]\tLoss: 1.694352\n",
      "Train Epoche: 1 [1891/96 (1970%)]\tLoss: 0.032127\n",
      "Train Epoche: 1 [1892/96 (1971%)]\tLoss: 0.430311\n",
      "Train Epoche: 1 [1893/96 (1972%)]\tLoss: 33.656544\n",
      "Train Epoche: 1 [1894/96 (1973%)]\tLoss: 0.006289\n",
      "Train Epoche: 1 [1895/96 (1974%)]\tLoss: 3.586239\n",
      "Train Epoche: 1 [1896/96 (1975%)]\tLoss: 184.640121\n",
      "Train Epoche: 1 [1897/96 (1976%)]\tLoss: 7.484877\n",
      "Train Epoche: 1 [1898/96 (1977%)]\tLoss: 9.952633\n",
      "Train Epoche: 1 [1899/96 (1978%)]\tLoss: 4.530464\n",
      "Train Epoche: 1 [1900/96 (1979%)]\tLoss: 10.432860\n",
      "Train Epoche: 1 [1901/96 (1980%)]\tLoss: 6.541821\n",
      "Train Epoche: 1 [1902/96 (1981%)]\tLoss: 48.768761\n",
      "Train Epoche: 1 [1903/96 (1982%)]\tLoss: 12.419693\n",
      "Train Epoche: 1 [1904/96 (1983%)]\tLoss: 7.864738\n",
      "Train Epoche: 1 [1905/96 (1984%)]\tLoss: 7.521544\n",
      "Train Epoche: 1 [1906/96 (1985%)]\tLoss: 11.231028\n",
      "Train Epoche: 1 [1907/96 (1986%)]\tLoss: 24.264374\n",
      "Train Epoche: 1 [1908/96 (1988%)]\tLoss: 0.198533\n",
      "Train Epoche: 1 [1909/96 (1989%)]\tLoss: 2.004404\n",
      "Train Epoche: 1 [1910/96 (1990%)]\tLoss: 13.354193\n",
      "Train Epoche: 1 [1911/96 (1991%)]\tLoss: 0.089821\n",
      "Train Epoche: 1 [1912/96 (1992%)]\tLoss: 80.802231\n",
      "Train Epoche: 1 [1913/96 (1993%)]\tLoss: 36.154903\n",
      "Train Epoche: 1 [1914/96 (1994%)]\tLoss: 17.172697\n",
      "Train Epoche: 1 [1915/96 (1995%)]\tLoss: 6.835114\n",
      "Train Epoche: 1 [1916/96 (1996%)]\tLoss: 0.219728\n",
      "Train Epoche: 1 [1917/96 (1997%)]\tLoss: 30.504421\n",
      "Train Epoche: 1 [1918/96 (1998%)]\tLoss: 0.027788\n",
      "Train Epoche: 1 [1919/96 (1999%)]\tLoss: 0.007937\n",
      "Train Epoche: 1 [1920/96 (2000%)]\tLoss: 0.412953\n",
      "Train Epoche: 1 [1921/96 (2001%)]\tLoss: 2.404802\n",
      "Train Epoche: 1 [1922/96 (2002%)]\tLoss: 1.175125\n",
      "Train Epoche: 1 [1923/96 (2003%)]\tLoss: 17.995710\n",
      "Train Epoche: 1 [1924/96 (2004%)]\tLoss: 272.192413\n",
      "Train Epoche: 1 [1925/96 (2005%)]\tLoss: 0.152562\n",
      "Train Epoche: 1 [1926/96 (2006%)]\tLoss: 6.173024\n",
      "Train Epoche: 1 [1927/96 (2007%)]\tLoss: 0.145161\n",
      "Train Epoche: 1 [1928/96 (2008%)]\tLoss: 8.221480\n",
      "Train Epoche: 1 [1929/96 (2009%)]\tLoss: 8.514186\n",
      "Train Epoche: 1 [1930/96 (2010%)]\tLoss: 0.014261\n",
      "Train Epoche: 1 [1931/96 (2011%)]\tLoss: 0.560503\n",
      "Train Epoche: 1 [1932/96 (2012%)]\tLoss: 17.019480\n",
      "Train Epoche: 1 [1933/96 (2014%)]\tLoss: 12.813600\n",
      "Train Epoche: 1 [1934/96 (2015%)]\tLoss: 0.088629\n",
      "Train Epoche: 1 [1935/96 (2016%)]\tLoss: 2.772949\n",
      "Train Epoche: 1 [1936/96 (2017%)]\tLoss: 0.798737\n",
      "Train Epoche: 1 [1937/96 (2018%)]\tLoss: 195.568512\n",
      "Train Epoche: 1 [1938/96 (2019%)]\tLoss: 3.209589\n",
      "Train Epoche: 1 [1939/96 (2020%)]\tLoss: 8.250393\n",
      "Train Epoche: 1 [1940/96 (2021%)]\tLoss: 12.249386\n",
      "Train Epoche: 1 [1941/96 (2022%)]\tLoss: 4.440507\n",
      "Train Epoche: 1 [1942/96 (2023%)]\tLoss: 2.007121\n",
      "Train Epoche: 1 [1943/96 (2024%)]\tLoss: 2.092965\n",
      "Train Epoche: 1 [1944/96 (2025%)]\tLoss: 1.201234\n",
      "Train Epoche: 1 [1945/96 (2026%)]\tLoss: 2.396736\n",
      "Train Epoche: 1 [1946/96 (2027%)]\tLoss: 2.178573\n",
      "Train Epoche: 1 [1947/96 (2028%)]\tLoss: 9.459210\n",
      "Train Epoche: 1 [1948/96 (2029%)]\tLoss: 2.230131\n",
      "Train Epoche: 1 [1949/96 (2030%)]\tLoss: 0.909365\n",
      "Train Epoche: 1 [1950/96 (2031%)]\tLoss: 18.289886\n",
      "Train Epoche: 1 [1951/96 (2032%)]\tLoss: 2.548010\n",
      "Train Epoche: 1 [1952/96 (2033%)]\tLoss: 2.031724\n",
      "Train Epoche: 1 [1953/96 (2034%)]\tLoss: 0.010074\n",
      "Train Epoche: 1 [1954/96 (2035%)]\tLoss: 18.201611\n",
      "Train Epoche: 1 [1955/96 (2036%)]\tLoss: 85.188683\n",
      "Train Epoche: 1 [1956/96 (2038%)]\tLoss: 0.127441\n",
      "Train Epoche: 1 [1957/96 (2039%)]\tLoss: 1.144079\n",
      "Train Epoche: 1 [1958/96 (2040%)]\tLoss: 2.622005\n",
      "Train Epoche: 1 [1959/96 (2041%)]\tLoss: 0.332050\n",
      "Train Epoche: 1 [1960/96 (2042%)]\tLoss: 1.717358\n",
      "Train Epoche: 1 [1961/96 (2043%)]\tLoss: 48.135220\n",
      "Train Epoche: 1 [1962/96 (2044%)]\tLoss: 10.835592\n",
      "Train Epoche: 1 [1963/96 (2045%)]\tLoss: 40.962704\n",
      "Train Epoche: 1 [1964/96 (2046%)]\tLoss: 16.004761\n",
      "Train Epoche: 1 [1965/96 (2047%)]\tLoss: 0.701708\n",
      "Train Epoche: 1 [1966/96 (2048%)]\tLoss: 2.656697\n",
      "Train Epoche: 1 [1967/96 (2049%)]\tLoss: 70.520874\n",
      "Train Epoche: 1 [1968/96 (2050%)]\tLoss: 0.003110\n",
      "Train Epoche: 1 [1969/96 (2051%)]\tLoss: 4.092570\n",
      "Train Epoche: 1 [1970/96 (2052%)]\tLoss: 0.020470\n",
      "Train Epoche: 1 [1971/96 (2053%)]\tLoss: 2.848290\n",
      "Train Epoche: 1 [1972/96 (2054%)]\tLoss: 8.078704\n",
      "Train Epoche: 1 [1973/96 (2055%)]\tLoss: 8.841624\n",
      "Train Epoche: 1 [1974/96 (2056%)]\tLoss: 28.361219\n",
      "Train Epoche: 1 [1975/96 (2057%)]\tLoss: 0.510684\n",
      "Train Epoche: 1 [1976/96 (2058%)]\tLoss: 0.004021\n",
      "Train Epoche: 1 [1977/96 (2059%)]\tLoss: 12.511946\n",
      "Train Epoche: 1 [1978/96 (2060%)]\tLoss: 0.001976\n",
      "Train Epoche: 1 [1979/96 (2061%)]\tLoss: 0.124118\n",
      "Train Epoche: 1 [1980/96 (2062%)]\tLoss: 0.670159\n",
      "Train Epoche: 1 [1981/96 (2064%)]\tLoss: 65.584869\n",
      "Train Epoche: 1 [1982/96 (2065%)]\tLoss: 1.652825\n",
      "Train Epoche: 1 [1983/96 (2066%)]\tLoss: 1.271535\n",
      "Train Epoche: 1 [1984/96 (2067%)]\tLoss: 9.576695\n",
      "Train Epoche: 1 [1985/96 (2068%)]\tLoss: 2.369004\n",
      "Train Epoche: 1 [1986/96 (2069%)]\tLoss: 7.209637\n",
      "Train Epoche: 1 [1987/96 (2070%)]\tLoss: 0.251108\n",
      "Train Epoche: 1 [1988/96 (2071%)]\tLoss: 1.235438\n",
      "Train Epoche: 1 [1989/96 (2072%)]\tLoss: 55.432999\n",
      "Train Epoche: 1 [1990/96 (2073%)]\tLoss: 0.322572\n",
      "Train Epoche: 1 [1991/96 (2074%)]\tLoss: 0.940242\n",
      "Train Epoche: 1 [1992/96 (2075%)]\tLoss: 5.721834\n",
      "Train Epoche: 1 [1993/96 (2076%)]\tLoss: 3.293370\n",
      "Train Epoche: 1 [1994/96 (2077%)]\tLoss: 38.555088\n",
      "Train Epoche: 1 [1995/96 (2078%)]\tLoss: 19.726360\n",
      "Train Epoche: 1 [1996/96 (2079%)]\tLoss: 0.755298\n",
      "Train Epoche: 1 [1997/96 (2080%)]\tLoss: 2.432935\n",
      "Train Epoche: 1 [1998/96 (2081%)]\tLoss: 8.880863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1999/96 (2082%)]\tLoss: 9.435318\n",
      "Train Epoche: 1 [2000/96 (2083%)]\tLoss: 21.090992\n",
      "Train Epoche: 1 [2001/96 (2084%)]\tLoss: 0.036275\n",
      "Train Epoche: 1 [2002/96 (2085%)]\tLoss: 7.135013\n",
      "Train Epoche: 1 [2003/96 (2086%)]\tLoss: 0.119305\n",
      "Train Epoche: 1 [2004/96 (2088%)]\tLoss: 16.061384\n",
      "Train Epoche: 1 [2005/96 (2089%)]\tLoss: 0.093579\n",
      "Train Epoche: 1 [2006/96 (2090%)]\tLoss: 0.080829\n",
      "Train Epoche: 1 [2007/96 (2091%)]\tLoss: 0.544386\n",
      "Train Epoche: 1 [2008/96 (2092%)]\tLoss: 18.681650\n",
      "Train Epoche: 1 [2009/96 (2093%)]\tLoss: 3.160978\n",
      "Train Epoche: 1 [2010/96 (2094%)]\tLoss: 0.241116\n",
      "Train Epoche: 1 [2011/96 (2095%)]\tLoss: 22.722010\n",
      "Train Epoche: 1 [2012/96 (2096%)]\tLoss: 0.002093\n",
      "Train Epoche: 1 [2013/96 (2097%)]\tLoss: 3.382404\n",
      "Train Epoche: 1 [2014/96 (2098%)]\tLoss: 9.230198\n",
      "Train Epoche: 1 [2015/96 (2099%)]\tLoss: 1.835955\n",
      "Train Epoche: 1 [2016/96 (2100%)]\tLoss: 0.390942\n",
      "Train Epoche: 1 [2017/96 (2101%)]\tLoss: 5.539013\n",
      "Train Epoche: 1 [2018/96 (2102%)]\tLoss: 22.222404\n",
      "Train Epoche: 1 [2019/96 (2103%)]\tLoss: 0.037496\n",
      "Train Epoche: 1 [2020/96 (2104%)]\tLoss: 39.286194\n",
      "Train Epoche: 1 [2021/96 (2105%)]\tLoss: 0.525807\n",
      "Train Epoche: 1 [2022/96 (2106%)]\tLoss: 3.146190\n",
      "Train Epoche: 1 [2023/96 (2107%)]\tLoss: 0.657401\n",
      "Train Epoche: 1 [2024/96 (2108%)]\tLoss: 5.154591\n",
      "Train Epoche: 1 [2025/96 (2109%)]\tLoss: 18.591841\n",
      "Train Epoche: 1 [2026/96 (2110%)]\tLoss: 63.822678\n",
      "Train Epoche: 2 [0/96 (0%)]\tLoss: 37.685375\n",
      "Train Epoche: 2 [1/96 (1%)]\tLoss: 5.943445\n",
      "Train Epoche: 2 [2/96 (2%)]\tLoss: 13.584167\n",
      "Train Epoche: 2 [3/96 (3%)]\tLoss: 39.562527\n",
      "Train Epoche: 2 [4/96 (4%)]\tLoss: 102.517090\n",
      "Train Epoche: 2 [5/96 (5%)]\tLoss: 0.443732\n",
      "Train Epoche: 2 [6/96 (6%)]\tLoss: 5.892732\n",
      "Train Epoche: 2 [7/96 (7%)]\tLoss: 44.750622\n",
      "Train Epoche: 2 [8/96 (8%)]\tLoss: 14.801295\n",
      "Train Epoche: 2 [9/96 (9%)]\tLoss: 0.100435\n",
      "Train Epoche: 2 [10/96 (10%)]\tLoss: 332.174133\n",
      "Train Epoche: 2 [11/96 (11%)]\tLoss: 11.450080\n",
      "Train Epoche: 2 [12/96 (12%)]\tLoss: 10.410620\n",
      "Train Epoche: 2 [13/96 (14%)]\tLoss: 0.255516\n",
      "Train Epoche: 2 [14/96 (15%)]\tLoss: 3.315058\n",
      "Train Epoche: 2 [15/96 (16%)]\tLoss: 8.633614\n",
      "Train Epoche: 2 [16/96 (17%)]\tLoss: 5.476181\n",
      "Train Epoche: 2 [17/96 (18%)]\tLoss: 8.764265\n",
      "Train Epoche: 2 [18/96 (19%)]\tLoss: 3.976719\n",
      "Train Epoche: 2 [19/96 (20%)]\tLoss: 1.252858\n",
      "Train Epoche: 2 [20/96 (21%)]\tLoss: 38.193707\n",
      "Train Epoche: 2 [21/96 (22%)]\tLoss: 5.357643\n",
      "Train Epoche: 2 [22/96 (23%)]\tLoss: 2.180997\n",
      "Train Epoche: 2 [23/96 (24%)]\tLoss: 4.458037\n",
      "Train Epoche: 2 [24/96 (25%)]\tLoss: 105.971184\n",
      "Train Epoche: 2 [25/96 (26%)]\tLoss: 7.326448\n",
      "Train Epoche: 2 [26/96 (27%)]\tLoss: 91.796356\n",
      "Train Epoche: 2 [27/96 (28%)]\tLoss: 4.968346\n",
      "Train Epoche: 2 [28/96 (29%)]\tLoss: 0.015344\n",
      "Train Epoche: 2 [29/96 (30%)]\tLoss: 81.773521\n",
      "Train Epoche: 2 [30/96 (31%)]\tLoss: 8.822427\n",
      "Train Epoche: 2 [31/96 (32%)]\tLoss: 4.498626\n",
      "Train Epoche: 2 [32/96 (33%)]\tLoss: 6.195901\n",
      "Train Epoche: 2 [33/96 (34%)]\tLoss: 5.236118\n",
      "Train Epoche: 2 [34/96 (35%)]\tLoss: 16.949860\n",
      "Train Epoche: 2 [35/96 (36%)]\tLoss: 8.610787\n",
      "Train Epoche: 2 [36/96 (38%)]\tLoss: 36.564007\n",
      "Train Epoche: 2 [37/96 (39%)]\tLoss: 0.154963\n",
      "Train Epoche: 2 [38/96 (40%)]\tLoss: 28.776735\n",
      "Train Epoche: 2 [39/96 (41%)]\tLoss: 26.510042\n",
      "Train Epoche: 2 [40/96 (42%)]\tLoss: 16.978556\n",
      "Train Epoche: 2 [41/96 (43%)]\tLoss: 1.903958\n",
      "Train Epoche: 2 [42/96 (44%)]\tLoss: 18.993235\n",
      "Train Epoche: 2 [43/96 (45%)]\tLoss: 0.222001\n",
      "Train Epoche: 2 [44/96 (46%)]\tLoss: 393.790619\n",
      "Train Epoche: 2 [45/96 (47%)]\tLoss: 0.674558\n",
      "Train Epoche: 2 [46/96 (48%)]\tLoss: 8.068727\n",
      "Train Epoche: 2 [47/96 (49%)]\tLoss: 0.351088\n",
      "Train Epoche: 2 [48/96 (50%)]\tLoss: 3.396956\n",
      "Train Epoche: 2 [49/96 (51%)]\tLoss: 4.962260\n",
      "Train Epoche: 2 [50/96 (52%)]\tLoss: 108.060692\n",
      "Train Epoche: 2 [51/96 (53%)]\tLoss: 3.063186\n",
      "Train Epoche: 2 [52/96 (54%)]\tLoss: 0.182493\n",
      "Train Epoche: 2 [53/96 (55%)]\tLoss: 0.632043\n",
      "Train Epoche: 2 [54/96 (56%)]\tLoss: 53.770863\n",
      "Train Epoche: 2 [55/96 (57%)]\tLoss: 3.787555\n",
      "Train Epoche: 2 [56/96 (58%)]\tLoss: 15.727625\n",
      "Train Epoche: 2 [57/96 (59%)]\tLoss: 1.115663\n",
      "Train Epoche: 2 [58/96 (60%)]\tLoss: 0.382194\n",
      "Train Epoche: 2 [59/96 (61%)]\tLoss: 0.657354\n",
      "Train Epoche: 2 [60/96 (62%)]\tLoss: 18.114141\n",
      "Train Epoche: 2 [61/96 (64%)]\tLoss: 2.735742\n",
      "Train Epoche: 2 [62/96 (65%)]\tLoss: 30.728012\n",
      "Train Epoche: 2 [63/96 (66%)]\tLoss: 3.193917\n",
      "Train Epoche: 2 [64/96 (67%)]\tLoss: 1.874416\n",
      "Train Epoche: 2 [65/96 (68%)]\tLoss: 1.942930\n",
      "Train Epoche: 2 [66/96 (69%)]\tLoss: 111.486481\n",
      "Train Epoche: 2 [67/96 (70%)]\tLoss: 0.070305\n",
      "Train Epoche: 2 [68/96 (71%)]\tLoss: 14.360103\n",
      "Train Epoche: 2 [69/96 (72%)]\tLoss: 5.388507\n",
      "Train Epoche: 2 [70/96 (73%)]\tLoss: 69.008789\n",
      "Train Epoche: 2 [71/96 (74%)]\tLoss: 3.214157\n",
      "Train Epoche: 2 [72/96 (75%)]\tLoss: 5.415367\n",
      "Train Epoche: 2 [73/96 (76%)]\tLoss: 0.513738\n",
      "Train Epoche: 2 [74/96 (77%)]\tLoss: 0.071510\n",
      "Train Epoche: 2 [75/96 (78%)]\tLoss: 4.639800\n",
      "Train Epoche: 2 [76/96 (79%)]\tLoss: 49.471874\n",
      "Train Epoche: 2 [77/96 (80%)]\tLoss: 0.855253\n",
      "Train Epoche: 2 [78/96 (81%)]\tLoss: 1.007682\n",
      "Train Epoche: 2 [79/96 (82%)]\tLoss: 0.135878\n",
      "Train Epoche: 2 [80/96 (83%)]\tLoss: 0.026095\n",
      "Train Epoche: 2 [81/96 (84%)]\tLoss: 33.848957\n",
      "Train Epoche: 2 [82/96 (85%)]\tLoss: 6.370577\n",
      "Train Epoche: 2 [83/96 (86%)]\tLoss: 72.345932\n",
      "Train Epoche: 2 [84/96 (88%)]\tLoss: 54.776451\n",
      "Train Epoche: 2 [85/96 (89%)]\tLoss: 7.860645\n",
      "Train Epoche: 2 [86/96 (90%)]\tLoss: 4.919120\n",
      "Train Epoche: 2 [87/96 (91%)]\tLoss: 8.561826\n",
      "Train Epoche: 2 [88/96 (92%)]\tLoss: 19.377548\n",
      "Train Epoche: 2 [89/96 (93%)]\tLoss: 0.937603\n",
      "Train Epoche: 2 [90/96 (94%)]\tLoss: 6.563923\n",
      "Train Epoche: 2 [91/96 (95%)]\tLoss: 3.515480\n",
      "Train Epoche: 2 [92/96 (96%)]\tLoss: 3.680503\n",
      "Train Epoche: 2 [93/96 (97%)]\tLoss: 13.436636\n",
      "Train Epoche: 2 [94/96 (98%)]\tLoss: 2.818013\n",
      "Train Epoche: 2 [95/96 (99%)]\tLoss: 4.583435\n",
      "Train Epoche: 2 [96/96 (100%)]\tLoss: 36.464630\n",
      "Train Epoche: 2 [97/96 (101%)]\tLoss: 233.658066\n",
      "Train Epoche: 2 [98/96 (102%)]\tLoss: 0.032673\n",
      "Train Epoche: 2 [99/96 (103%)]\tLoss: 152.557663\n",
      "Train Epoche: 2 [100/96 (104%)]\tLoss: 3.800016\n",
      "Train Epoche: 2 [101/96 (105%)]\tLoss: 4.734902\n",
      "Train Epoche: 2 [102/96 (106%)]\tLoss: 6.699271\n",
      "Train Epoche: 2 [103/96 (107%)]\tLoss: 0.931903\n",
      "Train Epoche: 2 [104/96 (108%)]\tLoss: 0.074007\n",
      "Train Epoche: 2 [105/96 (109%)]\tLoss: 22.950432\n",
      "Train Epoche: 2 [106/96 (110%)]\tLoss: 0.102025\n",
      "Train Epoche: 2 [107/96 (111%)]\tLoss: 5.843363\n",
      "Train Epoche: 2 [108/96 (112%)]\tLoss: 222.207321\n",
      "Train Epoche: 2 [109/96 (114%)]\tLoss: 1.311408\n",
      "Train Epoche: 2 [110/96 (115%)]\tLoss: 0.133093\n",
      "Train Epoche: 2 [111/96 (116%)]\tLoss: 13.084080\n",
      "Train Epoche: 2 [112/96 (117%)]\tLoss: 14.642598\n",
      "Train Epoche: 2 [113/96 (118%)]\tLoss: 6.277561\n",
      "Train Epoche: 2 [114/96 (119%)]\tLoss: 4.273528\n",
      "Train Epoche: 2 [115/96 (120%)]\tLoss: 148.262466\n",
      "Train Epoche: 2 [116/96 (121%)]\tLoss: 0.108258\n",
      "Train Epoche: 2 [117/96 (122%)]\tLoss: 0.394460\n",
      "Train Epoche: 2 [118/96 (123%)]\tLoss: 1.154387\n",
      "Train Epoche: 2 [119/96 (124%)]\tLoss: 1.203644\n",
      "Train Epoche: 2 [120/96 (125%)]\tLoss: 0.784803\n",
      "Train Epoche: 2 [121/96 (126%)]\tLoss: 0.532155\n",
      "Train Epoche: 2 [122/96 (127%)]\tLoss: 5.963159\n",
      "Train Epoche: 2 [123/96 (128%)]\tLoss: 107.879051\n",
      "Train Epoche: 2 [124/96 (129%)]\tLoss: 3.107811\n",
      "Train Epoche: 2 [125/96 (130%)]\tLoss: 68.269188\n",
      "Train Epoche: 2 [126/96 (131%)]\tLoss: 1.031894\n",
      "Train Epoche: 2 [127/96 (132%)]\tLoss: 5.297084\n",
      "Train Epoche: 2 [128/96 (133%)]\tLoss: 171.630020\n",
      "Train Epoche: 2 [129/96 (134%)]\tLoss: 6.576175\n",
      "Train Epoche: 2 [130/96 (135%)]\tLoss: 7.231794\n",
      "Train Epoche: 2 [131/96 (136%)]\tLoss: 1.975330\n",
      "Train Epoche: 2 [132/96 (138%)]\tLoss: 2.834167\n",
      "Train Epoche: 2 [133/96 (139%)]\tLoss: 0.414828\n",
      "Train Epoche: 2 [134/96 (140%)]\tLoss: 2.818106\n",
      "Train Epoche: 2 [135/96 (141%)]\tLoss: 116.049141\n",
      "Train Epoche: 2 [136/96 (142%)]\tLoss: 11.803323\n",
      "Train Epoche: 2 [137/96 (143%)]\tLoss: 20.859074\n",
      "Train Epoche: 2 [138/96 (144%)]\tLoss: 6.275382\n",
      "Train Epoche: 2 [139/96 (145%)]\tLoss: 66.915009\n",
      "Train Epoche: 2 [140/96 (146%)]\tLoss: 12.967035\n",
      "Train Epoche: 2 [141/96 (147%)]\tLoss: 24.774965\n",
      "Train Epoche: 2 [142/96 (148%)]\tLoss: 90.558861\n",
      "Train Epoche: 2 [143/96 (149%)]\tLoss: 15.347423\n",
      "Train Epoche: 2 [144/96 (150%)]\tLoss: 58.012741\n",
      "Train Epoche: 2 [145/96 (151%)]\tLoss: 1.587983\n",
      "Train Epoche: 2 [146/96 (152%)]\tLoss: 0.019705\n",
      "Train Epoche: 2 [147/96 (153%)]\tLoss: 62.023300\n",
      "Train Epoche: 2 [148/96 (154%)]\tLoss: 389.990631\n",
      "Train Epoche: 2 [149/96 (155%)]\tLoss: 0.244933\n",
      "Train Epoche: 2 [150/96 (156%)]\tLoss: 2.114753\n",
      "Train Epoche: 2 [151/96 (157%)]\tLoss: 0.128966\n",
      "Train Epoche: 2 [152/96 (158%)]\tLoss: 12.530276\n",
      "Train Epoche: 2 [153/96 (159%)]\tLoss: 14.081203\n",
      "Train Epoche: 2 [154/96 (160%)]\tLoss: 40.056911\n",
      "Train Epoche: 2 [155/96 (161%)]\tLoss: 0.140168\n",
      "Train Epoche: 2 [156/96 (162%)]\tLoss: 75.502106\n",
      "Train Epoche: 2 [157/96 (164%)]\tLoss: 1.599886\n",
      "Train Epoche: 2 [158/96 (165%)]\tLoss: 1.635793\n",
      "Train Epoche: 2 [159/96 (166%)]\tLoss: 121.681999\n",
      "Train Epoche: 2 [160/96 (167%)]\tLoss: 2.587099\n",
      "Train Epoche: 2 [161/96 (168%)]\tLoss: 6.395871\n",
      "Train Epoche: 2 [162/96 (169%)]\tLoss: 12.196273\n",
      "Train Epoche: 2 [163/96 (170%)]\tLoss: 7.849174\n",
      "Train Epoche: 2 [164/96 (171%)]\tLoss: 12.470336\n",
      "Train Epoche: 2 [165/96 (172%)]\tLoss: 0.001142\n",
      "Train Epoche: 2 [166/96 (173%)]\tLoss: 65.986649\n",
      "Train Epoche: 2 [167/96 (174%)]\tLoss: 3.623783\n",
      "Train Epoche: 2 [168/96 (175%)]\tLoss: 2.220450\n",
      "Train Epoche: 2 [169/96 (176%)]\tLoss: 0.054562\n",
      "Train Epoche: 2 [170/96 (177%)]\tLoss: 0.026111\n",
      "Train Epoche: 2 [171/96 (178%)]\tLoss: 0.000638\n",
      "Train Epoche: 2 [172/96 (179%)]\tLoss: 6.581586\n",
      "Train Epoche: 2 [173/96 (180%)]\tLoss: 3.397635\n",
      "Train Epoche: 2 [174/96 (181%)]\tLoss: 0.069560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [175/96 (182%)]\tLoss: 14.269645\n",
      "Train Epoche: 2 [176/96 (183%)]\tLoss: 0.102667\n",
      "Train Epoche: 2 [177/96 (184%)]\tLoss: 7.009807\n",
      "Train Epoche: 2 [178/96 (185%)]\tLoss: 3.800698\n",
      "Train Epoche: 2 [179/96 (186%)]\tLoss: 9.230661\n",
      "Train Epoche: 2 [180/96 (188%)]\tLoss: 0.429582\n",
      "Train Epoche: 2 [181/96 (189%)]\tLoss: 6.106580\n",
      "Train Epoche: 2 [182/96 (190%)]\tLoss: 0.105628\n",
      "Train Epoche: 2 [183/96 (191%)]\tLoss: 2.603009\n",
      "Train Epoche: 2 [184/96 (192%)]\tLoss: 0.801522\n",
      "Train Epoche: 2 [185/96 (193%)]\tLoss: 4.106574\n",
      "Train Epoche: 2 [186/96 (194%)]\tLoss: 0.017952\n",
      "Train Epoche: 2 [187/96 (195%)]\tLoss: 8.554829\n",
      "Train Epoche: 2 [188/96 (196%)]\tLoss: 10.017641\n",
      "Train Epoche: 2 [189/96 (197%)]\tLoss: 16.407578\n",
      "Train Epoche: 2 [190/96 (198%)]\tLoss: 0.978394\n",
      "Train Epoche: 2 [191/96 (199%)]\tLoss: 20.108549\n",
      "Train Epoche: 2 [192/96 (200%)]\tLoss: 0.002829\n",
      "Train Epoche: 2 [193/96 (201%)]\tLoss: 4.464914\n",
      "Train Epoche: 2 [194/96 (202%)]\tLoss: 1.244553\n",
      "Train Epoche: 2 [195/96 (203%)]\tLoss: 4.899603\n",
      "Train Epoche: 2 [196/96 (204%)]\tLoss: 0.635721\n",
      "Train Epoche: 2 [197/96 (205%)]\tLoss: 3.001925\n",
      "Train Epoche: 2 [198/96 (206%)]\tLoss: 0.199298\n",
      "Train Epoche: 2 [199/96 (207%)]\tLoss: 0.114729\n",
      "Train Epoche: 2 [200/96 (208%)]\tLoss: 0.368476\n",
      "Train Epoche: 2 [201/96 (209%)]\tLoss: 0.458440\n",
      "Train Epoche: 2 [202/96 (210%)]\tLoss: 6.212406\n",
      "Train Epoche: 2 [203/96 (211%)]\tLoss: 4.461634\n",
      "Train Epoche: 2 [204/96 (212%)]\tLoss: 6.112926\n",
      "Train Epoche: 2 [205/96 (214%)]\tLoss: 4.605929\n",
      "Train Epoche: 2 [206/96 (215%)]\tLoss: 4.824130\n",
      "Train Epoche: 2 [207/96 (216%)]\tLoss: 29.982407\n",
      "Train Epoche: 2 [208/96 (217%)]\tLoss: 30.065817\n",
      "Train Epoche: 2 [209/96 (218%)]\tLoss: 237.745789\n",
      "Train Epoche: 2 [210/96 (219%)]\tLoss: 0.418007\n",
      "Train Epoche: 2 [211/96 (220%)]\tLoss: 3.493120\n",
      "Train Epoche: 2 [212/96 (221%)]\tLoss: 0.412022\n",
      "Train Epoche: 2 [213/96 (222%)]\tLoss: 0.671279\n",
      "Train Epoche: 2 [214/96 (223%)]\tLoss: 8.261726\n",
      "Train Epoche: 2 [215/96 (224%)]\tLoss: 0.441793\n",
      "Train Epoche: 2 [216/96 (225%)]\tLoss: 1.540756\n",
      "Train Epoche: 2 [217/96 (226%)]\tLoss: 9.035988\n",
      "Train Epoche: 2 [218/96 (227%)]\tLoss: 5.386608\n",
      "Train Epoche: 2 [219/96 (228%)]\tLoss: 5.118450\n",
      "Train Epoche: 2 [220/96 (229%)]\tLoss: 1.404994\n",
      "Train Epoche: 2 [221/96 (230%)]\tLoss: 5.848787\n",
      "Train Epoche: 2 [222/96 (231%)]\tLoss: 24.668276\n",
      "Train Epoche: 2 [223/96 (232%)]\tLoss: 0.482219\n",
      "Train Epoche: 2 [224/96 (233%)]\tLoss: 46.504604\n",
      "Train Epoche: 2 [225/96 (234%)]\tLoss: 4.982038\n",
      "Train Epoche: 2 [226/96 (235%)]\tLoss: 16.036947\n",
      "Train Epoche: 2 [227/96 (236%)]\tLoss: 6.447846\n",
      "Train Epoche: 2 [228/96 (238%)]\tLoss: 1.687440\n",
      "Train Epoche: 2 [229/96 (239%)]\tLoss: 6.718864\n",
      "Train Epoche: 2 [230/96 (240%)]\tLoss: 15.890059\n",
      "Train Epoche: 2 [231/96 (241%)]\tLoss: 268.669098\n",
      "Train Epoche: 2 [232/96 (242%)]\tLoss: 3.146711\n",
      "Train Epoche: 2 [233/96 (243%)]\tLoss: 0.290882\n",
      "Train Epoche: 2 [234/96 (244%)]\tLoss: 61.372746\n",
      "Train Epoche: 2 [235/96 (245%)]\tLoss: 11.751263\n",
      "Train Epoche: 2 [236/96 (246%)]\tLoss: 339.018707\n",
      "Train Epoche: 2 [237/96 (247%)]\tLoss: 1.344013\n",
      "Train Epoche: 2 [238/96 (248%)]\tLoss: 0.615599\n",
      "Train Epoche: 2 [239/96 (249%)]\tLoss: 2.060773\n",
      "Train Epoche: 2 [240/96 (250%)]\tLoss: 4.743495\n",
      "Train Epoche: 2 [241/96 (251%)]\tLoss: 2.452347\n",
      "Train Epoche: 2 [242/96 (252%)]\tLoss: 1.844943\n",
      "Train Epoche: 2 [243/96 (253%)]\tLoss: 6.998999\n",
      "Train Epoche: 2 [244/96 (254%)]\tLoss: 8.281360\n",
      "Train Epoche: 2 [245/96 (255%)]\tLoss: 3.133364\n",
      "Train Epoche: 2 [246/96 (256%)]\tLoss: 24.616512\n",
      "Train Epoche: 2 [247/96 (257%)]\tLoss: 3.934554\n",
      "Train Epoche: 2 [248/96 (258%)]\tLoss: 0.935674\n",
      "Train Epoche: 2 [249/96 (259%)]\tLoss: 1.132358\n",
      "Train Epoche: 2 [250/96 (260%)]\tLoss: 36.199574\n",
      "Train Epoche: 2 [251/96 (261%)]\tLoss: 0.159861\n",
      "Train Epoche: 2 [252/96 (262%)]\tLoss: 1.389920\n",
      "Train Epoche: 2 [253/96 (264%)]\tLoss: 0.644066\n",
      "Train Epoche: 2 [254/96 (265%)]\tLoss: 5.580439\n",
      "Train Epoche: 2 [255/96 (266%)]\tLoss: 0.472433\n",
      "Train Epoche: 2 [256/96 (267%)]\tLoss: 8.375772\n",
      "Train Epoche: 2 [257/96 (268%)]\tLoss: 4.907844\n",
      "Train Epoche: 2 [258/96 (269%)]\tLoss: 13.124914\n",
      "Train Epoche: 2 [259/96 (270%)]\tLoss: 0.757590\n",
      "Train Epoche: 2 [260/96 (271%)]\tLoss: 2.933596\n",
      "Train Epoche: 2 [261/96 (272%)]\tLoss: 1.218969\n",
      "Train Epoche: 2 [262/96 (273%)]\tLoss: 0.076718\n",
      "Train Epoche: 2 [263/96 (274%)]\tLoss: 1.833886\n",
      "Train Epoche: 2 [264/96 (275%)]\tLoss: 0.170091\n",
      "Train Epoche: 2 [265/96 (276%)]\tLoss: 25.016922\n",
      "Train Epoche: 2 [266/96 (277%)]\tLoss: 1.893892\n",
      "Train Epoche: 2 [267/96 (278%)]\tLoss: 188.448135\n",
      "Train Epoche: 2 [268/96 (279%)]\tLoss: 5.122181\n",
      "Train Epoche: 2 [269/96 (280%)]\tLoss: 0.135546\n",
      "Train Epoche: 2 [270/96 (281%)]\tLoss: 0.757022\n",
      "Train Epoche: 2 [271/96 (282%)]\tLoss: 1.404429\n",
      "Train Epoche: 2 [272/96 (283%)]\tLoss: 2.220676\n",
      "Train Epoche: 2 [273/96 (284%)]\tLoss: 2.037102\n",
      "Train Epoche: 2 [274/96 (285%)]\tLoss: 2.127357\n",
      "Train Epoche: 2 [275/96 (286%)]\tLoss: 0.099917\n",
      "Train Epoche: 2 [276/96 (288%)]\tLoss: 23.648294\n",
      "Train Epoche: 2 [277/96 (289%)]\tLoss: 2.888676\n",
      "Train Epoche: 2 [278/96 (290%)]\tLoss: 2.011138\n",
      "Train Epoche: 2 [279/96 (291%)]\tLoss: 2.284769\n",
      "Train Epoche: 2 [280/96 (292%)]\tLoss: 5.843675\n",
      "Train Epoche: 2 [281/96 (293%)]\tLoss: 14.997087\n",
      "Train Epoche: 2 [282/96 (294%)]\tLoss: 0.577658\n",
      "Train Epoche: 2 [283/96 (295%)]\tLoss: 0.703639\n",
      "Train Epoche: 2 [284/96 (296%)]\tLoss: 1.102328\n",
      "Train Epoche: 2 [285/96 (297%)]\tLoss: 5.931220\n",
      "Train Epoche: 2 [286/96 (298%)]\tLoss: 0.734233\n",
      "Train Epoche: 2 [287/96 (299%)]\tLoss: 0.744359\n",
      "Train Epoche: 2 [288/96 (300%)]\tLoss: 0.324607\n",
      "Train Epoche: 2 [289/96 (301%)]\tLoss: 0.002735\n",
      "Train Epoche: 2 [290/96 (302%)]\tLoss: 0.179207\n",
      "Train Epoche: 2 [291/96 (303%)]\tLoss: 0.304940\n",
      "Train Epoche: 2 [292/96 (304%)]\tLoss: 16.357567\n",
      "Train Epoche: 2 [293/96 (305%)]\tLoss: 8.154486\n",
      "Train Epoche: 2 [294/96 (306%)]\tLoss: 1.766011\n",
      "Train Epoche: 2 [295/96 (307%)]\tLoss: 2.553013\n",
      "Train Epoche: 2 [296/96 (308%)]\tLoss: 0.303751\n",
      "Train Epoche: 2 [297/96 (309%)]\tLoss: 1.079847\n",
      "Train Epoche: 2 [298/96 (310%)]\tLoss: 3.103979\n",
      "Train Epoche: 2 [299/96 (311%)]\tLoss: 0.001166\n",
      "Train Epoche: 2 [300/96 (312%)]\tLoss: 1.876745\n",
      "Train Epoche: 2 [301/96 (314%)]\tLoss: 31.231045\n",
      "Train Epoche: 2 [302/96 (315%)]\tLoss: 23.751234\n",
      "Train Epoche: 2 [303/96 (316%)]\tLoss: 4.081689\n",
      "Train Epoche: 2 [304/96 (317%)]\tLoss: 0.000232\n",
      "Train Epoche: 2 [305/96 (318%)]\tLoss: 1.434841\n",
      "Train Epoche: 2 [306/96 (319%)]\tLoss: 3.212967\n",
      "Train Epoche: 2 [307/96 (320%)]\tLoss: 4.074553\n",
      "Train Epoche: 2 [308/96 (321%)]\tLoss: 157.502167\n",
      "Train Epoche: 2 [309/96 (322%)]\tLoss: 5.599916\n",
      "Train Epoche: 2 [310/96 (323%)]\tLoss: 25.055038\n",
      "Train Epoche: 2 [311/96 (324%)]\tLoss: 0.136051\n",
      "Train Epoche: 2 [312/96 (325%)]\tLoss: 1.058486\n",
      "Train Epoche: 2 [313/96 (326%)]\tLoss: 0.043831\n",
      "Train Epoche: 2 [314/96 (327%)]\tLoss: 91.647568\n",
      "Train Epoche: 2 [315/96 (328%)]\tLoss: 2.558598\n",
      "Train Epoche: 2 [316/96 (329%)]\tLoss: 0.597532\n",
      "Train Epoche: 2 [317/96 (330%)]\tLoss: 1.400376\n",
      "Train Epoche: 2 [318/96 (331%)]\tLoss: 6.766579\n",
      "Train Epoche: 2 [319/96 (332%)]\tLoss: 0.000283\n",
      "Train Epoche: 2 [320/96 (333%)]\tLoss: 0.100080\n",
      "Train Epoche: 2 [321/96 (334%)]\tLoss: 0.052973\n",
      "Train Epoche: 2 [322/96 (335%)]\tLoss: 0.188177\n",
      "Train Epoche: 2 [323/96 (336%)]\tLoss: 21.983681\n",
      "Train Epoche: 2 [324/96 (338%)]\tLoss: 0.865514\n",
      "Train Epoche: 2 [325/96 (339%)]\tLoss: 0.298800\n",
      "Train Epoche: 2 [326/96 (340%)]\tLoss: 0.172120\n",
      "Train Epoche: 2 [327/96 (341%)]\tLoss: 1.908522\n",
      "Train Epoche: 2 [328/96 (342%)]\tLoss: 0.042165\n",
      "Train Epoche: 2 [329/96 (343%)]\tLoss: 33.601173\n",
      "Train Epoche: 2 [330/96 (344%)]\tLoss: 20.491938\n",
      "Train Epoche: 2 [331/96 (345%)]\tLoss: 97.380585\n",
      "Train Epoche: 2 [332/96 (346%)]\tLoss: 0.404084\n",
      "Train Epoche: 2 [333/96 (347%)]\tLoss: 9.591965\n",
      "Train Epoche: 2 [334/96 (348%)]\tLoss: 6.431365\n",
      "Train Epoche: 2 [335/96 (349%)]\tLoss: 6.586050\n",
      "Train Epoche: 2 [336/96 (350%)]\tLoss: 76.612045\n",
      "Train Epoche: 2 [337/96 (351%)]\tLoss: 10.786995\n",
      "Train Epoche: 2 [338/96 (352%)]\tLoss: 2.895045\n",
      "Train Epoche: 2 [339/96 (353%)]\tLoss: 2.261269\n",
      "Train Epoche: 2 [340/96 (354%)]\tLoss: 4.437401\n",
      "Train Epoche: 2 [341/96 (355%)]\tLoss: 17.596069\n",
      "Train Epoche: 2 [342/96 (356%)]\tLoss: 3.308254\n",
      "Train Epoche: 2 [343/96 (357%)]\tLoss: 1.419213\n",
      "Train Epoche: 2 [344/96 (358%)]\tLoss: 122.740364\n",
      "Train Epoche: 2 [345/96 (359%)]\tLoss: 339.967712\n",
      "Train Epoche: 2 [346/96 (360%)]\tLoss: 0.115232\n",
      "Train Epoche: 2 [347/96 (361%)]\tLoss: 1.255907\n",
      "Train Epoche: 2 [348/96 (362%)]\tLoss: 5.341095\n",
      "Train Epoche: 2 [349/96 (364%)]\tLoss: 1.848934\n",
      "Train Epoche: 2 [350/96 (365%)]\tLoss: 0.645588\n",
      "Train Epoche: 2 [351/96 (366%)]\tLoss: 29.371067\n",
      "Train Epoche: 2 [352/96 (367%)]\tLoss: 77.724022\n",
      "Train Epoche: 2 [353/96 (368%)]\tLoss: 15.610704\n",
      "Train Epoche: 2 [354/96 (369%)]\tLoss: 2.444596\n",
      "Train Epoche: 2 [355/96 (370%)]\tLoss: 6.499870\n",
      "Train Epoche: 2 [356/96 (371%)]\tLoss: 162.641602\n",
      "Train Epoche: 2 [357/96 (372%)]\tLoss: 12.192296\n",
      "Train Epoche: 2 [358/96 (373%)]\tLoss: 4.209875\n",
      "Train Epoche: 2 [359/96 (374%)]\tLoss: 2.725903\n",
      "Train Epoche: 2 [360/96 (375%)]\tLoss: 8.764141\n",
      "Train Epoche: 2 [361/96 (376%)]\tLoss: 2.022293\n",
      "Train Epoche: 2 [362/96 (377%)]\tLoss: 0.050445\n",
      "Train Epoche: 2 [363/96 (378%)]\tLoss: 1.577792\n",
      "Train Epoche: 2 [364/96 (379%)]\tLoss: 115.755486\n",
      "Train Epoche: 2 [365/96 (380%)]\tLoss: 0.055502\n",
      "Train Epoche: 2 [366/96 (381%)]\tLoss: 6.194271\n",
      "Train Epoche: 2 [367/96 (382%)]\tLoss: 5.825267\n",
      "Train Epoche: 2 [368/96 (383%)]\tLoss: 0.418223\n",
      "Train Epoche: 2 [369/96 (384%)]\tLoss: 11.764948\n",
      "Train Epoche: 2 [370/96 (385%)]\tLoss: 5.051417\n",
      "Train Epoche: 2 [371/96 (386%)]\tLoss: 8.610438\n",
      "Train Epoche: 2 [372/96 (388%)]\tLoss: 22.539694\n",
      "Train Epoche: 2 [373/96 (389%)]\tLoss: 2.875152\n",
      "Train Epoche: 2 [374/96 (390%)]\tLoss: 25.353058\n",
      "Train Epoche: 2 [375/96 (391%)]\tLoss: 0.556682\n",
      "Train Epoche: 2 [376/96 (392%)]\tLoss: 87.957588\n",
      "Train Epoche: 2 [377/96 (393%)]\tLoss: 1.726633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [378/96 (394%)]\tLoss: 0.768977\n",
      "Train Epoche: 2 [379/96 (395%)]\tLoss: 0.249190\n",
      "Train Epoche: 2 [380/96 (396%)]\tLoss: 0.670922\n",
      "Train Epoche: 2 [381/96 (397%)]\tLoss: 3.299701\n",
      "Train Epoche: 2 [382/96 (398%)]\tLoss: 0.221313\n",
      "Train Epoche: 2 [383/96 (399%)]\tLoss: 0.772473\n",
      "Train Epoche: 2 [384/96 (400%)]\tLoss: 1.491269\n",
      "Train Epoche: 2 [385/96 (401%)]\tLoss: 0.255655\n",
      "Train Epoche: 2 [386/96 (402%)]\tLoss: 13.457850\n",
      "Train Epoche: 2 [387/96 (403%)]\tLoss: 20.529936\n",
      "Train Epoche: 2 [388/96 (404%)]\tLoss: 1.191002\n",
      "Train Epoche: 2 [389/96 (405%)]\tLoss: 0.612201\n",
      "Train Epoche: 2 [390/96 (406%)]\tLoss: 3.428213\n",
      "Train Epoche: 2 [391/96 (407%)]\tLoss: 6.639929\n",
      "Train Epoche: 2 [392/96 (408%)]\tLoss: 28.617361\n",
      "Train Epoche: 2 [393/96 (409%)]\tLoss: 224.513397\n",
      "Train Epoche: 2 [394/96 (410%)]\tLoss: 2.150707\n",
      "Train Epoche: 2 [395/96 (411%)]\tLoss: 8.063332\n",
      "Train Epoche: 2 [396/96 (412%)]\tLoss: 0.002999\n",
      "Train Epoche: 2 [397/96 (414%)]\tLoss: 0.129460\n",
      "Train Epoche: 2 [398/96 (415%)]\tLoss: 3.048869\n",
      "Train Epoche: 2 [399/96 (416%)]\tLoss: 0.018923\n",
      "Train Epoche: 2 [400/96 (417%)]\tLoss: 4.197248\n",
      "Train Epoche: 2 [401/96 (418%)]\tLoss: 7.846462\n",
      "Train Epoche: 2 [402/96 (419%)]\tLoss: 9.575603\n",
      "Train Epoche: 2 [403/96 (420%)]\tLoss: 2.831478\n",
      "Train Epoche: 2 [404/96 (421%)]\tLoss: 6.251903\n",
      "Train Epoche: 2 [405/96 (422%)]\tLoss: 0.368908\n",
      "Train Epoche: 2 [406/96 (423%)]\tLoss: 3.008612\n",
      "Train Epoche: 2 [407/96 (424%)]\tLoss: 0.259949\n",
      "Train Epoche: 2 [408/96 (425%)]\tLoss: 5.495561\n",
      "Train Epoche: 2 [409/96 (426%)]\tLoss: 4.623784\n",
      "Train Epoche: 2 [410/96 (427%)]\tLoss: 14.858044\n",
      "Train Epoche: 2 [411/96 (428%)]\tLoss: 0.086956\n",
      "Train Epoche: 2 [412/96 (429%)]\tLoss: 76.221687\n",
      "Train Epoche: 2 [413/96 (430%)]\tLoss: 14.177245\n",
      "Train Epoche: 2 [414/96 (431%)]\tLoss: 0.013299\n",
      "Train Epoche: 2 [415/96 (432%)]\tLoss: 15.354724\n",
      "Train Epoche: 2 [416/96 (433%)]\tLoss: 0.146472\n",
      "Train Epoche: 2 [417/96 (434%)]\tLoss: 5.810159\n",
      "Train Epoche: 2 [418/96 (435%)]\tLoss: 0.878819\n",
      "Train Epoche: 2 [419/96 (436%)]\tLoss: 4.551400\n",
      "Train Epoche: 2 [420/96 (438%)]\tLoss: 4.085709\n",
      "Train Epoche: 2 [421/96 (439%)]\tLoss: 13.638811\n",
      "Train Epoche: 2 [422/96 (440%)]\tLoss: 0.001160\n",
      "Train Epoche: 2 [423/96 (441%)]\tLoss: 138.352539\n",
      "Train Epoche: 2 [424/96 (442%)]\tLoss: 1.899182\n",
      "Train Epoche: 2 [425/96 (443%)]\tLoss: 2.383502\n",
      "Train Epoche: 2 [426/96 (444%)]\tLoss: 0.130797\n",
      "Train Epoche: 2 [427/96 (445%)]\tLoss: 0.006652\n",
      "Train Epoche: 2 [428/96 (446%)]\tLoss: 64.589195\n",
      "Train Epoche: 2 [429/96 (447%)]\tLoss: 3.774848\n",
      "Train Epoche: 2 [430/96 (448%)]\tLoss: 24.073563\n",
      "Train Epoche: 2 [431/96 (449%)]\tLoss: 3.993613\n",
      "Train Epoche: 2 [432/96 (450%)]\tLoss: 4.864128\n",
      "Train Epoche: 2 [433/96 (451%)]\tLoss: 0.528672\n",
      "Train Epoche: 2 [434/96 (452%)]\tLoss: 1.252339\n",
      "Train Epoche: 2 [435/96 (453%)]\tLoss: 59.523552\n",
      "Train Epoche: 2 [436/96 (454%)]\tLoss: 23.787537\n",
      "Train Epoche: 2 [437/96 (455%)]\tLoss: 24.220226\n",
      "Train Epoche: 2 [438/96 (456%)]\tLoss: 2.774787\n",
      "Train Epoche: 2 [439/96 (457%)]\tLoss: 3.708272\n",
      "Train Epoche: 2 [440/96 (458%)]\tLoss: 16.829496\n",
      "Train Epoche: 2 [441/96 (459%)]\tLoss: 1.531079\n",
      "Train Epoche: 2 [442/96 (460%)]\tLoss: 2.636594\n",
      "Train Epoche: 2 [443/96 (461%)]\tLoss: 0.467386\n",
      "Train Epoche: 2 [444/96 (462%)]\tLoss: 17.976053\n",
      "Train Epoche: 2 [445/96 (464%)]\tLoss: 2.167952\n",
      "Train Epoche: 2 [446/96 (465%)]\tLoss: 142.101089\n",
      "Train Epoche: 2 [447/96 (466%)]\tLoss: 13.070892\n",
      "Train Epoche: 2 [448/96 (467%)]\tLoss: 0.645560\n",
      "Train Epoche: 2 [449/96 (468%)]\tLoss: 18.418110\n",
      "Train Epoche: 2 [450/96 (469%)]\tLoss: 10.775077\n",
      "Train Epoche: 2 [451/96 (470%)]\tLoss: 3.126204\n",
      "Train Epoche: 2 [452/96 (471%)]\tLoss: 11.999486\n",
      "Train Epoche: 2 [453/96 (472%)]\tLoss: 2.524347\n",
      "Train Epoche: 2 [454/96 (473%)]\tLoss: 8.926519\n",
      "Train Epoche: 2 [455/96 (474%)]\tLoss: 0.493146\n",
      "Train Epoche: 2 [456/96 (475%)]\tLoss: 71.321663\n",
      "Train Epoche: 2 [457/96 (476%)]\tLoss: 0.875016\n",
      "Train Epoche: 2 [458/96 (477%)]\tLoss: 2.364042\n",
      "Train Epoche: 2 [459/96 (478%)]\tLoss: 0.388234\n",
      "Train Epoche: 2 [460/96 (479%)]\tLoss: 135.675812\n",
      "Train Epoche: 2 [461/96 (480%)]\tLoss: 1.204589\n",
      "Train Epoche: 2 [462/96 (481%)]\tLoss: 3.126400\n",
      "Train Epoche: 2 [463/96 (482%)]\tLoss: 0.751993\n",
      "Train Epoche: 2 [464/96 (483%)]\tLoss: 188.990540\n",
      "Train Epoche: 2 [465/96 (484%)]\tLoss: 2.490551\n",
      "Train Epoche: 2 [466/96 (485%)]\tLoss: 3.478913\n",
      "Train Epoche: 2 [467/96 (486%)]\tLoss: 87.124672\n",
      "Train Epoche: 2 [468/96 (488%)]\tLoss: 0.233573\n",
      "Train Epoche: 2 [469/96 (489%)]\tLoss: 3.944196\n",
      "Train Epoche: 2 [470/96 (490%)]\tLoss: 19.850161\n",
      "Train Epoche: 2 [471/96 (491%)]\tLoss: 26.940481\n",
      "Train Epoche: 2 [472/96 (492%)]\tLoss: 0.016708\n",
      "Train Epoche: 2 [473/96 (493%)]\tLoss: 3.980139\n",
      "Train Epoche: 2 [474/96 (494%)]\tLoss: 0.018706\n",
      "Train Epoche: 2 [475/96 (495%)]\tLoss: 1.474754\n",
      "Train Epoche: 2 [476/96 (496%)]\tLoss: 37.650398\n",
      "Train Epoche: 2 [477/96 (497%)]\tLoss: 15.968019\n",
      "Train Epoche: 2 [478/96 (498%)]\tLoss: 10.729952\n",
      "Train Epoche: 2 [479/96 (499%)]\tLoss: 5.450021\n",
      "Train Epoche: 2 [480/96 (500%)]\tLoss: 83.363991\n",
      "Train Epoche: 2 [481/96 (501%)]\tLoss: 2.429264\n",
      "Train Epoche: 2 [482/96 (502%)]\tLoss: 9.761703\n",
      "Train Epoche: 2 [483/96 (503%)]\tLoss: 10.761113\n",
      "Train Epoche: 2 [484/96 (504%)]\tLoss: 26.879730\n",
      "Train Epoche: 2 [485/96 (505%)]\tLoss: 41.263828\n",
      "Train Epoche: 2 [486/96 (506%)]\tLoss: 0.231016\n",
      "Train Epoche: 2 [487/96 (507%)]\tLoss: 10.676472\n",
      "Train Epoche: 2 [488/96 (508%)]\tLoss: 5.507976\n",
      "Train Epoche: 2 [489/96 (509%)]\tLoss: 3.452544\n",
      "Train Epoche: 2 [490/96 (510%)]\tLoss: 2.454346\n",
      "Train Epoche: 2 [491/96 (511%)]\tLoss: 1.222841\n",
      "Train Epoche: 2 [492/96 (512%)]\tLoss: 0.319549\n",
      "Train Epoche: 2 [493/96 (514%)]\tLoss: 10.749294\n",
      "Train Epoche: 2 [494/96 (515%)]\tLoss: 1.046081\n",
      "Train Epoche: 2 [495/96 (516%)]\tLoss: 6.271094\n",
      "Train Epoche: 2 [496/96 (517%)]\tLoss: 2.198446\n",
      "Train Epoche: 2 [497/96 (518%)]\tLoss: 30.901283\n",
      "Train Epoche: 2 [498/96 (519%)]\tLoss: 25.857813\n",
      "Train Epoche: 2 [499/96 (520%)]\tLoss: 0.000002\n",
      "Train Epoche: 2 [500/96 (521%)]\tLoss: 19.782701\n",
      "Train Epoche: 2 [501/96 (522%)]\tLoss: 0.378233\n",
      "Train Epoche: 2 [502/96 (523%)]\tLoss: 21.235235\n",
      "Train Epoche: 2 [503/96 (524%)]\tLoss: 0.000402\n",
      "Train Epoche: 2 [504/96 (525%)]\tLoss: 2.172813\n",
      "Train Epoche: 2 [505/96 (526%)]\tLoss: 53.229729\n",
      "Train Epoche: 2 [506/96 (527%)]\tLoss: 2.407753\n",
      "Train Epoche: 2 [507/96 (528%)]\tLoss: 19.287018\n",
      "Train Epoche: 2 [508/96 (529%)]\tLoss: 0.581340\n",
      "Train Epoche: 2 [509/96 (530%)]\tLoss: 1.507707\n",
      "Train Epoche: 2 [510/96 (531%)]\tLoss: 2.432792\n",
      "Train Epoche: 2 [511/96 (532%)]\tLoss: 0.366917\n",
      "Train Epoche: 2 [512/96 (533%)]\tLoss: 3.343078\n",
      "Train Epoche: 2 [513/96 (534%)]\tLoss: 1.703334\n",
      "Train Epoche: 2 [514/96 (535%)]\tLoss: 0.012434\n",
      "Train Epoche: 2 [515/96 (536%)]\tLoss: 0.058093\n",
      "Train Epoche: 2 [516/96 (538%)]\tLoss: 22.018385\n",
      "Train Epoche: 2 [517/96 (539%)]\tLoss: 47.698952\n",
      "Train Epoche: 2 [518/96 (540%)]\tLoss: 5.418789\n",
      "Train Epoche: 2 [519/96 (541%)]\tLoss: 66.006203\n",
      "Train Epoche: 2 [520/96 (542%)]\tLoss: 17.544853\n",
      "Train Epoche: 2 [521/96 (543%)]\tLoss: 151.818909\n",
      "Train Epoche: 2 [522/96 (544%)]\tLoss: 0.112016\n",
      "Train Epoche: 2 [523/96 (545%)]\tLoss: 98.118073\n",
      "Train Epoche: 2 [524/96 (546%)]\tLoss: 7.271209\n",
      "Train Epoche: 2 [525/96 (547%)]\tLoss: 1.296270\n",
      "Train Epoche: 2 [526/96 (548%)]\tLoss: 0.009225\n",
      "Train Epoche: 2 [527/96 (549%)]\tLoss: 0.648296\n",
      "Train Epoche: 2 [528/96 (550%)]\tLoss: 3.977818\n",
      "Train Epoche: 2 [529/96 (551%)]\tLoss: 8.568809\n",
      "Train Epoche: 2 [530/96 (552%)]\tLoss: 0.015462\n",
      "Train Epoche: 2 [531/96 (553%)]\tLoss: 12.977323\n",
      "Train Epoche: 2 [532/96 (554%)]\tLoss: 0.523187\n",
      "Train Epoche: 2 [533/96 (555%)]\tLoss: 121.706825\n",
      "Train Epoche: 2 [534/96 (556%)]\tLoss: 12.348330\n",
      "Train Epoche: 2 [535/96 (557%)]\tLoss: 2.733391\n",
      "Train Epoche: 2 [536/96 (558%)]\tLoss: 0.012219\n",
      "Train Epoche: 2 [537/96 (559%)]\tLoss: 0.513531\n",
      "Train Epoche: 2 [538/96 (560%)]\tLoss: 0.068458\n",
      "Train Epoche: 2 [539/96 (561%)]\tLoss: 1.129356\n",
      "Train Epoche: 2 [540/96 (562%)]\tLoss: 2.289165\n",
      "Train Epoche: 2 [541/96 (564%)]\tLoss: 0.642242\n",
      "Train Epoche: 2 [542/96 (565%)]\tLoss: 1.756263\n",
      "Train Epoche: 2 [543/96 (566%)]\tLoss: 0.103533\n",
      "Train Epoche: 2 [544/96 (567%)]\tLoss: 0.027859\n",
      "Train Epoche: 2 [545/96 (568%)]\tLoss: 0.209513\n",
      "Train Epoche: 2 [546/96 (569%)]\tLoss: 5.789948\n",
      "Train Epoche: 2 [547/96 (570%)]\tLoss: 1.400810\n",
      "Train Epoche: 2 [548/96 (571%)]\tLoss: 70.873985\n",
      "Train Epoche: 2 [549/96 (572%)]\tLoss: 0.335756\n",
      "Train Epoche: 2 [550/96 (573%)]\tLoss: 0.227369\n",
      "Train Epoche: 2 [551/96 (574%)]\tLoss: 5.408884\n",
      "Train Epoche: 2 [552/96 (575%)]\tLoss: 4.946409\n",
      "Train Epoche: 2 [553/96 (576%)]\tLoss: 312.264954\n",
      "Train Epoche: 2 [554/96 (577%)]\tLoss: 4.608779\n",
      "Train Epoche: 2 [555/96 (578%)]\tLoss: 27.126862\n",
      "Train Epoche: 2 [556/96 (579%)]\tLoss: 2.752949\n",
      "Train Epoche: 2 [557/96 (580%)]\tLoss: 8.725037\n",
      "Train Epoche: 2 [558/96 (581%)]\tLoss: 3.456095\n",
      "Train Epoche: 2 [559/96 (582%)]\tLoss: 1.492615\n",
      "Train Epoche: 2 [560/96 (583%)]\tLoss: 1.215971\n",
      "Train Epoche: 2 [561/96 (584%)]\tLoss: 7.698940\n",
      "Train Epoche: 2 [562/96 (585%)]\tLoss: 6.733407\n",
      "Train Epoche: 2 [563/96 (586%)]\tLoss: 0.672428\n",
      "Train Epoche: 2 [564/96 (588%)]\tLoss: 60.699001\n",
      "Train Epoche: 2 [565/96 (589%)]\tLoss: 1.283035\n",
      "Train Epoche: 2 [566/96 (590%)]\tLoss: 20.927238\n",
      "Train Epoche: 2 [567/96 (591%)]\tLoss: 11.097825\n",
      "Train Epoche: 2 [568/96 (592%)]\tLoss: 0.035993\n",
      "Train Epoche: 2 [569/96 (593%)]\tLoss: 1.834419\n",
      "Train Epoche: 2 [570/96 (594%)]\tLoss: 38.890930\n",
      "Train Epoche: 2 [571/96 (595%)]\tLoss: 0.083746\n",
      "Train Epoche: 2 [572/96 (596%)]\tLoss: 0.322332\n",
      "Train Epoche: 2 [573/96 (597%)]\tLoss: 0.569802\n",
      "Train Epoche: 2 [574/96 (598%)]\tLoss: 2.141340\n",
      "Train Epoche: 2 [575/96 (599%)]\tLoss: 2.888853\n",
      "Train Epoche: 2 [576/96 (600%)]\tLoss: 27.504251\n",
      "Train Epoche: 2 [577/96 (601%)]\tLoss: 0.000001\n",
      "Train Epoche: 2 [578/96 (602%)]\tLoss: 20.959476\n",
      "Train Epoche: 2 [579/96 (603%)]\tLoss: 0.000010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [580/96 (604%)]\tLoss: 2.572411\n",
      "Train Epoche: 2 [581/96 (605%)]\tLoss: 4.941108\n",
      "Train Epoche: 2 [582/96 (606%)]\tLoss: 4.498958\n",
      "Train Epoche: 2 [583/96 (607%)]\tLoss: 0.808193\n",
      "Train Epoche: 2 [584/96 (608%)]\tLoss: 33.791019\n",
      "Train Epoche: 2 [585/96 (609%)]\tLoss: 2.185151\n",
      "Train Epoche: 2 [586/96 (610%)]\tLoss: 6.207829\n",
      "Train Epoche: 2 [587/96 (611%)]\tLoss: 29.622189\n",
      "Train Epoche: 2 [588/96 (612%)]\tLoss: 3.527359\n",
      "Train Epoche: 2 [589/96 (614%)]\tLoss: 14.078169\n",
      "Train Epoche: 2 [590/96 (615%)]\tLoss: 1.853508\n",
      "Train Epoche: 2 [591/96 (616%)]\tLoss: 1.931868\n",
      "Train Epoche: 2 [592/96 (617%)]\tLoss: 80.699753\n",
      "Train Epoche: 2 [593/96 (618%)]\tLoss: 3.157219\n",
      "Train Epoche: 2 [594/96 (619%)]\tLoss: 129.588715\n",
      "Train Epoche: 2 [595/96 (620%)]\tLoss: 8.345092\n",
      "Train Epoche: 2 [596/96 (621%)]\tLoss: 3.718541\n",
      "Train Epoche: 2 [597/96 (622%)]\tLoss: 0.576471\n",
      "Train Epoche: 2 [598/96 (623%)]\tLoss: 2.807297\n",
      "Train Epoche: 2 [599/96 (624%)]\tLoss: 0.000128\n",
      "Train Epoche: 2 [600/96 (625%)]\tLoss: 17.890528\n",
      "Train Epoche: 2 [601/96 (626%)]\tLoss: 9.660001\n",
      "Train Epoche: 2 [602/96 (627%)]\tLoss: 0.013026\n",
      "Train Epoche: 2 [603/96 (628%)]\tLoss: 0.102272\n",
      "Train Epoche: 2 [604/96 (629%)]\tLoss: 0.264836\n",
      "Train Epoche: 2 [605/96 (630%)]\tLoss: 31.239082\n",
      "Train Epoche: 2 [606/96 (631%)]\tLoss: 2.439887\n",
      "Train Epoche: 2 [607/96 (632%)]\tLoss: 313.483154\n",
      "Train Epoche: 2 [608/96 (633%)]\tLoss: 17.343365\n",
      "Train Epoche: 2 [609/96 (634%)]\tLoss: 9.704046\n",
      "Train Epoche: 2 [610/96 (635%)]\tLoss: 70.724411\n",
      "Train Epoche: 2 [611/96 (636%)]\tLoss: 9.938437\n",
      "Train Epoche: 2 [612/96 (638%)]\tLoss: 219.046127\n",
      "Train Epoche: 2 [613/96 (639%)]\tLoss: 59.061253\n",
      "Train Epoche: 2 [614/96 (640%)]\tLoss: 1.976370\n",
      "Train Epoche: 2 [615/96 (641%)]\tLoss: 3.106638\n",
      "Train Epoche: 2 [616/96 (642%)]\tLoss: 2.903419\n",
      "Train Epoche: 2 [617/96 (643%)]\tLoss: 339.648315\n",
      "Train Epoche: 2 [618/96 (644%)]\tLoss: 2.581911\n",
      "Train Epoche: 2 [619/96 (645%)]\tLoss: 50.666466\n",
      "Train Epoche: 2 [620/96 (646%)]\tLoss: 0.002558\n",
      "Train Epoche: 2 [621/96 (647%)]\tLoss: 1.450977\n",
      "Train Epoche: 2 [622/96 (648%)]\tLoss: 26.247143\n",
      "Train Epoche: 2 [623/96 (649%)]\tLoss: 32.877029\n",
      "Train Epoche: 2 [624/96 (650%)]\tLoss: 74.382629\n",
      "Train Epoche: 2 [625/96 (651%)]\tLoss: 412.343140\n",
      "Train Epoche: 2 [626/96 (652%)]\tLoss: 0.093464\n",
      "Train Epoche: 2 [627/96 (653%)]\tLoss: 2.466720\n",
      "Train Epoche: 2 [628/96 (654%)]\tLoss: 1.691022\n",
      "Train Epoche: 2 [629/96 (655%)]\tLoss: 168.942032\n",
      "Train Epoche: 2 [630/96 (656%)]\tLoss: 2.123763\n",
      "Train Epoche: 2 [631/96 (657%)]\tLoss: 0.120074\n",
      "Train Epoche: 2 [632/96 (658%)]\tLoss: 3.771799\n",
      "Train Epoche: 2 [633/96 (659%)]\tLoss: 8.519615\n",
      "Train Epoche: 2 [634/96 (660%)]\tLoss: 14.975327\n",
      "Train Epoche: 2 [635/96 (661%)]\tLoss: 0.053398\n",
      "Train Epoche: 2 [636/96 (662%)]\tLoss: 8.398508\n",
      "Train Epoche: 2 [637/96 (664%)]\tLoss: 0.398637\n",
      "Train Epoche: 2 [638/96 (665%)]\tLoss: 2.828667\n",
      "Train Epoche: 2 [639/96 (666%)]\tLoss: 7.354974\n",
      "Train Epoche: 2 [640/96 (667%)]\tLoss: 7.455089\n",
      "Train Epoche: 2 [641/96 (668%)]\tLoss: 37.385151\n",
      "Train Epoche: 2 [642/96 (669%)]\tLoss: 53.321629\n",
      "Train Epoche: 2 [643/96 (670%)]\tLoss: 29.100143\n",
      "Train Epoche: 2 [644/96 (671%)]\tLoss: 0.091932\n",
      "Train Epoche: 2 [645/96 (672%)]\tLoss: 0.002286\n",
      "Train Epoche: 2 [646/96 (673%)]\tLoss: 5.687298\n",
      "Train Epoche: 2 [647/96 (674%)]\tLoss: 1.579351\n",
      "Train Epoche: 2 [648/96 (675%)]\tLoss: 1.662963\n",
      "Train Epoche: 2 [649/96 (676%)]\tLoss: 40.339184\n",
      "Train Epoche: 2 [650/96 (677%)]\tLoss: 1.143722\n",
      "Train Epoche: 2 [651/96 (678%)]\tLoss: 88.991287\n",
      "Train Epoche: 2 [652/96 (679%)]\tLoss: 14.862257\n",
      "Train Epoche: 2 [653/96 (680%)]\tLoss: 0.008284\n",
      "Train Epoche: 2 [654/96 (681%)]\tLoss: 2.587099\n",
      "Train Epoche: 2 [655/96 (682%)]\tLoss: 9.498270\n",
      "Train Epoche: 2 [656/96 (683%)]\tLoss: 1.049538\n",
      "Train Epoche: 2 [657/96 (684%)]\tLoss: 246.757629\n",
      "Train Epoche: 2 [658/96 (685%)]\tLoss: 9.332584\n",
      "Train Epoche: 2 [659/96 (686%)]\tLoss: 0.995561\n",
      "Train Epoche: 2 [660/96 (688%)]\tLoss: 40.493073\n",
      "Train Epoche: 2 [661/96 (689%)]\tLoss: 3.712138\n",
      "Train Epoche: 2 [662/96 (690%)]\tLoss: 19.004164\n",
      "Train Epoche: 2 [663/96 (691%)]\tLoss: 15.521455\n",
      "Train Epoche: 2 [664/96 (692%)]\tLoss: 9.437855\n",
      "Train Epoche: 2 [665/96 (693%)]\tLoss: 11.956447\n",
      "Train Epoche: 2 [666/96 (694%)]\tLoss: 0.688558\n",
      "Train Epoche: 2 [667/96 (695%)]\tLoss: 0.310475\n",
      "Train Epoche: 2 [668/96 (696%)]\tLoss: 0.979415\n",
      "Train Epoche: 2 [669/96 (697%)]\tLoss: 60.214111\n",
      "Train Epoche: 2 [670/96 (698%)]\tLoss: 0.779826\n",
      "Train Epoche: 2 [671/96 (699%)]\tLoss: 13.346408\n",
      "Train Epoche: 2 [672/96 (700%)]\tLoss: 2.390970\n",
      "Train Epoche: 2 [673/96 (701%)]\tLoss: 8.384242\n",
      "Train Epoche: 2 [674/96 (702%)]\tLoss: 3.832211\n",
      "Train Epoche: 2 [675/96 (703%)]\tLoss: 0.867451\n",
      "Train Epoche: 2 [676/96 (704%)]\tLoss: 3.804817\n",
      "Train Epoche: 2 [677/96 (705%)]\tLoss: 0.979831\n",
      "Train Epoche: 2 [678/96 (706%)]\tLoss: 1.264319\n",
      "Train Epoche: 2 [679/96 (707%)]\tLoss: 0.355918\n",
      "Train Epoche: 2 [680/96 (708%)]\tLoss: 11.473362\n",
      "Train Epoche: 2 [681/96 (709%)]\tLoss: 7.509794\n",
      "Train Epoche: 2 [682/96 (710%)]\tLoss: 1.945364\n",
      "Train Epoche: 2 [683/96 (711%)]\tLoss: 0.070702\n",
      "Train Epoche: 2 [684/96 (712%)]\tLoss: 17.531122\n",
      "Train Epoche: 2 [685/96 (714%)]\tLoss: 18.557226\n",
      "Train Epoche: 2 [686/96 (715%)]\tLoss: 3.798443\n",
      "Train Epoche: 2 [687/96 (716%)]\tLoss: 3.010217\n",
      "Train Epoche: 2 [688/96 (717%)]\tLoss: 12.343457\n",
      "Train Epoche: 2 [689/96 (718%)]\tLoss: 0.137387\n",
      "Train Epoche: 2 [690/96 (719%)]\tLoss: 2.701632\n",
      "Train Epoche: 2 [691/96 (720%)]\tLoss: 1.256616\n",
      "Train Epoche: 2 [692/96 (721%)]\tLoss: 72.765434\n",
      "Train Epoche: 2 [693/96 (722%)]\tLoss: 29.717354\n",
      "Train Epoche: 2 [694/96 (723%)]\tLoss: 0.002048\n",
      "Train Epoche: 2 [695/96 (724%)]\tLoss: 3.664232\n",
      "Train Epoche: 2 [696/96 (725%)]\tLoss: 0.324739\n",
      "Train Epoche: 2 [697/96 (726%)]\tLoss: 11.508881\n",
      "Train Epoche: 2 [698/96 (727%)]\tLoss: 5.522418\n",
      "Train Epoche: 2 [699/96 (728%)]\tLoss: 10.666524\n",
      "Train Epoche: 2 [700/96 (729%)]\tLoss: 10.026910\n",
      "Train Epoche: 2 [701/96 (730%)]\tLoss: 4.556693\n",
      "Train Epoche: 2 [702/96 (731%)]\tLoss: 26.993650\n",
      "Train Epoche: 2 [703/96 (732%)]\tLoss: 0.840250\n",
      "Train Epoche: 2 [704/96 (733%)]\tLoss: 0.103611\n",
      "Train Epoche: 2 [705/96 (734%)]\tLoss: 9.040908\n",
      "Train Epoche: 2 [706/96 (735%)]\tLoss: 16.674597\n",
      "Train Epoche: 2 [707/96 (736%)]\tLoss: 7.262540\n",
      "Train Epoche: 2 [708/96 (738%)]\tLoss: 1.227070\n",
      "Train Epoche: 2 [709/96 (739%)]\tLoss: 0.568313\n",
      "Train Epoche: 2 [710/96 (740%)]\tLoss: 0.173706\n",
      "Train Epoche: 2 [711/96 (741%)]\tLoss: 0.343381\n",
      "Train Epoche: 2 [712/96 (742%)]\tLoss: 0.027145\n",
      "Train Epoche: 2 [713/96 (743%)]\tLoss: 1.364944\n",
      "Train Epoche: 2 [714/96 (744%)]\tLoss: 0.027947\n",
      "Train Epoche: 2 [715/96 (745%)]\tLoss: 9.414829\n",
      "Train Epoche: 2 [716/96 (746%)]\tLoss: 0.097291\n",
      "Train Epoche: 2 [717/96 (747%)]\tLoss: 4.537091\n",
      "Train Epoche: 2 [718/96 (748%)]\tLoss: 0.623020\n",
      "Train Epoche: 2 [719/96 (749%)]\tLoss: 3.541687\n",
      "Train Epoche: 2 [720/96 (750%)]\tLoss: 0.194198\n",
      "Train Epoche: 2 [721/96 (751%)]\tLoss: 0.323448\n",
      "Train Epoche: 2 [722/96 (752%)]\tLoss: 1.086299\n",
      "Train Epoche: 2 [723/96 (753%)]\tLoss: 4.225790\n",
      "Train Epoche: 2 [724/96 (754%)]\tLoss: 3.870258\n",
      "Train Epoche: 2 [725/96 (755%)]\tLoss: 0.847816\n",
      "Train Epoche: 2 [726/96 (756%)]\tLoss: 0.009454\n",
      "Train Epoche: 2 [727/96 (757%)]\tLoss: 1.096232\n",
      "Train Epoche: 2 [728/96 (758%)]\tLoss: 15.133787\n",
      "Train Epoche: 2 [729/96 (759%)]\tLoss: 0.579499\n",
      "Train Epoche: 2 [730/96 (760%)]\tLoss: 6.030659\n",
      "Train Epoche: 2 [731/96 (761%)]\tLoss: 191.391525\n",
      "Train Epoche: 2 [732/96 (762%)]\tLoss: 60.837399\n",
      "Train Epoche: 2 [733/96 (764%)]\tLoss: 1.475475\n",
      "Train Epoche: 2 [734/96 (765%)]\tLoss: 2.967495\n",
      "Train Epoche: 2 [735/96 (766%)]\tLoss: 1.547580\n",
      "Train Epoche: 2 [736/96 (767%)]\tLoss: 43.698086\n",
      "Train Epoche: 2 [737/96 (768%)]\tLoss: 4.489019\n",
      "Train Epoche: 2 [738/96 (769%)]\tLoss: 1.880160\n",
      "Train Epoche: 2 [739/96 (770%)]\tLoss: 0.113720\n",
      "Train Epoche: 2 [740/96 (771%)]\tLoss: 0.289499\n",
      "Train Epoche: 2 [741/96 (772%)]\tLoss: 53.760838\n",
      "Train Epoche: 2 [742/96 (773%)]\tLoss: 2.432081\n",
      "Train Epoche: 2 [743/96 (774%)]\tLoss: 0.209105\n",
      "Train Epoche: 2 [744/96 (775%)]\tLoss: 0.469926\n",
      "Train Epoche: 2 [745/96 (776%)]\tLoss: 0.000111\n",
      "Train Epoche: 2 [746/96 (777%)]\tLoss: 5.357430\n",
      "Train Epoche: 2 [747/96 (778%)]\tLoss: 47.541267\n",
      "Train Epoche: 2 [748/96 (779%)]\tLoss: 6.873327\n",
      "Train Epoche: 2 [749/96 (780%)]\tLoss: 230.811951\n",
      "Train Epoche: 2 [750/96 (781%)]\tLoss: 12.432454\n",
      "Train Epoche: 2 [751/96 (782%)]\tLoss: 0.141012\n",
      "Train Epoche: 2 [752/96 (783%)]\tLoss: 1.907747\n",
      "Train Epoche: 2 [753/96 (784%)]\tLoss: 0.005504\n",
      "Train Epoche: 2 [754/96 (785%)]\tLoss: 4.639812\n",
      "Train Epoche: 2 [755/96 (786%)]\tLoss: 3.429170\n",
      "Train Epoche: 2 [756/96 (788%)]\tLoss: 2.598118\n",
      "Train Epoche: 2 [757/96 (789%)]\tLoss: 14.483889\n",
      "Train Epoche: 2 [758/96 (790%)]\tLoss: 8.670405\n",
      "Train Epoche: 2 [759/96 (791%)]\tLoss: 1.458202\n",
      "Train Epoche: 2 [760/96 (792%)]\tLoss: 10.558899\n",
      "Train Epoche: 2 [761/96 (793%)]\tLoss: 11.295392\n",
      "Train Epoche: 2 [762/96 (794%)]\tLoss: 2.528883\n",
      "Train Epoche: 2 [763/96 (795%)]\tLoss: 21.037786\n",
      "Train Epoche: 2 [764/96 (796%)]\tLoss: 44.855026\n",
      "Train Epoche: 2 [765/96 (797%)]\tLoss: 4.740648\n",
      "Train Epoche: 2 [766/96 (798%)]\tLoss: 40.175060\n",
      "Train Epoche: 2 [767/96 (799%)]\tLoss: 28.547245\n",
      "Train Epoche: 2 [768/96 (800%)]\tLoss: 14.705857\n",
      "Train Epoche: 2 [769/96 (801%)]\tLoss: 91.508827\n",
      "Train Epoche: 2 [770/96 (802%)]\tLoss: 5.542129\n",
      "Train Epoche: 2 [771/96 (803%)]\tLoss: 16.439779\n",
      "Train Epoche: 2 [772/96 (804%)]\tLoss: 61.201923\n",
      "Train Epoche: 2 [773/96 (805%)]\tLoss: 3.916570\n",
      "Train Epoche: 2 [774/96 (806%)]\tLoss: 41.657150\n",
      "Train Epoche: 2 [775/96 (807%)]\tLoss: 49.562393\n",
      "Train Epoche: 2 [776/96 (808%)]\tLoss: 5.557353\n",
      "Train Epoche: 2 [777/96 (809%)]\tLoss: 32.898743\n",
      "Train Epoche: 2 [778/96 (810%)]\tLoss: 26.559620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [779/96 (811%)]\tLoss: 9.074446\n",
      "Train Epoche: 2 [780/96 (812%)]\tLoss: 0.242599\n",
      "Train Epoche: 2 [781/96 (814%)]\tLoss: 1.618287\n",
      "Train Epoche: 2 [782/96 (815%)]\tLoss: 94.195061\n",
      "Train Epoche: 2 [783/96 (816%)]\tLoss: 3.389058\n",
      "Train Epoche: 2 [784/96 (817%)]\tLoss: 0.661628\n",
      "Train Epoche: 2 [785/96 (818%)]\tLoss: 0.271517\n",
      "Train Epoche: 2 [786/96 (819%)]\tLoss: 0.094125\n",
      "Train Epoche: 2 [787/96 (820%)]\tLoss: 1.491908\n",
      "Train Epoche: 2 [788/96 (821%)]\tLoss: 1.188748\n",
      "Train Epoche: 2 [789/96 (822%)]\tLoss: 0.000237\n",
      "Train Epoche: 2 [790/96 (823%)]\tLoss: 5.769386\n",
      "Train Epoche: 2 [791/96 (824%)]\tLoss: 12.145209\n",
      "Train Epoche: 2 [792/96 (825%)]\tLoss: 55.211411\n",
      "Train Epoche: 2 [793/96 (826%)]\tLoss: 41.465611\n",
      "Train Epoche: 2 [794/96 (827%)]\tLoss: 53.533485\n",
      "Train Epoche: 2 [795/96 (828%)]\tLoss: 220.769135\n",
      "Train Epoche: 2 [796/96 (829%)]\tLoss: 44.395493\n",
      "Train Epoche: 2 [797/96 (830%)]\tLoss: 0.795292\n",
      "Train Epoche: 2 [798/96 (831%)]\tLoss: 1.079476\n",
      "Train Epoche: 2 [799/96 (832%)]\tLoss: 4.583247\n",
      "Train Epoche: 2 [800/96 (833%)]\tLoss: 51.704033\n",
      "Train Epoche: 2 [801/96 (834%)]\tLoss: 36.473488\n",
      "Train Epoche: 2 [802/96 (835%)]\tLoss: 3.576549\n",
      "Train Epoche: 2 [803/96 (836%)]\tLoss: 12.839954\n",
      "Train Epoche: 2 [804/96 (838%)]\tLoss: 4.349833\n",
      "Train Epoche: 2 [805/96 (839%)]\tLoss: 251.551605\n",
      "Train Epoche: 2 [806/96 (840%)]\tLoss: 1.304479\n",
      "Train Epoche: 2 [807/96 (841%)]\tLoss: 22.188017\n",
      "Train Epoche: 2 [808/96 (842%)]\tLoss: 21.442430\n",
      "Train Epoche: 2 [809/96 (843%)]\tLoss: 383.575836\n",
      "Train Epoche: 2 [810/96 (844%)]\tLoss: 148.083466\n",
      "Train Epoche: 2 [811/96 (845%)]\tLoss: 1.534971\n",
      "Train Epoche: 2 [812/96 (846%)]\tLoss: 23.134012\n",
      "Train Epoche: 2 [813/96 (847%)]\tLoss: 0.615514\n",
      "Train Epoche: 2 [814/96 (848%)]\tLoss: 87.895760\n",
      "Train Epoche: 2 [815/96 (849%)]\tLoss: 1.234488\n",
      "Train Epoche: 2 [816/96 (850%)]\tLoss: 36.006191\n",
      "Train Epoche: 2 [817/96 (851%)]\tLoss: 2.750599\n",
      "Train Epoche: 2 [818/96 (852%)]\tLoss: 0.054752\n",
      "Train Epoche: 2 [819/96 (853%)]\tLoss: 7.113381\n",
      "Train Epoche: 2 [820/96 (854%)]\tLoss: 30.880135\n",
      "Train Epoche: 2 [821/96 (855%)]\tLoss: 4.219256\n",
      "Train Epoche: 2 [822/96 (856%)]\tLoss: 8.325560\n",
      "Train Epoche: 2 [823/96 (857%)]\tLoss: 3.353823\n",
      "Train Epoche: 2 [824/96 (858%)]\tLoss: 8.269058\n",
      "Train Epoche: 2 [825/96 (859%)]\tLoss: 0.448150\n",
      "Train Epoche: 2 [826/96 (860%)]\tLoss: 20.431887\n",
      "Train Epoche: 2 [827/96 (861%)]\tLoss: 0.081502\n",
      "Train Epoche: 2 [828/96 (862%)]\tLoss: 24.221708\n",
      "Train Epoche: 2 [829/96 (864%)]\tLoss: 221.044327\n",
      "Train Epoche: 2 [830/96 (865%)]\tLoss: 42.358601\n",
      "Train Epoche: 2 [831/96 (866%)]\tLoss: 2.695994\n",
      "Train Epoche: 2 [832/96 (867%)]\tLoss: 0.734591\n",
      "Train Epoche: 2 [833/96 (868%)]\tLoss: 41.268803\n",
      "Train Epoche: 2 [834/96 (869%)]\tLoss: 1.779107\n",
      "Train Epoche: 2 [835/96 (870%)]\tLoss: 1.422500\n",
      "Train Epoche: 2 [836/96 (871%)]\tLoss: 6.356506\n",
      "Train Epoche: 2 [837/96 (872%)]\tLoss: 108.592598\n",
      "Train Epoche: 2 [838/96 (873%)]\tLoss: 26.553329\n",
      "Train Epoche: 2 [839/96 (874%)]\tLoss: 10.650845\n",
      "Train Epoche: 2 [840/96 (875%)]\tLoss: 10.855969\n",
      "Train Epoche: 2 [841/96 (876%)]\tLoss: 0.230444\n",
      "Train Epoche: 2 [842/96 (877%)]\tLoss: 3.305592\n",
      "Train Epoche: 2 [843/96 (878%)]\tLoss: 25.418974\n",
      "Train Epoche: 2 [844/96 (879%)]\tLoss: 2.264718\n",
      "Train Epoche: 2 [845/96 (880%)]\tLoss: 2.025188\n",
      "Train Epoche: 2 [846/96 (881%)]\tLoss: 0.264662\n",
      "Train Epoche: 2 [847/96 (882%)]\tLoss: 0.389278\n",
      "Train Epoche: 2 [848/96 (883%)]\tLoss: 54.367008\n",
      "Train Epoche: 2 [849/96 (884%)]\tLoss: 0.001428\n",
      "Train Epoche: 2 [850/96 (885%)]\tLoss: 13.305669\n",
      "Train Epoche: 2 [851/96 (886%)]\tLoss: 151.486740\n",
      "Train Epoche: 2 [852/96 (888%)]\tLoss: 51.823109\n",
      "Train Epoche: 2 [853/96 (889%)]\tLoss: 13.583316\n",
      "Train Epoche: 2 [854/96 (890%)]\tLoss: 41.609066\n",
      "Train Epoche: 2 [855/96 (891%)]\tLoss: 8.438807\n",
      "Train Epoche: 2 [856/96 (892%)]\tLoss: 4.814934\n",
      "Train Epoche: 2 [857/96 (893%)]\tLoss: 26.867054\n",
      "Train Epoche: 2 [858/96 (894%)]\tLoss: 117.669472\n",
      "Train Epoche: 2 [859/96 (895%)]\tLoss: 13.965928\n",
      "Train Epoche: 2 [860/96 (896%)]\tLoss: 20.526056\n",
      "Train Epoche: 2 [861/96 (897%)]\tLoss: 12.562766\n",
      "Train Epoche: 2 [862/96 (898%)]\tLoss: 9.298139\n",
      "Train Epoche: 2 [863/96 (899%)]\tLoss: 1.847401\n",
      "Train Epoche: 2 [864/96 (900%)]\tLoss: 24.277523\n",
      "Train Epoche: 2 [865/96 (901%)]\tLoss: 111.352554\n",
      "Train Epoche: 2 [866/96 (902%)]\tLoss: 31.992479\n",
      "Train Epoche: 2 [867/96 (903%)]\tLoss: 1.012930\n",
      "Train Epoche: 2 [868/96 (904%)]\tLoss: 1.598385\n",
      "Train Epoche: 2 [869/96 (905%)]\tLoss: 2.275351\n",
      "Train Epoche: 2 [870/96 (906%)]\tLoss: 8.441844\n",
      "Train Epoche: 2 [871/96 (907%)]\tLoss: 0.129712\n",
      "Train Epoche: 2 [872/96 (908%)]\tLoss: 0.789259\n",
      "Train Epoche: 2 [873/96 (909%)]\tLoss: 57.733250\n",
      "Train Epoche: 2 [874/96 (910%)]\tLoss: 0.002423\n",
      "Train Epoche: 2 [875/96 (911%)]\tLoss: 7.373639\n",
      "Train Epoche: 2 [876/96 (912%)]\tLoss: 0.024751\n",
      "Train Epoche: 2 [877/96 (914%)]\tLoss: 2.498250\n",
      "Train Epoche: 2 [878/96 (915%)]\tLoss: 11.109926\n",
      "Train Epoche: 2 [879/96 (916%)]\tLoss: 10.926268\n",
      "Train Epoche: 2 [880/96 (917%)]\tLoss: 0.293100\n",
      "Train Epoche: 2 [881/96 (918%)]\tLoss: 0.208838\n",
      "Train Epoche: 2 [882/96 (919%)]\tLoss: 4.191220\n",
      "Train Epoche: 2 [883/96 (920%)]\tLoss: 36.779194\n",
      "Train Epoche: 2 [884/96 (921%)]\tLoss: 0.267189\n",
      "Train Epoche: 2 [885/96 (922%)]\tLoss: 17.163008\n",
      "Train Epoche: 2 [886/96 (923%)]\tLoss: 4.395050\n",
      "Train Epoche: 2 [887/96 (924%)]\tLoss: 4.698123\n",
      "Train Epoche: 2 [888/96 (925%)]\tLoss: 1.957047\n",
      "Train Epoche: 2 [889/96 (926%)]\tLoss: 0.002930\n",
      "Train Epoche: 2 [890/96 (927%)]\tLoss: 0.001935\n",
      "Train Epoche: 2 [891/96 (928%)]\tLoss: 12.456058\n",
      "Train Epoche: 2 [892/96 (929%)]\tLoss: 0.069402\n",
      "Train Epoche: 2 [893/96 (930%)]\tLoss: 0.000596\n",
      "Train Epoche: 2 [894/96 (931%)]\tLoss: 9.536311\n",
      "Train Epoche: 2 [895/96 (932%)]\tLoss: 0.096834\n",
      "Train Epoche: 2 [896/96 (933%)]\tLoss: 0.683835\n",
      "Train Epoche: 2 [897/96 (934%)]\tLoss: 2.444174\n",
      "Train Epoche: 2 [898/96 (935%)]\tLoss: 6.971515\n",
      "Train Epoche: 2 [899/96 (936%)]\tLoss: 1.692243\n",
      "Train Epoche: 2 [900/96 (938%)]\tLoss: 101.482559\n",
      "Train Epoche: 2 [901/96 (939%)]\tLoss: 3.702975\n",
      "Train Epoche: 2 [902/96 (940%)]\tLoss: 2.937570\n",
      "Train Epoche: 2 [903/96 (941%)]\tLoss: 2.224890\n",
      "Train Epoche: 2 [904/96 (942%)]\tLoss: 8.248952\n",
      "Train Epoche: 2 [905/96 (943%)]\tLoss: 81.753792\n",
      "Train Epoche: 2 [906/96 (944%)]\tLoss: 1.322887\n",
      "Train Epoche: 2 [907/96 (945%)]\tLoss: 14.800840\n",
      "Train Epoche: 2 [908/96 (946%)]\tLoss: 8.618970\n",
      "Train Epoche: 2 [909/96 (947%)]\tLoss: 2.939276\n",
      "Train Epoche: 2 [910/96 (948%)]\tLoss: 6.985084\n",
      "Train Epoche: 2 [911/96 (949%)]\tLoss: 20.808048\n",
      "Train Epoche: 2 [912/96 (950%)]\tLoss: 1.842229\n",
      "Train Epoche: 2 [913/96 (951%)]\tLoss: 0.631061\n",
      "Train Epoche: 2 [914/96 (952%)]\tLoss: 26.494350\n",
      "Train Epoche: 2 [915/96 (953%)]\tLoss: 4.835091\n",
      "Train Epoche: 2 [916/96 (954%)]\tLoss: 2.508445\n",
      "Train Epoche: 2 [917/96 (955%)]\tLoss: 10.641696\n",
      "Train Epoche: 2 [918/96 (956%)]\tLoss: 3.723488\n",
      "Train Epoche: 2 [919/96 (957%)]\tLoss: 32.834076\n",
      "Train Epoche: 2 [920/96 (958%)]\tLoss: 8.758507\n",
      "Train Epoche: 2 [921/96 (959%)]\tLoss: 3.207549\n",
      "Train Epoche: 2 [922/96 (960%)]\tLoss: 77.102295\n",
      "Train Epoche: 2 [923/96 (961%)]\tLoss: 37.357399\n",
      "Train Epoche: 2 [924/96 (962%)]\tLoss: 58.999165\n",
      "Train Epoche: 2 [925/96 (964%)]\tLoss: 30.615894\n",
      "Train Epoche: 2 [926/96 (965%)]\tLoss: 0.220083\n",
      "Train Epoche: 2 [927/96 (966%)]\tLoss: 1.812159\n",
      "Train Epoche: 2 [928/96 (967%)]\tLoss: 9.643510\n",
      "Train Epoche: 2 [929/96 (968%)]\tLoss: 4.385303\n",
      "Train Epoche: 2 [930/96 (969%)]\tLoss: 2.400251\n",
      "Train Epoche: 2 [931/96 (970%)]\tLoss: 3.624844\n",
      "Train Epoche: 2 [932/96 (971%)]\tLoss: 154.745041\n",
      "Train Epoche: 2 [933/96 (972%)]\tLoss: 363.279663\n",
      "Train Epoche: 2 [934/96 (973%)]\tLoss: 1.792732\n",
      "Train Epoche: 2 [935/96 (974%)]\tLoss: 75.485718\n",
      "Train Epoche: 2 [936/96 (975%)]\tLoss: 1.216947\n",
      "Train Epoche: 2 [937/96 (976%)]\tLoss: 0.010652\n",
      "Train Epoche: 2 [938/96 (977%)]\tLoss: 5.351719\n",
      "Train Epoche: 2 [939/96 (978%)]\tLoss: 41.791245\n",
      "Train Epoche: 2 [940/96 (979%)]\tLoss: 1.206274\n",
      "Train Epoche: 2 [941/96 (980%)]\tLoss: 101.696800\n",
      "Train Epoche: 2 [942/96 (981%)]\tLoss: 0.883464\n",
      "Train Epoche: 2 [943/96 (982%)]\tLoss: 4.011548\n",
      "Train Epoche: 2 [944/96 (983%)]\tLoss: 84.814857\n",
      "Train Epoche: 2 [945/96 (984%)]\tLoss: 40.459488\n",
      "Train Epoche: 2 [946/96 (985%)]\tLoss: 8.213438\n",
      "Train Epoche: 2 [947/96 (986%)]\tLoss: 22.965588\n",
      "Train Epoche: 2 [948/96 (988%)]\tLoss: 11.448566\n",
      "Train Epoche: 2 [949/96 (989%)]\tLoss: 36.115734\n",
      "Train Epoche: 2 [950/96 (990%)]\tLoss: 6.229446\n",
      "Train Epoche: 2 [951/96 (991%)]\tLoss: 2.777002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [952/96 (992%)]\tLoss: 11.983257\n",
      "Train Epoche: 2 [953/96 (993%)]\tLoss: 39.802879\n",
      "Train Epoche: 2 [954/96 (994%)]\tLoss: 192.883072\n",
      "Train Epoche: 2 [955/96 (995%)]\tLoss: 36.025089\n",
      "Train Epoche: 2 [956/96 (996%)]\tLoss: 62.833248\n",
      "Train Epoche: 2 [957/96 (997%)]\tLoss: 4.230525\n",
      "Train Epoche: 2 [958/96 (998%)]\tLoss: 57.939938\n",
      "Train Epoche: 2 [959/96 (999%)]\tLoss: 81.404282\n",
      "Train Epoche: 2 [960/96 (1000%)]\tLoss: 2.219886\n",
      "Train Epoche: 2 [961/96 (1001%)]\tLoss: 35.970757\n",
      "Train Epoche: 2 [962/96 (1002%)]\tLoss: 6.627521\n",
      "Train Epoche: 2 [963/96 (1003%)]\tLoss: 44.533886\n",
      "Train Epoche: 2 [964/96 (1004%)]\tLoss: 96.781914\n",
      "Train Epoche: 2 [965/96 (1005%)]\tLoss: 29.186018\n",
      "Train Epoche: 2 [966/96 (1006%)]\tLoss: 45.516987\n",
      "Train Epoche: 2 [967/96 (1007%)]\tLoss: 53.149090\n",
      "Train Epoche: 2 [968/96 (1008%)]\tLoss: 24.373125\n",
      "Train Epoche: 2 [969/96 (1009%)]\tLoss: 91.902939\n",
      "Train Epoche: 2 [970/96 (1010%)]\tLoss: 0.188114\n",
      "Train Epoche: 2 [971/96 (1011%)]\tLoss: 2.534271\n",
      "Train Epoche: 2 [972/96 (1012%)]\tLoss: 0.116754\n",
      "Train Epoche: 2 [973/96 (1014%)]\tLoss: 13.140819\n",
      "Train Epoche: 2 [974/96 (1015%)]\tLoss: 0.979062\n",
      "Train Epoche: 2 [975/96 (1016%)]\tLoss: 0.283943\n",
      "Train Epoche: 2 [976/96 (1017%)]\tLoss: 13.091615\n",
      "Train Epoche: 2 [977/96 (1018%)]\tLoss: 49.658367\n",
      "Train Epoche: 2 [978/96 (1019%)]\tLoss: 1.132018\n",
      "Train Epoche: 2 [979/96 (1020%)]\tLoss: 44.025723\n",
      "Train Epoche: 2 [980/96 (1021%)]\tLoss: 0.392021\n",
      "Train Epoche: 2 [981/96 (1022%)]\tLoss: 197.737122\n",
      "Train Epoche: 2 [982/96 (1023%)]\tLoss: 0.190014\n",
      "Train Epoche: 2 [983/96 (1024%)]\tLoss: 1.636482\n",
      "Train Epoche: 2 [984/96 (1025%)]\tLoss: 1.722879\n",
      "Train Epoche: 2 [985/96 (1026%)]\tLoss: 10.157785\n",
      "Train Epoche: 2 [986/96 (1027%)]\tLoss: 0.064389\n",
      "Train Epoche: 2 [987/96 (1028%)]\tLoss: 0.528421\n",
      "Train Epoche: 2 [988/96 (1029%)]\tLoss: 44.254005\n",
      "Train Epoche: 2 [989/96 (1030%)]\tLoss: 4.162970\n",
      "Train Epoche: 2 [990/96 (1031%)]\tLoss: 1.639183\n",
      "Train Epoche: 2 [991/96 (1032%)]\tLoss: 1.723773\n",
      "Train Epoche: 2 [992/96 (1033%)]\tLoss: 0.026173\n",
      "Train Epoche: 2 [993/96 (1034%)]\tLoss: 7.703696\n",
      "Train Epoche: 2 [994/96 (1035%)]\tLoss: 174.161987\n",
      "Train Epoche: 2 [995/96 (1036%)]\tLoss: 9.512666\n",
      "Train Epoche: 2 [996/96 (1038%)]\tLoss: 67.067238\n",
      "Train Epoche: 2 [997/96 (1039%)]\tLoss: 0.001182\n",
      "Train Epoche: 2 [998/96 (1040%)]\tLoss: 8.640066\n",
      "Train Epoche: 2 [999/96 (1041%)]\tLoss: 3.257009\n",
      "Train Epoche: 2 [1000/96 (1042%)]\tLoss: 3.182004\n",
      "Train Epoche: 2 [1001/96 (1043%)]\tLoss: 36.149811\n",
      "Train Epoche: 2 [1002/96 (1044%)]\tLoss: 0.049805\n",
      "Train Epoche: 2 [1003/96 (1045%)]\tLoss: 15.628547\n",
      "Train Epoche: 2 [1004/96 (1046%)]\tLoss: 6.381919\n",
      "Train Epoche: 2 [1005/96 (1047%)]\tLoss: 11.760637\n",
      "Train Epoche: 2 [1006/96 (1048%)]\tLoss: 2.369013\n",
      "Train Epoche: 2 [1007/96 (1049%)]\tLoss: 0.114317\n",
      "Train Epoche: 2 [1008/96 (1050%)]\tLoss: 3.807358\n",
      "Train Epoche: 2 [1009/96 (1051%)]\tLoss: 4.363342\n",
      "Train Epoche: 2 [1010/96 (1052%)]\tLoss: 1.191467\n",
      "Train Epoche: 2 [1011/96 (1053%)]\tLoss: 2.652304\n",
      "Train Epoche: 2 [1012/96 (1054%)]\tLoss: 211.795593\n",
      "Train Epoche: 2 [1013/96 (1055%)]\tLoss: 123.988892\n",
      "Train Epoche: 2 [1014/96 (1056%)]\tLoss: 23.170759\n",
      "Train Epoche: 2 [1015/96 (1057%)]\tLoss: 59.920971\n",
      "Train Epoche: 2 [1016/96 (1058%)]\tLoss: 94.157295\n",
      "Train Epoche: 2 [1017/96 (1059%)]\tLoss: 182.437149\n",
      "Train Epoche: 2 [1018/96 (1060%)]\tLoss: 52.539146\n",
      "Train Epoche: 2 [1019/96 (1061%)]\tLoss: 0.229346\n",
      "Train Epoche: 2 [1020/96 (1062%)]\tLoss: 12.330066\n",
      "Train Epoche: 2 [1021/96 (1064%)]\tLoss: 87.443977\n",
      "Train Epoche: 2 [1022/96 (1065%)]\tLoss: 77.184418\n",
      "Train Epoche: 2 [1023/96 (1066%)]\tLoss: 24.421373\n",
      "Train Epoche: 2 [1024/96 (1067%)]\tLoss: 0.488151\n",
      "Train Epoche: 2 [1025/96 (1068%)]\tLoss: 4.302482\n",
      "Train Epoche: 2 [1026/96 (1069%)]\tLoss: 17.399591\n",
      "Train Epoche: 2 [1027/96 (1070%)]\tLoss: 42.416355\n",
      "Train Epoche: 2 [1028/96 (1071%)]\tLoss: 56.719318\n",
      "Train Epoche: 2 [1029/96 (1072%)]\tLoss: 8.788025\n",
      "Train Epoche: 2 [1030/96 (1073%)]\tLoss: 3.285550\n",
      "Train Epoche: 2 [1031/96 (1074%)]\tLoss: 11.697537\n",
      "Train Epoche: 2 [1032/96 (1075%)]\tLoss: 4.942620\n",
      "Train Epoche: 2 [1033/96 (1076%)]\tLoss: 422.123291\n",
      "Train Epoche: 2 [1034/96 (1077%)]\tLoss: 194.243484\n",
      "Train Epoche: 2 [1035/96 (1078%)]\tLoss: 66.351128\n",
      "Train Epoche: 2 [1036/96 (1079%)]\tLoss: 0.754594\n",
      "Train Epoche: 2 [1037/96 (1080%)]\tLoss: 0.533754\n",
      "Train Epoche: 2 [1038/96 (1081%)]\tLoss: 24.700296\n",
      "Train Epoche: 2 [1039/96 (1082%)]\tLoss: 163.130768\n",
      "Train Epoche: 2 [1040/96 (1083%)]\tLoss: 183.407455\n",
      "Train Epoche: 2 [1041/96 (1084%)]\tLoss: 0.645723\n",
      "Train Epoche: 2 [1042/96 (1085%)]\tLoss: 0.004308\n",
      "Train Epoche: 2 [1043/96 (1086%)]\tLoss: 88.674355\n",
      "Train Epoche: 2 [1044/96 (1088%)]\tLoss: 209.380356\n",
      "Train Epoche: 2 [1045/96 (1089%)]\tLoss: 0.009087\n",
      "Train Epoche: 2 [1046/96 (1090%)]\tLoss: 0.272509\n",
      "Train Epoche: 2 [1047/96 (1091%)]\tLoss: 56.056683\n",
      "Train Epoche: 2 [1048/96 (1092%)]\tLoss: 9.952416\n",
      "Train Epoche: 2 [1049/96 (1093%)]\tLoss: 195.658539\n",
      "Train Epoche: 2 [1050/96 (1094%)]\tLoss: 46.882690\n",
      "Train Epoche: 2 [1051/96 (1095%)]\tLoss: 12.769423\n",
      "Train Epoche: 2 [1052/96 (1096%)]\tLoss: 13.401258\n",
      "Train Epoche: 2 [1053/96 (1097%)]\tLoss: 9.460416\n",
      "Train Epoche: 2 [1054/96 (1098%)]\tLoss: 4.847853\n",
      "Train Epoche: 2 [1055/96 (1099%)]\tLoss: 0.663551\n",
      "Train Epoche: 2 [1056/96 (1100%)]\tLoss: 9.667762\n",
      "Train Epoche: 2 [1057/96 (1101%)]\tLoss: 0.338341\n",
      "Train Epoche: 2 [1058/96 (1102%)]\tLoss: 1.941061\n",
      "Train Epoche: 2 [1059/96 (1103%)]\tLoss: 0.368644\n",
      "Train Epoche: 2 [1060/96 (1104%)]\tLoss: 0.165641\n",
      "Train Epoche: 2 [1061/96 (1105%)]\tLoss: 0.101241\n",
      "Train Epoche: 2 [1062/96 (1106%)]\tLoss: 5.204210\n",
      "Train Epoche: 2 [1063/96 (1107%)]\tLoss: 0.202372\n",
      "Train Epoche: 2 [1064/96 (1108%)]\tLoss: 4.380250\n",
      "Train Epoche: 2 [1065/96 (1109%)]\tLoss: 1.719944\n",
      "Train Epoche: 2 [1066/96 (1110%)]\tLoss: 45.730049\n",
      "Train Epoche: 2 [1067/96 (1111%)]\tLoss: 3.745408\n",
      "Train Epoche: 2 [1068/96 (1112%)]\tLoss: 14.582928\n",
      "Train Epoche: 2 [1069/96 (1114%)]\tLoss: 2.162645\n",
      "Train Epoche: 2 [1070/96 (1115%)]\tLoss: 43.936329\n",
      "Train Epoche: 2 [1071/96 (1116%)]\tLoss: 1.020967\n",
      "Train Epoche: 2 [1072/96 (1117%)]\tLoss: 0.081605\n",
      "Train Epoche: 2 [1073/96 (1118%)]\tLoss: 8.295038\n",
      "Train Epoche: 2 [1074/96 (1119%)]\tLoss: 2.920794\n",
      "Train Epoche: 2 [1075/96 (1120%)]\tLoss: 4.205884\n",
      "Train Epoche: 2 [1076/96 (1121%)]\tLoss: 6.292661\n",
      "Train Epoche: 2 [1077/96 (1122%)]\tLoss: 0.491367\n",
      "Train Epoche: 2 [1078/96 (1123%)]\tLoss: 0.281245\n",
      "Train Epoche: 2 [1079/96 (1124%)]\tLoss: 0.749892\n",
      "Train Epoche: 2 [1080/96 (1125%)]\tLoss: 0.529341\n",
      "Train Epoche: 2 [1081/96 (1126%)]\tLoss: 1.175495\n",
      "Train Epoche: 2 [1082/96 (1127%)]\tLoss: 1.218663\n",
      "Train Epoche: 2 [1083/96 (1128%)]\tLoss: 0.005397\n",
      "Train Epoche: 2 [1084/96 (1129%)]\tLoss: 8.793284\n",
      "Train Epoche: 2 [1085/96 (1130%)]\tLoss: 13.482530\n",
      "Train Epoche: 2 [1086/96 (1131%)]\tLoss: 0.106427\n",
      "Train Epoche: 2 [1087/96 (1132%)]\tLoss: 2.358119\n",
      "Train Epoche: 2 [1088/96 (1133%)]\tLoss: 10.274935\n",
      "Train Epoche: 2 [1089/96 (1134%)]\tLoss: 1.436744\n",
      "Train Epoche: 2 [1090/96 (1135%)]\tLoss: 2.059025\n",
      "Train Epoche: 2 [1091/96 (1136%)]\tLoss: 11.852435\n",
      "Train Epoche: 2 [1092/96 (1138%)]\tLoss: 2.648777\n",
      "Train Epoche: 2 [1093/96 (1139%)]\tLoss: 5.079427\n",
      "Train Epoche: 2 [1094/96 (1140%)]\tLoss: 59.472767\n",
      "Train Epoche: 2 [1095/96 (1141%)]\tLoss: 7.210830\n",
      "Train Epoche: 2 [1096/96 (1142%)]\tLoss: 14.821930\n",
      "Train Epoche: 2 [1097/96 (1143%)]\tLoss: 58.269958\n",
      "Train Epoche: 2 [1098/96 (1144%)]\tLoss: 6.918177\n",
      "Train Epoche: 2 [1099/96 (1145%)]\tLoss: 0.000152\n",
      "Train Epoche: 2 [1100/96 (1146%)]\tLoss: 87.780281\n",
      "Train Epoche: 2 [1101/96 (1147%)]\tLoss: 0.096576\n",
      "Train Epoche: 2 [1102/96 (1148%)]\tLoss: 0.058955\n",
      "Train Epoche: 2 [1103/96 (1149%)]\tLoss: 3.090410\n",
      "Train Epoche: 2 [1104/96 (1150%)]\tLoss: 32.157051\n",
      "Train Epoche: 2 [1105/96 (1151%)]\tLoss: 1.213371\n",
      "Train Epoche: 2 [1106/96 (1152%)]\tLoss: 7.521915\n",
      "Train Epoche: 2 [1107/96 (1153%)]\tLoss: 0.000672\n",
      "Train Epoche: 2 [1108/96 (1154%)]\tLoss: 4.511492\n",
      "Train Epoche: 2 [1109/96 (1155%)]\tLoss: 1.049585\n",
      "Train Epoche: 2 [1110/96 (1156%)]\tLoss: 362.260498\n",
      "Train Epoche: 2 [1111/96 (1157%)]\tLoss: 1.920033\n",
      "Train Epoche: 2 [1112/96 (1158%)]\tLoss: 4.883938\n",
      "Train Epoche: 2 [1113/96 (1159%)]\tLoss: 0.753057\n",
      "Train Epoche: 2 [1114/96 (1160%)]\tLoss: 0.251452\n",
      "Train Epoche: 2 [1115/96 (1161%)]\tLoss: 2.040229\n",
      "Train Epoche: 2 [1116/96 (1162%)]\tLoss: 6.576787\n",
      "Train Epoche: 2 [1117/96 (1164%)]\tLoss: 0.155766\n",
      "Train Epoche: 2 [1118/96 (1165%)]\tLoss: 0.000164\n",
      "Train Epoche: 2 [1119/96 (1166%)]\tLoss: 1.951458\n",
      "Train Epoche: 2 [1120/96 (1167%)]\tLoss: 2.269078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1121/96 (1168%)]\tLoss: 7.531726\n",
      "Train Epoche: 2 [1122/96 (1169%)]\tLoss: 11.466447\n",
      "Train Epoche: 2 [1123/96 (1170%)]\tLoss: 1.117296\n",
      "Train Epoche: 2 [1124/96 (1171%)]\tLoss: 2.685496\n",
      "Train Epoche: 2 [1125/96 (1172%)]\tLoss: 35.179768\n",
      "Train Epoche: 2 [1126/96 (1173%)]\tLoss: 1.507728\n",
      "Train Epoche: 2 [1127/96 (1174%)]\tLoss: 6.097484\n",
      "Train Epoche: 2 [1128/96 (1175%)]\tLoss: 5.943238\n",
      "Train Epoche: 2 [1129/96 (1176%)]\tLoss: 13.704893\n",
      "Train Epoche: 2 [1130/96 (1177%)]\tLoss: 20.240389\n",
      "Train Epoche: 2 [1131/96 (1178%)]\tLoss: 15.371284\n",
      "Train Epoche: 2 [1132/96 (1179%)]\tLoss: 20.661169\n",
      "Train Epoche: 2 [1133/96 (1180%)]\tLoss: 0.208920\n",
      "Train Epoche: 2 [1134/96 (1181%)]\tLoss: 72.256470\n",
      "Train Epoche: 2 [1135/96 (1182%)]\tLoss: 400.501648\n",
      "Train Epoche: 2 [1136/96 (1183%)]\tLoss: 217.849503\n",
      "Train Epoche: 2 [1137/96 (1184%)]\tLoss: 0.120355\n",
      "Train Epoche: 2 [1138/96 (1185%)]\tLoss: 100.714279\n",
      "Train Epoche: 2 [1139/96 (1186%)]\tLoss: 0.043454\n",
      "Train Epoche: 2 [1140/96 (1188%)]\tLoss: 6.679977\n",
      "Train Epoche: 2 [1141/96 (1189%)]\tLoss: 127.680458\n",
      "Train Epoche: 2 [1142/96 (1190%)]\tLoss: 0.149516\n",
      "Train Epoche: 2 [1143/96 (1191%)]\tLoss: 27.239244\n",
      "Train Epoche: 2 [1144/96 (1192%)]\tLoss: 1.260515\n",
      "Train Epoche: 2 [1145/96 (1193%)]\tLoss: 0.429465\n",
      "Train Epoche: 2 [1146/96 (1194%)]\tLoss: 11.427963\n",
      "Train Epoche: 2 [1147/96 (1195%)]\tLoss: 0.117711\n",
      "Train Epoche: 2 [1148/96 (1196%)]\tLoss: 0.966952\n",
      "Train Epoche: 2 [1149/96 (1197%)]\tLoss: 27.389538\n",
      "Train Epoche: 2 [1150/96 (1198%)]\tLoss: 0.015113\n",
      "Train Epoche: 2 [1151/96 (1199%)]\tLoss: 4.359715\n",
      "Train Epoche: 2 [1152/96 (1200%)]\tLoss: 3.870095\n",
      "Train Epoche: 2 [1153/96 (1201%)]\tLoss: 2.413945\n",
      "Train Epoche: 2 [1154/96 (1202%)]\tLoss: 52.553864\n",
      "Train Epoche: 2 [1155/96 (1203%)]\tLoss: 1.025560\n",
      "Train Epoche: 2 [1156/96 (1204%)]\tLoss: 2.454220\n",
      "Train Epoche: 2 [1157/96 (1205%)]\tLoss: 19.772947\n",
      "Train Epoche: 2 [1158/96 (1206%)]\tLoss: 3.695886\n",
      "Train Epoche: 2 [1159/96 (1207%)]\tLoss: 6.412775\n",
      "Train Epoche: 2 [1160/96 (1208%)]\tLoss: 7.795698\n",
      "Train Epoche: 2 [1161/96 (1209%)]\tLoss: 12.521285\n",
      "Train Epoche: 2 [1162/96 (1210%)]\tLoss: 48.371838\n",
      "Train Epoche: 2 [1163/96 (1211%)]\tLoss: 0.123121\n",
      "Train Epoche: 2 [1164/96 (1212%)]\tLoss: 0.047240\n",
      "Train Epoche: 2 [1165/96 (1214%)]\tLoss: 9.345221\n",
      "Train Epoche: 2 [1166/96 (1215%)]\tLoss: 0.026030\n",
      "Train Epoche: 2 [1167/96 (1216%)]\tLoss: 4.990412\n",
      "Train Epoche: 2 [1168/96 (1217%)]\tLoss: 11.126774\n",
      "Train Epoche: 2 [1169/96 (1218%)]\tLoss: 7.259179\n",
      "Train Epoche: 2 [1170/96 (1219%)]\tLoss: 0.000047\n",
      "Train Epoche: 2 [1171/96 (1220%)]\tLoss: 0.065847\n",
      "Train Epoche: 2 [1172/96 (1221%)]\tLoss: 7.240983\n",
      "Train Epoche: 2 [1173/96 (1222%)]\tLoss: 31.092991\n",
      "Train Epoche: 2 [1174/96 (1223%)]\tLoss: 0.520508\n",
      "Train Epoche: 2 [1175/96 (1224%)]\tLoss: 4.881491\n",
      "Train Epoche: 2 [1176/96 (1225%)]\tLoss: 31.143913\n",
      "Train Epoche: 2 [1177/96 (1226%)]\tLoss: 12.346607\n",
      "Train Epoche: 2 [1178/96 (1227%)]\tLoss: 6.624444\n",
      "Train Epoche: 2 [1179/96 (1228%)]\tLoss: 22.175846\n",
      "Train Epoche: 2 [1180/96 (1229%)]\tLoss: 14.842733\n",
      "Train Epoche: 2 [1181/96 (1230%)]\tLoss: 66.122086\n",
      "Train Epoche: 2 [1182/96 (1231%)]\tLoss: 3.813807\n",
      "Train Epoche: 2 [1183/96 (1232%)]\tLoss: 28.341482\n",
      "Train Epoche: 2 [1184/96 (1233%)]\tLoss: 11.026996\n",
      "Train Epoche: 2 [1185/96 (1234%)]\tLoss: 0.589725\n",
      "Train Epoche: 2 [1186/96 (1235%)]\tLoss: 8.789258\n",
      "Train Epoche: 2 [1187/96 (1236%)]\tLoss: 7.495150\n",
      "Train Epoche: 2 [1188/96 (1238%)]\tLoss: 42.033356\n",
      "Train Epoche: 2 [1189/96 (1239%)]\tLoss: 0.000793\n",
      "Train Epoche: 2 [1190/96 (1240%)]\tLoss: 2.325161\n",
      "Train Epoche: 2 [1191/96 (1241%)]\tLoss: 3.211147\n",
      "Train Epoche: 2 [1192/96 (1242%)]\tLoss: 1.121676\n",
      "Train Epoche: 2 [1193/96 (1243%)]\tLoss: 0.027380\n",
      "Train Epoche: 2 [1194/96 (1244%)]\tLoss: 0.593431\n",
      "Train Epoche: 2 [1195/96 (1245%)]\tLoss: 0.173468\n",
      "Train Epoche: 2 [1196/96 (1246%)]\tLoss: 3.622439\n",
      "Train Epoche: 2 [1197/96 (1247%)]\tLoss: 0.924125\n",
      "Train Epoche: 2 [1198/96 (1248%)]\tLoss: 181.456970\n",
      "Train Epoche: 2 [1199/96 (1249%)]\tLoss: 1.109395\n",
      "Train Epoche: 2 [1200/96 (1250%)]\tLoss: 1.772204\n",
      "Train Epoche: 2 [1201/96 (1251%)]\tLoss: 9.585598\n",
      "Train Epoche: 2 [1202/96 (1252%)]\tLoss: 0.399080\n",
      "Train Epoche: 2 [1203/96 (1253%)]\tLoss: 80.882729\n",
      "Train Epoche: 2 [1204/96 (1254%)]\tLoss: 10.251281\n",
      "Train Epoche: 2 [1205/96 (1255%)]\tLoss: 6.405841\n",
      "Train Epoche: 2 [1206/96 (1256%)]\tLoss: 1.982535\n",
      "Train Epoche: 2 [1207/96 (1257%)]\tLoss: 11.641299\n",
      "Train Epoche: 2 [1208/96 (1258%)]\tLoss: 0.271891\n",
      "Train Epoche: 2 [1209/96 (1259%)]\tLoss: 22.045153\n",
      "Train Epoche: 2 [1210/96 (1260%)]\tLoss: 12.937899\n",
      "Train Epoche: 2 [1211/96 (1261%)]\tLoss: 3.078255\n",
      "Train Epoche: 2 [1212/96 (1262%)]\tLoss: 5.871485\n",
      "Train Epoche: 2 [1213/96 (1264%)]\tLoss: 7.973247\n",
      "Train Epoche: 2 [1214/96 (1265%)]\tLoss: 3.870894\n",
      "Train Epoche: 2 [1215/96 (1266%)]\tLoss: 2.414216\n",
      "Train Epoche: 2 [1216/96 (1267%)]\tLoss: 2.043739\n",
      "Train Epoche: 2 [1217/96 (1268%)]\tLoss: 7.807090\n",
      "Train Epoche: 2 [1218/96 (1269%)]\tLoss: 0.688015\n",
      "Train Epoche: 2 [1219/96 (1270%)]\tLoss: 1.117517\n",
      "Train Epoche: 2 [1220/96 (1271%)]\tLoss: 3.986060\n",
      "Train Epoche: 2 [1221/96 (1272%)]\tLoss: 7.265218\n",
      "Train Epoche: 2 [1222/96 (1273%)]\tLoss: 1.759167\n",
      "Train Epoche: 2 [1223/96 (1274%)]\tLoss: 4.630498\n",
      "Train Epoche: 2 [1224/96 (1275%)]\tLoss: 2.289580\n",
      "Train Epoche: 2 [1225/96 (1276%)]\tLoss: 27.708832\n",
      "Train Epoche: 2 [1226/96 (1277%)]\tLoss: 4.421950\n",
      "Train Epoche: 2 [1227/96 (1278%)]\tLoss: 0.045148\n",
      "Train Epoche: 2 [1228/96 (1279%)]\tLoss: 0.007713\n",
      "Train Epoche: 2 [1229/96 (1280%)]\tLoss: 5.077566\n",
      "Train Epoche: 2 [1230/96 (1281%)]\tLoss: 0.622571\n",
      "Train Epoche: 2 [1231/96 (1282%)]\tLoss: 62.623207\n",
      "Train Epoche: 2 [1232/96 (1283%)]\tLoss: 3.100007\n",
      "Train Epoche: 2 [1233/96 (1284%)]\tLoss: 4.140827\n",
      "Train Epoche: 2 [1234/96 (1285%)]\tLoss: 0.561266\n",
      "Train Epoche: 2 [1235/96 (1286%)]\tLoss: 0.762235\n",
      "Train Epoche: 2 [1236/96 (1288%)]\tLoss: 2.570877\n",
      "Train Epoche: 2 [1237/96 (1289%)]\tLoss: 8.167204\n",
      "Train Epoche: 2 [1238/96 (1290%)]\tLoss: 0.071094\n",
      "Train Epoche: 2 [1239/96 (1291%)]\tLoss: 25.634285\n",
      "Train Epoche: 2 [1240/96 (1292%)]\tLoss: 2.104187\n",
      "Train Epoche: 2 [1241/96 (1293%)]\tLoss: 3.992734\n",
      "Train Epoche: 2 [1242/96 (1294%)]\tLoss: 0.008897\n",
      "Train Epoche: 2 [1243/96 (1295%)]\tLoss: 4.876817\n",
      "Train Epoche: 2 [1244/96 (1296%)]\tLoss: 0.538208\n",
      "Train Epoche: 2 [1245/96 (1297%)]\tLoss: 0.388760\n",
      "Train Epoche: 2 [1246/96 (1298%)]\tLoss: 0.541763\n",
      "Train Epoche: 2 [1247/96 (1299%)]\tLoss: 1.255351\n",
      "Train Epoche: 2 [1248/96 (1300%)]\tLoss: 4.947152\n",
      "Train Epoche: 2 [1249/96 (1301%)]\tLoss: 6.789600\n",
      "Train Epoche: 2 [1250/96 (1302%)]\tLoss: 1.320497\n",
      "Train Epoche: 2 [1251/96 (1303%)]\tLoss: 7.936517\n",
      "Train Epoche: 2 [1252/96 (1304%)]\tLoss: 0.025141\n",
      "Train Epoche: 2 [1253/96 (1305%)]\tLoss: 0.166319\n",
      "Train Epoche: 2 [1254/96 (1306%)]\tLoss: 47.330322\n",
      "Train Epoche: 2 [1255/96 (1307%)]\tLoss: 5.353515\n",
      "Train Epoche: 2 [1256/96 (1308%)]\tLoss: 0.074787\n",
      "Train Epoche: 2 [1257/96 (1309%)]\tLoss: 6.746639\n",
      "Train Epoche: 2 [1258/96 (1310%)]\tLoss: 8.813324\n",
      "Train Epoche: 2 [1259/96 (1311%)]\tLoss: 33.160072\n",
      "Train Epoche: 2 [1260/96 (1312%)]\tLoss: 5.227838\n",
      "Train Epoche: 2 [1261/96 (1314%)]\tLoss: 1.043826\n",
      "Train Epoche: 2 [1262/96 (1315%)]\tLoss: 2.101804\n",
      "Train Epoche: 2 [1263/96 (1316%)]\tLoss: 3.516605\n",
      "Train Epoche: 2 [1264/96 (1317%)]\tLoss: 0.273539\n",
      "Train Epoche: 2 [1265/96 (1318%)]\tLoss: 0.253107\n",
      "Train Epoche: 2 [1266/96 (1319%)]\tLoss: 310.421539\n",
      "Train Epoche: 2 [1267/96 (1320%)]\tLoss: 0.248131\n",
      "Train Epoche: 2 [1268/96 (1321%)]\tLoss: 4.514198\n",
      "Train Epoche: 2 [1269/96 (1322%)]\tLoss: 4.076338\n",
      "Train Epoche: 2 [1270/96 (1323%)]\tLoss: 0.019841\n",
      "Train Epoche: 2 [1271/96 (1324%)]\tLoss: 3.885874\n",
      "Train Epoche: 2 [1272/96 (1325%)]\tLoss: 0.592277\n",
      "Train Epoche: 2 [1273/96 (1326%)]\tLoss: 8.158784\n",
      "Train Epoche: 2 [1274/96 (1327%)]\tLoss: 218.300476\n",
      "Train Epoche: 2 [1275/96 (1328%)]\tLoss: 17.281855\n",
      "Train Epoche: 2 [1276/96 (1329%)]\tLoss: 15.558433\n",
      "Train Epoche: 2 [1277/96 (1330%)]\tLoss: 0.049023\n",
      "Train Epoche: 2 [1278/96 (1331%)]\tLoss: 7.331134\n",
      "Train Epoche: 2 [1279/96 (1332%)]\tLoss: 6.997127\n",
      "Train Epoche: 2 [1280/96 (1333%)]\tLoss: 14.269846\n",
      "Train Epoche: 2 [1281/96 (1334%)]\tLoss: 65.660461\n",
      "Train Epoche: 2 [1282/96 (1335%)]\tLoss: 2.795298\n",
      "Train Epoche: 2 [1283/96 (1336%)]\tLoss: 0.076498\n",
      "Train Epoche: 2 [1284/96 (1338%)]\tLoss: 3.048684\n",
      "Train Epoche: 2 [1285/96 (1339%)]\tLoss: 8.575750\n",
      "Train Epoche: 2 [1286/96 (1340%)]\tLoss: 1.171816\n",
      "Train Epoche: 2 [1287/96 (1341%)]\tLoss: 1.415150\n",
      "Train Epoche: 2 [1288/96 (1342%)]\tLoss: 63.606716\n",
      "Train Epoche: 2 [1289/96 (1343%)]\tLoss: 1.103400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1290/96 (1344%)]\tLoss: 7.704498\n",
      "Train Epoche: 2 [1291/96 (1345%)]\tLoss: 14.030961\n",
      "Train Epoche: 2 [1292/96 (1346%)]\tLoss: 3.109624\n",
      "Train Epoche: 2 [1293/96 (1347%)]\tLoss: 0.426055\n",
      "Train Epoche: 2 [1294/96 (1348%)]\tLoss: 1.918009\n",
      "Train Epoche: 2 [1295/96 (1349%)]\tLoss: 0.756696\n",
      "Train Epoche: 2 [1296/96 (1350%)]\tLoss: 0.002928\n",
      "Train Epoche: 2 [1297/96 (1351%)]\tLoss: 1.508893\n",
      "Train Epoche: 2 [1298/96 (1352%)]\tLoss: 0.671838\n",
      "Train Epoche: 2 [1299/96 (1353%)]\tLoss: 5.042324\n",
      "Train Epoche: 2 [1300/96 (1354%)]\tLoss: 11.496982\n",
      "Train Epoche: 2 [1301/96 (1355%)]\tLoss: 13.256251\n",
      "Train Epoche: 2 [1302/96 (1356%)]\tLoss: 0.968382\n",
      "Train Epoche: 2 [1303/96 (1357%)]\tLoss: 11.977293\n",
      "Train Epoche: 2 [1304/96 (1358%)]\tLoss: 1.493287\n",
      "Train Epoche: 2 [1305/96 (1359%)]\tLoss: 9.760426\n",
      "Train Epoche: 2 [1306/96 (1360%)]\tLoss: 10.573692\n",
      "Train Epoche: 2 [1307/96 (1361%)]\tLoss: 4.558366\n",
      "Train Epoche: 2 [1308/96 (1362%)]\tLoss: 0.107928\n",
      "Train Epoche: 2 [1309/96 (1364%)]\tLoss: 2.602504\n",
      "Train Epoche: 2 [1310/96 (1365%)]\tLoss: 0.016900\n",
      "Train Epoche: 2 [1311/96 (1366%)]\tLoss: 112.370705\n",
      "Train Epoche: 2 [1312/96 (1367%)]\tLoss: 10.734313\n",
      "Train Epoche: 2 [1313/96 (1368%)]\tLoss: 16.085999\n",
      "Train Epoche: 2 [1314/96 (1369%)]\tLoss: 26.746836\n",
      "Train Epoche: 2 [1315/96 (1370%)]\tLoss: 0.000041\n",
      "Train Epoche: 2 [1316/96 (1371%)]\tLoss: 8.675810\n",
      "Train Epoche: 2 [1317/96 (1372%)]\tLoss: 0.652452\n",
      "Train Epoche: 2 [1318/96 (1373%)]\tLoss: 201.243973\n",
      "Train Epoche: 2 [1319/96 (1374%)]\tLoss: 8.160138\n",
      "Train Epoche: 2 [1320/96 (1375%)]\tLoss: 0.822530\n",
      "Train Epoche: 2 [1321/96 (1376%)]\tLoss: 6.609144\n",
      "Train Epoche: 2 [1322/96 (1377%)]\tLoss: 0.000013\n",
      "Train Epoche: 2 [1323/96 (1378%)]\tLoss: 14.496569\n",
      "Train Epoche: 2 [1324/96 (1379%)]\tLoss: 16.075483\n",
      "Train Epoche: 2 [1325/96 (1380%)]\tLoss: 18.710482\n",
      "Train Epoche: 2 [1326/96 (1381%)]\tLoss: 5.139119\n",
      "Train Epoche: 2 [1327/96 (1382%)]\tLoss: 52.299747\n",
      "Train Epoche: 2 [1328/96 (1383%)]\tLoss: 40.377560\n",
      "Train Epoche: 2 [1329/96 (1384%)]\tLoss: 330.077362\n",
      "Train Epoche: 2 [1330/96 (1385%)]\tLoss: 1.129040\n",
      "Train Epoche: 2 [1331/96 (1386%)]\tLoss: 36.383316\n",
      "Train Epoche: 2 [1332/96 (1388%)]\tLoss: 32.375759\n",
      "Train Epoche: 2 [1333/96 (1389%)]\tLoss: 33.132683\n",
      "Train Epoche: 2 [1334/96 (1390%)]\tLoss: 32.968426\n",
      "Train Epoche: 2 [1335/96 (1391%)]\tLoss: 5.320292\n",
      "Train Epoche: 2 [1336/96 (1392%)]\tLoss: 0.042520\n",
      "Train Epoche: 2 [1337/96 (1393%)]\tLoss: 24.690845\n",
      "Train Epoche: 2 [1338/96 (1394%)]\tLoss: 2.255863\n",
      "Train Epoche: 2 [1339/96 (1395%)]\tLoss: 6.740462\n",
      "Train Epoche: 2 [1340/96 (1396%)]\tLoss: 57.278503\n",
      "Train Epoche: 2 [1341/96 (1397%)]\tLoss: 95.341522\n",
      "Train Epoche: 2 [1342/96 (1398%)]\tLoss: 0.591641\n",
      "Train Epoche: 2 [1343/96 (1399%)]\tLoss: 1.177618\n",
      "Train Epoche: 2 [1344/96 (1400%)]\tLoss: 97.689308\n",
      "Train Epoche: 2 [1345/96 (1401%)]\tLoss: 1.563509\n",
      "Train Epoche: 2 [1346/96 (1402%)]\tLoss: 5.124003\n",
      "Train Epoche: 2 [1347/96 (1403%)]\tLoss: 1.252694\n",
      "Train Epoche: 2 [1348/96 (1404%)]\tLoss: 0.259565\n",
      "Train Epoche: 2 [1349/96 (1405%)]\tLoss: 1.051990\n",
      "Train Epoche: 2 [1350/96 (1406%)]\tLoss: 8.640018\n",
      "Train Epoche: 2 [1351/96 (1407%)]\tLoss: 0.004120\n",
      "Train Epoche: 2 [1352/96 (1408%)]\tLoss: 4.857774\n",
      "Train Epoche: 2 [1353/96 (1409%)]\tLoss: 0.348563\n",
      "Train Epoche: 2 [1354/96 (1410%)]\tLoss: 3.548266\n",
      "Train Epoche: 2 [1355/96 (1411%)]\tLoss: 30.289541\n",
      "Train Epoche: 2 [1356/96 (1412%)]\tLoss: 8.398955\n",
      "Train Epoche: 2 [1357/96 (1414%)]\tLoss: 0.267147\n",
      "Train Epoche: 2 [1358/96 (1415%)]\tLoss: 5.333463\n",
      "Train Epoche: 2 [1359/96 (1416%)]\tLoss: 43.268757\n",
      "Train Epoche: 2 [1360/96 (1417%)]\tLoss: 0.115314\n",
      "Train Epoche: 2 [1361/96 (1418%)]\tLoss: 1.269561\n",
      "Train Epoche: 2 [1362/96 (1419%)]\tLoss: 4.690740\n",
      "Train Epoche: 2 [1363/96 (1420%)]\tLoss: 16.533644\n",
      "Train Epoche: 2 [1364/96 (1421%)]\tLoss: 143.351059\n",
      "Train Epoche: 2 [1365/96 (1422%)]\tLoss: 8.470092\n",
      "Train Epoche: 2 [1366/96 (1423%)]\tLoss: 22.063982\n",
      "Train Epoche: 2 [1367/96 (1424%)]\tLoss: 2.758168\n",
      "Train Epoche: 2 [1368/96 (1425%)]\tLoss: 92.357468\n",
      "Train Epoche: 2 [1369/96 (1426%)]\tLoss: 67.689545\n",
      "Train Epoche: 2 [1370/96 (1427%)]\tLoss: 3.000230\n",
      "Train Epoche: 2 [1371/96 (1428%)]\tLoss: 15.121413\n",
      "Train Epoche: 2 [1372/96 (1429%)]\tLoss: 17.085941\n",
      "Train Epoche: 2 [1373/96 (1430%)]\tLoss: 0.007789\n",
      "Train Epoche: 2 [1374/96 (1431%)]\tLoss: 23.967468\n",
      "Train Epoche: 2 [1375/96 (1432%)]\tLoss: 0.735156\n",
      "Train Epoche: 2 [1376/96 (1433%)]\tLoss: 14.778058\n",
      "Train Epoche: 2 [1377/96 (1434%)]\tLoss: 11.938916\n",
      "Train Epoche: 2 [1378/96 (1435%)]\tLoss: 3.242250\n",
      "Train Epoche: 2 [1379/96 (1436%)]\tLoss: 0.125665\n",
      "Train Epoche: 2 [1380/96 (1438%)]\tLoss: 0.378021\n",
      "Train Epoche: 2 [1381/96 (1439%)]\tLoss: 0.266617\n",
      "Train Epoche: 2 [1382/96 (1440%)]\tLoss: 19.854563\n",
      "Train Epoche: 2 [1383/96 (1441%)]\tLoss: 2.081047\n",
      "Train Epoche: 2 [1384/96 (1442%)]\tLoss: 0.026478\n",
      "Train Epoche: 2 [1385/96 (1443%)]\tLoss: 0.798843\n",
      "Train Epoche: 2 [1386/96 (1444%)]\tLoss: 2.654786\n",
      "Train Epoche: 2 [1387/96 (1445%)]\tLoss: 1.571604\n",
      "Train Epoche: 2 [1388/96 (1446%)]\tLoss: 32.291302\n",
      "Train Epoche: 2 [1389/96 (1447%)]\tLoss: 108.526703\n",
      "Train Epoche: 2 [1390/96 (1448%)]\tLoss: 14.137745\n",
      "Train Epoche: 2 [1391/96 (1449%)]\tLoss: 3.338287\n",
      "Train Epoche: 2 [1392/96 (1450%)]\tLoss: 1.980104\n",
      "Train Epoche: 2 [1393/96 (1451%)]\tLoss: 2.864076\n",
      "Train Epoche: 2 [1394/96 (1452%)]\tLoss: 19.930538\n",
      "Train Epoche: 2 [1395/96 (1453%)]\tLoss: 0.197188\n",
      "Train Epoche: 2 [1396/96 (1454%)]\tLoss: 6.153236\n",
      "Train Epoche: 2 [1397/96 (1455%)]\tLoss: 18.814035\n",
      "Train Epoche: 2 [1398/96 (1456%)]\tLoss: 0.316209\n",
      "Train Epoche: 2 [1399/96 (1457%)]\tLoss: 2.338888\n",
      "Train Epoche: 2 [1400/96 (1458%)]\tLoss: 5.551594\n",
      "Train Epoche: 2 [1401/96 (1459%)]\tLoss: 3.790841\n",
      "Train Epoche: 2 [1402/96 (1460%)]\tLoss: 0.155726\n",
      "Train Epoche: 2 [1403/96 (1461%)]\tLoss: 0.385068\n",
      "Train Epoche: 2 [1404/96 (1462%)]\tLoss: 6.131019\n",
      "Train Epoche: 2 [1405/96 (1464%)]\tLoss: 3.783512\n",
      "Train Epoche: 2 [1406/96 (1465%)]\tLoss: 14.124359\n",
      "Train Epoche: 2 [1407/96 (1466%)]\tLoss: 0.852828\n",
      "Train Epoche: 2 [1408/96 (1467%)]\tLoss: 1.527344\n",
      "Train Epoche: 2 [1409/96 (1468%)]\tLoss: 35.603279\n",
      "Train Epoche: 2 [1410/96 (1469%)]\tLoss: 0.001209\n",
      "Train Epoche: 2 [1411/96 (1470%)]\tLoss: 118.332382\n",
      "Train Epoche: 2 [1412/96 (1471%)]\tLoss: 4.571474\n",
      "Train Epoche: 2 [1413/96 (1472%)]\tLoss: 4.116482\n",
      "Train Epoche: 2 [1414/96 (1473%)]\tLoss: 0.051328\n",
      "Train Epoche: 2 [1415/96 (1474%)]\tLoss: 0.222525\n",
      "Train Epoche: 2 [1416/96 (1475%)]\tLoss: 2.576125\n",
      "Train Epoche: 2 [1417/96 (1476%)]\tLoss: 0.001652\n",
      "Train Epoche: 2 [1418/96 (1477%)]\tLoss: 109.282417\n",
      "Train Epoche: 2 [1419/96 (1478%)]\tLoss: 13.035733\n",
      "Train Epoche: 2 [1420/96 (1479%)]\tLoss: 16.899361\n",
      "Train Epoche: 2 [1421/96 (1480%)]\tLoss: 9.801266\n",
      "Train Epoche: 2 [1422/96 (1481%)]\tLoss: 1.288505\n",
      "Train Epoche: 2 [1423/96 (1482%)]\tLoss: 0.000509\n",
      "Train Epoche: 2 [1424/96 (1483%)]\tLoss: 8.097136\n",
      "Train Epoche: 2 [1425/96 (1484%)]\tLoss: 1.286018\n",
      "Train Epoche: 2 [1426/96 (1485%)]\tLoss: 1.516453\n",
      "Train Epoche: 2 [1427/96 (1486%)]\tLoss: 0.245002\n",
      "Train Epoche: 2 [1428/96 (1488%)]\tLoss: 0.689080\n",
      "Train Epoche: 2 [1429/96 (1489%)]\tLoss: 0.933225\n",
      "Train Epoche: 2 [1430/96 (1490%)]\tLoss: 0.805373\n",
      "Train Epoche: 2 [1431/96 (1491%)]\tLoss: 32.589390\n",
      "Train Epoche: 2 [1432/96 (1492%)]\tLoss: 11.607724\n",
      "Train Epoche: 2 [1433/96 (1493%)]\tLoss: 3.874552\n",
      "Train Epoche: 2 [1434/96 (1494%)]\tLoss: 1.752106\n",
      "Train Epoche: 2 [1435/96 (1495%)]\tLoss: 16.810085\n",
      "Train Epoche: 2 [1436/96 (1496%)]\tLoss: 2.597430\n",
      "Train Epoche: 2 [1437/96 (1497%)]\tLoss: 0.014795\n",
      "Train Epoche: 2 [1438/96 (1498%)]\tLoss: 3.658526\n",
      "Train Epoche: 2 [1439/96 (1499%)]\tLoss: 1.121138\n",
      "Train Epoche: 2 [1440/96 (1500%)]\tLoss: 0.147329\n",
      "Train Epoche: 2 [1441/96 (1501%)]\tLoss: 0.042361\n",
      "Train Epoche: 2 [1442/96 (1502%)]\tLoss: 167.340485\n",
      "Train Epoche: 2 [1443/96 (1503%)]\tLoss: 116.499603\n",
      "Train Epoche: 2 [1444/96 (1504%)]\tLoss: 0.000030\n",
      "Train Epoche: 2 [1445/96 (1505%)]\tLoss: 1.600470\n",
      "Train Epoche: 2 [1446/96 (1506%)]\tLoss: 0.174874\n",
      "Train Epoche: 2 [1447/96 (1507%)]\tLoss: 0.162380\n",
      "Train Epoche: 2 [1448/96 (1508%)]\tLoss: 5.320182\n",
      "Train Epoche: 2 [1449/96 (1509%)]\tLoss: 0.032554\n",
      "Train Epoche: 2 [1450/96 (1510%)]\tLoss: 13.529483\n",
      "Train Epoche: 2 [1451/96 (1511%)]\tLoss: 1.739953\n",
      "Train Epoche: 2 [1452/96 (1512%)]\tLoss: 2.196579\n",
      "Train Epoche: 2 [1453/96 (1514%)]\tLoss: 1.989848\n",
      "Train Epoche: 2 [1454/96 (1515%)]\tLoss: 0.134754\n",
      "Train Epoche: 2 [1455/96 (1516%)]\tLoss: 3.858498\n",
      "Train Epoche: 2 [1456/96 (1517%)]\tLoss: 8.428533\n",
      "Train Epoche: 2 [1457/96 (1518%)]\tLoss: 2.177939\n",
      "Train Epoche: 2 [1458/96 (1519%)]\tLoss: 1.727271\n",
      "Train Epoche: 2 [1459/96 (1520%)]\tLoss: 2.845584\n",
      "Train Epoche: 2 [1460/96 (1521%)]\tLoss: 0.555092\n",
      "Train Epoche: 2 [1461/96 (1522%)]\tLoss: 13.452959\n",
      "Train Epoche: 2 [1462/96 (1523%)]\tLoss: 0.151299\n",
      "Train Epoche: 2 [1463/96 (1524%)]\tLoss: 25.994656\n",
      "Train Epoche: 2 [1464/96 (1525%)]\tLoss: 13.170206\n",
      "Train Epoche: 2 [1465/96 (1526%)]\tLoss: 0.050059\n",
      "Train Epoche: 2 [1466/96 (1527%)]\tLoss: 2.270739\n",
      "Train Epoche: 2 [1467/96 (1528%)]\tLoss: 0.208800\n",
      "Train Epoche: 2 [1468/96 (1529%)]\tLoss: 113.386009\n",
      "Train Epoche: 2 [1469/96 (1530%)]\tLoss: 1.623431\n",
      "Train Epoche: 2 [1470/96 (1531%)]\tLoss: 0.750373\n",
      "Train Epoche: 2 [1471/96 (1532%)]\tLoss: 21.089958\n",
      "Train Epoche: 2 [1472/96 (1533%)]\tLoss: 1.350543\n",
      "Train Epoche: 2 [1473/96 (1534%)]\tLoss: 0.589003\n",
      "Train Epoche: 2 [1474/96 (1535%)]\tLoss: 34.238087\n",
      "Train Epoche: 2 [1475/96 (1536%)]\tLoss: 1.143549\n",
      "Train Epoche: 2 [1476/96 (1538%)]\tLoss: 0.003595\n",
      "Train Epoche: 2 [1477/96 (1539%)]\tLoss: 64.121201\n",
      "Train Epoche: 2 [1478/96 (1540%)]\tLoss: 0.326665\n",
      "Train Epoche: 2 [1479/96 (1541%)]\tLoss: 0.019389\n",
      "Train Epoche: 2 [1480/96 (1542%)]\tLoss: 1.575655\n",
      "Train Epoche: 2 [1481/96 (1543%)]\tLoss: 0.035756\n",
      "Train Epoche: 2 [1482/96 (1544%)]\tLoss: 13.451098\n",
      "Train Epoche: 2 [1483/96 (1545%)]\tLoss: 2.866432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1484/96 (1546%)]\tLoss: 6.521140\n",
      "Train Epoche: 2 [1485/96 (1547%)]\tLoss: 1.123543\n",
      "Train Epoche: 2 [1486/96 (1548%)]\tLoss: 150.118347\n",
      "Train Epoche: 2 [1487/96 (1549%)]\tLoss: 0.446307\n",
      "Train Epoche: 2 [1488/96 (1550%)]\tLoss: 1.045957\n",
      "Train Epoche: 2 [1489/96 (1551%)]\tLoss: 0.170251\n",
      "Train Epoche: 2 [1490/96 (1552%)]\tLoss: 5.498825\n",
      "Train Epoche: 2 [1491/96 (1553%)]\tLoss: 7.061254\n",
      "Train Epoche: 2 [1492/96 (1554%)]\tLoss: 9.477402\n",
      "Train Epoche: 2 [1493/96 (1555%)]\tLoss: 14.596543\n",
      "Train Epoche: 2 [1494/96 (1556%)]\tLoss: 124.413765\n",
      "Train Epoche: 2 [1495/96 (1557%)]\tLoss: 3.811489\n",
      "Train Epoche: 2 [1496/96 (1558%)]\tLoss: 19.027872\n",
      "Train Epoche: 2 [1497/96 (1559%)]\tLoss: 6.571994\n",
      "Train Epoche: 2 [1498/96 (1560%)]\tLoss: 3.732929\n",
      "Train Epoche: 2 [1499/96 (1561%)]\tLoss: 8.466651\n",
      "Train Epoche: 2 [1500/96 (1562%)]\tLoss: 0.168597\n",
      "Train Epoche: 2 [1501/96 (1564%)]\tLoss: 3.198810\n",
      "Train Epoche: 2 [1502/96 (1565%)]\tLoss: 12.773104\n",
      "Train Epoche: 2 [1503/96 (1566%)]\tLoss: 2.983948\n",
      "Train Epoche: 2 [1504/96 (1567%)]\tLoss: 6.344242\n",
      "Train Epoche: 2 [1505/96 (1568%)]\tLoss: 0.515332\n",
      "Train Epoche: 2 [1506/96 (1569%)]\tLoss: 1.369338\n",
      "Train Epoche: 2 [1507/96 (1570%)]\tLoss: 0.071966\n",
      "Train Epoche: 2 [1508/96 (1571%)]\tLoss: 4.052632\n",
      "Train Epoche: 2 [1509/96 (1572%)]\tLoss: 1.340277\n",
      "Train Epoche: 2 [1510/96 (1573%)]\tLoss: 7.786325\n",
      "Train Epoche: 2 [1511/96 (1574%)]\tLoss: 0.035541\n",
      "Train Epoche: 2 [1512/96 (1575%)]\tLoss: 4.473390\n",
      "Train Epoche: 2 [1513/96 (1576%)]\tLoss: 0.812258\n",
      "Train Epoche: 2 [1514/96 (1577%)]\tLoss: 9.221444\n",
      "Train Epoche: 2 [1515/96 (1578%)]\tLoss: 14.300433\n",
      "Train Epoche: 2 [1516/96 (1579%)]\tLoss: 14.827945\n",
      "Train Epoche: 2 [1517/96 (1580%)]\tLoss: 0.248800\n",
      "Train Epoche: 2 [1518/96 (1581%)]\tLoss: 5.131032\n",
      "Train Epoche: 2 [1519/96 (1582%)]\tLoss: 19.569176\n",
      "Train Epoche: 2 [1520/96 (1583%)]\tLoss: 0.040571\n",
      "Train Epoche: 2 [1521/96 (1584%)]\tLoss: 0.058714\n",
      "Train Epoche: 2 [1522/96 (1585%)]\tLoss: 4.942862\n",
      "Train Epoche: 2 [1523/96 (1586%)]\tLoss: 33.720596\n",
      "Train Epoche: 2 [1524/96 (1588%)]\tLoss: 13.021049\n",
      "Train Epoche: 2 [1525/96 (1589%)]\tLoss: 0.020953\n",
      "Train Epoche: 2 [1526/96 (1590%)]\tLoss: 48.336399\n",
      "Train Epoche: 2 [1527/96 (1591%)]\tLoss: 2.934996\n",
      "Train Epoche: 2 [1528/96 (1592%)]\tLoss: 0.281715\n",
      "Train Epoche: 2 [1529/96 (1593%)]\tLoss: 11.813403\n",
      "Train Epoche: 2 [1530/96 (1594%)]\tLoss: 0.013017\n",
      "Train Epoche: 2 [1531/96 (1595%)]\tLoss: 4.805813\n",
      "Train Epoche: 2 [1532/96 (1596%)]\tLoss: 17.964668\n",
      "Train Epoche: 2 [1533/96 (1597%)]\tLoss: 7.303534\n",
      "Train Epoche: 2 [1534/96 (1598%)]\tLoss: 0.333835\n",
      "Train Epoche: 2 [1535/96 (1599%)]\tLoss: 0.271600\n",
      "Train Epoche: 2 [1536/96 (1600%)]\tLoss: 0.011587\n",
      "Train Epoche: 2 [1537/96 (1601%)]\tLoss: 0.789703\n",
      "Train Epoche: 2 [1538/96 (1602%)]\tLoss: 29.484541\n",
      "Train Epoche: 2 [1539/96 (1603%)]\tLoss: 0.004055\n",
      "Train Epoche: 2 [1540/96 (1604%)]\tLoss: 5.391241\n",
      "Train Epoche: 2 [1541/96 (1605%)]\tLoss: 2.420913\n",
      "Train Epoche: 2 [1542/96 (1606%)]\tLoss: 70.182755\n",
      "Train Epoche: 2 [1543/96 (1607%)]\tLoss: 2.354067\n",
      "Train Epoche: 2 [1544/96 (1608%)]\tLoss: 0.674828\n",
      "Train Epoche: 2 [1545/96 (1609%)]\tLoss: 0.082169\n",
      "Train Epoche: 2 [1546/96 (1610%)]\tLoss: 2.902633\n",
      "Train Epoche: 2 [1547/96 (1611%)]\tLoss: 0.620277\n",
      "Train Epoche: 2 [1548/96 (1612%)]\tLoss: 4.953742\n",
      "Train Epoche: 2 [1549/96 (1614%)]\tLoss: 0.932278\n",
      "Train Epoche: 2 [1550/96 (1615%)]\tLoss: 3.280672\n",
      "Train Epoche: 2 [1551/96 (1616%)]\tLoss: 11.699891\n",
      "Train Epoche: 2 [1552/96 (1617%)]\tLoss: 0.129454\n",
      "Train Epoche: 2 [1553/96 (1618%)]\tLoss: 25.264990\n",
      "Train Epoche: 2 [1554/96 (1619%)]\tLoss: 0.017124\n",
      "Train Epoche: 2 [1555/96 (1620%)]\tLoss: 7.772625\n",
      "Train Epoche: 2 [1556/96 (1621%)]\tLoss: 0.202825\n",
      "Train Epoche: 2 [1557/96 (1622%)]\tLoss: 0.798443\n",
      "Train Epoche: 2 [1558/96 (1623%)]\tLoss: 42.746204\n",
      "Train Epoche: 2 [1559/96 (1624%)]\tLoss: 1.266873\n",
      "Train Epoche: 2 [1560/96 (1625%)]\tLoss: 0.434784\n",
      "Train Epoche: 2 [1561/96 (1626%)]\tLoss: 4.787322\n",
      "Train Epoche: 2 [1562/96 (1627%)]\tLoss: 1.385924\n",
      "Train Epoche: 2 [1563/96 (1628%)]\tLoss: 3.601306\n",
      "Train Epoche: 2 [1564/96 (1629%)]\tLoss: 0.774682\n",
      "Train Epoche: 2 [1565/96 (1630%)]\tLoss: 1.242675\n",
      "Train Epoche: 2 [1566/96 (1631%)]\tLoss: 1.291862\n",
      "Train Epoche: 2 [1567/96 (1632%)]\tLoss: 23.223627\n",
      "Train Epoche: 2 [1568/96 (1633%)]\tLoss: 3.658581\n",
      "Train Epoche: 2 [1569/96 (1634%)]\tLoss: 41.547119\n",
      "Train Epoche: 2 [1570/96 (1635%)]\tLoss: 51.636578\n",
      "Train Epoche: 2 [1571/96 (1636%)]\tLoss: 10.744796\n",
      "Train Epoche: 2 [1572/96 (1638%)]\tLoss: 1.104766\n",
      "Train Epoche: 2 [1573/96 (1639%)]\tLoss: 4.163674\n",
      "Train Epoche: 2 [1574/96 (1640%)]\tLoss: 4.094874\n",
      "Train Epoche: 2 [1575/96 (1641%)]\tLoss: 43.512611\n",
      "Train Epoche: 2 [1576/96 (1642%)]\tLoss: 0.097033\n",
      "Train Epoche: 2 [1577/96 (1643%)]\tLoss: 7.287337\n",
      "Train Epoche: 2 [1578/96 (1644%)]\tLoss: 3.824823\n",
      "Train Epoche: 2 [1579/96 (1645%)]\tLoss: 3.454668\n",
      "Train Epoche: 2 [1580/96 (1646%)]\tLoss: 28.630547\n",
      "Train Epoche: 2 [1581/96 (1647%)]\tLoss: 0.345682\n",
      "Train Epoche: 2 [1582/96 (1648%)]\tLoss: 0.072368\n",
      "Train Epoche: 2 [1583/96 (1649%)]\tLoss: 0.066854\n",
      "Train Epoche: 2 [1584/96 (1650%)]\tLoss: 0.611492\n",
      "Train Epoche: 2 [1585/96 (1651%)]\tLoss: 0.124271\n",
      "Train Epoche: 2 [1586/96 (1652%)]\tLoss: 3.389592\n",
      "Train Epoche: 2 [1587/96 (1653%)]\tLoss: 44.949173\n",
      "Train Epoche: 2 [1588/96 (1654%)]\tLoss: 2.190898\n",
      "Train Epoche: 2 [1589/96 (1655%)]\tLoss: 0.511981\n",
      "Train Epoche: 2 [1590/96 (1656%)]\tLoss: 0.017366\n",
      "Train Epoche: 2 [1591/96 (1657%)]\tLoss: 0.125638\n",
      "Train Epoche: 2 [1592/96 (1658%)]\tLoss: 9.536387\n",
      "Train Epoche: 2 [1593/96 (1659%)]\tLoss: 8.095085\n",
      "Train Epoche: 2 [1594/96 (1660%)]\tLoss: 6.362395\n",
      "Train Epoche: 2 [1595/96 (1661%)]\tLoss: 1.521441\n",
      "Train Epoche: 2 [1596/96 (1662%)]\tLoss: 0.956237\n",
      "Train Epoche: 2 [1597/96 (1664%)]\tLoss: 4.355481\n",
      "Train Epoche: 2 [1598/96 (1665%)]\tLoss: 0.794670\n",
      "Train Epoche: 2 [1599/96 (1666%)]\tLoss: 1.236223\n",
      "Train Epoche: 2 [1600/96 (1667%)]\tLoss: 214.330063\n",
      "Train Epoche: 2 [1601/96 (1668%)]\tLoss: 1.993079\n",
      "Train Epoche: 2 [1602/96 (1669%)]\tLoss: 21.498983\n",
      "Train Epoche: 2 [1603/96 (1670%)]\tLoss: 15.018700\n",
      "Train Epoche: 2 [1604/96 (1671%)]\tLoss: 11.023201\n",
      "Train Epoche: 2 [1605/96 (1672%)]\tLoss: 56.323151\n",
      "Train Epoche: 2 [1606/96 (1673%)]\tLoss: 8.670900\n",
      "Train Epoche: 2 [1607/96 (1674%)]\tLoss: 0.022412\n",
      "Train Epoche: 2 [1608/96 (1675%)]\tLoss: 1.646894\n",
      "Train Epoche: 2 [1609/96 (1676%)]\tLoss: 4.722808\n",
      "Train Epoche: 2 [1610/96 (1677%)]\tLoss: 10.262124\n",
      "Train Epoche: 2 [1611/96 (1678%)]\tLoss: 14.044796\n",
      "Train Epoche: 2 [1612/96 (1679%)]\tLoss: 9.053535\n",
      "Train Epoche: 2 [1613/96 (1680%)]\tLoss: 7.543619\n",
      "Train Epoche: 2 [1614/96 (1681%)]\tLoss: 174.438370\n",
      "Train Epoche: 2 [1615/96 (1682%)]\tLoss: 3.475118\n",
      "Train Epoche: 2 [1616/96 (1683%)]\tLoss: 8.405043\n",
      "Train Epoche: 2 [1617/96 (1684%)]\tLoss: 5.486552\n",
      "Train Epoche: 2 [1618/96 (1685%)]\tLoss: 5.676859\n",
      "Train Epoche: 2 [1619/96 (1686%)]\tLoss: 443.590027\n",
      "Train Epoche: 2 [1620/96 (1688%)]\tLoss: 47.815155\n",
      "Train Epoche: 2 [1621/96 (1689%)]\tLoss: 22.984043\n",
      "Train Epoche: 2 [1622/96 (1690%)]\tLoss: 0.003132\n",
      "Train Epoche: 2 [1623/96 (1691%)]\tLoss: 263.960358\n",
      "Train Epoche: 2 [1624/96 (1692%)]\tLoss: 2.224848\n",
      "Train Epoche: 2 [1625/96 (1693%)]\tLoss: 12.045033\n",
      "Train Epoche: 2 [1626/96 (1694%)]\tLoss: 6.824285\n",
      "Train Epoche: 2 [1627/96 (1695%)]\tLoss: 33.572289\n",
      "Train Epoche: 2 [1628/96 (1696%)]\tLoss: 2.787344\n",
      "Train Epoche: 2 [1629/96 (1697%)]\tLoss: 2.546626\n",
      "Train Epoche: 2 [1630/96 (1698%)]\tLoss: 120.261452\n",
      "Train Epoche: 2 [1631/96 (1699%)]\tLoss: 0.525121\n",
      "Train Epoche: 2 [1632/96 (1700%)]\tLoss: 3.774489\n",
      "Train Epoche: 2 [1633/96 (1701%)]\tLoss: 8.306710\n",
      "Train Epoche: 2 [1634/96 (1702%)]\tLoss: 3.872890\n",
      "Train Epoche: 2 [1635/96 (1703%)]\tLoss: 6.824903\n",
      "Train Epoche: 2 [1636/96 (1704%)]\tLoss: 1.876021\n",
      "Train Epoche: 2 [1637/96 (1705%)]\tLoss: 13.474529\n",
      "Train Epoche: 2 [1638/96 (1706%)]\tLoss: 17.710348\n",
      "Train Epoche: 2 [1639/96 (1707%)]\tLoss: 6.165388\n",
      "Train Epoche: 2 [1640/96 (1708%)]\tLoss: 0.213807\n",
      "Train Epoche: 2 [1641/96 (1709%)]\tLoss: 0.245008\n",
      "Train Epoche: 2 [1642/96 (1710%)]\tLoss: 6.891541\n",
      "Train Epoche: 2 [1643/96 (1711%)]\tLoss: 7.719655\n",
      "Train Epoche: 2 [1644/96 (1712%)]\tLoss: 122.507881\n",
      "Train Epoche: 2 [1645/96 (1714%)]\tLoss: 0.904040\n",
      "Train Epoche: 2 [1646/96 (1715%)]\tLoss: 31.753077\n",
      "Train Epoche: 2 [1647/96 (1716%)]\tLoss: 0.906014\n",
      "Train Epoche: 2 [1648/96 (1717%)]\tLoss: 7.448457\n",
      "Train Epoche: 2 [1649/96 (1718%)]\tLoss: 1.940824\n",
      "Train Epoche: 2 [1650/96 (1719%)]\tLoss: 0.036443\n",
      "Train Epoche: 2 [1651/96 (1720%)]\tLoss: 2.289113\n",
      "Train Epoche: 2 [1652/96 (1721%)]\tLoss: 11.867438\n",
      "Train Epoche: 2 [1653/96 (1722%)]\tLoss: 63.035202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1654/96 (1723%)]\tLoss: 39.082626\n",
      "Train Epoche: 2 [1655/96 (1724%)]\tLoss: 34.753536\n",
      "Train Epoche: 2 [1656/96 (1725%)]\tLoss: 36.168724\n",
      "Train Epoche: 2 [1657/96 (1726%)]\tLoss: 15.145869\n",
      "Train Epoche: 2 [1658/96 (1727%)]\tLoss: 2.708393\n",
      "Train Epoche: 2 [1659/96 (1728%)]\tLoss: 12.712004\n",
      "Train Epoche: 2 [1660/96 (1729%)]\tLoss: 4.942406\n",
      "Train Epoche: 2 [1661/96 (1730%)]\tLoss: 4.867212\n",
      "Train Epoche: 2 [1662/96 (1731%)]\tLoss: 15.290351\n",
      "Train Epoche: 2 [1663/96 (1732%)]\tLoss: 13.176811\n",
      "Train Epoche: 2 [1664/96 (1733%)]\tLoss: 0.030573\n",
      "Train Epoche: 2 [1665/96 (1734%)]\tLoss: 0.316481\n",
      "Train Epoche: 2 [1666/96 (1735%)]\tLoss: 2.010838\n",
      "Train Epoche: 2 [1667/96 (1736%)]\tLoss: 0.379905\n",
      "Train Epoche: 2 [1668/96 (1738%)]\tLoss: 0.010421\n",
      "Train Epoche: 2 [1669/96 (1739%)]\tLoss: 0.112121\n",
      "Train Epoche: 2 [1670/96 (1740%)]\tLoss: 0.164814\n",
      "Train Epoche: 2 [1671/96 (1741%)]\tLoss: 2.153069\n",
      "Train Epoche: 2 [1672/96 (1742%)]\tLoss: 21.407953\n",
      "Train Epoche: 2 [1673/96 (1743%)]\tLoss: 77.739395\n",
      "Train Epoche: 2 [1674/96 (1744%)]\tLoss: 0.565515\n",
      "Train Epoche: 2 [1675/96 (1745%)]\tLoss: 2.414923\n",
      "Train Epoche: 2 [1676/96 (1746%)]\tLoss: 10.877032\n",
      "Train Epoche: 2 [1677/96 (1747%)]\tLoss: 1.337192\n",
      "Train Epoche: 2 [1678/96 (1748%)]\tLoss: 93.117615\n",
      "Train Epoche: 2 [1679/96 (1749%)]\tLoss: 3.632927\n",
      "Train Epoche: 2 [1680/96 (1750%)]\tLoss: 0.582420\n",
      "Train Epoche: 2 [1681/96 (1751%)]\tLoss: 0.172617\n",
      "Train Epoche: 2 [1682/96 (1752%)]\tLoss: 21.557056\n",
      "Train Epoche: 2 [1683/96 (1753%)]\tLoss: 0.118011\n",
      "Train Epoche: 2 [1684/96 (1754%)]\tLoss: 16.617346\n",
      "Train Epoche: 2 [1685/96 (1755%)]\tLoss: 269.607208\n",
      "Train Epoche: 2 [1686/96 (1756%)]\tLoss: 0.436014\n",
      "Train Epoche: 2 [1687/96 (1757%)]\tLoss: 0.358496\n",
      "Train Epoche: 2 [1688/96 (1758%)]\tLoss: 8.252974\n",
      "Train Epoche: 2 [1689/96 (1759%)]\tLoss: 25.928804\n",
      "Train Epoche: 2 [1690/96 (1760%)]\tLoss: 7.561189\n",
      "Train Epoche: 2 [1691/96 (1761%)]\tLoss: 53.106049\n",
      "Train Epoche: 2 [1692/96 (1762%)]\tLoss: 1.231396\n",
      "Train Epoche: 2 [1693/96 (1764%)]\tLoss: 6.980878\n",
      "Train Epoche: 2 [1694/96 (1765%)]\tLoss: 18.933290\n",
      "Train Epoche: 2 [1695/96 (1766%)]\tLoss: 4.038708\n",
      "Train Epoche: 2 [1696/96 (1767%)]\tLoss: 0.583562\n",
      "Train Epoche: 2 [1697/96 (1768%)]\tLoss: 34.718117\n",
      "Train Epoche: 2 [1698/96 (1769%)]\tLoss: 36.391842\n",
      "Train Epoche: 2 [1699/96 (1770%)]\tLoss: 48.137665\n",
      "Train Epoche: 2 [1700/96 (1771%)]\tLoss: 1.550205\n",
      "Train Epoche: 2 [1701/96 (1772%)]\tLoss: 1.239155\n",
      "Train Epoche: 2 [1702/96 (1773%)]\tLoss: 2.912302\n",
      "Train Epoche: 2 [1703/96 (1774%)]\tLoss: 2.278644\n",
      "Train Epoche: 2 [1704/96 (1775%)]\tLoss: 0.240161\n",
      "Train Epoche: 2 [1705/96 (1776%)]\tLoss: 233.949310\n",
      "Train Epoche: 2 [1706/96 (1777%)]\tLoss: 1.881536\n",
      "Train Epoche: 2 [1707/96 (1778%)]\tLoss: 41.357784\n",
      "Train Epoche: 2 [1708/96 (1779%)]\tLoss: 22.764906\n",
      "Train Epoche: 2 [1709/96 (1780%)]\tLoss: 18.541092\n",
      "Train Epoche: 2 [1710/96 (1781%)]\tLoss: 0.194170\n",
      "Train Epoche: 2 [1711/96 (1782%)]\tLoss: 5.582782\n",
      "Train Epoche: 2 [1712/96 (1783%)]\tLoss: 0.719326\n",
      "Train Epoche: 2 [1713/96 (1784%)]\tLoss: 30.451015\n",
      "Train Epoche: 2 [1714/96 (1785%)]\tLoss: 0.062837\n",
      "Train Epoche: 2 [1715/96 (1786%)]\tLoss: 47.964359\n",
      "Train Epoche: 2 [1716/96 (1788%)]\tLoss: 1.644626\n",
      "Train Epoche: 2 [1717/96 (1789%)]\tLoss: 9.895617\n",
      "Train Epoche: 2 [1718/96 (1790%)]\tLoss: 7.170501\n",
      "Train Epoche: 2 [1719/96 (1791%)]\tLoss: 3.941776\n",
      "Train Epoche: 2 [1720/96 (1792%)]\tLoss: 2.039727\n",
      "Train Epoche: 2 [1721/96 (1793%)]\tLoss: 20.227123\n",
      "Train Epoche: 2 [1722/96 (1794%)]\tLoss: 5.799938\n",
      "Train Epoche: 2 [1723/96 (1795%)]\tLoss: 1.125338\n",
      "Train Epoche: 2 [1724/96 (1796%)]\tLoss: 1.435919\n",
      "Train Epoche: 2 [1725/96 (1797%)]\tLoss: 0.108271\n",
      "Train Epoche: 2 [1726/96 (1798%)]\tLoss: 7.820285\n",
      "Train Epoche: 2 [1727/96 (1799%)]\tLoss: 49.938896\n",
      "Train Epoche: 2 [1728/96 (1800%)]\tLoss: 18.105358\n",
      "Train Epoche: 2 [1729/96 (1801%)]\tLoss: 0.295161\n",
      "Train Epoche: 2 [1730/96 (1802%)]\tLoss: 1.749249\n",
      "Train Epoche: 2 [1731/96 (1803%)]\tLoss: 1.455143\n",
      "Train Epoche: 2 [1732/96 (1804%)]\tLoss: 5.136352\n",
      "Train Epoche: 2 [1733/96 (1805%)]\tLoss: 0.141387\n",
      "Train Epoche: 2 [1734/96 (1806%)]\tLoss: 8.106216\n",
      "Train Epoche: 2 [1735/96 (1807%)]\tLoss: 6.768403\n",
      "Train Epoche: 2 [1736/96 (1808%)]\tLoss: 7.045532\n",
      "Train Epoche: 2 [1737/96 (1809%)]\tLoss: 5.612224\n",
      "Train Epoche: 2 [1738/96 (1810%)]\tLoss: 14.978951\n",
      "Train Epoche: 2 [1739/96 (1811%)]\tLoss: 1.995137\n",
      "Train Epoche: 2 [1740/96 (1812%)]\tLoss: 0.027458\n",
      "Train Epoche: 2 [1741/96 (1814%)]\tLoss: 34.025013\n",
      "Train Epoche: 2 [1742/96 (1815%)]\tLoss: 24.315256\n",
      "Train Epoche: 2 [1743/96 (1816%)]\tLoss: 23.328568\n",
      "Train Epoche: 2 [1744/96 (1817%)]\tLoss: 0.025116\n",
      "Train Epoche: 2 [1745/96 (1818%)]\tLoss: 9.961649\n",
      "Train Epoche: 2 [1746/96 (1819%)]\tLoss: 22.561920\n",
      "Train Epoche: 2 [1747/96 (1820%)]\tLoss: 14.736572\n",
      "Train Epoche: 2 [1748/96 (1821%)]\tLoss: 54.458771\n",
      "Train Epoche: 2 [1749/96 (1822%)]\tLoss: 5.080897\n",
      "Train Epoche: 2 [1750/96 (1823%)]\tLoss: 1.930452\n",
      "Train Epoche: 2 [1751/96 (1824%)]\tLoss: 0.452876\n",
      "Train Epoche: 2 [1752/96 (1825%)]\tLoss: 1.657765\n",
      "Train Epoche: 2 [1753/96 (1826%)]\tLoss: 0.345233\n",
      "Train Epoche: 2 [1754/96 (1827%)]\tLoss: 32.986683\n",
      "Train Epoche: 2 [1755/96 (1828%)]\tLoss: 2.203351\n",
      "Train Epoche: 2 [1756/96 (1829%)]\tLoss: 38.293488\n",
      "Train Epoche: 2 [1757/96 (1830%)]\tLoss: 1.104043\n",
      "Train Epoche: 2 [1758/96 (1831%)]\tLoss: 54.175674\n",
      "Train Epoche: 2 [1759/96 (1832%)]\tLoss: 68.292542\n",
      "Train Epoche: 2 [1760/96 (1833%)]\tLoss: 9.852557\n",
      "Train Epoche: 2 [1761/96 (1834%)]\tLoss: 2.547530\n",
      "Train Epoche: 2 [1762/96 (1835%)]\tLoss: 16.413214\n",
      "Train Epoche: 2 [1763/96 (1836%)]\tLoss: 293.929901\n",
      "Train Epoche: 2 [1764/96 (1838%)]\tLoss: 128.134842\n",
      "Train Epoche: 2 [1765/96 (1839%)]\tLoss: 19.734545\n",
      "Train Epoche: 2 [1766/96 (1840%)]\tLoss: 4.567842\n",
      "Train Epoche: 2 [1767/96 (1841%)]\tLoss: 30.395908\n",
      "Train Epoche: 2 [1768/96 (1842%)]\tLoss: 0.162091\n",
      "Train Epoche: 2 [1769/96 (1843%)]\tLoss: 10.074862\n",
      "Train Epoche: 2 [1770/96 (1844%)]\tLoss: 129.288895\n",
      "Train Epoche: 2 [1771/96 (1845%)]\tLoss: 139.352768\n",
      "Train Epoche: 2 [1772/96 (1846%)]\tLoss: 9.240446\n",
      "Train Epoche: 2 [1773/96 (1847%)]\tLoss: 64.889175\n",
      "Train Epoche: 2 [1774/96 (1848%)]\tLoss: 5.437017\n",
      "Train Epoche: 2 [1775/96 (1849%)]\tLoss: 1.059209\n",
      "Train Epoche: 2 [1776/96 (1850%)]\tLoss: 0.021855\n",
      "Train Epoche: 2 [1777/96 (1851%)]\tLoss: 5.286968\n",
      "Train Epoche: 2 [1778/96 (1852%)]\tLoss: 7.089975\n",
      "Train Epoche: 2 [1779/96 (1853%)]\tLoss: 18.020477\n",
      "Train Epoche: 2 [1780/96 (1854%)]\tLoss: 12.083994\n",
      "Train Epoche: 2 [1781/96 (1855%)]\tLoss: 3.334425\n",
      "Train Epoche: 2 [1782/96 (1856%)]\tLoss: 1.141874\n",
      "Train Epoche: 2 [1783/96 (1857%)]\tLoss: 74.988640\n",
      "Train Epoche: 2 [1784/96 (1858%)]\tLoss: 14.681649\n",
      "Train Epoche: 2 [1785/96 (1859%)]\tLoss: 20.355297\n",
      "Train Epoche: 2 [1786/96 (1860%)]\tLoss: 0.470793\n",
      "Train Epoche: 2 [1787/96 (1861%)]\tLoss: 0.044993\n",
      "Train Epoche: 2 [1788/96 (1862%)]\tLoss: 66.775772\n",
      "Train Epoche: 2 [1789/96 (1864%)]\tLoss: 17.878401\n",
      "Train Epoche: 2 [1790/96 (1865%)]\tLoss: 0.799270\n",
      "Train Epoche: 2 [1791/96 (1866%)]\tLoss: 2.532983\n",
      "Train Epoche: 2 [1792/96 (1867%)]\tLoss: 16.971090\n",
      "Train Epoche: 2 [1793/96 (1868%)]\tLoss: 0.003685\n",
      "Train Epoche: 2 [1794/96 (1869%)]\tLoss: 14.658273\n",
      "Train Epoche: 2 [1795/96 (1870%)]\tLoss: 0.839913\n",
      "Train Epoche: 2 [1796/96 (1871%)]\tLoss: 16.517965\n",
      "Train Epoche: 2 [1797/96 (1872%)]\tLoss: 1.061951\n",
      "Train Epoche: 2 [1798/96 (1873%)]\tLoss: 1.159765\n",
      "Train Epoche: 2 [1799/96 (1874%)]\tLoss: 11.784591\n",
      "Train Epoche: 2 [1800/96 (1875%)]\tLoss: 0.003840\n",
      "Train Epoche: 2 [1801/96 (1876%)]\tLoss: 1.046367\n",
      "Train Epoche: 2 [1802/96 (1877%)]\tLoss: 3.582807\n",
      "Train Epoche: 2 [1803/96 (1878%)]\tLoss: 1.165798\n",
      "Train Epoche: 2 [1804/96 (1879%)]\tLoss: 34.651371\n",
      "Train Epoche: 2 [1805/96 (1880%)]\tLoss: 174.418167\n",
      "Train Epoche: 2 [1806/96 (1881%)]\tLoss: 0.561604\n",
      "Train Epoche: 2 [1807/96 (1882%)]\tLoss: 0.000031\n",
      "Train Epoche: 2 [1808/96 (1883%)]\tLoss: 5.189322\n",
      "Train Epoche: 2 [1809/96 (1884%)]\tLoss: 77.431122\n",
      "Train Epoche: 2 [1810/96 (1885%)]\tLoss: 48.848900\n",
      "Train Epoche: 2 [1811/96 (1886%)]\tLoss: 20.964975\n",
      "Train Epoche: 2 [1812/96 (1888%)]\tLoss: 4.046203\n",
      "Train Epoche: 2 [1813/96 (1889%)]\tLoss: 74.079750\n",
      "Train Epoche: 2 [1814/96 (1890%)]\tLoss: 6.433994\n",
      "Train Epoche: 2 [1815/96 (1891%)]\tLoss: 5.393954\n",
      "Train Epoche: 2 [1816/96 (1892%)]\tLoss: 27.472572\n",
      "Train Epoche: 2 [1817/96 (1893%)]\tLoss: 25.717049\n",
      "Train Epoche: 2 [1818/96 (1894%)]\tLoss: 37.325626\n",
      "Train Epoche: 2 [1819/96 (1895%)]\tLoss: 34.301994\n",
      "Train Epoche: 2 [1820/96 (1896%)]\tLoss: 267.815033\n",
      "Train Epoche: 2 [1821/96 (1897%)]\tLoss: 0.000006\n",
      "Train Epoche: 2 [1822/96 (1898%)]\tLoss: 0.400828\n",
      "Train Epoche: 2 [1823/96 (1899%)]\tLoss: 0.047443\n",
      "Train Epoche: 2 [1824/96 (1900%)]\tLoss: 6.718951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1825/96 (1901%)]\tLoss: 130.649811\n",
      "Train Epoche: 2 [1826/96 (1902%)]\tLoss: 62.953697\n",
      "Train Epoche: 2 [1827/96 (1903%)]\tLoss: 2.028829\n",
      "Train Epoche: 2 [1828/96 (1904%)]\tLoss: 0.004272\n",
      "Train Epoche: 2 [1829/96 (1905%)]\tLoss: 25.485195\n",
      "Train Epoche: 2 [1830/96 (1906%)]\tLoss: 3.005789\n",
      "Train Epoche: 2 [1831/96 (1907%)]\tLoss: 3.352307\n",
      "Train Epoche: 2 [1832/96 (1908%)]\tLoss: 13.554232\n",
      "Train Epoche: 2 [1833/96 (1909%)]\tLoss: 43.945255\n",
      "Train Epoche: 2 [1834/96 (1910%)]\tLoss: 12.441471\n",
      "Train Epoche: 2 [1835/96 (1911%)]\tLoss: 12.203967\n",
      "Train Epoche: 2 [1836/96 (1912%)]\tLoss: 2.658786\n",
      "Train Epoche: 2 [1837/96 (1914%)]\tLoss: 2.425836\n",
      "Train Epoche: 2 [1838/96 (1915%)]\tLoss: 9.072783\n",
      "Train Epoche: 2 [1839/96 (1916%)]\tLoss: 0.786165\n",
      "Train Epoche: 2 [1840/96 (1917%)]\tLoss: 2.454734\n",
      "Train Epoche: 2 [1841/96 (1918%)]\tLoss: 157.833221\n",
      "Train Epoche: 2 [1842/96 (1919%)]\tLoss: 6.418626\n",
      "Train Epoche: 2 [1843/96 (1920%)]\tLoss: 3.190279\n",
      "Train Epoche: 2 [1844/96 (1921%)]\tLoss: 41.699238\n",
      "Train Epoche: 2 [1845/96 (1922%)]\tLoss: 6.773562\n",
      "Train Epoche: 2 [1846/96 (1923%)]\tLoss: 0.416906\n",
      "Train Epoche: 2 [1847/96 (1924%)]\tLoss: 36.541058\n",
      "Train Epoche: 2 [1848/96 (1925%)]\tLoss: 0.871777\n",
      "Train Epoche: 2 [1849/96 (1926%)]\tLoss: 2.443204\n",
      "Train Epoche: 2 [1850/96 (1927%)]\tLoss: 5.170176\n",
      "Train Epoche: 2 [1851/96 (1928%)]\tLoss: 3.693956\n",
      "Train Epoche: 2 [1852/96 (1929%)]\tLoss: 0.292723\n",
      "Train Epoche: 2 [1853/96 (1930%)]\tLoss: 0.101728\n",
      "Train Epoche: 2 [1854/96 (1931%)]\tLoss: 227.116531\n",
      "Train Epoche: 2 [1855/96 (1932%)]\tLoss: 44.602173\n",
      "Train Epoche: 2 [1856/96 (1933%)]\tLoss: 0.063370\n",
      "Train Epoche: 2 [1857/96 (1934%)]\tLoss: 0.079722\n",
      "Train Epoche: 2 [1858/96 (1935%)]\tLoss: 4.830378\n",
      "Train Epoche: 2 [1859/96 (1936%)]\tLoss: 11.995456\n",
      "Train Epoche: 2 [1860/96 (1938%)]\tLoss: 18.902744\n",
      "Train Epoche: 2 [1861/96 (1939%)]\tLoss: 2.635067\n",
      "Train Epoche: 2 [1862/96 (1940%)]\tLoss: 15.995331\n",
      "Train Epoche: 2 [1863/96 (1941%)]\tLoss: 57.511555\n",
      "Train Epoche: 2 [1864/96 (1942%)]\tLoss: 0.388121\n",
      "Train Epoche: 2 [1865/96 (1943%)]\tLoss: 0.942258\n",
      "Train Epoche: 2 [1866/96 (1944%)]\tLoss: 4.131406\n",
      "Train Epoche: 2 [1867/96 (1945%)]\tLoss: 2.812412\n",
      "Train Epoche: 2 [1868/96 (1946%)]\tLoss: 4.198474\n",
      "Train Epoche: 2 [1869/96 (1947%)]\tLoss: 18.936884\n",
      "Train Epoche: 2 [1870/96 (1948%)]\tLoss: 0.130041\n",
      "Train Epoche: 2 [1871/96 (1949%)]\tLoss: 1.982688\n",
      "Train Epoche: 2 [1872/96 (1950%)]\tLoss: 0.339031\n",
      "Train Epoche: 2 [1873/96 (1951%)]\tLoss: 1.470599\n",
      "Train Epoche: 2 [1874/96 (1952%)]\tLoss: 0.006119\n",
      "Train Epoche: 2 [1875/96 (1953%)]\tLoss: 17.846510\n",
      "Train Epoche: 2 [1876/96 (1954%)]\tLoss: 3.609401\n",
      "Train Epoche: 2 [1877/96 (1955%)]\tLoss: 3.319366\n",
      "Train Epoche: 2 [1878/96 (1956%)]\tLoss: 35.929996\n",
      "Train Epoche: 2 [1879/96 (1957%)]\tLoss: 62.602863\n",
      "Train Epoche: 2 [1880/96 (1958%)]\tLoss: 6.425958\n",
      "Train Epoche: 2 [1881/96 (1959%)]\tLoss: 9.927927\n",
      "Train Epoche: 2 [1882/96 (1960%)]\tLoss: 5.236594\n",
      "Train Epoche: 2 [1883/96 (1961%)]\tLoss: 7.209402\n",
      "Train Epoche: 2 [1884/96 (1962%)]\tLoss: 326.388428\n",
      "Train Epoche: 2 [1885/96 (1964%)]\tLoss: 0.492864\n",
      "Train Epoche: 2 [1886/96 (1965%)]\tLoss: 3.434573\n",
      "Train Epoche: 2 [1887/96 (1966%)]\tLoss: 2.485985\n",
      "Train Epoche: 2 [1888/96 (1967%)]\tLoss: 5.876589\n",
      "Train Epoche: 2 [1889/96 (1968%)]\tLoss: 2.352566\n",
      "Train Epoche: 2 [1890/96 (1969%)]\tLoss: 0.137495\n",
      "Train Epoche: 2 [1891/96 (1970%)]\tLoss: 2.800850\n",
      "Train Epoche: 2 [1892/96 (1971%)]\tLoss: 0.009790\n",
      "Train Epoche: 2 [1893/96 (1972%)]\tLoss: 15.897717\n",
      "Train Epoche: 2 [1894/96 (1973%)]\tLoss: 0.395853\n",
      "Train Epoche: 2 [1895/96 (1974%)]\tLoss: 7.217767\n",
      "Train Epoche: 2 [1896/96 (1975%)]\tLoss: 74.481613\n",
      "Train Epoche: 2 [1897/96 (1976%)]\tLoss: 3.495591\n",
      "Train Epoche: 2 [1898/96 (1977%)]\tLoss: 0.549527\n",
      "Train Epoche: 2 [1899/96 (1978%)]\tLoss: 27.332510\n",
      "Train Epoche: 2 [1900/96 (1979%)]\tLoss: 10.604272\n",
      "Train Epoche: 2 [1901/96 (1980%)]\tLoss: 5.537447\n",
      "Train Epoche: 2 [1902/96 (1981%)]\tLoss: 0.869359\n",
      "Train Epoche: 2 [1903/96 (1982%)]\tLoss: 2.866568\n",
      "Train Epoche: 2 [1904/96 (1983%)]\tLoss: 5.290473\n",
      "Train Epoche: 2 [1905/96 (1984%)]\tLoss: 6.148799\n",
      "Train Epoche: 2 [1906/96 (1985%)]\tLoss: 1.113900\n",
      "Train Epoche: 2 [1907/96 (1986%)]\tLoss: 6.128148\n",
      "Train Epoche: 2 [1908/96 (1988%)]\tLoss: 2.122982\n",
      "Train Epoche: 2 [1909/96 (1989%)]\tLoss: 0.264453\n",
      "Train Epoche: 2 [1910/96 (1990%)]\tLoss: 37.002125\n",
      "Train Epoche: 2 [1911/96 (1991%)]\tLoss: 5.478725\n",
      "Train Epoche: 2 [1912/96 (1992%)]\tLoss: 80.047127\n",
      "Train Epoche: 2 [1913/96 (1993%)]\tLoss: 42.271675\n",
      "Train Epoche: 2 [1914/96 (1994%)]\tLoss: 0.044282\n",
      "Train Epoche: 2 [1915/96 (1995%)]\tLoss: 7.591001\n",
      "Train Epoche: 2 [1916/96 (1996%)]\tLoss: 1.047065\n",
      "Train Epoche: 2 [1917/96 (1997%)]\tLoss: 6.511234\n",
      "Train Epoche: 2 [1918/96 (1998%)]\tLoss: 0.090658\n",
      "Train Epoche: 2 [1919/96 (1999%)]\tLoss: 1.687248\n",
      "Train Epoche: 2 [1920/96 (2000%)]\tLoss: 2.371136\n",
      "Train Epoche: 2 [1921/96 (2001%)]\tLoss: 0.633144\n",
      "Train Epoche: 2 [1922/96 (2002%)]\tLoss: 0.000511\n",
      "Train Epoche: 2 [1923/96 (2003%)]\tLoss: 8.152373\n",
      "Train Epoche: 2 [1924/96 (2004%)]\tLoss: 254.124786\n",
      "Train Epoche: 2 [1925/96 (2005%)]\tLoss: 0.925304\n",
      "Train Epoche: 2 [1926/96 (2006%)]\tLoss: 0.360599\n",
      "Train Epoche: 2 [1927/96 (2007%)]\tLoss: 0.427335\n",
      "Train Epoche: 2 [1928/96 (2008%)]\tLoss: 1.636713\n",
      "Train Epoche: 2 [1929/96 (2009%)]\tLoss: 2.453927\n",
      "Train Epoche: 2 [1930/96 (2010%)]\tLoss: 0.093542\n",
      "Train Epoche: 2 [1931/96 (2011%)]\tLoss: 0.083573\n",
      "Train Epoche: 2 [1932/96 (2012%)]\tLoss: 1.757639\n",
      "Train Epoche: 2 [1933/96 (2014%)]\tLoss: 0.427701\n",
      "Train Epoche: 2 [1934/96 (2015%)]\tLoss: 1.918593\n",
      "Train Epoche: 2 [1935/96 (2016%)]\tLoss: 27.339071\n",
      "Train Epoche: 2 [1936/96 (2017%)]\tLoss: 4.899219\n",
      "Train Epoche: 2 [1937/96 (2018%)]\tLoss: 243.016525\n",
      "Train Epoche: 2 [1938/96 (2019%)]\tLoss: 8.525844\n",
      "Train Epoche: 2 [1939/96 (2020%)]\tLoss: 1.682200\n",
      "Train Epoche: 2 [1940/96 (2021%)]\tLoss: 7.760836\n",
      "Train Epoche: 2 [1941/96 (2022%)]\tLoss: 25.204981\n",
      "Train Epoche: 2 [1942/96 (2023%)]\tLoss: 22.147284\n",
      "Train Epoche: 2 [1943/96 (2024%)]\tLoss: 1.246653\n",
      "Train Epoche: 2 [1944/96 (2025%)]\tLoss: 0.073448\n",
      "Train Epoche: 2 [1945/96 (2026%)]\tLoss: 2.313684\n",
      "Train Epoche: 2 [1946/96 (2027%)]\tLoss: 0.042329\n",
      "Train Epoche: 2 [1947/96 (2028%)]\tLoss: 0.003768\n",
      "Train Epoche: 2 [1948/96 (2029%)]\tLoss: 2.258270\n",
      "Train Epoche: 2 [1949/96 (2030%)]\tLoss: 0.065320\n",
      "Train Epoche: 2 [1950/96 (2031%)]\tLoss: 0.003007\n",
      "Train Epoche: 2 [1951/96 (2032%)]\tLoss: 6.591116\n",
      "Train Epoche: 2 [1952/96 (2033%)]\tLoss: 6.290403\n",
      "Train Epoche: 2 [1953/96 (2034%)]\tLoss: 2.246382\n",
      "Train Epoche: 2 [1954/96 (2035%)]\tLoss: 4.051975\n",
      "Train Epoche: 2 [1955/96 (2036%)]\tLoss: 76.438683\n",
      "Train Epoche: 2 [1956/96 (2038%)]\tLoss: 33.397099\n",
      "Train Epoche: 2 [1957/96 (2039%)]\tLoss: 0.554503\n",
      "Train Epoche: 2 [1958/96 (2040%)]\tLoss: 0.302219\n",
      "Train Epoche: 2 [1959/96 (2041%)]\tLoss: 0.552622\n",
      "Train Epoche: 2 [1960/96 (2042%)]\tLoss: 3.439714\n",
      "Train Epoche: 2 [1961/96 (2043%)]\tLoss: 18.057377\n",
      "Train Epoche: 2 [1962/96 (2044%)]\tLoss: 1.251148\n",
      "Train Epoche: 2 [1963/96 (2045%)]\tLoss: 0.242823\n",
      "Train Epoche: 2 [1964/96 (2046%)]\tLoss: 23.491764\n",
      "Train Epoche: 2 [1965/96 (2047%)]\tLoss: 0.082450\n",
      "Train Epoche: 2 [1966/96 (2048%)]\tLoss: 9.017724\n",
      "Train Epoche: 2 [1967/96 (2049%)]\tLoss: 17.934406\n",
      "Train Epoche: 2 [1968/96 (2050%)]\tLoss: 1.637732\n",
      "Train Epoche: 2 [1969/96 (2051%)]\tLoss: 0.022320\n",
      "Train Epoche: 2 [1970/96 (2052%)]\tLoss: 26.184916\n",
      "Train Epoche: 2 [1971/96 (2053%)]\tLoss: 1.880766\n",
      "Train Epoche: 2 [1972/96 (2054%)]\tLoss: 2.810046\n",
      "Train Epoche: 2 [1973/96 (2055%)]\tLoss: 47.623394\n",
      "Train Epoche: 2 [1974/96 (2056%)]\tLoss: 0.823773\n",
      "Train Epoche: 2 [1975/96 (2057%)]\tLoss: 9.154229\n",
      "Train Epoche: 2 [1976/96 (2058%)]\tLoss: 2.858989\n",
      "Train Epoche: 2 [1977/96 (2059%)]\tLoss: 20.002010\n",
      "Train Epoche: 2 [1978/96 (2060%)]\tLoss: 6.930743\n",
      "Train Epoche: 2 [1979/96 (2061%)]\tLoss: 28.181107\n",
      "Train Epoche: 2 [1980/96 (2062%)]\tLoss: 8.164926\n",
      "Train Epoche: 2 [1981/96 (2064%)]\tLoss: 6.540323\n",
      "Train Epoche: 2 [1982/96 (2065%)]\tLoss: 1.247051\n",
      "Train Epoche: 2 [1983/96 (2066%)]\tLoss: 19.332102\n",
      "Train Epoche: 2 [1984/96 (2067%)]\tLoss: 3.007633\n",
      "Train Epoche: 2 [1985/96 (2068%)]\tLoss: 0.813281\n",
      "Train Epoche: 2 [1986/96 (2069%)]\tLoss: 22.709572\n",
      "Train Epoche: 2 [1987/96 (2070%)]\tLoss: 0.241630\n",
      "Train Epoche: 2 [1988/96 (2071%)]\tLoss: 0.349458\n",
      "Train Epoche: 2 [1989/96 (2072%)]\tLoss: 115.292213\n",
      "Train Epoche: 2 [1990/96 (2073%)]\tLoss: 3.679940\n",
      "Train Epoche: 2 [1991/96 (2074%)]\tLoss: 2.997974\n",
      "Train Epoche: 2 [1992/96 (2075%)]\tLoss: 41.266033\n",
      "Train Epoche: 2 [1993/96 (2076%)]\tLoss: 0.902319\n",
      "Train Epoche: 2 [1994/96 (2077%)]\tLoss: 6.655046\n",
      "Train Epoche: 2 [1995/96 (2078%)]\tLoss: 4.631648\n",
      "Train Epoche: 2 [1996/96 (2079%)]\tLoss: 1.801724\n",
      "Train Epoche: 2 [1997/96 (2080%)]\tLoss: 1.258562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1998/96 (2081%)]\tLoss: 0.410831\n",
      "Train Epoche: 2 [1999/96 (2082%)]\tLoss: 19.643311\n",
      "Train Epoche: 2 [2000/96 (2083%)]\tLoss: 0.122183\n",
      "Train Epoche: 2 [2001/96 (2084%)]\tLoss: 26.134605\n",
      "Train Epoche: 2 [2002/96 (2085%)]\tLoss: 9.318709\n",
      "Train Epoche: 2 [2003/96 (2086%)]\tLoss: 47.483421\n",
      "Train Epoche: 2 [2004/96 (2088%)]\tLoss: 15.541194\n",
      "Train Epoche: 2 [2005/96 (2089%)]\tLoss: 12.792089\n",
      "Train Epoche: 2 [2006/96 (2090%)]\tLoss: 18.575445\n",
      "Train Epoche: 2 [2007/96 (2091%)]\tLoss: 6.460464\n",
      "Train Epoche: 2 [2008/96 (2092%)]\tLoss: 0.832951\n",
      "Train Epoche: 2 [2009/96 (2093%)]\tLoss: 0.765098\n",
      "Train Epoche: 2 [2010/96 (2094%)]\tLoss: 7.184505\n",
      "Train Epoche: 2 [2011/96 (2095%)]\tLoss: 15.165279\n",
      "Train Epoche: 2 [2012/96 (2096%)]\tLoss: 9.145385\n",
      "Train Epoche: 2 [2013/96 (2097%)]\tLoss: 2.339927\n",
      "Train Epoche: 2 [2014/96 (2098%)]\tLoss: 74.068901\n",
      "Train Epoche: 2 [2015/96 (2099%)]\tLoss: 1.584949\n",
      "Train Epoche: 2 [2016/96 (2100%)]\tLoss: 0.425728\n",
      "Train Epoche: 2 [2017/96 (2101%)]\tLoss: 0.452395\n",
      "Train Epoche: 2 [2018/96 (2102%)]\tLoss: 26.978600\n",
      "Train Epoche: 2 [2019/96 (2103%)]\tLoss: 0.506166\n",
      "Train Epoche: 2 [2020/96 (2104%)]\tLoss: 25.811390\n",
      "Train Epoche: 2 [2021/96 (2105%)]\tLoss: 0.059310\n",
      "Train Epoche: 2 [2022/96 (2106%)]\tLoss: 29.108458\n",
      "Train Epoche: 2 [2023/96 (2107%)]\tLoss: 6.719666\n",
      "Train Epoche: 2 [2024/96 (2108%)]\tLoss: 0.121863\n",
      "Train Epoche: 2 [2025/96 (2109%)]\tLoss: 1.403512\n",
      "Train Epoche: 2 [2026/96 (2110%)]\tLoss: 33.964039\n",
      "Train Epoche: 3 [0/96 (0%)]\tLoss: 33.528019\n",
      "Train Epoche: 3 [1/96 (1%)]\tLoss: 10.361647\n",
      "Train Epoche: 3 [2/96 (2%)]\tLoss: 35.519997\n",
      "Train Epoche: 3 [3/96 (3%)]\tLoss: 52.705139\n",
      "Train Epoche: 3 [4/96 (4%)]\tLoss: 118.419685\n",
      "Train Epoche: 3 [5/96 (5%)]\tLoss: 18.268391\n",
      "Train Epoche: 3 [6/96 (6%)]\tLoss: 2.082383\n",
      "Train Epoche: 3 [7/96 (7%)]\tLoss: 39.765656\n",
      "Train Epoche: 3 [8/96 (8%)]\tLoss: 15.948292\n",
      "Train Epoche: 3 [9/96 (9%)]\tLoss: 1.874248\n",
      "Train Epoche: 3 [10/96 (10%)]\tLoss: 311.446198\n",
      "Train Epoche: 3 [11/96 (11%)]\tLoss: 2.962559\n",
      "Train Epoche: 3 [12/96 (12%)]\tLoss: 7.177782\n",
      "Train Epoche: 3 [13/96 (14%)]\tLoss: 0.075493\n",
      "Train Epoche: 3 [14/96 (15%)]\tLoss: 22.054415\n",
      "Train Epoche: 3 [15/96 (16%)]\tLoss: 0.190211\n",
      "Train Epoche: 3 [16/96 (17%)]\tLoss: 1.482034\n",
      "Train Epoche: 3 [17/96 (18%)]\tLoss: 13.896439\n",
      "Train Epoche: 3 [18/96 (19%)]\tLoss: 8.937065\n",
      "Train Epoche: 3 [19/96 (20%)]\tLoss: 0.660716\n",
      "Train Epoche: 3 [20/96 (21%)]\tLoss: 39.038162\n",
      "Train Epoche: 3 [21/96 (22%)]\tLoss: 12.798072\n",
      "Train Epoche: 3 [22/96 (23%)]\tLoss: 9.786205\n",
      "Train Epoche: 3 [23/96 (24%)]\tLoss: 14.678967\n",
      "Train Epoche: 3 [24/96 (25%)]\tLoss: 55.160160\n",
      "Train Epoche: 3 [25/96 (26%)]\tLoss: 13.681503\n",
      "Train Epoche: 3 [26/96 (27%)]\tLoss: 83.167091\n",
      "Train Epoche: 3 [27/96 (28%)]\tLoss: 8.219471\n",
      "Train Epoche: 3 [28/96 (29%)]\tLoss: 3.740105\n",
      "Train Epoche: 3 [29/96 (30%)]\tLoss: 20.554869\n",
      "Train Epoche: 3 [30/96 (31%)]\tLoss: 11.164818\n",
      "Train Epoche: 3 [31/96 (32%)]\tLoss: 6.362679\n",
      "Train Epoche: 3 [32/96 (33%)]\tLoss: 3.059977\n",
      "Train Epoche: 3 [33/96 (34%)]\tLoss: 20.448099\n",
      "Train Epoche: 3 [34/96 (35%)]\tLoss: 10.046047\n",
      "Train Epoche: 3 [35/96 (36%)]\tLoss: 1.551671\n",
      "Train Epoche: 3 [36/96 (38%)]\tLoss: 19.347595\n",
      "Train Epoche: 3 [37/96 (39%)]\tLoss: 0.267727\n",
      "Train Epoche: 3 [38/96 (40%)]\tLoss: 17.821936\n",
      "Train Epoche: 3 [39/96 (41%)]\tLoss: 47.379044\n",
      "Train Epoche: 3 [40/96 (42%)]\tLoss: 19.457554\n",
      "Train Epoche: 3 [41/96 (43%)]\tLoss: 1.936109\n",
      "Train Epoche: 3 [42/96 (44%)]\tLoss: 83.057213\n",
      "Train Epoche: 3 [43/96 (45%)]\tLoss: 0.002227\n",
      "Train Epoche: 3 [44/96 (46%)]\tLoss: 375.674652\n",
      "Train Epoche: 3 [45/96 (47%)]\tLoss: 0.373901\n",
      "Train Epoche: 3 [46/96 (48%)]\tLoss: 1.822854\n",
      "Train Epoche: 3 [47/96 (49%)]\tLoss: 0.970824\n",
      "Train Epoche: 3 [48/96 (50%)]\tLoss: 4.651059\n",
      "Train Epoche: 3 [49/96 (51%)]\tLoss: 2.519512\n",
      "Train Epoche: 3 [50/96 (52%)]\tLoss: 138.782364\n",
      "Train Epoche: 3 [51/96 (53%)]\tLoss: 2.856805\n",
      "Train Epoche: 3 [52/96 (54%)]\tLoss: 0.186810\n",
      "Train Epoche: 3 [53/96 (55%)]\tLoss: 0.515694\n",
      "Train Epoche: 3 [54/96 (56%)]\tLoss: 67.300385\n",
      "Train Epoche: 3 [55/96 (57%)]\tLoss: 0.013373\n",
      "Train Epoche: 3 [56/96 (58%)]\tLoss: 17.872183\n",
      "Train Epoche: 3 [57/96 (59%)]\tLoss: 4.243558\n",
      "Train Epoche: 3 [58/96 (60%)]\tLoss: 0.007606\n",
      "Train Epoche: 3 [59/96 (61%)]\tLoss: 0.001345\n",
      "Train Epoche: 3 [60/96 (62%)]\tLoss: 60.386662\n",
      "Train Epoche: 3 [61/96 (64%)]\tLoss: 4.935403\n",
      "Train Epoche: 3 [62/96 (65%)]\tLoss: 8.367411\n",
      "Train Epoche: 3 [63/96 (66%)]\tLoss: 2.413455\n",
      "Train Epoche: 3 [64/96 (67%)]\tLoss: 1.696918\n",
      "Train Epoche: 3 [65/96 (68%)]\tLoss: 0.773210\n",
      "Train Epoche: 3 [66/96 (69%)]\tLoss: 50.721073\n",
      "Train Epoche: 3 [67/96 (70%)]\tLoss: 0.247189\n",
      "Train Epoche: 3 [68/96 (71%)]\tLoss: 6.113458\n",
      "Train Epoche: 3 [69/96 (72%)]\tLoss: 0.637787\n",
      "Train Epoche: 3 [70/96 (73%)]\tLoss: 49.314209\n",
      "Train Epoche: 3 [71/96 (74%)]\tLoss: 0.083745\n",
      "Train Epoche: 3 [72/96 (75%)]\tLoss: 10.220709\n",
      "Train Epoche: 3 [73/96 (76%)]\tLoss: 0.276460\n",
      "Train Epoche: 3 [74/96 (77%)]\tLoss: 2.156203\n",
      "Train Epoche: 3 [75/96 (78%)]\tLoss: 0.000949\n",
      "Train Epoche: 3 [76/96 (79%)]\tLoss: 40.180534\n",
      "Train Epoche: 3 [77/96 (80%)]\tLoss: 0.000029\n",
      "Train Epoche: 3 [78/96 (81%)]\tLoss: 1.116338\n",
      "Train Epoche: 3 [79/96 (82%)]\tLoss: 5.845757\n",
      "Train Epoche: 3 [80/96 (83%)]\tLoss: 1.324151\n",
      "Train Epoche: 3 [81/96 (84%)]\tLoss: 0.756026\n",
      "Train Epoche: 3 [82/96 (85%)]\tLoss: 0.593253\n",
      "Train Epoche: 3 [83/96 (86%)]\tLoss: 0.438460\n",
      "Train Epoche: 3 [84/96 (88%)]\tLoss: 6.383080\n",
      "Train Epoche: 3 [85/96 (89%)]\tLoss: 7.428507\n",
      "Train Epoche: 3 [86/96 (90%)]\tLoss: 11.495459\n",
      "Train Epoche: 3 [87/96 (91%)]\tLoss: 4.282775\n",
      "Train Epoche: 3 [88/96 (92%)]\tLoss: 3.411131\n",
      "Train Epoche: 3 [89/96 (93%)]\tLoss: 0.002032\n",
      "Train Epoche: 3 [90/96 (94%)]\tLoss: 0.001613\n",
      "Train Epoche: 3 [91/96 (95%)]\tLoss: 2.468714\n",
      "Train Epoche: 3 [92/96 (96%)]\tLoss: 4.596197\n",
      "Train Epoche: 3 [93/96 (97%)]\tLoss: 31.112925\n",
      "Train Epoche: 3 [94/96 (98%)]\tLoss: 2.752744\n",
      "Train Epoche: 3 [95/96 (99%)]\tLoss: 14.876982\n",
      "Train Epoche: 3 [96/96 (100%)]\tLoss: 52.175140\n",
      "Train Epoche: 3 [97/96 (101%)]\tLoss: 113.547394\n",
      "Train Epoche: 3 [98/96 (102%)]\tLoss: 1.409108\n",
      "Train Epoche: 3 [99/96 (103%)]\tLoss: 113.687042\n",
      "Train Epoche: 3 [100/96 (104%)]\tLoss: 5.531400\n",
      "Train Epoche: 3 [101/96 (105%)]\tLoss: 5.975566\n",
      "Train Epoche: 3 [102/96 (106%)]\tLoss: 10.989292\n",
      "Train Epoche: 3 [103/96 (107%)]\tLoss: 0.882769\n",
      "Train Epoche: 3 [104/96 (108%)]\tLoss: 1.305624\n",
      "Train Epoche: 3 [105/96 (109%)]\tLoss: 19.423830\n",
      "Train Epoche: 3 [106/96 (110%)]\tLoss: 2.963679\n",
      "Train Epoche: 3 [107/96 (111%)]\tLoss: 13.720552\n",
      "Train Epoche: 3 [108/96 (112%)]\tLoss: 204.660980\n",
      "Train Epoche: 3 [109/96 (114%)]\tLoss: 0.974485\n",
      "Train Epoche: 3 [110/96 (115%)]\tLoss: 0.123170\n",
      "Train Epoche: 3 [111/96 (116%)]\tLoss: 5.917647\n",
      "Train Epoche: 3 [112/96 (117%)]\tLoss: 8.326278\n",
      "Train Epoche: 3 [113/96 (118%)]\tLoss: 9.373594\n",
      "Train Epoche: 3 [114/96 (119%)]\tLoss: 3.526011\n",
      "Train Epoche: 3 [115/96 (120%)]\tLoss: 111.949013\n",
      "Train Epoche: 3 [116/96 (121%)]\tLoss: 0.001207\n",
      "Train Epoche: 3 [117/96 (122%)]\tLoss: 1.005187\n",
      "Train Epoche: 3 [118/96 (123%)]\tLoss: 32.015728\n",
      "Train Epoche: 3 [119/96 (124%)]\tLoss: 0.363634\n",
      "Train Epoche: 3 [120/96 (125%)]\tLoss: 0.098598\n",
      "Train Epoche: 3 [121/96 (126%)]\tLoss: 9.300855\n",
      "Train Epoche: 3 [122/96 (127%)]\tLoss: 12.686361\n",
      "Train Epoche: 3 [123/96 (128%)]\tLoss: 0.015161\n",
      "Train Epoche: 3 [124/96 (129%)]\tLoss: 3.405047\n",
      "Train Epoche: 3 [125/96 (130%)]\tLoss: 73.379784\n",
      "Train Epoche: 3 [126/96 (131%)]\tLoss: 1.137259\n",
      "Train Epoche: 3 [127/96 (132%)]\tLoss: 0.258672\n",
      "Train Epoche: 3 [128/96 (133%)]\tLoss: 117.855759\n",
      "Train Epoche: 3 [129/96 (134%)]\tLoss: 1.033748\n",
      "Train Epoche: 3 [130/96 (135%)]\tLoss: 0.879636\n",
      "Train Epoche: 3 [131/96 (136%)]\tLoss: 4.013526\n",
      "Train Epoche: 3 [132/96 (138%)]\tLoss: 1.025853\n",
      "Train Epoche: 3 [133/96 (139%)]\tLoss: 0.450216\n",
      "Train Epoche: 3 [134/96 (140%)]\tLoss: 0.814025\n",
      "Train Epoche: 3 [135/96 (141%)]\tLoss: 130.919220\n",
      "Train Epoche: 3 [136/96 (142%)]\tLoss: 0.137171\n",
      "Train Epoche: 3 [137/96 (143%)]\tLoss: 4.038364\n",
      "Train Epoche: 3 [138/96 (144%)]\tLoss: 0.051348\n",
      "Train Epoche: 3 [139/96 (145%)]\tLoss: 34.710720\n",
      "Train Epoche: 3 [140/96 (146%)]\tLoss: 0.389715\n",
      "Train Epoche: 3 [141/96 (147%)]\tLoss: 3.099051\n",
      "Train Epoche: 3 [142/96 (148%)]\tLoss: 49.601273\n",
      "Train Epoche: 3 [143/96 (149%)]\tLoss: 2.954525\n",
      "Train Epoche: 3 [144/96 (150%)]\tLoss: 49.413292\n",
      "Train Epoche: 3 [145/96 (151%)]\tLoss: 24.662477\n",
      "Train Epoche: 3 [146/96 (152%)]\tLoss: 3.373356\n",
      "Train Epoche: 3 [147/96 (153%)]\tLoss: 4.626994\n",
      "Train Epoche: 3 [148/96 (154%)]\tLoss: 357.421631\n",
      "Train Epoche: 3 [149/96 (155%)]\tLoss: 0.329339\n",
      "Train Epoche: 3 [150/96 (156%)]\tLoss: 1.828014\n",
      "Train Epoche: 3 [151/96 (157%)]\tLoss: 0.396492\n",
      "Train Epoche: 3 [152/96 (158%)]\tLoss: 1.373318\n",
      "Train Epoche: 3 [153/96 (159%)]\tLoss: 0.018793\n",
      "Train Epoche: 3 [154/96 (160%)]\tLoss: 38.759190\n",
      "Train Epoche: 3 [155/96 (161%)]\tLoss: 0.617554\n",
      "Train Epoche: 3 [156/96 (162%)]\tLoss: 47.445511\n",
      "Train Epoche: 3 [157/96 (164%)]\tLoss: 8.337341\n",
      "Train Epoche: 3 [158/96 (165%)]\tLoss: 0.262380\n",
      "Train Epoche: 3 [159/96 (166%)]\tLoss: 90.173409\n",
      "Train Epoche: 3 [160/96 (167%)]\tLoss: 1.206973\n",
      "Train Epoche: 3 [161/96 (168%)]\tLoss: 25.017532\n",
      "Train Epoche: 3 [162/96 (169%)]\tLoss: 27.640169\n",
      "Train Epoche: 3 [163/96 (170%)]\tLoss: 62.939217\n",
      "Train Epoche: 3 [164/96 (171%)]\tLoss: 0.908327\n",
      "Train Epoche: 3 [165/96 (172%)]\tLoss: 16.423157\n",
      "Train Epoche: 3 [166/96 (173%)]\tLoss: 38.441357\n",
      "Train Epoche: 3 [167/96 (174%)]\tLoss: 158.263458\n",
      "Train Epoche: 3 [168/96 (175%)]\tLoss: 2.612492\n",
      "Train Epoche: 3 [169/96 (176%)]\tLoss: 0.581519\n",
      "Train Epoche: 3 [170/96 (177%)]\tLoss: 0.131167\n",
      "Train Epoche: 3 [171/96 (178%)]\tLoss: 1.491604\n",
      "Train Epoche: 3 [172/96 (179%)]\tLoss: 35.426403\n",
      "Train Epoche: 3 [173/96 (180%)]\tLoss: 99.332916\n",
      "Train Epoche: 3 [174/96 (181%)]\tLoss: 3.957042\n",
      "Train Epoche: 3 [175/96 (182%)]\tLoss: 17.491262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [176/96 (183%)]\tLoss: 18.611120\n",
      "Train Epoche: 3 [177/96 (184%)]\tLoss: 23.617813\n",
      "Train Epoche: 3 [178/96 (185%)]\tLoss: 0.216636\n",
      "Train Epoche: 3 [179/96 (186%)]\tLoss: 6.393898\n",
      "Train Epoche: 3 [180/96 (188%)]\tLoss: 11.332798\n",
      "Train Epoche: 3 [181/96 (189%)]\tLoss: 22.641218\n",
      "Train Epoche: 3 [182/96 (190%)]\tLoss: 4.728639\n",
      "Train Epoche: 3 [183/96 (191%)]\tLoss: 9.977187\n",
      "Train Epoche: 3 [184/96 (192%)]\tLoss: 4.363460\n",
      "Train Epoche: 3 [185/96 (193%)]\tLoss: 12.891902\n",
      "Train Epoche: 3 [186/96 (194%)]\tLoss: 1.673827\n",
      "Train Epoche: 3 [187/96 (195%)]\tLoss: 0.006138\n",
      "Train Epoche: 3 [188/96 (196%)]\tLoss: 20.394865\n",
      "Train Epoche: 3 [189/96 (197%)]\tLoss: 25.378572\n",
      "Train Epoche: 3 [190/96 (198%)]\tLoss: 1.811869\n",
      "Train Epoche: 3 [191/96 (199%)]\tLoss: 31.547129\n",
      "Train Epoche: 3 [192/96 (200%)]\tLoss: 0.165906\n",
      "Train Epoche: 3 [193/96 (201%)]\tLoss: 4.640954\n",
      "Train Epoche: 3 [194/96 (202%)]\tLoss: 3.360974\n",
      "Train Epoche: 3 [195/96 (203%)]\tLoss: 23.808155\n",
      "Train Epoche: 3 [196/96 (204%)]\tLoss: 10.354409\n",
      "Train Epoche: 3 [197/96 (205%)]\tLoss: 5.236541\n",
      "Train Epoche: 3 [198/96 (206%)]\tLoss: 30.311893\n",
      "Train Epoche: 3 [199/96 (207%)]\tLoss: 1.506703\n",
      "Train Epoche: 3 [200/96 (208%)]\tLoss: 1.368055\n",
      "Train Epoche: 3 [201/96 (209%)]\tLoss: 3.128980\n",
      "Train Epoche: 3 [202/96 (210%)]\tLoss: 15.446941\n",
      "Train Epoche: 3 [203/96 (211%)]\tLoss: 10.820849\n",
      "Train Epoche: 3 [204/96 (212%)]\tLoss: 6.158711\n",
      "Train Epoche: 3 [205/96 (214%)]\tLoss: 0.845533\n",
      "Train Epoche: 3 [206/96 (215%)]\tLoss: 9.029831\n",
      "Train Epoche: 3 [207/96 (216%)]\tLoss: 1.591761\n",
      "Train Epoche: 3 [208/96 (217%)]\tLoss: 0.014053\n",
      "Train Epoche: 3 [209/96 (218%)]\tLoss: 147.638046\n",
      "Train Epoche: 3 [210/96 (219%)]\tLoss: 0.000204\n",
      "Train Epoche: 3 [211/96 (220%)]\tLoss: 12.078134\n",
      "Train Epoche: 3 [212/96 (221%)]\tLoss: 4.885867\n",
      "Train Epoche: 3 [213/96 (222%)]\tLoss: 6.353460\n",
      "Train Epoche: 3 [214/96 (223%)]\tLoss: 0.403767\n",
      "Train Epoche: 3 [215/96 (224%)]\tLoss: 1.164976\n",
      "Train Epoche: 3 [216/96 (225%)]\tLoss: 11.824459\n",
      "Train Epoche: 3 [217/96 (226%)]\tLoss: 5.964146\n",
      "Train Epoche: 3 [218/96 (227%)]\tLoss: 1.781188\n",
      "Train Epoche: 3 [219/96 (228%)]\tLoss: 0.687723\n",
      "Train Epoche: 3 [220/96 (229%)]\tLoss: 2.912501\n",
      "Train Epoche: 3 [221/96 (230%)]\tLoss: 5.632605\n",
      "Train Epoche: 3 [222/96 (231%)]\tLoss: 1.211092\n",
      "Train Epoche: 3 [223/96 (232%)]\tLoss: 5.602556\n",
      "Train Epoche: 3 [224/96 (233%)]\tLoss: 20.664619\n",
      "Train Epoche: 3 [225/96 (234%)]\tLoss: 8.522923\n",
      "Train Epoche: 3 [226/96 (235%)]\tLoss: 4.394306\n",
      "Train Epoche: 3 [227/96 (236%)]\tLoss: 1.045276\n",
      "Train Epoche: 3 [228/96 (238%)]\tLoss: 30.298904\n",
      "Train Epoche: 3 [229/96 (239%)]\tLoss: 2.237218\n",
      "Train Epoche: 3 [230/96 (240%)]\tLoss: 1.769486\n",
      "Train Epoche: 3 [231/96 (241%)]\tLoss: 164.693039\n",
      "Train Epoche: 3 [232/96 (242%)]\tLoss: 1.488056\n",
      "Train Epoche: 3 [233/96 (243%)]\tLoss: 0.028411\n",
      "Train Epoche: 3 [234/96 (244%)]\tLoss: 0.243117\n",
      "Train Epoche: 3 [235/96 (245%)]\tLoss: 32.320377\n",
      "Train Epoche: 3 [236/96 (246%)]\tLoss: 228.756561\n",
      "Train Epoche: 3 [237/96 (247%)]\tLoss: 19.625435\n",
      "Train Epoche: 3 [238/96 (248%)]\tLoss: 3.786742\n",
      "Train Epoche: 3 [239/96 (249%)]\tLoss: 27.820290\n",
      "Train Epoche: 3 [240/96 (250%)]\tLoss: 10.790472\n",
      "Train Epoche: 3 [241/96 (251%)]\tLoss: 1.213806\n",
      "Train Epoche: 3 [242/96 (252%)]\tLoss: 5.697701\n",
      "Train Epoche: 3 [243/96 (253%)]\tLoss: 1.279438\n",
      "Train Epoche: 3 [244/96 (254%)]\tLoss: 0.104845\n",
      "Train Epoche: 3 [245/96 (255%)]\tLoss: 5.019145\n",
      "Train Epoche: 3 [246/96 (256%)]\tLoss: 18.289730\n",
      "Train Epoche: 3 [247/96 (257%)]\tLoss: 2.361530\n",
      "Train Epoche: 3 [248/96 (258%)]\tLoss: 1.354578\n",
      "Train Epoche: 3 [249/96 (259%)]\tLoss: 25.091331\n",
      "Train Epoche: 3 [250/96 (260%)]\tLoss: 14.037355\n",
      "Train Epoche: 3 [251/96 (261%)]\tLoss: 0.565789\n",
      "Train Epoche: 3 [252/96 (262%)]\tLoss: 1.436559\n",
      "Train Epoche: 3 [253/96 (264%)]\tLoss: 3.907097\n",
      "Train Epoche: 3 [254/96 (265%)]\tLoss: 7.149296\n",
      "Train Epoche: 3 [255/96 (266%)]\tLoss: 1.020473\n",
      "Train Epoche: 3 [256/96 (267%)]\tLoss: 9.907345\n",
      "Train Epoche: 3 [257/96 (268%)]\tLoss: 0.682532\n",
      "Train Epoche: 3 [258/96 (269%)]\tLoss: 6.515866\n",
      "Train Epoche: 3 [259/96 (270%)]\tLoss: 10.700039\n",
      "Train Epoche: 3 [260/96 (271%)]\tLoss: 20.678965\n",
      "Train Epoche: 3 [261/96 (272%)]\tLoss: 0.030470\n",
      "Train Epoche: 3 [262/96 (273%)]\tLoss: 0.279548\n",
      "Train Epoche: 3 [263/96 (274%)]\tLoss: 1.699069\n",
      "Train Epoche: 3 [264/96 (275%)]\tLoss: 0.538783\n",
      "Train Epoche: 3 [265/96 (276%)]\tLoss: 4.088015\n",
      "Train Epoche: 3 [266/96 (277%)]\tLoss: 2.393380\n",
      "Train Epoche: 3 [267/96 (278%)]\tLoss: 132.710876\n",
      "Train Epoche: 3 [268/96 (279%)]\tLoss: 2.035679\n",
      "Train Epoche: 3 [269/96 (280%)]\tLoss: 5.602927\n",
      "Train Epoche: 3 [270/96 (281%)]\tLoss: 0.409619\n",
      "Train Epoche: 3 [271/96 (282%)]\tLoss: 0.489062\n",
      "Train Epoche: 3 [272/96 (283%)]\tLoss: 0.042703\n",
      "Train Epoche: 3 [273/96 (284%)]\tLoss: 0.085135\n",
      "Train Epoche: 3 [274/96 (285%)]\tLoss: 0.826186\n",
      "Train Epoche: 3 [275/96 (286%)]\tLoss: 0.064541\n",
      "Train Epoche: 3 [276/96 (288%)]\tLoss: 17.252462\n",
      "Train Epoche: 3 [277/96 (289%)]\tLoss: 0.799527\n",
      "Train Epoche: 3 [278/96 (290%)]\tLoss: 2.067949\n",
      "Train Epoche: 3 [279/96 (291%)]\tLoss: 0.307528\n",
      "Train Epoche: 3 [280/96 (292%)]\tLoss: 1.892518\n",
      "Train Epoche: 3 [281/96 (293%)]\tLoss: 26.251047\n",
      "Train Epoche: 3 [282/96 (294%)]\tLoss: 16.036421\n",
      "Train Epoche: 3 [283/96 (295%)]\tLoss: 4.467921\n",
      "Train Epoche: 3 [284/96 (296%)]\tLoss: 5.679977\n",
      "Train Epoche: 3 [285/96 (297%)]\tLoss: 6.939367\n",
      "Train Epoche: 3 [286/96 (298%)]\tLoss: 0.319193\n",
      "Train Epoche: 3 [287/96 (299%)]\tLoss: 0.183582\n",
      "Train Epoche: 3 [288/96 (300%)]\tLoss: 2.720891\n",
      "Train Epoche: 3 [289/96 (301%)]\tLoss: 53.332268\n",
      "Train Epoche: 3 [290/96 (302%)]\tLoss: 4.999385\n",
      "Train Epoche: 3 [291/96 (303%)]\tLoss: 0.258930\n",
      "Train Epoche: 3 [292/96 (304%)]\tLoss: 1.127719\n",
      "Train Epoche: 3 [293/96 (305%)]\tLoss: 14.340320\n",
      "Train Epoche: 3 [294/96 (306%)]\tLoss: 0.418497\n",
      "Train Epoche: 3 [295/96 (307%)]\tLoss: 0.851514\n",
      "Train Epoche: 3 [296/96 (308%)]\tLoss: 1.169212\n",
      "Train Epoche: 3 [297/96 (309%)]\tLoss: 3.789077\n",
      "Train Epoche: 3 [298/96 (310%)]\tLoss: 0.146016\n",
      "Train Epoche: 3 [299/96 (311%)]\tLoss: 1.366841\n",
      "Train Epoche: 3 [300/96 (312%)]\tLoss: 8.978029\n",
      "Train Epoche: 3 [301/96 (314%)]\tLoss: 33.480392\n",
      "Train Epoche: 3 [302/96 (315%)]\tLoss: 5.517008\n",
      "Train Epoche: 3 [303/96 (316%)]\tLoss: 7.222492\n",
      "Train Epoche: 3 [304/96 (317%)]\tLoss: 4.377765\n",
      "Train Epoche: 3 [305/96 (318%)]\tLoss: 6.906230\n",
      "Train Epoche: 3 [306/96 (319%)]\tLoss: 9.344329\n",
      "Train Epoche: 3 [307/96 (320%)]\tLoss: 1.567536\n",
      "Train Epoche: 3 [308/96 (321%)]\tLoss: 52.218643\n",
      "Train Epoche: 3 [309/96 (322%)]\tLoss: 1.217686\n",
      "Train Epoche: 3 [310/96 (323%)]\tLoss: 1.906746\n",
      "Train Epoche: 3 [311/96 (324%)]\tLoss: 0.000927\n",
      "Train Epoche: 3 [312/96 (325%)]\tLoss: 10.375840\n",
      "Train Epoche: 3 [313/96 (326%)]\tLoss: 2.864666\n",
      "Train Epoche: 3 [314/96 (327%)]\tLoss: 63.116882\n",
      "Train Epoche: 3 [315/96 (328%)]\tLoss: 4.260337\n",
      "Train Epoche: 3 [316/96 (329%)]\tLoss: 0.217578\n",
      "Train Epoche: 3 [317/96 (330%)]\tLoss: 0.516395\n",
      "Train Epoche: 3 [318/96 (331%)]\tLoss: 16.661024\n",
      "Train Epoche: 3 [319/96 (332%)]\tLoss: 0.169179\n",
      "Train Epoche: 3 [320/96 (333%)]\tLoss: 5.941797\n",
      "Train Epoche: 3 [321/96 (334%)]\tLoss: 8.813805\n",
      "Train Epoche: 3 [322/96 (335%)]\tLoss: 0.000848\n",
      "Train Epoche: 3 [323/96 (336%)]\tLoss: 0.370615\n",
      "Train Epoche: 3 [324/96 (338%)]\tLoss: 0.567004\n",
      "Train Epoche: 3 [325/96 (339%)]\tLoss: 3.120611\n",
      "Train Epoche: 3 [326/96 (340%)]\tLoss: 1.299861\n",
      "Train Epoche: 3 [327/96 (341%)]\tLoss: 0.034220\n",
      "Train Epoche: 3 [328/96 (342%)]\tLoss: 0.992893\n",
      "Train Epoche: 3 [329/96 (343%)]\tLoss: 58.424976\n",
      "Train Epoche: 3 [330/96 (344%)]\tLoss: 11.750814\n",
      "Train Epoche: 3 [331/96 (345%)]\tLoss: 118.754990\n",
      "Train Epoche: 3 [332/96 (346%)]\tLoss: 0.106241\n",
      "Train Epoche: 3 [333/96 (347%)]\tLoss: 0.138319\n",
      "Train Epoche: 3 [334/96 (348%)]\tLoss: 4.989883\n",
      "Train Epoche: 3 [335/96 (349%)]\tLoss: 2.335629\n",
      "Train Epoche: 3 [336/96 (350%)]\tLoss: 56.530258\n",
      "Train Epoche: 3 [337/96 (351%)]\tLoss: 5.940681\n",
      "Train Epoche: 3 [338/96 (352%)]\tLoss: 0.311612\n",
      "Train Epoche: 3 [339/96 (353%)]\tLoss: 1.841266\n",
      "Train Epoche: 3 [340/96 (354%)]\tLoss: 11.006724\n",
      "Train Epoche: 3 [341/96 (355%)]\tLoss: 0.393189\n",
      "Train Epoche: 3 [342/96 (356%)]\tLoss: 0.678322\n",
      "Train Epoche: 3 [343/96 (357%)]\tLoss: 0.012424\n",
      "Train Epoche: 3 [344/96 (358%)]\tLoss: 40.923187\n",
      "Train Epoche: 3 [345/96 (359%)]\tLoss: 234.708023\n",
      "Train Epoche: 3 [346/96 (360%)]\tLoss: 1.116292\n",
      "Train Epoche: 3 [347/96 (361%)]\tLoss: 0.727417\n",
      "Train Epoche: 3 [348/96 (362%)]\tLoss: 0.623293\n",
      "Train Epoche: 3 [349/96 (364%)]\tLoss: 0.049933\n",
      "Train Epoche: 3 [350/96 (365%)]\tLoss: 0.596552\n",
      "Train Epoche: 3 [351/96 (366%)]\tLoss: 9.326764\n",
      "Train Epoche: 3 [352/96 (367%)]\tLoss: 86.001221\n",
      "Train Epoche: 3 [353/96 (368%)]\tLoss: 4.412093\n",
      "Train Epoche: 3 [354/96 (369%)]\tLoss: 0.733512\n",
      "Train Epoche: 3 [355/96 (370%)]\tLoss: 2.219744\n",
      "Train Epoche: 3 [356/96 (371%)]\tLoss: 159.573837\n",
      "Train Epoche: 3 [357/96 (372%)]\tLoss: 49.967796\n",
      "Train Epoche: 3 [358/96 (373%)]\tLoss: 1.289044\n",
      "Train Epoche: 3 [359/96 (374%)]\tLoss: 0.019679\n",
      "Train Epoche: 3 [360/96 (375%)]\tLoss: 4.419319\n",
      "Train Epoche: 3 [361/96 (376%)]\tLoss: 2.914011\n",
      "Train Epoche: 3 [362/96 (377%)]\tLoss: 0.657319\n",
      "Train Epoche: 3 [363/96 (378%)]\tLoss: 0.003476\n",
      "Train Epoche: 3 [364/96 (379%)]\tLoss: 17.964855\n",
      "Train Epoche: 3 [365/96 (380%)]\tLoss: 13.696654\n",
      "Train Epoche: 3 [366/96 (381%)]\tLoss: 1.602728\n",
      "Train Epoche: 3 [367/96 (382%)]\tLoss: 3.325838\n",
      "Train Epoche: 3 [368/96 (383%)]\tLoss: 0.658135\n",
      "Train Epoche: 3 [369/96 (384%)]\tLoss: 3.146728\n",
      "Train Epoche: 3 [370/96 (385%)]\tLoss: 9.254001\n",
      "Train Epoche: 3 [371/96 (386%)]\tLoss: 0.564846\n",
      "Train Epoche: 3 [372/96 (388%)]\tLoss: 39.835136\n",
      "Train Epoche: 3 [373/96 (389%)]\tLoss: 0.113755\n",
      "Train Epoche: 3 [374/96 (390%)]\tLoss: 50.676014\n",
      "Train Epoche: 3 [375/96 (391%)]\tLoss: 5.485174\n",
      "Train Epoche: 3 [376/96 (392%)]\tLoss: 33.619915\n",
      "Train Epoche: 3 [377/96 (393%)]\tLoss: 1.135091\n",
      "Train Epoche: 3 [378/96 (394%)]\tLoss: 0.511432\n",
      "Train Epoche: 3 [379/96 (395%)]\tLoss: 2.033161\n",
      "Train Epoche: 3 [380/96 (396%)]\tLoss: 1.495762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [381/96 (397%)]\tLoss: 21.270012\n",
      "Train Epoche: 3 [382/96 (398%)]\tLoss: 0.062337\n",
      "Train Epoche: 3 [383/96 (399%)]\tLoss: 0.714245\n",
      "Train Epoche: 3 [384/96 (400%)]\tLoss: 18.124258\n",
      "Train Epoche: 3 [385/96 (401%)]\tLoss: 0.149555\n",
      "Train Epoche: 3 [386/96 (402%)]\tLoss: 17.259125\n",
      "Train Epoche: 3 [387/96 (403%)]\tLoss: 9.271227\n",
      "Train Epoche: 3 [388/96 (404%)]\tLoss: 11.723781\n",
      "Train Epoche: 3 [389/96 (405%)]\tLoss: 0.450602\n",
      "Train Epoche: 3 [390/96 (406%)]\tLoss: 25.788700\n",
      "Train Epoche: 3 [391/96 (407%)]\tLoss: 2.134511\n",
      "Train Epoche: 3 [392/96 (408%)]\tLoss: 16.120626\n",
      "Train Epoche: 3 [393/96 (409%)]\tLoss: 234.663437\n",
      "Train Epoche: 3 [394/96 (410%)]\tLoss: 26.655802\n",
      "Train Epoche: 3 [395/96 (411%)]\tLoss: 3.636353\n",
      "Train Epoche: 3 [396/96 (412%)]\tLoss: 0.446858\n",
      "Train Epoche: 3 [397/96 (414%)]\tLoss: 3.067723\n",
      "Train Epoche: 3 [398/96 (415%)]\tLoss: 3.003837\n",
      "Train Epoche: 3 [399/96 (416%)]\tLoss: 0.019530\n",
      "Train Epoche: 3 [400/96 (417%)]\tLoss: 1.368512\n",
      "Train Epoche: 3 [401/96 (418%)]\tLoss: 5.528451\n",
      "Train Epoche: 3 [402/96 (419%)]\tLoss: 1.192979\n",
      "Train Epoche: 3 [403/96 (420%)]\tLoss: 3.493695\n",
      "Train Epoche: 3 [404/96 (421%)]\tLoss: 3.744640\n",
      "Train Epoche: 3 [405/96 (422%)]\tLoss: 3.009969\n",
      "Train Epoche: 3 [406/96 (423%)]\tLoss: 2.404710\n",
      "Train Epoche: 3 [407/96 (424%)]\tLoss: 2.764174\n",
      "Train Epoche: 3 [408/96 (425%)]\tLoss: 0.007727\n",
      "Train Epoche: 3 [409/96 (426%)]\tLoss: 1.543799\n",
      "Train Epoche: 3 [410/96 (427%)]\tLoss: 0.028339\n",
      "Train Epoche: 3 [411/96 (428%)]\tLoss: 4.301846\n",
      "Train Epoche: 3 [412/96 (429%)]\tLoss: 30.928612\n",
      "Train Epoche: 3 [413/96 (430%)]\tLoss: 2.196902\n",
      "Train Epoche: 3 [414/96 (431%)]\tLoss: 0.041262\n",
      "Train Epoche: 3 [415/96 (432%)]\tLoss: 8.019158\n",
      "Train Epoche: 3 [416/96 (433%)]\tLoss: 1.485347\n",
      "Train Epoche: 3 [417/96 (434%)]\tLoss: 1.285151\n",
      "Train Epoche: 3 [418/96 (435%)]\tLoss: 2.403680\n",
      "Train Epoche: 3 [419/96 (436%)]\tLoss: 0.098349\n",
      "Train Epoche: 3 [420/96 (438%)]\tLoss: 3.710316\n",
      "Train Epoche: 3 [421/96 (439%)]\tLoss: 8.546557\n",
      "Train Epoche: 3 [422/96 (440%)]\tLoss: 5.878022\n",
      "Train Epoche: 3 [423/96 (441%)]\tLoss: 78.477173\n",
      "Train Epoche: 3 [424/96 (442%)]\tLoss: 6.853213\n",
      "Train Epoche: 3 [425/96 (443%)]\tLoss: 5.702792\n",
      "Train Epoche: 3 [426/96 (444%)]\tLoss: 0.612181\n",
      "Train Epoche: 3 [427/96 (445%)]\tLoss: 0.542242\n",
      "Train Epoche: 3 [428/96 (446%)]\tLoss: 10.589482\n",
      "Train Epoche: 3 [429/96 (447%)]\tLoss: 1.920240\n",
      "Train Epoche: 3 [430/96 (448%)]\tLoss: 7.450732\n",
      "Train Epoche: 3 [431/96 (449%)]\tLoss: 2.463734\n",
      "Train Epoche: 3 [432/96 (450%)]\tLoss: 0.514587\n",
      "Train Epoche: 3 [433/96 (451%)]\tLoss: 1.589565\n",
      "Train Epoche: 3 [434/96 (452%)]\tLoss: 1.269800\n",
      "Train Epoche: 3 [435/96 (453%)]\tLoss: 4.398781\n",
      "Train Epoche: 3 [436/96 (454%)]\tLoss: 14.947927\n",
      "Train Epoche: 3 [437/96 (455%)]\tLoss: 30.937544\n",
      "Train Epoche: 3 [438/96 (456%)]\tLoss: 0.197328\n",
      "Train Epoche: 3 [439/96 (457%)]\tLoss: 1.753509\n",
      "Train Epoche: 3 [440/96 (458%)]\tLoss: 26.128853\n",
      "Train Epoche: 3 [441/96 (459%)]\tLoss: 4.246966\n",
      "Train Epoche: 3 [442/96 (460%)]\tLoss: 1.129749\n",
      "Train Epoche: 3 [443/96 (461%)]\tLoss: 2.023149\n",
      "Train Epoche: 3 [444/96 (462%)]\tLoss: 20.263109\n",
      "Train Epoche: 3 [445/96 (464%)]\tLoss: 2.397925\n",
      "Train Epoche: 3 [446/96 (465%)]\tLoss: 82.584641\n",
      "Train Epoche: 3 [447/96 (466%)]\tLoss: 3.647189\n",
      "Train Epoche: 3 [448/96 (467%)]\tLoss: 8.931558\n",
      "Train Epoche: 3 [449/96 (468%)]\tLoss: 3.089496\n",
      "Train Epoche: 3 [450/96 (469%)]\tLoss: 20.980034\n",
      "Train Epoche: 3 [451/96 (470%)]\tLoss: 4.195669\n",
      "Train Epoche: 3 [452/96 (471%)]\tLoss: 24.137009\n",
      "Train Epoche: 3 [453/96 (472%)]\tLoss: 0.031600\n",
      "Train Epoche: 3 [454/96 (473%)]\tLoss: 18.608507\n",
      "Train Epoche: 3 [455/96 (474%)]\tLoss: 0.303721\n",
      "Train Epoche: 3 [456/96 (475%)]\tLoss: 13.645638\n",
      "Train Epoche: 3 [457/96 (476%)]\tLoss: 2.414474\n",
      "Train Epoche: 3 [458/96 (477%)]\tLoss: 5.416250\n",
      "Train Epoche: 3 [459/96 (478%)]\tLoss: 0.162273\n",
      "Train Epoche: 3 [460/96 (479%)]\tLoss: 74.895073\n",
      "Train Epoche: 3 [461/96 (480%)]\tLoss: 0.729533\n",
      "Train Epoche: 3 [462/96 (481%)]\tLoss: 1.742787\n",
      "Train Epoche: 3 [463/96 (482%)]\tLoss: 1.698556\n",
      "Train Epoche: 3 [464/96 (483%)]\tLoss: 94.800690\n",
      "Train Epoche: 3 [465/96 (484%)]\tLoss: 5.261664\n",
      "Train Epoche: 3 [466/96 (485%)]\tLoss: 0.000447\n",
      "Train Epoche: 3 [467/96 (486%)]\tLoss: 0.295775\n",
      "Train Epoche: 3 [468/96 (488%)]\tLoss: 0.008460\n",
      "Train Epoche: 3 [469/96 (489%)]\tLoss: 22.030863\n",
      "Train Epoche: 3 [470/96 (490%)]\tLoss: 15.782812\n",
      "Train Epoche: 3 [471/96 (491%)]\tLoss: 0.725851\n",
      "Train Epoche: 3 [472/96 (492%)]\tLoss: 0.247031\n",
      "Train Epoche: 3 [473/96 (493%)]\tLoss: 1.367700\n",
      "Train Epoche: 3 [474/96 (494%)]\tLoss: 0.477882\n",
      "Train Epoche: 3 [475/96 (495%)]\tLoss: 47.105179\n",
      "Train Epoche: 3 [476/96 (496%)]\tLoss: 0.511793\n",
      "Train Epoche: 3 [477/96 (497%)]\tLoss: 4.046184\n",
      "Train Epoche: 3 [478/96 (498%)]\tLoss: 8.262889\n",
      "Train Epoche: 3 [479/96 (499%)]\tLoss: 20.795004\n",
      "Train Epoche: 3 [480/96 (500%)]\tLoss: 14.268967\n",
      "Train Epoche: 3 [481/96 (501%)]\tLoss: 2.694672\n",
      "Train Epoche: 3 [482/96 (502%)]\tLoss: 18.735424\n",
      "Train Epoche: 3 [483/96 (503%)]\tLoss: 4.647123\n",
      "Train Epoche: 3 [484/96 (504%)]\tLoss: 5.109705\n",
      "Train Epoche: 3 [485/96 (505%)]\tLoss: 3.582018\n",
      "Train Epoche: 3 [486/96 (506%)]\tLoss: 1.119530\n",
      "Train Epoche: 3 [487/96 (507%)]\tLoss: 6.516911\n",
      "Train Epoche: 3 [488/96 (508%)]\tLoss: 7.454552\n",
      "Train Epoche: 3 [489/96 (509%)]\tLoss: 6.778338\n",
      "Train Epoche: 3 [490/96 (510%)]\tLoss: 9.161577\n",
      "Train Epoche: 3 [491/96 (511%)]\tLoss: 47.696842\n",
      "Train Epoche: 3 [492/96 (512%)]\tLoss: 0.286106\n",
      "Train Epoche: 3 [493/96 (514%)]\tLoss: 5.812778\n",
      "Train Epoche: 3 [494/96 (515%)]\tLoss: 0.000704\n",
      "Train Epoche: 3 [495/96 (516%)]\tLoss: 44.295471\n",
      "Train Epoche: 3 [496/96 (517%)]\tLoss: 9.426245\n",
      "Train Epoche: 3 [497/96 (518%)]\tLoss: 44.537704\n",
      "Train Epoche: 3 [498/96 (519%)]\tLoss: 29.235298\n",
      "Train Epoche: 3 [499/96 (520%)]\tLoss: 1.515112\n",
      "Train Epoche: 3 [500/96 (521%)]\tLoss: 12.749508\n",
      "Train Epoche: 3 [501/96 (522%)]\tLoss: 1.395786\n",
      "Train Epoche: 3 [502/96 (523%)]\tLoss: 18.263645\n",
      "Train Epoche: 3 [503/96 (524%)]\tLoss: 1.031382\n",
      "Train Epoche: 3 [504/96 (525%)]\tLoss: 1.338950\n",
      "Train Epoche: 3 [505/96 (526%)]\tLoss: 13.095004\n",
      "Train Epoche: 3 [506/96 (527%)]\tLoss: 2.465046\n",
      "Train Epoche: 3 [507/96 (528%)]\tLoss: 32.137459\n",
      "Train Epoche: 3 [508/96 (529%)]\tLoss: 4.629936\n",
      "Train Epoche: 3 [509/96 (530%)]\tLoss: 2.139679\n",
      "Train Epoche: 3 [510/96 (531%)]\tLoss: 19.640816\n",
      "Train Epoche: 3 [511/96 (532%)]\tLoss: 0.143285\n",
      "Train Epoche: 3 [512/96 (533%)]\tLoss: 3.295383\n",
      "Train Epoche: 3 [513/96 (534%)]\tLoss: 0.982173\n",
      "Train Epoche: 3 [514/96 (535%)]\tLoss: 8.821927\n",
      "Train Epoche: 3 [515/96 (536%)]\tLoss: 0.222173\n",
      "Train Epoche: 3 [516/96 (538%)]\tLoss: 2.344329\n",
      "Train Epoche: 3 [517/96 (539%)]\tLoss: 105.789223\n",
      "Train Epoche: 3 [518/96 (540%)]\tLoss: 3.984185\n",
      "Train Epoche: 3 [519/96 (541%)]\tLoss: 11.508409\n",
      "Train Epoche: 3 [520/96 (542%)]\tLoss: 6.859078\n",
      "Train Epoche: 3 [521/96 (543%)]\tLoss: 89.996231\n",
      "Train Epoche: 3 [522/96 (544%)]\tLoss: 4.111975\n",
      "Train Epoche: 3 [523/96 (545%)]\tLoss: 56.493004\n",
      "Train Epoche: 3 [524/96 (546%)]\tLoss: 7.025501\n",
      "Train Epoche: 3 [525/96 (547%)]\tLoss: 0.348308\n",
      "Train Epoche: 3 [526/96 (548%)]\tLoss: 0.477142\n",
      "Train Epoche: 3 [527/96 (549%)]\tLoss: 0.355074\n",
      "Train Epoche: 3 [528/96 (550%)]\tLoss: 0.117830\n",
      "Train Epoche: 3 [529/96 (551%)]\tLoss: 0.526946\n",
      "Train Epoche: 3 [530/96 (552%)]\tLoss: 41.362896\n",
      "Train Epoche: 3 [531/96 (553%)]\tLoss: 0.518742\n",
      "Train Epoche: 3 [532/96 (554%)]\tLoss: 0.021155\n",
      "Train Epoche: 3 [533/96 (555%)]\tLoss: 77.394508\n",
      "Train Epoche: 3 [534/96 (556%)]\tLoss: 1.193720\n",
      "Train Epoche: 3 [535/96 (557%)]\tLoss: 0.000529\n",
      "Train Epoche: 3 [536/96 (558%)]\tLoss: 6.133144\n",
      "Train Epoche: 3 [537/96 (559%)]\tLoss: 10.159019\n",
      "Train Epoche: 3 [538/96 (560%)]\tLoss: 0.013970\n",
      "Train Epoche: 3 [539/96 (561%)]\tLoss: 0.885980\n",
      "Train Epoche: 3 [540/96 (562%)]\tLoss: 0.021279\n",
      "Train Epoche: 3 [541/96 (564%)]\tLoss: 10.493681\n",
      "Train Epoche: 3 [542/96 (565%)]\tLoss: 2.398498\n",
      "Train Epoche: 3 [543/96 (566%)]\tLoss: 14.415000\n",
      "Train Epoche: 3 [544/96 (567%)]\tLoss: 2.523820\n",
      "Train Epoche: 3 [545/96 (568%)]\tLoss: 0.144208\n",
      "Train Epoche: 3 [546/96 (569%)]\tLoss: 14.958990\n",
      "Train Epoche: 3 [547/96 (570%)]\tLoss: 1.688012\n",
      "Train Epoche: 3 [548/96 (571%)]\tLoss: 49.962673\n",
      "Train Epoche: 3 [549/96 (572%)]\tLoss: 2.069750\n",
      "Train Epoche: 3 [550/96 (573%)]\tLoss: 2.919092\n",
      "Train Epoche: 3 [551/96 (574%)]\tLoss: 58.238457\n",
      "Train Epoche: 3 [552/96 (575%)]\tLoss: 0.451144\n",
      "Train Epoche: 3 [553/96 (576%)]\tLoss: 204.740036\n",
      "Train Epoche: 3 [554/96 (577%)]\tLoss: 7.479300\n",
      "Train Epoche: 3 [555/96 (578%)]\tLoss: 12.550370\n",
      "Train Epoche: 3 [556/96 (579%)]\tLoss: 0.132668\n",
      "Train Epoche: 3 [557/96 (580%)]\tLoss: 2.503980\n",
      "Train Epoche: 3 [558/96 (581%)]\tLoss: 2.765359\n",
      "Train Epoche: 3 [559/96 (582%)]\tLoss: 0.183331\n",
      "Train Epoche: 3 [560/96 (583%)]\tLoss: 19.724022\n",
      "Train Epoche: 3 [561/96 (584%)]\tLoss: 2.510717\n",
      "Train Epoche: 3 [562/96 (585%)]\tLoss: 6.755153\n",
      "Train Epoche: 3 [563/96 (586%)]\tLoss: 4.307136\n",
      "Train Epoche: 3 [564/96 (588%)]\tLoss: 20.842138\n",
      "Train Epoche: 3 [565/96 (589%)]\tLoss: 48.166492\n",
      "Train Epoche: 3 [566/96 (590%)]\tLoss: 4.966409\n",
      "Train Epoche: 3 [567/96 (591%)]\tLoss: 14.352637\n",
      "Train Epoche: 3 [568/96 (592%)]\tLoss: 16.329546\n",
      "Train Epoche: 3 [569/96 (593%)]\tLoss: 18.176296\n",
      "Train Epoche: 3 [570/96 (594%)]\tLoss: 0.918305\n",
      "Train Epoche: 3 [571/96 (595%)]\tLoss: 0.283051\n",
      "Train Epoche: 3 [572/96 (596%)]\tLoss: 0.002792\n",
      "Train Epoche: 3 [573/96 (597%)]\tLoss: 0.105610\n",
      "Train Epoche: 3 [574/96 (598%)]\tLoss: 0.230301\n",
      "Train Epoche: 3 [575/96 (599%)]\tLoss: 1.513731\n",
      "Train Epoche: 3 [576/96 (600%)]\tLoss: 37.875885\n",
      "Train Epoche: 3 [577/96 (601%)]\tLoss: 5.793231\n",
      "Train Epoche: 3 [578/96 (602%)]\tLoss: 12.492017\n",
      "Train Epoche: 3 [579/96 (603%)]\tLoss: 1.548063\n",
      "Train Epoche: 3 [580/96 (604%)]\tLoss: 1.121580\n",
      "Train Epoche: 3 [581/96 (605%)]\tLoss: 3.148369\n",
      "Train Epoche: 3 [582/96 (606%)]\tLoss: 1.740232\n",
      "Train Epoche: 3 [583/96 (607%)]\tLoss: 1.212757\n",
      "Train Epoche: 3 [584/96 (608%)]\tLoss: 16.430477\n",
      "Train Epoche: 3 [585/96 (609%)]\tLoss: 18.093414\n",
      "Train Epoche: 3 [586/96 (610%)]\tLoss: 12.072771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [587/96 (611%)]\tLoss: 8.323281\n",
      "Train Epoche: 3 [588/96 (612%)]\tLoss: 4.133860\n",
      "Train Epoche: 3 [589/96 (614%)]\tLoss: 23.219896\n",
      "Train Epoche: 3 [590/96 (615%)]\tLoss: 4.719218\n",
      "Train Epoche: 3 [591/96 (616%)]\tLoss: 3.879984\n",
      "Train Epoche: 3 [592/96 (617%)]\tLoss: 1.748704\n",
      "Train Epoche: 3 [593/96 (618%)]\tLoss: 4.457195\n",
      "Train Epoche: 3 [594/96 (619%)]\tLoss: 116.729050\n",
      "Train Epoche: 3 [595/96 (620%)]\tLoss: 0.864173\n",
      "Train Epoche: 3 [596/96 (621%)]\tLoss: 0.402624\n",
      "Train Epoche: 3 [597/96 (622%)]\tLoss: 2.219564\n",
      "Train Epoche: 3 [598/96 (623%)]\tLoss: 3.916294\n",
      "Train Epoche: 3 [599/96 (624%)]\tLoss: 3.624400\n",
      "Train Epoche: 3 [600/96 (625%)]\tLoss: 8.263031\n",
      "Train Epoche: 3 [601/96 (626%)]\tLoss: 1.478391\n",
      "Train Epoche: 3 [602/96 (627%)]\tLoss: 0.741303\n",
      "Train Epoche: 3 [603/96 (628%)]\tLoss: 0.904552\n",
      "Train Epoche: 3 [604/96 (629%)]\tLoss: 1.048412\n",
      "Train Epoche: 3 [605/96 (630%)]\tLoss: 12.832587\n",
      "Train Epoche: 3 [606/96 (631%)]\tLoss: 0.413864\n",
      "Train Epoche: 3 [607/96 (632%)]\tLoss: 305.311859\n",
      "Train Epoche: 3 [608/96 (633%)]\tLoss: 12.481975\n",
      "Train Epoche: 3 [609/96 (634%)]\tLoss: 11.237424\n",
      "Train Epoche: 3 [610/96 (635%)]\tLoss: 66.270058\n",
      "Train Epoche: 3 [611/96 (636%)]\tLoss: 1.566949\n",
      "Train Epoche: 3 [612/96 (638%)]\tLoss: 25.428110\n",
      "Train Epoche: 3 [613/96 (639%)]\tLoss: 86.707809\n",
      "Train Epoche: 3 [614/96 (640%)]\tLoss: 0.520024\n",
      "Train Epoche: 3 [615/96 (641%)]\tLoss: 2.846827\n",
      "Train Epoche: 3 [616/96 (642%)]\tLoss: 2.449326\n",
      "Train Epoche: 3 [617/96 (643%)]\tLoss: 226.187088\n",
      "Train Epoche: 3 [618/96 (644%)]\tLoss: 7.944311\n",
      "Train Epoche: 3 [619/96 (645%)]\tLoss: 55.840591\n",
      "Train Epoche: 3 [620/96 (646%)]\tLoss: 0.294706\n",
      "Train Epoche: 3 [621/96 (647%)]\tLoss: 0.020661\n",
      "Train Epoche: 3 [622/96 (648%)]\tLoss: 25.260878\n",
      "Train Epoche: 3 [623/96 (649%)]\tLoss: 4.093643\n",
      "Train Epoche: 3 [624/96 (650%)]\tLoss: 60.484970\n",
      "Train Epoche: 3 [625/96 (651%)]\tLoss: 371.198029\n",
      "Train Epoche: 3 [626/96 (652%)]\tLoss: 12.985610\n",
      "Train Epoche: 3 [627/96 (653%)]\tLoss: 11.948612\n",
      "Train Epoche: 3 [628/96 (654%)]\tLoss: 28.871222\n",
      "Train Epoche: 3 [629/96 (655%)]\tLoss: 96.095398\n",
      "Train Epoche: 3 [630/96 (656%)]\tLoss: 22.377117\n",
      "Train Epoche: 3 [631/96 (657%)]\tLoss: 2.776882\n",
      "Train Epoche: 3 [632/96 (658%)]\tLoss: 0.847311\n",
      "Train Epoche: 3 [633/96 (659%)]\tLoss: 16.658110\n",
      "Train Epoche: 3 [634/96 (660%)]\tLoss: 1.085863\n",
      "Train Epoche: 3 [635/96 (661%)]\tLoss: 9.808146\n",
      "Train Epoche: 3 [636/96 (662%)]\tLoss: 11.559544\n",
      "Train Epoche: 3 [637/96 (664%)]\tLoss: 0.711932\n",
      "Train Epoche: 3 [638/96 (665%)]\tLoss: 5.682580\n",
      "Train Epoche: 3 [639/96 (666%)]\tLoss: 8.137950\n",
      "Train Epoche: 3 [640/96 (667%)]\tLoss: 1.005944\n",
      "Train Epoche: 3 [641/96 (668%)]\tLoss: 4.554472\n",
      "Train Epoche: 3 [642/96 (669%)]\tLoss: 4.419323\n",
      "Train Epoche: 3 [643/96 (670%)]\tLoss: 29.132523\n",
      "Train Epoche: 3 [644/96 (671%)]\tLoss: 1.518967\n",
      "Train Epoche: 3 [645/96 (672%)]\tLoss: 17.554714\n",
      "Train Epoche: 3 [646/96 (673%)]\tLoss: 6.968570\n",
      "Train Epoche: 3 [647/96 (674%)]\tLoss: 2.170259\n",
      "Train Epoche: 3 [648/96 (675%)]\tLoss: 8.614726\n",
      "Train Epoche: 3 [649/96 (676%)]\tLoss: 43.093884\n",
      "Train Epoche: 3 [650/96 (677%)]\tLoss: 1.329310\n",
      "Train Epoche: 3 [651/96 (678%)]\tLoss: 11.538309\n",
      "Train Epoche: 3 [652/96 (679%)]\tLoss: 11.217774\n",
      "Train Epoche: 3 [653/96 (680%)]\tLoss: 1.896688\n",
      "Train Epoche: 3 [654/96 (681%)]\tLoss: 10.128239\n",
      "Train Epoche: 3 [655/96 (682%)]\tLoss: 6.995262\n",
      "Train Epoche: 3 [656/96 (683%)]\tLoss: 3.041930\n",
      "Train Epoche: 3 [657/96 (684%)]\tLoss: 224.972427\n",
      "Train Epoche: 3 [658/96 (685%)]\tLoss: 17.996037\n",
      "Train Epoche: 3 [659/96 (686%)]\tLoss: 0.026722\n",
      "Train Epoche: 3 [660/96 (688%)]\tLoss: 14.372363\n",
      "Train Epoche: 3 [661/96 (689%)]\tLoss: 2.633469\n",
      "Train Epoche: 3 [662/96 (690%)]\tLoss: 13.124804\n",
      "Train Epoche: 3 [663/96 (691%)]\tLoss: 9.575420\n",
      "Train Epoche: 3 [664/96 (692%)]\tLoss: 0.309664\n",
      "Train Epoche: 3 [665/96 (693%)]\tLoss: 29.492878\n",
      "Train Epoche: 3 [666/96 (694%)]\tLoss: 7.849035\n",
      "Train Epoche: 3 [667/96 (695%)]\tLoss: 0.003279\n",
      "Train Epoche: 3 [668/96 (696%)]\tLoss: 5.420970\n",
      "Train Epoche: 3 [669/96 (697%)]\tLoss: 66.269928\n",
      "Train Epoche: 3 [670/96 (698%)]\tLoss: 1.778685\n",
      "Train Epoche: 3 [671/96 (699%)]\tLoss: 16.884722\n",
      "Train Epoche: 3 [672/96 (700%)]\tLoss: 0.019427\n",
      "Train Epoche: 3 [673/96 (701%)]\tLoss: 8.893805\n",
      "Train Epoche: 3 [674/96 (702%)]\tLoss: 3.234908\n",
      "Train Epoche: 3 [675/96 (703%)]\tLoss: 2.014112\n",
      "Train Epoche: 3 [676/96 (704%)]\tLoss: 11.732920\n",
      "Train Epoche: 3 [677/96 (705%)]\tLoss: 1.814174\n",
      "Train Epoche: 3 [678/96 (706%)]\tLoss: 4.642990\n",
      "Train Epoche: 3 [679/96 (707%)]\tLoss: 0.095616\n",
      "Train Epoche: 3 [680/96 (708%)]\tLoss: 8.447576\n",
      "Train Epoche: 3 [681/96 (709%)]\tLoss: 12.986143\n",
      "Train Epoche: 3 [682/96 (710%)]\tLoss: 0.005568\n",
      "Train Epoche: 3 [683/96 (711%)]\tLoss: 2.455036\n",
      "Train Epoche: 3 [684/96 (712%)]\tLoss: 13.978672\n",
      "Train Epoche: 3 [685/96 (714%)]\tLoss: 4.867662\n",
      "Train Epoche: 3 [686/96 (715%)]\tLoss: 25.461727\n",
      "Train Epoche: 3 [687/96 (716%)]\tLoss: 0.028983\n",
      "Train Epoche: 3 [688/96 (717%)]\tLoss: 1.006940\n",
      "Train Epoche: 3 [689/96 (718%)]\tLoss: 2.233713\n",
      "Train Epoche: 3 [690/96 (719%)]\tLoss: 0.105355\n",
      "Train Epoche: 3 [691/96 (720%)]\tLoss: 6.699232\n",
      "Train Epoche: 3 [692/96 (721%)]\tLoss: 5.618332\n",
      "Train Epoche: 3 [693/96 (722%)]\tLoss: 21.059111\n",
      "Train Epoche: 3 [694/96 (723%)]\tLoss: 0.395554\n",
      "Train Epoche: 3 [695/96 (724%)]\tLoss: 1.861741\n",
      "Train Epoche: 3 [696/96 (725%)]\tLoss: 11.250999\n",
      "Train Epoche: 3 [697/96 (726%)]\tLoss: 10.075413\n",
      "Train Epoche: 3 [698/96 (727%)]\tLoss: 1.322643\n",
      "Train Epoche: 3 [699/96 (728%)]\tLoss: 2.574185\n",
      "Train Epoche: 3 [700/96 (729%)]\tLoss: 7.557843\n",
      "Train Epoche: 3 [701/96 (730%)]\tLoss: 0.460697\n",
      "Train Epoche: 3 [702/96 (731%)]\tLoss: 2.926387\n",
      "Train Epoche: 3 [703/96 (732%)]\tLoss: 14.807364\n",
      "Train Epoche: 3 [704/96 (733%)]\tLoss: 0.070733\n",
      "Train Epoche: 3 [705/96 (734%)]\tLoss: 0.719563\n",
      "Train Epoche: 3 [706/96 (735%)]\tLoss: 8.775924\n",
      "Train Epoche: 3 [707/96 (736%)]\tLoss: 9.892989\n",
      "Train Epoche: 3 [708/96 (738%)]\tLoss: 0.108809\n",
      "Train Epoche: 3 [709/96 (739%)]\tLoss: 0.046643\n",
      "Train Epoche: 3 [710/96 (740%)]\tLoss: 0.000174\n",
      "Train Epoche: 3 [711/96 (741%)]\tLoss: 0.021351\n",
      "Train Epoche: 3 [712/96 (742%)]\tLoss: 12.351399\n",
      "Train Epoche: 3 [713/96 (743%)]\tLoss: 2.833403\n",
      "Train Epoche: 3 [714/96 (744%)]\tLoss: 3.311479\n",
      "Train Epoche: 3 [715/96 (745%)]\tLoss: 12.026167\n",
      "Train Epoche: 3 [716/96 (746%)]\tLoss: 0.141961\n",
      "Train Epoche: 3 [717/96 (747%)]\tLoss: 2.941193\n",
      "Train Epoche: 3 [718/96 (748%)]\tLoss: 1.359910\n",
      "Train Epoche: 3 [719/96 (749%)]\tLoss: 65.660812\n",
      "Train Epoche: 3 [720/96 (750%)]\tLoss: 0.033256\n",
      "Train Epoche: 3 [721/96 (751%)]\tLoss: 0.254541\n",
      "Train Epoche: 3 [722/96 (752%)]\tLoss: 1.429925\n",
      "Train Epoche: 3 [723/96 (753%)]\tLoss: 9.092643\n",
      "Train Epoche: 3 [724/96 (754%)]\tLoss: 0.441303\n",
      "Train Epoche: 3 [725/96 (755%)]\tLoss: 1.080773\n",
      "Train Epoche: 3 [726/96 (756%)]\tLoss: 0.516621\n",
      "Train Epoche: 3 [727/96 (757%)]\tLoss: 0.176520\n",
      "Train Epoche: 3 [728/96 (758%)]\tLoss: 27.246222\n",
      "Train Epoche: 3 [729/96 (759%)]\tLoss: 2.842412\n",
      "Train Epoche: 3 [730/96 (760%)]\tLoss: 9.362196\n",
      "Train Epoche: 3 [731/96 (761%)]\tLoss: 212.574570\n",
      "Train Epoche: 3 [732/96 (762%)]\tLoss: 62.827473\n",
      "Train Epoche: 3 [733/96 (764%)]\tLoss: 1.073636\n",
      "Train Epoche: 3 [734/96 (765%)]\tLoss: 0.002645\n",
      "Train Epoche: 3 [735/96 (766%)]\tLoss: 0.019674\n",
      "Train Epoche: 3 [736/96 (767%)]\tLoss: 5.135883\n",
      "Train Epoche: 3 [737/96 (768%)]\tLoss: 5.184560\n",
      "Train Epoche: 3 [738/96 (769%)]\tLoss: 0.386622\n",
      "Train Epoche: 3 [739/96 (770%)]\tLoss: 0.723377\n",
      "Train Epoche: 3 [740/96 (771%)]\tLoss: 2.227169\n",
      "Train Epoche: 3 [741/96 (772%)]\tLoss: 20.279785\n",
      "Train Epoche: 3 [742/96 (773%)]\tLoss: 0.162731\n",
      "Train Epoche: 3 [743/96 (774%)]\tLoss: 0.202759\n",
      "Train Epoche: 3 [744/96 (775%)]\tLoss: 0.000494\n",
      "Train Epoche: 3 [745/96 (776%)]\tLoss: 0.841008\n",
      "Train Epoche: 3 [746/96 (777%)]\tLoss: 0.382299\n",
      "Train Epoche: 3 [747/96 (778%)]\tLoss: 41.680668\n",
      "Train Epoche: 3 [748/96 (779%)]\tLoss: 0.903420\n",
      "Train Epoche: 3 [749/96 (780%)]\tLoss: 169.212570\n",
      "Train Epoche: 3 [750/96 (781%)]\tLoss: 6.444320\n",
      "Train Epoche: 3 [751/96 (782%)]\tLoss: 1.259123\n",
      "Train Epoche: 3 [752/96 (783%)]\tLoss: 2.138409\n",
      "Train Epoche: 3 [753/96 (784%)]\tLoss: 2.186066\n",
      "Train Epoche: 3 [754/96 (785%)]\tLoss: 49.584549\n",
      "Train Epoche: 3 [755/96 (786%)]\tLoss: 7.086765\n",
      "Train Epoche: 3 [756/96 (788%)]\tLoss: 15.090352\n",
      "Train Epoche: 3 [757/96 (789%)]\tLoss: 13.966195\n",
      "Train Epoche: 3 [758/96 (790%)]\tLoss: 28.772766\n",
      "Train Epoche: 3 [759/96 (791%)]\tLoss: 0.040716\n",
      "Train Epoche: 3 [760/96 (792%)]\tLoss: 11.294431\n",
      "Train Epoche: 3 [761/96 (793%)]\tLoss: 15.614947\n",
      "Train Epoche: 3 [762/96 (794%)]\tLoss: 5.362480\n",
      "Train Epoche: 3 [763/96 (795%)]\tLoss: 0.027680\n",
      "Train Epoche: 3 [764/96 (796%)]\tLoss: 50.560654\n",
      "Train Epoche: 3 [765/96 (797%)]\tLoss: 3.444959\n",
      "Train Epoche: 3 [766/96 (798%)]\tLoss: 71.167915\n",
      "Train Epoche: 3 [767/96 (799%)]\tLoss: 39.900856\n",
      "Train Epoche: 3 [768/96 (800%)]\tLoss: 7.585029\n",
      "Train Epoche: 3 [769/96 (801%)]\tLoss: 103.399475\n",
      "Train Epoche: 3 [770/96 (802%)]\tLoss: 0.212596\n",
      "Train Epoche: 3 [771/96 (803%)]\tLoss: 12.206326\n",
      "Train Epoche: 3 [772/96 (804%)]\tLoss: 57.032257\n",
      "Train Epoche: 3 [773/96 (805%)]\tLoss: 7.810085\n",
      "Train Epoche: 3 [774/96 (806%)]\tLoss: 15.532083\n",
      "Train Epoche: 3 [775/96 (807%)]\tLoss: 19.707977\n",
      "Train Epoche: 3 [776/96 (808%)]\tLoss: 11.550019\n",
      "Train Epoche: 3 [777/96 (809%)]\tLoss: 2.814287\n",
      "Train Epoche: 3 [778/96 (810%)]\tLoss: 22.838907\n",
      "Train Epoche: 3 [779/96 (811%)]\tLoss: 5.791745\n",
      "Train Epoche: 3 [780/96 (812%)]\tLoss: 6.511234\n",
      "Train Epoche: 3 [781/96 (814%)]\tLoss: 0.712697\n",
      "Train Epoche: 3 [782/96 (815%)]\tLoss: 102.122406\n",
      "Train Epoche: 3 [783/96 (816%)]\tLoss: 0.008288\n",
      "Train Epoche: 3 [784/96 (817%)]\tLoss: 0.159680\n",
      "Train Epoche: 3 [785/96 (818%)]\tLoss: 0.343093\n",
      "Train Epoche: 3 [786/96 (819%)]\tLoss: 6.854159\n",
      "Train Epoche: 3 [787/96 (820%)]\tLoss: 3.930218\n",
      "Train Epoche: 3 [788/96 (821%)]\tLoss: 0.307827\n",
      "Train Epoche: 3 [789/96 (822%)]\tLoss: 0.006800\n",
      "Train Epoche: 3 [790/96 (823%)]\tLoss: 1.353834\n",
      "Train Epoche: 3 [791/96 (824%)]\tLoss: 6.175333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [792/96 (825%)]\tLoss: 16.347816\n",
      "Train Epoche: 3 [793/96 (826%)]\tLoss: 0.752371\n",
      "Train Epoche: 3 [794/96 (827%)]\tLoss: 1.002412\n",
      "Train Epoche: 3 [795/96 (828%)]\tLoss: 87.050446\n",
      "Train Epoche: 3 [796/96 (829%)]\tLoss: 71.482399\n",
      "Train Epoche: 3 [797/96 (830%)]\tLoss: 0.087596\n",
      "Train Epoche: 3 [798/96 (831%)]\tLoss: 91.185837\n",
      "Train Epoche: 3 [799/96 (832%)]\tLoss: 2.274658\n",
      "Train Epoche: 3 [800/96 (833%)]\tLoss: 108.032959\n",
      "Train Epoche: 3 [801/96 (834%)]\tLoss: 86.508682\n",
      "Train Epoche: 3 [802/96 (835%)]\tLoss: 0.986820\n",
      "Train Epoche: 3 [803/96 (836%)]\tLoss: 0.056493\n",
      "Train Epoche: 3 [804/96 (838%)]\tLoss: 2.467112\n",
      "Train Epoche: 3 [805/96 (839%)]\tLoss: 42.990788\n",
      "Train Epoche: 3 [806/96 (840%)]\tLoss: 21.859528\n",
      "Train Epoche: 3 [807/96 (841%)]\tLoss: 6.990532\n",
      "Train Epoche: 3 [808/96 (842%)]\tLoss: 44.818386\n",
      "Train Epoche: 3 [809/96 (843%)]\tLoss: 376.984192\n",
      "Train Epoche: 3 [810/96 (844%)]\tLoss: 165.983566\n",
      "Train Epoche: 3 [811/96 (845%)]\tLoss: 13.925659\n",
      "Train Epoche: 3 [812/96 (846%)]\tLoss: 15.759274\n",
      "Train Epoche: 3 [813/96 (847%)]\tLoss: 3.472898\n",
      "Train Epoche: 3 [814/96 (848%)]\tLoss: 118.205048\n",
      "Train Epoche: 3 [815/96 (849%)]\tLoss: 4.388133\n",
      "Train Epoche: 3 [816/96 (850%)]\tLoss: 106.436584\n",
      "Train Epoche: 3 [817/96 (851%)]\tLoss: 0.507476\n",
      "Train Epoche: 3 [818/96 (852%)]\tLoss: 0.293782\n",
      "Train Epoche: 3 [819/96 (853%)]\tLoss: 9.969753\n",
      "Train Epoche: 3 [820/96 (854%)]\tLoss: 0.223788\n",
      "Train Epoche: 3 [821/96 (855%)]\tLoss: 0.529607\n",
      "Train Epoche: 3 [822/96 (856%)]\tLoss: 1.799984\n",
      "Train Epoche: 3 [823/96 (857%)]\tLoss: 0.705282\n",
      "Train Epoche: 3 [824/96 (858%)]\tLoss: 2.928555\n",
      "Train Epoche: 3 [825/96 (859%)]\tLoss: 0.215969\n",
      "Train Epoche: 3 [826/96 (860%)]\tLoss: 83.617447\n",
      "Train Epoche: 3 [827/96 (861%)]\tLoss: 1.618834\n",
      "Train Epoche: 3 [828/96 (862%)]\tLoss: 21.337254\n",
      "Train Epoche: 3 [829/96 (864%)]\tLoss: 115.581223\n",
      "Train Epoche: 3 [830/96 (865%)]\tLoss: 22.224005\n",
      "Train Epoche: 3 [831/96 (866%)]\tLoss: 8.677273\n",
      "Train Epoche: 3 [832/96 (867%)]\tLoss: 0.109849\n",
      "Train Epoche: 3 [833/96 (868%)]\tLoss: 58.575195\n",
      "Train Epoche: 3 [834/96 (869%)]\tLoss: 19.274120\n",
      "Train Epoche: 3 [835/96 (870%)]\tLoss: 7.738121\n",
      "Train Epoche: 3 [836/96 (871%)]\tLoss: 0.932575\n",
      "Train Epoche: 3 [837/96 (872%)]\tLoss: 86.136147\n",
      "Train Epoche: 3 [838/96 (873%)]\tLoss: 3.653311\n",
      "Train Epoche: 3 [839/96 (874%)]\tLoss: 0.372588\n",
      "Train Epoche: 3 [840/96 (875%)]\tLoss: 0.034264\n",
      "Train Epoche: 3 [841/96 (876%)]\tLoss: 1.235093\n",
      "Train Epoche: 3 [842/96 (877%)]\tLoss: 47.789780\n",
      "Train Epoche: 3 [843/96 (878%)]\tLoss: 43.370480\n",
      "Train Epoche: 3 [844/96 (879%)]\tLoss: 0.602725\n",
      "Train Epoche: 3 [845/96 (880%)]\tLoss: 1.653848\n",
      "Train Epoche: 3 [846/96 (881%)]\tLoss: 1.447499\n",
      "Train Epoche: 3 [847/96 (882%)]\tLoss: 0.326277\n",
      "Train Epoche: 3 [848/96 (883%)]\tLoss: 51.238983\n",
      "Train Epoche: 3 [849/96 (884%)]\tLoss: 0.622067\n",
      "Train Epoche: 3 [850/96 (885%)]\tLoss: 37.779186\n",
      "Train Epoche: 3 [851/96 (886%)]\tLoss: 138.401794\n",
      "Train Epoche: 3 [852/96 (888%)]\tLoss: 45.609543\n",
      "Train Epoche: 3 [853/96 (889%)]\tLoss: 4.045387\n",
      "Train Epoche: 3 [854/96 (890%)]\tLoss: 47.393471\n",
      "Train Epoche: 3 [855/96 (891%)]\tLoss: 2.343071\n",
      "Train Epoche: 3 [856/96 (892%)]\tLoss: 49.958805\n",
      "Train Epoche: 3 [857/96 (893%)]\tLoss: 19.771454\n",
      "Train Epoche: 3 [858/96 (894%)]\tLoss: 90.394600\n",
      "Train Epoche: 3 [859/96 (895%)]\tLoss: 0.741132\n",
      "Train Epoche: 3 [860/96 (896%)]\tLoss: 0.201767\n",
      "Train Epoche: 3 [861/96 (897%)]\tLoss: 1.353541\n",
      "Train Epoche: 3 [862/96 (898%)]\tLoss: 0.045540\n",
      "Train Epoche: 3 [863/96 (899%)]\tLoss: 17.665972\n",
      "Train Epoche: 3 [864/96 (900%)]\tLoss: 19.493374\n",
      "Train Epoche: 3 [865/96 (901%)]\tLoss: 106.365776\n",
      "Train Epoche: 3 [866/96 (902%)]\tLoss: 35.691833\n",
      "Train Epoche: 3 [867/96 (903%)]\tLoss: 0.494688\n",
      "Train Epoche: 3 [868/96 (904%)]\tLoss: 1.912724\n",
      "Train Epoche: 3 [869/96 (905%)]\tLoss: 6.095381\n",
      "Train Epoche: 3 [870/96 (906%)]\tLoss: 2.504112\n",
      "Train Epoche: 3 [871/96 (907%)]\tLoss: 1.718364\n",
      "Train Epoche: 3 [872/96 (908%)]\tLoss: 2.355883\n",
      "Train Epoche: 3 [873/96 (909%)]\tLoss: 18.551605\n",
      "Train Epoche: 3 [874/96 (910%)]\tLoss: 0.566269\n",
      "Train Epoche: 3 [875/96 (911%)]\tLoss: 44.259792\n",
      "Train Epoche: 3 [876/96 (912%)]\tLoss: 0.384806\n",
      "Train Epoche: 3 [877/96 (914%)]\tLoss: 1.617632\n",
      "Train Epoche: 3 [878/96 (915%)]\tLoss: 0.003355\n",
      "Train Epoche: 3 [879/96 (916%)]\tLoss: 0.693173\n",
      "Train Epoche: 3 [880/96 (917%)]\tLoss: 11.742518\n",
      "Train Epoche: 3 [881/96 (918%)]\tLoss: 0.289867\n",
      "Train Epoche: 3 [882/96 (919%)]\tLoss: 6.513337\n",
      "Train Epoche: 3 [883/96 (920%)]\tLoss: 51.445694\n",
      "Train Epoche: 3 [884/96 (921%)]\tLoss: 4.393627\n",
      "Train Epoche: 3 [885/96 (922%)]\tLoss: 0.135308\n",
      "Train Epoche: 3 [886/96 (923%)]\tLoss: 13.376192\n",
      "Train Epoche: 3 [887/96 (924%)]\tLoss: 13.098275\n",
      "Train Epoche: 3 [888/96 (925%)]\tLoss: 5.996919\n",
      "Train Epoche: 3 [889/96 (926%)]\tLoss: 0.145161\n",
      "Train Epoche: 3 [890/96 (927%)]\tLoss: 0.163927\n",
      "Train Epoche: 3 [891/96 (928%)]\tLoss: 10.707905\n",
      "Train Epoche: 3 [892/96 (929%)]\tLoss: 0.353038\n",
      "Train Epoche: 3 [893/96 (930%)]\tLoss: 0.001805\n",
      "Train Epoche: 3 [894/96 (931%)]\tLoss: 14.551908\n",
      "Train Epoche: 3 [895/96 (932%)]\tLoss: 0.083116\n",
      "Train Epoche: 3 [896/96 (933%)]\tLoss: 5.191791\n",
      "Train Epoche: 3 [897/96 (934%)]\tLoss: 1.165522\n",
      "Train Epoche: 3 [898/96 (935%)]\tLoss: 13.457004\n",
      "Train Epoche: 3 [899/96 (936%)]\tLoss: 0.413974\n",
      "Train Epoche: 3 [900/96 (938%)]\tLoss: 101.954224\n",
      "Train Epoche: 3 [901/96 (939%)]\tLoss: 1.935607\n",
      "Train Epoche: 3 [902/96 (940%)]\tLoss: 1.896488\n",
      "Train Epoche: 3 [903/96 (941%)]\tLoss: 18.831951\n",
      "Train Epoche: 3 [904/96 (942%)]\tLoss: 32.279728\n",
      "Train Epoche: 3 [905/96 (943%)]\tLoss: 68.160744\n",
      "Train Epoche: 3 [906/96 (944%)]\tLoss: 0.091701\n",
      "Train Epoche: 3 [907/96 (945%)]\tLoss: 2.846800\n",
      "Train Epoche: 3 [908/96 (946%)]\tLoss: 2.429538\n",
      "Train Epoche: 3 [909/96 (947%)]\tLoss: 3.180514\n",
      "Train Epoche: 3 [910/96 (948%)]\tLoss: 5.229902\n",
      "Train Epoche: 3 [911/96 (949%)]\tLoss: 22.767254\n",
      "Train Epoche: 3 [912/96 (950%)]\tLoss: 1.148947\n",
      "Train Epoche: 3 [913/96 (951%)]\tLoss: 2.494339\n",
      "Train Epoche: 3 [914/96 (952%)]\tLoss: 77.355949\n",
      "Train Epoche: 3 [915/96 (953%)]\tLoss: 5.168507\n",
      "Train Epoche: 3 [916/96 (954%)]\tLoss: 0.366683\n",
      "Train Epoche: 3 [917/96 (955%)]\tLoss: 2.115044\n",
      "Train Epoche: 3 [918/96 (956%)]\tLoss: 10.876863\n",
      "Train Epoche: 3 [919/96 (957%)]\tLoss: 81.320465\n",
      "Train Epoche: 3 [920/96 (958%)]\tLoss: 16.725653\n",
      "Train Epoche: 3 [921/96 (959%)]\tLoss: 0.822342\n",
      "Train Epoche: 3 [922/96 (960%)]\tLoss: 63.138615\n",
      "Train Epoche: 3 [923/96 (961%)]\tLoss: 0.001437\n",
      "Train Epoche: 3 [924/96 (962%)]\tLoss: 0.548732\n",
      "Train Epoche: 3 [925/96 (964%)]\tLoss: 7.507269\n",
      "Train Epoche: 3 [926/96 (965%)]\tLoss: 0.104824\n",
      "Train Epoche: 3 [927/96 (966%)]\tLoss: 0.002819\n",
      "Train Epoche: 3 [928/96 (967%)]\tLoss: 0.682907\n",
      "Train Epoche: 3 [929/96 (968%)]\tLoss: 0.127702\n",
      "Train Epoche: 3 [930/96 (969%)]\tLoss: 20.420616\n",
      "Train Epoche: 3 [931/96 (970%)]\tLoss: 2.007565\n",
      "Train Epoche: 3 [932/96 (971%)]\tLoss: 36.605247\n",
      "Train Epoche: 3 [933/96 (972%)]\tLoss: 301.451019\n",
      "Train Epoche: 3 [934/96 (973%)]\tLoss: 0.325827\n",
      "Train Epoche: 3 [935/96 (974%)]\tLoss: 8.500867\n",
      "Train Epoche: 3 [936/96 (975%)]\tLoss: 0.016285\n",
      "Train Epoche: 3 [937/96 (976%)]\tLoss: 9.745513\n",
      "Train Epoche: 3 [938/96 (977%)]\tLoss: 1.901740\n",
      "Train Epoche: 3 [939/96 (978%)]\tLoss: 2.915268\n",
      "Train Epoche: 3 [940/96 (979%)]\tLoss: 0.210609\n",
      "Train Epoche: 3 [941/96 (980%)]\tLoss: 47.601479\n",
      "Train Epoche: 3 [942/96 (981%)]\tLoss: 2.446281\n",
      "Train Epoche: 3 [943/96 (982%)]\tLoss: 3.491498\n",
      "Train Epoche: 3 [944/96 (983%)]\tLoss: 3.483612\n",
      "Train Epoche: 3 [945/96 (984%)]\tLoss: 15.582829\n",
      "Train Epoche: 3 [946/96 (985%)]\tLoss: 8.228630\n",
      "Train Epoche: 3 [947/96 (986%)]\tLoss: 7.075789\n",
      "Train Epoche: 3 [948/96 (988%)]\tLoss: 0.295889\n",
      "Train Epoche: 3 [949/96 (989%)]\tLoss: 0.581362\n",
      "Train Epoche: 3 [950/96 (990%)]\tLoss: 0.374606\n",
      "Train Epoche: 3 [951/96 (991%)]\tLoss: 0.005161\n",
      "Train Epoche: 3 [952/96 (992%)]\tLoss: 14.335336\n",
      "Train Epoche: 3 [953/96 (993%)]\tLoss: 44.458271\n",
      "Train Epoche: 3 [954/96 (994%)]\tLoss: 153.562943\n",
      "Train Epoche: 3 [955/96 (995%)]\tLoss: 1.227228\n",
      "Train Epoche: 3 [956/96 (996%)]\tLoss: 50.703266\n",
      "Train Epoche: 3 [957/96 (997%)]\tLoss: 4.143026\n",
      "Train Epoche: 3 [958/96 (998%)]\tLoss: 7.595933\n",
      "Train Epoche: 3 [959/96 (999%)]\tLoss: 105.415268\n",
      "Train Epoche: 3 [960/96 (1000%)]\tLoss: 16.242674\n",
      "Train Epoche: 3 [961/96 (1001%)]\tLoss: 17.376940\n",
      "Train Epoche: 3 [962/96 (1002%)]\tLoss: 26.347124\n",
      "Train Epoche: 3 [963/96 (1003%)]\tLoss: 29.553951\n",
      "Train Epoche: 3 [964/96 (1004%)]\tLoss: 112.096848\n",
      "Train Epoche: 3 [965/96 (1005%)]\tLoss: 45.035889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [966/96 (1006%)]\tLoss: 115.593262\n",
      "Train Epoche: 3 [967/96 (1007%)]\tLoss: 10.375177\n",
      "Train Epoche: 3 [968/96 (1008%)]\tLoss: 26.511417\n",
      "Train Epoche: 3 [969/96 (1009%)]\tLoss: 74.908920\n",
      "Train Epoche: 3 [970/96 (1010%)]\tLoss: 2.719925\n",
      "Train Epoche: 3 [971/96 (1011%)]\tLoss: 0.175408\n",
      "Train Epoche: 3 [972/96 (1012%)]\tLoss: 0.003580\n",
      "Train Epoche: 3 [973/96 (1014%)]\tLoss: 24.280592\n",
      "Train Epoche: 3 [974/96 (1015%)]\tLoss: 3.019841\n",
      "Train Epoche: 3 [975/96 (1016%)]\tLoss: 3.567739\n",
      "Train Epoche: 3 [976/96 (1017%)]\tLoss: 27.165916\n",
      "Train Epoche: 3 [977/96 (1018%)]\tLoss: 51.274918\n",
      "Train Epoche: 3 [978/96 (1019%)]\tLoss: 0.322287\n",
      "Train Epoche: 3 [979/96 (1020%)]\tLoss: 4.332535\n",
      "Train Epoche: 3 [980/96 (1021%)]\tLoss: 0.164961\n",
      "Train Epoche: 3 [981/96 (1022%)]\tLoss: 174.021683\n",
      "Train Epoche: 3 [982/96 (1023%)]\tLoss: 1.736909\n",
      "Train Epoche: 3 [983/96 (1024%)]\tLoss: 0.067911\n",
      "Train Epoche: 3 [984/96 (1025%)]\tLoss: 11.165267\n",
      "Train Epoche: 3 [985/96 (1026%)]\tLoss: 30.073944\n",
      "Train Epoche: 3 [986/96 (1027%)]\tLoss: 7.225581\n",
      "Train Epoche: 3 [987/96 (1028%)]\tLoss: 8.606829\n",
      "Train Epoche: 3 [988/96 (1029%)]\tLoss: 79.442627\n",
      "Train Epoche: 3 [989/96 (1030%)]\tLoss: 10.575683\n",
      "Train Epoche: 3 [990/96 (1031%)]\tLoss: 2.008601\n",
      "Train Epoche: 3 [991/96 (1032%)]\tLoss: 0.036907\n",
      "Train Epoche: 3 [992/96 (1033%)]\tLoss: 0.185382\n",
      "Train Epoche: 3 [993/96 (1034%)]\tLoss: 0.954115\n",
      "Train Epoche: 3 [994/96 (1035%)]\tLoss: 117.326080\n",
      "Train Epoche: 3 [995/96 (1036%)]\tLoss: 12.176091\n",
      "Train Epoche: 3 [996/96 (1038%)]\tLoss: 73.490265\n",
      "Train Epoche: 3 [997/96 (1039%)]\tLoss: 0.581871\n",
      "Train Epoche: 3 [998/96 (1040%)]\tLoss: 5.567817\n",
      "Train Epoche: 3 [999/96 (1041%)]\tLoss: 0.300987\n",
      "Train Epoche: 3 [1000/96 (1042%)]\tLoss: 9.607449\n",
      "Train Epoche: 3 [1001/96 (1043%)]\tLoss: 23.547209\n",
      "Train Epoche: 3 [1002/96 (1044%)]\tLoss: 2.757915\n",
      "Train Epoche: 3 [1003/96 (1045%)]\tLoss: 8.962212\n",
      "Train Epoche: 3 [1004/96 (1046%)]\tLoss: 1.521942\n",
      "Train Epoche: 3 [1005/96 (1047%)]\tLoss: 1.601102\n",
      "Train Epoche: 3 [1006/96 (1048%)]\tLoss: 0.168991\n",
      "Train Epoche: 3 [1007/96 (1049%)]\tLoss: 9.549079\n",
      "Train Epoche: 3 [1008/96 (1050%)]\tLoss: 0.146361\n",
      "Train Epoche: 3 [1009/96 (1051%)]\tLoss: 0.022128\n",
      "Train Epoche: 3 [1010/96 (1052%)]\tLoss: 9.290550\n",
      "Train Epoche: 3 [1011/96 (1053%)]\tLoss: 15.474780\n",
      "Train Epoche: 3 [1012/96 (1054%)]\tLoss: 199.550430\n",
      "Train Epoche: 3 [1013/96 (1055%)]\tLoss: 43.940376\n",
      "Train Epoche: 3 [1014/96 (1056%)]\tLoss: 111.965981\n",
      "Train Epoche: 3 [1015/96 (1057%)]\tLoss: 53.132462\n",
      "Train Epoche: 3 [1016/96 (1058%)]\tLoss: 136.681396\n",
      "Train Epoche: 3 [1017/96 (1059%)]\tLoss: 190.567505\n",
      "Train Epoche: 3 [1018/96 (1060%)]\tLoss: 21.932755\n",
      "Train Epoche: 3 [1019/96 (1061%)]\tLoss: 34.840443\n",
      "Train Epoche: 3 [1020/96 (1062%)]\tLoss: 160.373306\n",
      "Train Epoche: 3 [1021/96 (1064%)]\tLoss: 24.530178\n",
      "Train Epoche: 3 [1022/96 (1065%)]\tLoss: 48.320904\n",
      "Train Epoche: 3 [1023/96 (1066%)]\tLoss: 6.709583\n",
      "Train Epoche: 3 [1024/96 (1067%)]\tLoss: 3.699709\n",
      "Train Epoche: 3 [1025/96 (1068%)]\tLoss: 25.482649\n",
      "Train Epoche: 3 [1026/96 (1069%)]\tLoss: 1.554002\n",
      "Train Epoche: 3 [1027/96 (1070%)]\tLoss: 68.237358\n",
      "Train Epoche: 3 [1028/96 (1071%)]\tLoss: 54.403694\n",
      "Train Epoche: 3 [1029/96 (1072%)]\tLoss: 2.849742\n",
      "Train Epoche: 3 [1030/96 (1073%)]\tLoss: 2.635351\n",
      "Train Epoche: 3 [1031/96 (1074%)]\tLoss: 13.133633\n",
      "Train Epoche: 3 [1032/96 (1075%)]\tLoss: 2.241941\n",
      "Train Epoche: 3 [1033/96 (1076%)]\tLoss: 301.859863\n",
      "Train Epoche: 3 [1034/96 (1077%)]\tLoss: 228.833893\n",
      "Train Epoche: 3 [1035/96 (1078%)]\tLoss: 60.135063\n",
      "Train Epoche: 3 [1036/96 (1079%)]\tLoss: 1.729376\n",
      "Train Epoche: 3 [1037/96 (1080%)]\tLoss: 0.105998\n",
      "Train Epoche: 3 [1038/96 (1081%)]\tLoss: 14.510094\n",
      "Train Epoche: 3 [1039/96 (1082%)]\tLoss: 104.100822\n",
      "Train Epoche: 3 [1040/96 (1083%)]\tLoss: 91.285988\n",
      "Train Epoche: 3 [1041/96 (1084%)]\tLoss: 0.423645\n",
      "Train Epoche: 3 [1042/96 (1085%)]\tLoss: 3.567081\n",
      "Train Epoche: 3 [1043/96 (1086%)]\tLoss: 7.114485\n",
      "Train Epoche: 3 [1044/96 (1088%)]\tLoss: 138.426498\n",
      "Train Epoche: 3 [1045/96 (1089%)]\tLoss: 3.550551\n",
      "Train Epoche: 3 [1046/96 (1090%)]\tLoss: 5.105909\n",
      "Train Epoche: 3 [1047/96 (1091%)]\tLoss: 17.856882\n",
      "Train Epoche: 3 [1048/96 (1092%)]\tLoss: 23.275976\n",
      "Train Epoche: 3 [1049/96 (1093%)]\tLoss: 76.657677\n",
      "Train Epoche: 3 [1050/96 (1094%)]\tLoss: 8.544934\n",
      "Train Epoche: 3 [1051/96 (1095%)]\tLoss: 247.477295\n",
      "Train Epoche: 3 [1052/96 (1096%)]\tLoss: 23.173872\n",
      "Train Epoche: 3 [1053/96 (1097%)]\tLoss: 10.664917\n",
      "Train Epoche: 3 [1054/96 (1098%)]\tLoss: 22.912472\n",
      "Train Epoche: 3 [1055/96 (1099%)]\tLoss: 1.458532\n",
      "Train Epoche: 3 [1056/96 (1100%)]\tLoss: 6.819728\n",
      "Train Epoche: 3 [1057/96 (1101%)]\tLoss: 0.601583\n",
      "Train Epoche: 3 [1058/96 (1102%)]\tLoss: 3.616654\n",
      "Train Epoche: 3 [1059/96 (1103%)]\tLoss: 39.683620\n",
      "Train Epoche: 3 [1060/96 (1104%)]\tLoss: 31.004087\n",
      "Train Epoche: 3 [1061/96 (1105%)]\tLoss: 0.567340\n",
      "Train Epoche: 3 [1062/96 (1106%)]\tLoss: 12.226313\n",
      "Train Epoche: 3 [1063/96 (1107%)]\tLoss: 3.794041\n",
      "Train Epoche: 3 [1064/96 (1108%)]\tLoss: 0.092938\n",
      "Train Epoche: 3 [1065/96 (1109%)]\tLoss: 2.539051\n",
      "Train Epoche: 3 [1066/96 (1110%)]\tLoss: 75.483192\n",
      "Train Epoche: 3 [1067/96 (1111%)]\tLoss: 4.509896\n",
      "Train Epoche: 3 [1068/96 (1112%)]\tLoss: 10.351550\n",
      "Train Epoche: 3 [1069/96 (1114%)]\tLoss: 4.095708\n",
      "Train Epoche: 3 [1070/96 (1115%)]\tLoss: 130.925171\n",
      "Train Epoche: 3 [1071/96 (1116%)]\tLoss: 2.413591\n",
      "Train Epoche: 3 [1072/96 (1117%)]\tLoss: 3.357625\n",
      "Train Epoche: 3 [1073/96 (1118%)]\tLoss: 2.481294\n",
      "Train Epoche: 3 [1074/96 (1119%)]\tLoss: 20.484468\n",
      "Train Epoche: 3 [1075/96 (1120%)]\tLoss: 18.933407\n",
      "Train Epoche: 3 [1076/96 (1121%)]\tLoss: 6.101184\n",
      "Train Epoche: 3 [1077/96 (1122%)]\tLoss: 6.170257\n",
      "Train Epoche: 3 [1078/96 (1123%)]\tLoss: 0.001285\n",
      "Train Epoche: 3 [1079/96 (1124%)]\tLoss: 3.363345\n",
      "Train Epoche: 3 [1080/96 (1125%)]\tLoss: 6.795807\n",
      "Train Epoche: 3 [1081/96 (1126%)]\tLoss: 2.226675\n",
      "Train Epoche: 3 [1082/96 (1127%)]\tLoss: 9.564711\n",
      "Train Epoche: 3 [1083/96 (1128%)]\tLoss: 4.027467\n",
      "Train Epoche: 3 [1084/96 (1129%)]\tLoss: 23.865892\n",
      "Train Epoche: 3 [1085/96 (1130%)]\tLoss: 15.153219\n",
      "Train Epoche: 3 [1086/96 (1131%)]\tLoss: 0.012654\n",
      "Train Epoche: 3 [1087/96 (1132%)]\tLoss: 0.910532\n",
      "Train Epoche: 3 [1088/96 (1133%)]\tLoss: 6.527061\n",
      "Train Epoche: 3 [1089/96 (1134%)]\tLoss: 6.706327\n",
      "Train Epoche: 3 [1090/96 (1135%)]\tLoss: 6.718690\n",
      "Train Epoche: 3 [1091/96 (1136%)]\tLoss: 2.185061\n",
      "Train Epoche: 3 [1092/96 (1138%)]\tLoss: 38.603065\n",
      "Train Epoche: 3 [1093/96 (1139%)]\tLoss: 2.163692\n",
      "Train Epoche: 3 [1094/96 (1140%)]\tLoss: 0.779728\n",
      "Train Epoche: 3 [1095/96 (1141%)]\tLoss: 1.840764\n",
      "Train Epoche: 3 [1096/96 (1142%)]\tLoss: 7.643302\n",
      "Train Epoche: 3 [1097/96 (1143%)]\tLoss: 17.528864\n",
      "Train Epoche: 3 [1098/96 (1144%)]\tLoss: 1.258169\n",
      "Train Epoche: 3 [1099/96 (1145%)]\tLoss: 8.140396\n",
      "Train Epoche: 3 [1100/96 (1146%)]\tLoss: 94.050316\n",
      "Train Epoche: 3 [1101/96 (1147%)]\tLoss: 0.188983\n",
      "Train Epoche: 3 [1102/96 (1148%)]\tLoss: 0.403909\n",
      "Train Epoche: 3 [1103/96 (1149%)]\tLoss: 12.949029\n",
      "Train Epoche: 3 [1104/96 (1150%)]\tLoss: 18.043091\n",
      "Train Epoche: 3 [1105/96 (1151%)]\tLoss: 15.919341\n",
      "Train Epoche: 3 [1106/96 (1152%)]\tLoss: 12.212725\n",
      "Train Epoche: 3 [1107/96 (1153%)]\tLoss: 1.460062\n",
      "Train Epoche: 3 [1108/96 (1154%)]\tLoss: 20.460806\n",
      "Train Epoche: 3 [1109/96 (1155%)]\tLoss: 5.019637\n",
      "Train Epoche: 3 [1110/96 (1156%)]\tLoss: 283.680420\n",
      "Train Epoche: 3 [1111/96 (1157%)]\tLoss: 2.671767\n",
      "Train Epoche: 3 [1112/96 (1158%)]\tLoss: 4.697403\n",
      "Train Epoche: 3 [1113/96 (1159%)]\tLoss: 0.975550\n",
      "Train Epoche: 3 [1114/96 (1160%)]\tLoss: 7.035970\n",
      "Train Epoche: 3 [1115/96 (1161%)]\tLoss: 18.910740\n",
      "Train Epoche: 3 [1116/96 (1162%)]\tLoss: 13.492536\n",
      "Train Epoche: 3 [1117/96 (1164%)]\tLoss: 43.237381\n",
      "Train Epoche: 3 [1118/96 (1165%)]\tLoss: 3.968815\n",
      "Train Epoche: 3 [1119/96 (1166%)]\tLoss: 2.041629\n",
      "Train Epoche: 3 [1120/96 (1167%)]\tLoss: 1.903416\n",
      "Train Epoche: 3 [1121/96 (1168%)]\tLoss: 4.233667\n",
      "Train Epoche: 3 [1122/96 (1169%)]\tLoss: 33.013210\n",
      "Train Epoche: 3 [1123/96 (1170%)]\tLoss: 14.519715\n",
      "Train Epoche: 3 [1124/96 (1171%)]\tLoss: 30.624210\n",
      "Train Epoche: 3 [1125/96 (1172%)]\tLoss: 90.487000\n",
      "Train Epoche: 3 [1126/96 (1173%)]\tLoss: 1.000721\n",
      "Train Epoche: 3 [1127/96 (1174%)]\tLoss: 4.972061\n",
      "Train Epoche: 3 [1128/96 (1175%)]\tLoss: 0.688419\n",
      "Train Epoche: 3 [1129/96 (1176%)]\tLoss: 12.329640\n",
      "Train Epoche: 3 [1130/96 (1177%)]\tLoss: 4.913732\n",
      "Train Epoche: 3 [1131/96 (1178%)]\tLoss: 0.868740\n",
      "Train Epoche: 3 [1132/96 (1179%)]\tLoss: 12.311226\n",
      "Train Epoche: 3 [1133/96 (1180%)]\tLoss: 0.000898\n",
      "Train Epoche: 3 [1134/96 (1181%)]\tLoss: 48.180206\n",
      "Train Epoche: 3 [1135/96 (1182%)]\tLoss: 314.227509\n",
      "Train Epoche: 3 [1136/96 (1183%)]\tLoss: 230.680283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1137/96 (1184%)]\tLoss: 1.109720\n",
      "Train Epoche: 3 [1138/96 (1185%)]\tLoss: 0.013217\n",
      "Train Epoche: 3 [1139/96 (1186%)]\tLoss: 52.876564\n",
      "Train Epoche: 3 [1140/96 (1188%)]\tLoss: 9.550117\n",
      "Train Epoche: 3 [1141/96 (1189%)]\tLoss: 44.168797\n",
      "Train Epoche: 3 [1142/96 (1190%)]\tLoss: 12.623656\n",
      "Train Epoche: 3 [1143/96 (1191%)]\tLoss: 8.935526\n",
      "Train Epoche: 3 [1144/96 (1192%)]\tLoss: 0.657306\n",
      "Train Epoche: 3 [1145/96 (1193%)]\tLoss: 11.794223\n",
      "Train Epoche: 3 [1146/96 (1194%)]\tLoss: 17.799461\n",
      "Train Epoche: 3 [1147/96 (1195%)]\tLoss: 0.062872\n",
      "Train Epoche: 3 [1148/96 (1196%)]\tLoss: 0.834350\n",
      "Train Epoche: 3 [1149/96 (1197%)]\tLoss: 27.122253\n",
      "Train Epoche: 3 [1150/96 (1198%)]\tLoss: 6.482094\n",
      "Train Epoche: 3 [1151/96 (1199%)]\tLoss: 3.795291\n",
      "Train Epoche: 3 [1152/96 (1200%)]\tLoss: 1.182027\n",
      "Train Epoche: 3 [1153/96 (1201%)]\tLoss: 3.326311\n",
      "Train Epoche: 3 [1154/96 (1202%)]\tLoss: 54.524582\n",
      "Train Epoche: 3 [1155/96 (1203%)]\tLoss: 1.802674\n",
      "Train Epoche: 3 [1156/96 (1204%)]\tLoss: 8.656615\n",
      "Train Epoche: 3 [1157/96 (1205%)]\tLoss: 34.191059\n",
      "Train Epoche: 3 [1158/96 (1206%)]\tLoss: 7.629723\n",
      "Train Epoche: 3 [1159/96 (1207%)]\tLoss: 1.953713\n",
      "Train Epoche: 3 [1160/96 (1208%)]\tLoss: 0.449965\n",
      "Train Epoche: 3 [1161/96 (1209%)]\tLoss: 17.070177\n",
      "Train Epoche: 3 [1162/96 (1210%)]\tLoss: 49.250526\n",
      "Train Epoche: 3 [1163/96 (1211%)]\tLoss: 0.417355\n",
      "Train Epoche: 3 [1164/96 (1212%)]\tLoss: 0.254226\n",
      "Train Epoche: 3 [1165/96 (1214%)]\tLoss: 0.092107\n",
      "Train Epoche: 3 [1166/96 (1215%)]\tLoss: 12.143741\n",
      "Train Epoche: 3 [1167/96 (1216%)]\tLoss: 40.894993\n",
      "Train Epoche: 3 [1168/96 (1217%)]\tLoss: 14.661216\n",
      "Train Epoche: 3 [1169/96 (1218%)]\tLoss: 0.143397\n",
      "Train Epoche: 3 [1170/96 (1219%)]\tLoss: 1.310271\n",
      "Train Epoche: 3 [1171/96 (1220%)]\tLoss: 9.664352\n",
      "Train Epoche: 3 [1172/96 (1221%)]\tLoss: 7.339228\n",
      "Train Epoche: 3 [1173/96 (1222%)]\tLoss: 0.906943\n",
      "Train Epoche: 3 [1174/96 (1223%)]\tLoss: 3.511392\n",
      "Train Epoche: 3 [1175/96 (1224%)]\tLoss: 8.961806\n",
      "Train Epoche: 3 [1176/96 (1225%)]\tLoss: 43.349880\n",
      "Train Epoche: 3 [1177/96 (1226%)]\tLoss: 9.173456\n",
      "Train Epoche: 3 [1178/96 (1227%)]\tLoss: 14.134171\n",
      "Train Epoche: 3 [1179/96 (1228%)]\tLoss: 21.576401\n",
      "Train Epoche: 3 [1180/96 (1229%)]\tLoss: 53.242268\n",
      "Train Epoche: 3 [1181/96 (1230%)]\tLoss: 26.360823\n",
      "Train Epoche: 3 [1182/96 (1231%)]\tLoss: 0.622161\n",
      "Train Epoche: 3 [1183/96 (1232%)]\tLoss: 8.896421\n",
      "Train Epoche: 3 [1184/96 (1233%)]\tLoss: 18.029539\n",
      "Train Epoche: 3 [1185/96 (1234%)]\tLoss: 0.002801\n",
      "Train Epoche: 3 [1186/96 (1235%)]\tLoss: 4.435746\n",
      "Train Epoche: 3 [1187/96 (1236%)]\tLoss: 7.368787\n",
      "Train Epoche: 3 [1188/96 (1238%)]\tLoss: 14.151561\n",
      "Train Epoche: 3 [1189/96 (1239%)]\tLoss: 4.567482\n",
      "Train Epoche: 3 [1190/96 (1240%)]\tLoss: 2.934738\n",
      "Train Epoche: 3 [1191/96 (1241%)]\tLoss: 6.831871\n",
      "Train Epoche: 3 [1192/96 (1242%)]\tLoss: 0.476664\n",
      "Train Epoche: 3 [1193/96 (1243%)]\tLoss: 1.392720\n",
      "Train Epoche: 3 [1194/96 (1244%)]\tLoss: 0.923675\n",
      "Train Epoche: 3 [1195/96 (1245%)]\tLoss: 4.682130\n",
      "Train Epoche: 3 [1196/96 (1246%)]\tLoss: 11.812171\n",
      "Train Epoche: 3 [1197/96 (1247%)]\tLoss: 0.475550\n",
      "Train Epoche: 3 [1198/96 (1248%)]\tLoss: 164.414551\n",
      "Train Epoche: 3 [1199/96 (1249%)]\tLoss: 4.987020\n",
      "Train Epoche: 3 [1200/96 (1250%)]\tLoss: 0.687534\n",
      "Train Epoche: 3 [1201/96 (1251%)]\tLoss: 41.212189\n",
      "Train Epoche: 3 [1202/96 (1252%)]\tLoss: 1.289196\n",
      "Train Epoche: 3 [1203/96 (1253%)]\tLoss: 55.854603\n",
      "Train Epoche: 3 [1204/96 (1254%)]\tLoss: 13.950921\n",
      "Train Epoche: 3 [1205/96 (1255%)]\tLoss: 2.484868\n",
      "Train Epoche: 3 [1206/96 (1256%)]\tLoss: 1.785069\n",
      "Train Epoche: 3 [1207/96 (1257%)]\tLoss: 12.713642\n",
      "Train Epoche: 3 [1208/96 (1258%)]\tLoss: 0.959767\n",
      "Train Epoche: 3 [1209/96 (1259%)]\tLoss: 18.767767\n",
      "Train Epoche: 3 [1210/96 (1260%)]\tLoss: 5.799417\n",
      "Train Epoche: 3 [1211/96 (1261%)]\tLoss: 4.108727\n",
      "Train Epoche: 3 [1212/96 (1262%)]\tLoss: 0.768151\n",
      "Train Epoche: 3 [1213/96 (1264%)]\tLoss: 2.291673\n",
      "Train Epoche: 3 [1214/96 (1265%)]\tLoss: 0.000749\n",
      "Train Epoche: 3 [1215/96 (1266%)]\tLoss: 5.941778\n",
      "Train Epoche: 3 [1216/96 (1267%)]\tLoss: 21.659683\n",
      "Train Epoche: 3 [1217/96 (1268%)]\tLoss: 4.154175\n",
      "Train Epoche: 3 [1218/96 (1269%)]\tLoss: 0.049058\n",
      "Train Epoche: 3 [1219/96 (1270%)]\tLoss: 0.122551\n",
      "Train Epoche: 3 [1220/96 (1271%)]\tLoss: 7.875382\n",
      "Train Epoche: 3 [1221/96 (1272%)]\tLoss: 0.069714\n",
      "Train Epoche: 3 [1222/96 (1273%)]\tLoss: 0.321903\n",
      "Train Epoche: 3 [1223/96 (1274%)]\tLoss: 0.571958\n",
      "Train Epoche: 3 [1224/96 (1275%)]\tLoss: 3.059784\n",
      "Train Epoche: 3 [1225/96 (1276%)]\tLoss: 0.703033\n",
      "Train Epoche: 3 [1226/96 (1277%)]\tLoss: 3.031872\n",
      "Train Epoche: 3 [1227/96 (1278%)]\tLoss: 2.545651\n",
      "Train Epoche: 3 [1228/96 (1279%)]\tLoss: 0.001387\n",
      "Train Epoche: 3 [1229/96 (1280%)]\tLoss: 0.971083\n",
      "Train Epoche: 3 [1230/96 (1281%)]\tLoss: 0.249727\n",
      "Train Epoche: 3 [1231/96 (1282%)]\tLoss: 27.343420\n",
      "Train Epoche: 3 [1232/96 (1283%)]\tLoss: 2.906331\n",
      "Train Epoche: 3 [1233/96 (1284%)]\tLoss: 0.545873\n",
      "Train Epoche: 3 [1234/96 (1285%)]\tLoss: 0.002215\n",
      "Train Epoche: 3 [1235/96 (1286%)]\tLoss: 1.831655\n",
      "Train Epoche: 3 [1236/96 (1288%)]\tLoss: 6.657719\n",
      "Train Epoche: 3 [1237/96 (1289%)]\tLoss: 17.260941\n",
      "Train Epoche: 3 [1238/96 (1290%)]\tLoss: 0.020074\n",
      "Train Epoche: 3 [1239/96 (1291%)]\tLoss: 26.201725\n",
      "Train Epoche: 3 [1240/96 (1292%)]\tLoss: 0.179790\n",
      "Train Epoche: 3 [1241/96 (1293%)]\tLoss: 0.201394\n",
      "Train Epoche: 3 [1242/96 (1294%)]\tLoss: 2.503363\n",
      "Train Epoche: 3 [1243/96 (1295%)]\tLoss: 4.158042\n",
      "Train Epoche: 3 [1244/96 (1296%)]\tLoss: 0.352296\n",
      "Train Epoche: 3 [1245/96 (1297%)]\tLoss: 9.967044\n",
      "Train Epoche: 3 [1246/96 (1298%)]\tLoss: 51.268425\n",
      "Train Epoche: 3 [1247/96 (1299%)]\tLoss: 3.648777\n",
      "Train Epoche: 3 [1248/96 (1300%)]\tLoss: 15.852302\n",
      "Train Epoche: 3 [1249/96 (1301%)]\tLoss: 51.154942\n",
      "Train Epoche: 3 [1250/96 (1302%)]\tLoss: 0.449652\n",
      "Train Epoche: 3 [1251/96 (1303%)]\tLoss: 0.696924\n",
      "Train Epoche: 3 [1252/96 (1304%)]\tLoss: 1.048307\n",
      "Train Epoche: 3 [1253/96 (1305%)]\tLoss: 6.498367\n",
      "Train Epoche: 3 [1254/96 (1306%)]\tLoss: 10.454329\n",
      "Train Epoche: 3 [1255/96 (1307%)]\tLoss: 0.582082\n",
      "Train Epoche: 3 [1256/96 (1308%)]\tLoss: 7.001823\n",
      "Train Epoche: 3 [1257/96 (1309%)]\tLoss: 2.719200\n",
      "Train Epoche: 3 [1258/96 (1310%)]\tLoss: 6.877606\n",
      "Train Epoche: 3 [1259/96 (1311%)]\tLoss: 9.514513\n",
      "Train Epoche: 3 [1260/96 (1312%)]\tLoss: 20.674767\n",
      "Train Epoche: 3 [1261/96 (1314%)]\tLoss: 16.440908\n",
      "Train Epoche: 3 [1262/96 (1315%)]\tLoss: 4.997927\n",
      "Train Epoche: 3 [1263/96 (1316%)]\tLoss: 8.376699\n",
      "Train Epoche: 3 [1264/96 (1317%)]\tLoss: 3.623173\n",
      "Train Epoche: 3 [1265/96 (1318%)]\tLoss: 0.000730\n",
      "Train Epoche: 3 [1266/96 (1319%)]\tLoss: 125.401123\n",
      "Train Epoche: 3 [1267/96 (1320%)]\tLoss: 1.288239\n",
      "Train Epoche: 3 [1268/96 (1321%)]\tLoss: 0.282802\n",
      "Train Epoche: 3 [1269/96 (1322%)]\tLoss: 2.544427\n",
      "Train Epoche: 3 [1270/96 (1323%)]\tLoss: 2.474497\n",
      "Train Epoche: 3 [1271/96 (1324%)]\tLoss: 4.585596\n",
      "Train Epoche: 3 [1272/96 (1325%)]\tLoss: 0.004579\n",
      "Train Epoche: 3 [1273/96 (1326%)]\tLoss: 9.791988\n",
      "Train Epoche: 3 [1274/96 (1327%)]\tLoss: 134.332077\n",
      "Train Epoche: 3 [1275/96 (1328%)]\tLoss: 8.295730\n",
      "Train Epoche: 3 [1276/96 (1329%)]\tLoss: 3.242105\n",
      "Train Epoche: 3 [1277/96 (1330%)]\tLoss: 0.501750\n",
      "Train Epoche: 3 [1278/96 (1331%)]\tLoss: 3.574382\n",
      "Train Epoche: 3 [1279/96 (1332%)]\tLoss: 2.529186\n",
      "Train Epoche: 3 [1280/96 (1333%)]\tLoss: 11.827621\n",
      "Train Epoche: 3 [1281/96 (1334%)]\tLoss: 20.486395\n",
      "Train Epoche: 3 [1282/96 (1335%)]\tLoss: 7.338230\n",
      "Train Epoche: 3 [1283/96 (1336%)]\tLoss: 0.061946\n",
      "Train Epoche: 3 [1284/96 (1338%)]\tLoss: 32.370201\n",
      "Train Epoche: 3 [1285/96 (1339%)]\tLoss: 2.009197\n",
      "Train Epoche: 3 [1286/96 (1340%)]\tLoss: 16.851742\n",
      "Train Epoche: 3 [1287/96 (1341%)]\tLoss: 3.955299\n",
      "Train Epoche: 3 [1288/96 (1342%)]\tLoss: 39.712704\n",
      "Train Epoche: 3 [1289/96 (1343%)]\tLoss: 0.000951\n",
      "Train Epoche: 3 [1290/96 (1344%)]\tLoss: 1.539128\n",
      "Train Epoche: 3 [1291/96 (1345%)]\tLoss: 19.596413\n",
      "Train Epoche: 3 [1292/96 (1346%)]\tLoss: 0.439415\n",
      "Train Epoche: 3 [1293/96 (1347%)]\tLoss: 1.257514\n",
      "Train Epoche: 3 [1294/96 (1348%)]\tLoss: 4.210598\n",
      "Train Epoche: 3 [1295/96 (1349%)]\tLoss: 4.165254\n",
      "Train Epoche: 3 [1296/96 (1350%)]\tLoss: 0.328810\n",
      "Train Epoche: 3 [1297/96 (1351%)]\tLoss: 0.233997\n",
      "Train Epoche: 3 [1298/96 (1352%)]\tLoss: 5.209946\n",
      "Train Epoche: 3 [1299/96 (1353%)]\tLoss: 0.881972\n",
      "Train Epoche: 3 [1300/96 (1354%)]\tLoss: 25.755062\n",
      "Train Epoche: 3 [1301/96 (1355%)]\tLoss: 1.535963\n",
      "Train Epoche: 3 [1302/96 (1356%)]\tLoss: 34.647888\n",
      "Train Epoche: 3 [1303/96 (1357%)]\tLoss: 1.484637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1304/96 (1358%)]\tLoss: 1.931791\n",
      "Train Epoche: 3 [1305/96 (1359%)]\tLoss: 6.767131\n",
      "Train Epoche: 3 [1306/96 (1360%)]\tLoss: 9.090618\n",
      "Train Epoche: 3 [1307/96 (1361%)]\tLoss: 12.236919\n",
      "Train Epoche: 3 [1308/96 (1362%)]\tLoss: 4.760975\n",
      "Train Epoche: 3 [1309/96 (1364%)]\tLoss: 4.266280\n",
      "Train Epoche: 3 [1310/96 (1365%)]\tLoss: 0.835084\n",
      "Train Epoche: 3 [1311/96 (1366%)]\tLoss: 58.869858\n",
      "Train Epoche: 3 [1312/96 (1367%)]\tLoss: 1.093748\n",
      "Train Epoche: 3 [1313/96 (1368%)]\tLoss: 12.377637\n",
      "Train Epoche: 3 [1314/96 (1369%)]\tLoss: 27.728228\n",
      "Train Epoche: 3 [1315/96 (1370%)]\tLoss: 20.529617\n",
      "Train Epoche: 3 [1316/96 (1371%)]\tLoss: 0.925976\n",
      "Train Epoche: 3 [1317/96 (1372%)]\tLoss: 0.056918\n",
      "Train Epoche: 3 [1318/96 (1373%)]\tLoss: 183.921249\n",
      "Train Epoche: 3 [1319/96 (1374%)]\tLoss: 1.991264\n",
      "Train Epoche: 3 [1320/96 (1375%)]\tLoss: 0.013902\n",
      "Train Epoche: 3 [1321/96 (1376%)]\tLoss: 7.154320\n",
      "Train Epoche: 3 [1322/96 (1377%)]\tLoss: 0.444646\n",
      "Train Epoche: 3 [1323/96 (1378%)]\tLoss: 14.951123\n",
      "Train Epoche: 3 [1324/96 (1379%)]\tLoss: 1.694400\n",
      "Train Epoche: 3 [1325/96 (1380%)]\tLoss: 31.779974\n",
      "Train Epoche: 3 [1326/96 (1381%)]\tLoss: 1.201481\n",
      "Train Epoche: 3 [1327/96 (1382%)]\tLoss: 25.136351\n",
      "Train Epoche: 3 [1328/96 (1383%)]\tLoss: 19.643106\n",
      "Train Epoche: 3 [1329/96 (1384%)]\tLoss: 325.221924\n",
      "Train Epoche: 3 [1330/96 (1385%)]\tLoss: 0.111525\n",
      "Train Epoche: 3 [1331/96 (1386%)]\tLoss: 61.507149\n",
      "Train Epoche: 3 [1332/96 (1388%)]\tLoss: 20.074173\n",
      "Train Epoche: 3 [1333/96 (1389%)]\tLoss: 21.406841\n",
      "Train Epoche: 3 [1334/96 (1390%)]\tLoss: 2.744061\n",
      "Train Epoche: 3 [1335/96 (1391%)]\tLoss: 16.978922\n",
      "Train Epoche: 3 [1336/96 (1392%)]\tLoss: 15.211143\n",
      "Train Epoche: 3 [1337/96 (1393%)]\tLoss: 6.213071\n",
      "Train Epoche: 3 [1338/96 (1394%)]\tLoss: 2.055164\n",
      "Train Epoche: 3 [1339/96 (1395%)]\tLoss: 5.214871\n",
      "Train Epoche: 3 [1340/96 (1396%)]\tLoss: 16.967068\n",
      "Train Epoche: 3 [1341/96 (1397%)]\tLoss: 66.852913\n",
      "Train Epoche: 3 [1342/96 (1398%)]\tLoss: 5.437294\n",
      "Train Epoche: 3 [1343/96 (1399%)]\tLoss: 9.682309\n",
      "Train Epoche: 3 [1344/96 (1400%)]\tLoss: 8.615980\n",
      "Train Epoche: 3 [1345/96 (1401%)]\tLoss: 0.242910\n",
      "Train Epoche: 3 [1346/96 (1402%)]\tLoss: 4.437064\n",
      "Train Epoche: 3 [1347/96 (1403%)]\tLoss: 5.156133\n",
      "Train Epoche: 3 [1348/96 (1404%)]\tLoss: 0.326703\n",
      "Train Epoche: 3 [1349/96 (1405%)]\tLoss: 2.886072\n",
      "Train Epoche: 3 [1350/96 (1406%)]\tLoss: 2.757430\n",
      "Train Epoche: 3 [1351/96 (1407%)]\tLoss: 0.359378\n",
      "Train Epoche: 3 [1352/96 (1408%)]\tLoss: 13.061888\n",
      "Train Epoche: 3 [1353/96 (1409%)]\tLoss: 23.351318\n",
      "Train Epoche: 3 [1354/96 (1410%)]\tLoss: 0.734889\n",
      "Train Epoche: 3 [1355/96 (1411%)]\tLoss: 13.968515\n",
      "Train Epoche: 3 [1356/96 (1412%)]\tLoss: 4.745306\n",
      "Train Epoche: 3 [1357/96 (1414%)]\tLoss: 0.068741\n",
      "Train Epoche: 3 [1358/96 (1415%)]\tLoss: 1.504571\n",
      "Train Epoche: 3 [1359/96 (1416%)]\tLoss: 0.837810\n",
      "Train Epoche: 3 [1360/96 (1417%)]\tLoss: 0.603982\n",
      "Train Epoche: 3 [1361/96 (1418%)]\tLoss: 5.567814\n",
      "Train Epoche: 3 [1362/96 (1419%)]\tLoss: 0.242540\n",
      "Train Epoche: 3 [1363/96 (1420%)]\tLoss: 22.958931\n",
      "Train Epoche: 3 [1364/96 (1421%)]\tLoss: 138.244202\n",
      "Train Epoche: 3 [1365/96 (1422%)]\tLoss: 6.765656\n",
      "Train Epoche: 3 [1366/96 (1423%)]\tLoss: 0.461084\n",
      "Train Epoche: 3 [1367/96 (1424%)]\tLoss: 6.197539\n",
      "Train Epoche: 3 [1368/96 (1425%)]\tLoss: 1.293606\n",
      "Train Epoche: 3 [1369/96 (1426%)]\tLoss: 35.188107\n",
      "Train Epoche: 3 [1370/96 (1427%)]\tLoss: 0.512926\n",
      "Train Epoche: 3 [1371/96 (1428%)]\tLoss: 22.896990\n",
      "Train Epoche: 3 [1372/96 (1429%)]\tLoss: 8.601463\n",
      "Train Epoche: 3 [1373/96 (1430%)]\tLoss: 8.193858\n",
      "Train Epoche: 3 [1374/96 (1431%)]\tLoss: 14.818772\n",
      "Train Epoche: 3 [1375/96 (1432%)]\tLoss: 0.034401\n",
      "Train Epoche: 3 [1376/96 (1433%)]\tLoss: 0.005923\n",
      "Train Epoche: 3 [1377/96 (1434%)]\tLoss: 0.438290\n",
      "Train Epoche: 3 [1378/96 (1435%)]\tLoss: 19.067678\n",
      "Train Epoche: 3 [1379/96 (1436%)]\tLoss: 18.815285\n",
      "Train Epoche: 3 [1380/96 (1438%)]\tLoss: 2.711793\n",
      "Train Epoche: 3 [1381/96 (1439%)]\tLoss: 0.030404\n",
      "Train Epoche: 3 [1382/96 (1440%)]\tLoss: 0.302763\n",
      "Train Epoche: 3 [1383/96 (1441%)]\tLoss: 4.380214\n",
      "Train Epoche: 3 [1384/96 (1442%)]\tLoss: 0.062688\n",
      "Train Epoche: 3 [1385/96 (1443%)]\tLoss: 4.395757\n",
      "Train Epoche: 3 [1386/96 (1444%)]\tLoss: 6.435151\n",
      "Train Epoche: 3 [1387/96 (1445%)]\tLoss: 0.185857\n",
      "Train Epoche: 3 [1388/96 (1446%)]\tLoss: 8.144196\n",
      "Train Epoche: 3 [1389/96 (1447%)]\tLoss: 38.177010\n",
      "Train Epoche: 3 [1390/96 (1448%)]\tLoss: 10.487311\n",
      "Train Epoche: 3 [1391/96 (1449%)]\tLoss: 2.805685\n",
      "Train Epoche: 3 [1392/96 (1450%)]\tLoss: 0.470775\n",
      "Train Epoche: 3 [1393/96 (1451%)]\tLoss: 15.179670\n",
      "Train Epoche: 3 [1394/96 (1452%)]\tLoss: 0.055722\n",
      "Train Epoche: 3 [1395/96 (1453%)]\tLoss: 0.085582\n",
      "Train Epoche: 3 [1396/96 (1454%)]\tLoss: 2.394740\n",
      "Train Epoche: 3 [1397/96 (1455%)]\tLoss: 14.834791\n",
      "Train Epoche: 3 [1398/96 (1456%)]\tLoss: 0.000561\n",
      "Train Epoche: 3 [1399/96 (1457%)]\tLoss: 1.073264\n",
      "Train Epoche: 3 [1400/96 (1458%)]\tLoss: 2.615366\n",
      "Train Epoche: 3 [1401/96 (1459%)]\tLoss: 3.909925\n",
      "Train Epoche: 3 [1402/96 (1460%)]\tLoss: 0.024568\n",
      "Train Epoche: 3 [1403/96 (1461%)]\tLoss: 0.434189\n",
      "Train Epoche: 3 [1404/96 (1462%)]\tLoss: 2.630948\n",
      "Train Epoche: 3 [1405/96 (1464%)]\tLoss: 6.568332\n",
      "Train Epoche: 3 [1406/96 (1465%)]\tLoss: 8.457282\n",
      "Train Epoche: 3 [1407/96 (1466%)]\tLoss: 1.136439\n",
      "Train Epoche: 3 [1408/96 (1467%)]\tLoss: 15.181777\n",
      "Train Epoche: 3 [1409/96 (1468%)]\tLoss: 5.909009\n",
      "Train Epoche: 3 [1410/96 (1469%)]\tLoss: 0.775292\n",
      "Train Epoche: 3 [1411/96 (1470%)]\tLoss: 68.984673\n",
      "Train Epoche: 3 [1412/96 (1471%)]\tLoss: 5.367987\n",
      "Train Epoche: 3 [1413/96 (1472%)]\tLoss: 1.602577\n",
      "Train Epoche: 3 [1414/96 (1473%)]\tLoss: 7.312954\n",
      "Train Epoche: 3 [1415/96 (1474%)]\tLoss: 0.778915\n",
      "Train Epoche: 3 [1416/96 (1475%)]\tLoss: 2.744077\n",
      "Train Epoche: 3 [1417/96 (1476%)]\tLoss: 0.612613\n",
      "Train Epoche: 3 [1418/96 (1477%)]\tLoss: 73.647705\n",
      "Train Epoche: 3 [1419/96 (1478%)]\tLoss: 2.703828\n",
      "Train Epoche: 3 [1420/96 (1479%)]\tLoss: 7.885242\n",
      "Train Epoche: 3 [1421/96 (1480%)]\tLoss: 9.461789\n",
      "Train Epoche: 3 [1422/96 (1481%)]\tLoss: 1.910905\n",
      "Train Epoche: 3 [1423/96 (1482%)]\tLoss: 0.031977\n",
      "Train Epoche: 3 [1424/96 (1483%)]\tLoss: 3.273558\n",
      "Train Epoche: 3 [1425/96 (1484%)]\tLoss: 1.154430\n",
      "Train Epoche: 3 [1426/96 (1485%)]\tLoss: 0.110822\n",
      "Train Epoche: 3 [1427/96 (1486%)]\tLoss: 1.596242\n",
      "Train Epoche: 3 [1428/96 (1488%)]\tLoss: 3.548262\n",
      "Train Epoche: 3 [1429/96 (1489%)]\tLoss: 0.859791\n",
      "Train Epoche: 3 [1430/96 (1490%)]\tLoss: 0.647593\n",
      "Train Epoche: 3 [1431/96 (1491%)]\tLoss: 1.016547\n",
      "Train Epoche: 3 [1432/96 (1492%)]\tLoss: 41.274181\n",
      "Train Epoche: 3 [1433/96 (1493%)]\tLoss: 7.550983\n",
      "Train Epoche: 3 [1434/96 (1494%)]\tLoss: 18.289267\n",
      "Train Epoche: 3 [1435/96 (1495%)]\tLoss: 23.401554\n",
      "Train Epoche: 3 [1436/96 (1496%)]\tLoss: 6.070120\n",
      "Train Epoche: 3 [1437/96 (1497%)]\tLoss: 7.380084\n",
      "Train Epoche: 3 [1438/96 (1498%)]\tLoss: 3.336686\n",
      "Train Epoche: 3 [1439/96 (1499%)]\tLoss: 1.586151\n",
      "Train Epoche: 3 [1440/96 (1500%)]\tLoss: 6.944679\n",
      "Train Epoche: 3 [1441/96 (1501%)]\tLoss: 1.766538\n",
      "Train Epoche: 3 [1442/96 (1502%)]\tLoss: 97.192757\n",
      "Train Epoche: 3 [1443/96 (1503%)]\tLoss: 48.688595\n",
      "Train Epoche: 3 [1444/96 (1504%)]\tLoss: 0.332285\n",
      "Train Epoche: 3 [1445/96 (1505%)]\tLoss: 1.242212\n",
      "Train Epoche: 3 [1446/96 (1506%)]\tLoss: 10.407868\n",
      "Train Epoche: 3 [1447/96 (1507%)]\tLoss: 8.297691\n",
      "Train Epoche: 3 [1448/96 (1508%)]\tLoss: 9.694102\n",
      "Train Epoche: 3 [1449/96 (1509%)]\tLoss: 8.057970\n",
      "Train Epoche: 3 [1450/96 (1510%)]\tLoss: 13.027340\n",
      "Train Epoche: 3 [1451/96 (1511%)]\tLoss: 22.236029\n",
      "Train Epoche: 3 [1452/96 (1512%)]\tLoss: 1.810920\n",
      "Train Epoche: 3 [1453/96 (1514%)]\tLoss: 0.845239\n",
      "Train Epoche: 3 [1454/96 (1515%)]\tLoss: 0.072328\n",
      "Train Epoche: 3 [1455/96 (1516%)]\tLoss: 1.851236\n",
      "Train Epoche: 3 [1456/96 (1517%)]\tLoss: 3.833122\n",
      "Train Epoche: 3 [1457/96 (1518%)]\tLoss: 31.234392\n",
      "Train Epoche: 3 [1458/96 (1519%)]\tLoss: 3.290807\n",
      "Train Epoche: 3 [1459/96 (1520%)]\tLoss: 0.166337\n",
      "Train Epoche: 3 [1460/96 (1521%)]\tLoss: 2.087319\n",
      "Train Epoche: 3 [1461/96 (1522%)]\tLoss: 4.849030\n",
      "Train Epoche: 3 [1462/96 (1523%)]\tLoss: 6.441507\n",
      "Train Epoche: 3 [1463/96 (1524%)]\tLoss: 16.078373\n",
      "Train Epoche: 3 [1464/96 (1525%)]\tLoss: 4.456082\n",
      "Train Epoche: 3 [1465/96 (1526%)]\tLoss: 0.283539\n",
      "Train Epoche: 3 [1466/96 (1527%)]\tLoss: 8.378505\n",
      "Train Epoche: 3 [1467/96 (1528%)]\tLoss: 0.537928\n",
      "Train Epoche: 3 [1468/96 (1529%)]\tLoss: 26.382166\n",
      "Train Epoche: 3 [1469/96 (1530%)]\tLoss: 2.212436\n",
      "Train Epoche: 3 [1470/96 (1531%)]\tLoss: 6.709578\n",
      "Train Epoche: 3 [1471/96 (1532%)]\tLoss: 1.502837\n",
      "Train Epoche: 3 [1472/96 (1533%)]\tLoss: 6.960425\n",
      "Train Epoche: 3 [1473/96 (1534%)]\tLoss: 0.156031\n",
      "Train Epoche: 3 [1474/96 (1535%)]\tLoss: 29.490040\n",
      "Train Epoche: 3 [1475/96 (1536%)]\tLoss: 2.470580\n",
      "Train Epoche: 3 [1476/96 (1538%)]\tLoss: 1.710327\n",
      "Train Epoche: 3 [1477/96 (1539%)]\tLoss: 32.649456\n",
      "Train Epoche: 3 [1478/96 (1540%)]\tLoss: 20.039902\n",
      "Train Epoche: 3 [1479/96 (1541%)]\tLoss: 0.043695\n",
      "Train Epoche: 3 [1480/96 (1542%)]\tLoss: 2.336716\n",
      "Train Epoche: 3 [1481/96 (1543%)]\tLoss: 3.922385\n",
      "Train Epoche: 3 [1482/96 (1544%)]\tLoss: 42.279488\n",
      "Train Epoche: 3 [1483/96 (1545%)]\tLoss: 0.896507\n",
      "Train Epoche: 3 [1484/96 (1546%)]\tLoss: 3.595527\n",
      "Train Epoche: 3 [1485/96 (1547%)]\tLoss: 2.508812\n",
      "Train Epoche: 3 [1486/96 (1548%)]\tLoss: 131.745163\n",
      "Train Epoche: 3 [1487/96 (1549%)]\tLoss: 0.007658\n",
      "Train Epoche: 3 [1488/96 (1550%)]\tLoss: 0.942565\n",
      "Train Epoche: 3 [1489/96 (1551%)]\tLoss: 1.982174\n",
      "Train Epoche: 3 [1490/96 (1552%)]\tLoss: 0.608965\n",
      "Train Epoche: 3 [1491/96 (1553%)]\tLoss: 0.137308\n",
      "Train Epoche: 3 [1492/96 (1554%)]\tLoss: 0.021764\n",
      "Train Epoche: 3 [1493/96 (1555%)]\tLoss: 0.514183\n",
      "Train Epoche: 3 [1494/96 (1556%)]\tLoss: 77.303154\n",
      "Train Epoche: 3 [1495/96 (1557%)]\tLoss: 1.168047\n",
      "Train Epoche: 3 [1496/96 (1558%)]\tLoss: 5.908846\n",
      "Train Epoche: 3 [1497/96 (1559%)]\tLoss: 2.438643\n",
      "Train Epoche: 3 [1498/96 (1560%)]\tLoss: 2.322543\n",
      "Train Epoche: 3 [1499/96 (1561%)]\tLoss: 1.075117\n",
      "Train Epoche: 3 [1500/96 (1562%)]\tLoss: 0.875747\n",
      "Train Epoche: 3 [1501/96 (1564%)]\tLoss: 0.500189\n",
      "Train Epoche: 3 [1502/96 (1565%)]\tLoss: 3.973901\n",
      "Train Epoche: 3 [1503/96 (1566%)]\tLoss: 2.006841\n",
      "Train Epoche: 3 [1504/96 (1567%)]\tLoss: 3.767829\n",
      "Train Epoche: 3 [1505/96 (1568%)]\tLoss: 0.186161\n",
      "Train Epoche: 3 [1506/96 (1569%)]\tLoss: 0.475371\n",
      "Train Epoche: 3 [1507/96 (1570%)]\tLoss: 2.700220\n",
      "Train Epoche: 3 [1508/96 (1571%)]\tLoss: 3.347629\n",
      "Train Epoche: 3 [1509/96 (1572%)]\tLoss: 1.450635\n",
      "Train Epoche: 3 [1510/96 (1573%)]\tLoss: 0.685747\n",
      "Train Epoche: 3 [1511/96 (1574%)]\tLoss: 20.838106\n",
      "Train Epoche: 3 [1512/96 (1575%)]\tLoss: 1.531331\n",
      "Train Epoche: 3 [1513/96 (1576%)]\tLoss: 0.055769\n",
      "Train Epoche: 3 [1514/96 (1577%)]\tLoss: 2.347642\n",
      "Train Epoche: 3 [1515/96 (1578%)]\tLoss: 10.330757\n",
      "Train Epoche: 3 [1516/96 (1579%)]\tLoss: 0.171004\n",
      "Train Epoche: 3 [1517/96 (1580%)]\tLoss: 18.788067\n",
      "Train Epoche: 3 [1518/96 (1581%)]\tLoss: 1.400779\n",
      "Train Epoche: 3 [1519/96 (1582%)]\tLoss: 4.975510\n",
      "Train Epoche: 3 [1520/96 (1583%)]\tLoss: 0.174285\n",
      "Train Epoche: 3 [1521/96 (1584%)]\tLoss: 1.419126\n",
      "Train Epoche: 3 [1522/96 (1585%)]\tLoss: 14.647882\n",
      "Train Epoche: 3 [1523/96 (1586%)]\tLoss: 1.522368\n",
      "Train Epoche: 3 [1524/96 (1588%)]\tLoss: 9.503902\n",
      "Train Epoche: 3 [1525/96 (1589%)]\tLoss: 17.834667\n",
      "Train Epoche: 3 [1526/96 (1590%)]\tLoss: 6.936028\n",
      "Train Epoche: 3 [1527/96 (1591%)]\tLoss: 3.376115\n",
      "Train Epoche: 3 [1528/96 (1592%)]\tLoss: 4.074263\n",
      "Train Epoche: 3 [1529/96 (1593%)]\tLoss: 10.914431\n",
      "Train Epoche: 3 [1530/96 (1594%)]\tLoss: 0.356784\n",
      "Train Epoche: 3 [1531/96 (1595%)]\tLoss: 1.379130\n",
      "Train Epoche: 3 [1532/96 (1596%)]\tLoss: 42.540207\n",
      "Train Epoche: 3 [1533/96 (1597%)]\tLoss: 12.975327\n",
      "Train Epoche: 3 [1534/96 (1598%)]\tLoss: 6.575442\n",
      "Train Epoche: 3 [1535/96 (1599%)]\tLoss: 9.023567\n",
      "Train Epoche: 3 [1536/96 (1600%)]\tLoss: 1.077681\n",
      "Train Epoche: 3 [1537/96 (1601%)]\tLoss: 0.282616\n",
      "Train Epoche: 3 [1538/96 (1602%)]\tLoss: 33.266640\n",
      "Train Epoche: 3 [1539/96 (1603%)]\tLoss: 17.232012\n",
      "Train Epoche: 3 [1540/96 (1604%)]\tLoss: 6.980030\n",
      "Train Epoche: 3 [1541/96 (1605%)]\tLoss: 3.015766\n",
      "Train Epoche: 3 [1542/96 (1606%)]\tLoss: 35.899170\n",
      "Train Epoche: 3 [1543/96 (1607%)]\tLoss: 10.364410\n",
      "Train Epoche: 3 [1544/96 (1608%)]\tLoss: 0.613300\n",
      "Train Epoche: 3 [1545/96 (1609%)]\tLoss: 0.031987\n",
      "Train Epoche: 3 [1546/96 (1610%)]\tLoss: 1.312544\n",
      "Train Epoche: 3 [1547/96 (1611%)]\tLoss: 1.761589\n",
      "Train Epoche: 3 [1548/96 (1612%)]\tLoss: 13.723138\n",
      "Train Epoche: 3 [1549/96 (1614%)]\tLoss: 0.043715\n",
      "Train Epoche: 3 [1550/96 (1615%)]\tLoss: 6.901680\n",
      "Train Epoche: 3 [1551/96 (1616%)]\tLoss: 8.745811\n",
      "Train Epoche: 3 [1552/96 (1617%)]\tLoss: 10.798243\n",
      "Train Epoche: 3 [1553/96 (1618%)]\tLoss: 40.660618\n",
      "Train Epoche: 3 [1554/96 (1619%)]\tLoss: 2.818048\n",
      "Train Epoche: 3 [1555/96 (1620%)]\tLoss: 15.241181\n",
      "Train Epoche: 3 [1556/96 (1621%)]\tLoss: 0.066682\n",
      "Train Epoche: 3 [1557/96 (1622%)]\tLoss: 1.507532\n",
      "Train Epoche: 3 [1558/96 (1623%)]\tLoss: 26.385193\n",
      "Train Epoche: 3 [1559/96 (1624%)]\tLoss: 0.019059\n",
      "Train Epoche: 3 [1560/96 (1625%)]\tLoss: 39.525166\n",
      "Train Epoche: 3 [1561/96 (1626%)]\tLoss: 9.752117\n",
      "Train Epoche: 3 [1562/96 (1627%)]\tLoss: 0.017490\n",
      "Train Epoche: 3 [1563/96 (1628%)]\tLoss: 0.416149\n",
      "Train Epoche: 3 [1564/96 (1629%)]\tLoss: 0.716636\n",
      "Train Epoche: 3 [1565/96 (1630%)]\tLoss: 1.424595\n",
      "Train Epoche: 3 [1566/96 (1631%)]\tLoss: 4.913740\n",
      "Train Epoche: 3 [1567/96 (1632%)]\tLoss: 31.554768\n",
      "Train Epoche: 3 [1568/96 (1633%)]\tLoss: 2.311002\n",
      "Train Epoche: 3 [1569/96 (1634%)]\tLoss: 12.603077\n",
      "Train Epoche: 3 [1570/96 (1635%)]\tLoss: 30.095381\n",
      "Train Epoche: 3 [1571/96 (1636%)]\tLoss: 0.726439\n",
      "Train Epoche: 3 [1572/96 (1638%)]\tLoss: 1.639263\n",
      "Train Epoche: 3 [1573/96 (1639%)]\tLoss: 8.713277\n",
      "Train Epoche: 3 [1574/96 (1640%)]\tLoss: 0.528400\n",
      "Train Epoche: 3 [1575/96 (1641%)]\tLoss: 23.414120\n",
      "Train Epoche: 3 [1576/96 (1642%)]\tLoss: 0.440384\n",
      "Train Epoche: 3 [1577/96 (1643%)]\tLoss: 17.070099\n",
      "Train Epoche: 3 [1578/96 (1644%)]\tLoss: 1.319664\n",
      "Train Epoche: 3 [1579/96 (1645%)]\tLoss: 1.238892\n",
      "Train Epoche: 3 [1580/96 (1646%)]\tLoss: 4.335998\n",
      "Train Epoche: 3 [1581/96 (1647%)]\tLoss: 0.008200\n",
      "Train Epoche: 3 [1582/96 (1648%)]\tLoss: 0.237417\n",
      "Train Epoche: 3 [1583/96 (1649%)]\tLoss: 0.641880\n",
      "Train Epoche: 3 [1584/96 (1650%)]\tLoss: 0.238677\n",
      "Train Epoche: 3 [1585/96 (1651%)]\tLoss: 0.379409\n",
      "Train Epoche: 3 [1586/96 (1652%)]\tLoss: 0.659984\n",
      "Train Epoche: 3 [1587/96 (1653%)]\tLoss: 3.702039\n",
      "Train Epoche: 3 [1588/96 (1654%)]\tLoss: 0.395360\n",
      "Train Epoche: 3 [1589/96 (1655%)]\tLoss: 0.559158\n",
      "Train Epoche: 3 [1590/96 (1656%)]\tLoss: 0.015197\n",
      "Train Epoche: 3 [1591/96 (1657%)]\tLoss: 0.226222\n",
      "Train Epoche: 3 [1592/96 (1658%)]\tLoss: 1.220748\n",
      "Train Epoche: 3 [1593/96 (1659%)]\tLoss: 4.519233\n",
      "Train Epoche: 3 [1594/96 (1660%)]\tLoss: 0.023479\n",
      "Train Epoche: 3 [1595/96 (1661%)]\tLoss: 3.995836\n",
      "Train Epoche: 3 [1596/96 (1662%)]\tLoss: 5.132625\n",
      "Train Epoche: 3 [1597/96 (1664%)]\tLoss: 4.541968\n",
      "Train Epoche: 3 [1598/96 (1665%)]\tLoss: 0.427567\n",
      "Train Epoche: 3 [1599/96 (1666%)]\tLoss: 3.895991\n",
      "Train Epoche: 3 [1600/96 (1667%)]\tLoss: 134.543686\n",
      "Train Epoche: 3 [1601/96 (1668%)]\tLoss: 0.324301\n",
      "Train Epoche: 3 [1602/96 (1669%)]\tLoss: 3.093252\n",
      "Train Epoche: 3 [1603/96 (1670%)]\tLoss: 22.796049\n",
      "Train Epoche: 3 [1604/96 (1671%)]\tLoss: 9.138225\n",
      "Train Epoche: 3 [1605/96 (1672%)]\tLoss: 21.755507\n",
      "Train Epoche: 3 [1606/96 (1673%)]\tLoss: 1.601597\n",
      "Train Epoche: 3 [1607/96 (1674%)]\tLoss: 0.054422\n",
      "Train Epoche: 3 [1608/96 (1675%)]\tLoss: 0.113394\n",
      "Train Epoche: 3 [1609/96 (1676%)]\tLoss: 5.791967\n",
      "Train Epoche: 3 [1610/96 (1677%)]\tLoss: 9.308512\n",
      "Train Epoche: 3 [1611/96 (1678%)]\tLoss: 31.087759\n",
      "Train Epoche: 3 [1612/96 (1679%)]\tLoss: 3.669556\n",
      "Train Epoche: 3 [1613/96 (1680%)]\tLoss: 14.494658\n",
      "Train Epoche: 3 [1614/96 (1681%)]\tLoss: 149.540237\n",
      "Train Epoche: 3 [1615/96 (1682%)]\tLoss: 0.694699\n",
      "Train Epoche: 3 [1616/96 (1683%)]\tLoss: 1.941883\n",
      "Train Epoche: 3 [1617/96 (1684%)]\tLoss: 2.625225\n",
      "Train Epoche: 3 [1618/96 (1685%)]\tLoss: 42.891373\n",
      "Train Epoche: 3 [1619/96 (1686%)]\tLoss: 224.476624\n",
      "Train Epoche: 3 [1620/96 (1688%)]\tLoss: 19.354584\n",
      "Train Epoche: 3 [1621/96 (1689%)]\tLoss: 16.320641\n",
      "Train Epoche: 3 [1622/96 (1690%)]\tLoss: 9.250349\n",
      "Train Epoche: 3 [1623/96 (1691%)]\tLoss: 236.838898\n",
      "Train Epoche: 3 [1624/96 (1692%)]\tLoss: 6.970130\n",
      "Train Epoche: 3 [1625/96 (1693%)]\tLoss: 2.028473\n",
      "Train Epoche: 3 [1626/96 (1694%)]\tLoss: 12.791795\n",
      "Train Epoche: 3 [1627/96 (1695%)]\tLoss: 12.119585\n",
      "Train Epoche: 3 [1628/96 (1696%)]\tLoss: 1.651249\n",
      "Train Epoche: 3 [1629/96 (1697%)]\tLoss: 9.228344\n",
      "Train Epoche: 3 [1630/96 (1698%)]\tLoss: 33.735287\n",
      "Train Epoche: 3 [1631/96 (1699%)]\tLoss: 1.580480\n",
      "Train Epoche: 3 [1632/96 (1700%)]\tLoss: 0.069210\n",
      "Train Epoche: 3 [1633/96 (1701%)]\tLoss: 0.988321\n",
      "Train Epoche: 3 [1634/96 (1702%)]\tLoss: 45.944180\n",
      "Train Epoche: 3 [1635/96 (1703%)]\tLoss: 0.801741\n",
      "Train Epoche: 3 [1636/96 (1704%)]\tLoss: 1.300871\n",
      "Train Epoche: 3 [1637/96 (1705%)]\tLoss: 3.920810\n",
      "Train Epoche: 3 [1638/96 (1706%)]\tLoss: 18.445190\n",
      "Train Epoche: 3 [1639/96 (1707%)]\tLoss: 34.495266\n",
      "Train Epoche: 3 [1640/96 (1708%)]\tLoss: 2.145242\n",
      "Train Epoche: 3 [1641/96 (1709%)]\tLoss: 0.062897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1642/96 (1710%)]\tLoss: 0.874627\n",
      "Train Epoche: 3 [1643/96 (1711%)]\tLoss: 4.383068\n",
      "Train Epoche: 3 [1644/96 (1712%)]\tLoss: 116.897987\n",
      "Train Epoche: 3 [1645/96 (1714%)]\tLoss: 0.050920\n",
      "Train Epoche: 3 [1646/96 (1715%)]\tLoss: 0.076290\n",
      "Train Epoche: 3 [1647/96 (1716%)]\tLoss: 0.161793\n",
      "Train Epoche: 3 [1648/96 (1717%)]\tLoss: 0.002580\n",
      "Train Epoche: 3 [1649/96 (1718%)]\tLoss: 1.466665\n",
      "Train Epoche: 3 [1650/96 (1719%)]\tLoss: 0.118339\n",
      "Train Epoche: 3 [1651/96 (1720%)]\tLoss: 8.351981\n",
      "Train Epoche: 3 [1652/96 (1721%)]\tLoss: 21.024446\n",
      "Train Epoche: 3 [1653/96 (1722%)]\tLoss: 8.582432\n",
      "Train Epoche: 3 [1654/96 (1723%)]\tLoss: 14.927514\n",
      "Train Epoche: 3 [1655/96 (1724%)]\tLoss: 26.160822\n",
      "Train Epoche: 3 [1656/96 (1725%)]\tLoss: 35.181984\n",
      "Train Epoche: 3 [1657/96 (1726%)]\tLoss: 59.405064\n",
      "Train Epoche: 3 [1658/96 (1727%)]\tLoss: 0.810641\n",
      "Train Epoche: 3 [1659/96 (1728%)]\tLoss: 15.398074\n",
      "Train Epoche: 3 [1660/96 (1729%)]\tLoss: 1.704007\n",
      "Train Epoche: 3 [1661/96 (1730%)]\tLoss: 2.120368\n",
      "Train Epoche: 3 [1662/96 (1731%)]\tLoss: 14.048513\n",
      "Train Epoche: 3 [1663/96 (1732%)]\tLoss: 0.061801\n",
      "Train Epoche: 3 [1664/96 (1733%)]\tLoss: 0.463612\n",
      "Train Epoche: 3 [1665/96 (1734%)]\tLoss: 3.217483\n",
      "Train Epoche: 3 [1666/96 (1735%)]\tLoss: 0.323306\n",
      "Train Epoche: 3 [1667/96 (1736%)]\tLoss: 0.096987\n",
      "Train Epoche: 3 [1668/96 (1738%)]\tLoss: 11.020391\n",
      "Train Epoche: 3 [1669/96 (1739%)]\tLoss: 0.082899\n",
      "Train Epoche: 3 [1670/96 (1740%)]\tLoss: 0.918227\n",
      "Train Epoche: 3 [1671/96 (1741%)]\tLoss: 3.791878\n",
      "Train Epoche: 3 [1672/96 (1742%)]\tLoss: 15.246572\n",
      "Train Epoche: 3 [1673/96 (1743%)]\tLoss: 93.931656\n",
      "Train Epoche: 3 [1674/96 (1744%)]\tLoss: 18.488729\n",
      "Train Epoche: 3 [1675/96 (1745%)]\tLoss: 1.803947\n",
      "Train Epoche: 3 [1676/96 (1746%)]\tLoss: 0.016946\n",
      "Train Epoche: 3 [1677/96 (1747%)]\tLoss: 21.111456\n",
      "Train Epoche: 3 [1678/96 (1748%)]\tLoss: 64.931862\n",
      "Train Epoche: 3 [1679/96 (1749%)]\tLoss: 15.352004\n",
      "Train Epoche: 3 [1680/96 (1750%)]\tLoss: 11.757007\n",
      "Train Epoche: 3 [1681/96 (1751%)]\tLoss: 3.442194\n",
      "Train Epoche: 3 [1682/96 (1752%)]\tLoss: 10.276964\n",
      "Train Epoche: 3 [1683/96 (1753%)]\tLoss: 3.749185\n",
      "Train Epoche: 3 [1684/96 (1754%)]\tLoss: 10.680376\n",
      "Train Epoche: 3 [1685/96 (1755%)]\tLoss: 228.492889\n",
      "Train Epoche: 3 [1686/96 (1756%)]\tLoss: 0.555566\n",
      "Train Epoche: 3 [1687/96 (1757%)]\tLoss: 0.546425\n",
      "Train Epoche: 3 [1688/96 (1758%)]\tLoss: 0.326901\n",
      "Train Epoche: 3 [1689/96 (1759%)]\tLoss: 33.769348\n",
      "Train Epoche: 3 [1690/96 (1760%)]\tLoss: 4.781825\n",
      "Train Epoche: 3 [1691/96 (1761%)]\tLoss: 0.151653\n",
      "Train Epoche: 3 [1692/96 (1762%)]\tLoss: 0.035351\n",
      "Train Epoche: 3 [1693/96 (1764%)]\tLoss: 8.257514\n",
      "Train Epoche: 3 [1694/96 (1765%)]\tLoss: 29.953339\n",
      "Train Epoche: 3 [1695/96 (1766%)]\tLoss: 8.273723\n",
      "Train Epoche: 3 [1696/96 (1767%)]\tLoss: 15.201986\n",
      "Train Epoche: 3 [1697/96 (1768%)]\tLoss: 0.870892\n",
      "Train Epoche: 3 [1698/96 (1769%)]\tLoss: 35.602791\n",
      "Train Epoche: 3 [1699/96 (1770%)]\tLoss: 140.572693\n",
      "Train Epoche: 3 [1700/96 (1771%)]\tLoss: 16.901419\n",
      "Train Epoche: 3 [1701/96 (1772%)]\tLoss: 2.539404\n",
      "Train Epoche: 3 [1702/96 (1773%)]\tLoss: 2.546713\n",
      "Train Epoche: 3 [1703/96 (1774%)]\tLoss: 15.500834\n",
      "Train Epoche: 3 [1704/96 (1775%)]\tLoss: 1.775542\n",
      "Train Epoche: 3 [1705/96 (1776%)]\tLoss: 132.688797\n",
      "Train Epoche: 3 [1706/96 (1777%)]\tLoss: 4.661019\n",
      "Train Epoche: 3 [1707/96 (1778%)]\tLoss: 0.476114\n",
      "Train Epoche: 3 [1708/96 (1779%)]\tLoss: 21.161198\n",
      "Train Epoche: 3 [1709/96 (1780%)]\tLoss: 75.887909\n",
      "Train Epoche: 3 [1710/96 (1781%)]\tLoss: 5.938413\n",
      "Train Epoche: 3 [1711/96 (1782%)]\tLoss: 0.135258\n",
      "Train Epoche: 3 [1712/96 (1783%)]\tLoss: 12.488754\n",
      "Train Epoche: 3 [1713/96 (1784%)]\tLoss: 27.588541\n",
      "Train Epoche: 3 [1714/96 (1785%)]\tLoss: 0.236162\n",
      "Train Epoche: 3 [1715/96 (1786%)]\tLoss: 30.865107\n",
      "Train Epoche: 3 [1716/96 (1788%)]\tLoss: 0.005941\n",
      "Train Epoche: 3 [1717/96 (1789%)]\tLoss: 7.398020\n",
      "Train Epoche: 3 [1718/96 (1790%)]\tLoss: 5.372915\n",
      "Train Epoche: 3 [1719/96 (1791%)]\tLoss: 6.343067\n",
      "Train Epoche: 3 [1720/96 (1792%)]\tLoss: 64.235138\n",
      "Train Epoche: 3 [1721/96 (1793%)]\tLoss: 70.431946\n",
      "Train Epoche: 3 [1722/96 (1794%)]\tLoss: 0.197538\n",
      "Train Epoche: 3 [1723/96 (1795%)]\tLoss: 22.420647\n",
      "Train Epoche: 3 [1724/96 (1796%)]\tLoss: 13.994940\n",
      "Train Epoche: 3 [1725/96 (1797%)]\tLoss: 18.032843\n",
      "Train Epoche: 3 [1726/96 (1798%)]\tLoss: 0.530863\n",
      "Train Epoche: 3 [1727/96 (1799%)]\tLoss: 57.441509\n",
      "Train Epoche: 3 [1728/96 (1800%)]\tLoss: 4.016366\n",
      "Train Epoche: 3 [1729/96 (1801%)]\tLoss: 0.734943\n",
      "Train Epoche: 3 [1730/96 (1802%)]\tLoss: 39.931065\n",
      "Train Epoche: 3 [1731/96 (1803%)]\tLoss: 3.645433\n",
      "Train Epoche: 3 [1732/96 (1804%)]\tLoss: 3.482336\n",
      "Train Epoche: 3 [1733/96 (1805%)]\tLoss: 0.082014\n",
      "Train Epoche: 3 [1734/96 (1806%)]\tLoss: 21.571962\n",
      "Train Epoche: 3 [1735/96 (1807%)]\tLoss: 1.431511\n",
      "Train Epoche: 3 [1736/96 (1808%)]\tLoss: 0.005590\n",
      "Train Epoche: 3 [1737/96 (1809%)]\tLoss: 4.373023\n",
      "Train Epoche: 3 [1738/96 (1810%)]\tLoss: 43.812290\n",
      "Train Epoche: 3 [1739/96 (1811%)]\tLoss: 6.512317\n",
      "Train Epoche: 3 [1740/96 (1812%)]\tLoss: 4.709639\n",
      "Train Epoche: 3 [1741/96 (1814%)]\tLoss: 39.380474\n",
      "Train Epoche: 3 [1742/96 (1815%)]\tLoss: 98.316849\n",
      "Train Epoche: 3 [1743/96 (1816%)]\tLoss: 53.467148\n",
      "Train Epoche: 3 [1744/96 (1817%)]\tLoss: 3.849818\n",
      "Train Epoche: 3 [1745/96 (1818%)]\tLoss: 28.234392\n",
      "Train Epoche: 3 [1746/96 (1819%)]\tLoss: 15.127963\n",
      "Train Epoche: 3 [1747/96 (1820%)]\tLoss: 1.320548\n",
      "Train Epoche: 3 [1748/96 (1821%)]\tLoss: 3.820459\n",
      "Train Epoche: 3 [1749/96 (1822%)]\tLoss: 1.943728\n",
      "Train Epoche: 3 [1750/96 (1823%)]\tLoss: 3.368503\n",
      "Train Epoche: 3 [1751/96 (1824%)]\tLoss: 5.773515\n",
      "Train Epoche: 3 [1752/96 (1825%)]\tLoss: 12.345742\n",
      "Train Epoche: 3 [1753/96 (1826%)]\tLoss: 0.094869\n",
      "Train Epoche: 3 [1754/96 (1827%)]\tLoss: 57.503788\n",
      "Train Epoche: 3 [1755/96 (1828%)]\tLoss: 0.093326\n",
      "Train Epoche: 3 [1756/96 (1829%)]\tLoss: 42.747665\n",
      "Train Epoche: 3 [1757/96 (1830%)]\tLoss: 0.370670\n",
      "Train Epoche: 3 [1758/96 (1831%)]\tLoss: 0.405547\n",
      "Train Epoche: 3 [1759/96 (1832%)]\tLoss: 71.550446\n",
      "Train Epoche: 3 [1760/96 (1833%)]\tLoss: 5.152324\n",
      "Train Epoche: 3 [1761/96 (1834%)]\tLoss: 5.337079\n",
      "Train Epoche: 3 [1762/96 (1835%)]\tLoss: 3.055526\n",
      "Train Epoche: 3 [1763/96 (1836%)]\tLoss: 280.286346\n",
      "Train Epoche: 3 [1764/96 (1838%)]\tLoss: 68.358711\n",
      "Train Epoche: 3 [1765/96 (1839%)]\tLoss: 35.238121\n",
      "Train Epoche: 3 [1766/96 (1840%)]\tLoss: 1.804009\n",
      "Train Epoche: 3 [1767/96 (1841%)]\tLoss: 100.404617\n",
      "Train Epoche: 3 [1768/96 (1842%)]\tLoss: 1.353978\n",
      "Train Epoche: 3 [1769/96 (1843%)]\tLoss: 14.436247\n",
      "Train Epoche: 3 [1770/96 (1844%)]\tLoss: 53.428158\n",
      "Train Epoche: 3 [1771/96 (1845%)]\tLoss: 130.796600\n",
      "Train Epoche: 3 [1772/96 (1846%)]\tLoss: 0.678790\n",
      "Train Epoche: 3 [1773/96 (1847%)]\tLoss: 142.846329\n",
      "Train Epoche: 3 [1774/96 (1848%)]\tLoss: 4.624004\n",
      "Train Epoche: 3 [1775/96 (1849%)]\tLoss: 0.033871\n",
      "Train Epoche: 3 [1776/96 (1850%)]\tLoss: 0.748556\n",
      "Train Epoche: 3 [1777/96 (1851%)]\tLoss: 0.005123\n",
      "Train Epoche: 3 [1778/96 (1852%)]\tLoss: 14.059389\n",
      "Train Epoche: 3 [1779/96 (1853%)]\tLoss: 16.685612\n",
      "Train Epoche: 3 [1780/96 (1854%)]\tLoss: 11.850830\n",
      "Train Epoche: 3 [1781/96 (1855%)]\tLoss: 3.464542\n",
      "Train Epoche: 3 [1782/96 (1856%)]\tLoss: 4.804721\n",
      "Train Epoche: 3 [1783/96 (1857%)]\tLoss: 48.406944\n",
      "Train Epoche: 3 [1784/96 (1858%)]\tLoss: 17.426422\n",
      "Train Epoche: 3 [1785/96 (1859%)]\tLoss: 18.009508\n",
      "Train Epoche: 3 [1786/96 (1860%)]\tLoss: 1.844801\n",
      "Train Epoche: 3 [1787/96 (1861%)]\tLoss: 0.632969\n",
      "Train Epoche: 3 [1788/96 (1862%)]\tLoss: 49.317032\n",
      "Train Epoche: 3 [1789/96 (1864%)]\tLoss: 26.363899\n",
      "Train Epoche: 3 [1790/96 (1865%)]\tLoss: 14.450665\n",
      "Train Epoche: 3 [1791/96 (1866%)]\tLoss: 3.103131\n",
      "Train Epoche: 3 [1792/96 (1867%)]\tLoss: 18.193115\n",
      "Train Epoche: 3 [1793/96 (1868%)]\tLoss: 2.670922\n",
      "Train Epoche: 3 [1794/96 (1869%)]\tLoss: 11.907349\n",
      "Train Epoche: 3 [1795/96 (1870%)]\tLoss: 2.877545\n",
      "Train Epoche: 3 [1796/96 (1871%)]\tLoss: 9.398801\n",
      "Train Epoche: 3 [1797/96 (1872%)]\tLoss: 4.025060\n",
      "Train Epoche: 3 [1798/96 (1873%)]\tLoss: 0.541891\n",
      "Train Epoche: 3 [1799/96 (1874%)]\tLoss: 10.377132\n",
      "Train Epoche: 3 [1800/96 (1875%)]\tLoss: 3.415265\n",
      "Train Epoche: 3 [1801/96 (1876%)]\tLoss: 0.027017\n",
      "Train Epoche: 3 [1802/96 (1877%)]\tLoss: 3.561394\n",
      "Train Epoche: 3 [1803/96 (1878%)]\tLoss: 0.393448\n",
      "Train Epoche: 3 [1804/96 (1879%)]\tLoss: 23.690851\n",
      "Train Epoche: 3 [1805/96 (1880%)]\tLoss: 83.560158\n",
      "Train Epoche: 3 [1806/96 (1881%)]\tLoss: 0.052399\n",
      "Train Epoche: 3 [1807/96 (1882%)]\tLoss: 0.239688\n",
      "Train Epoche: 3 [1808/96 (1883%)]\tLoss: 31.616339\n",
      "Train Epoche: 3 [1809/96 (1884%)]\tLoss: 5.563735\n",
      "Train Epoche: 3 [1810/96 (1885%)]\tLoss: 24.532135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1811/96 (1886%)]\tLoss: 39.659546\n",
      "Train Epoche: 3 [1812/96 (1888%)]\tLoss: 4.862849\n",
      "Train Epoche: 3 [1813/96 (1889%)]\tLoss: 34.900238\n",
      "Train Epoche: 3 [1814/96 (1890%)]\tLoss: 0.431199\n",
      "Train Epoche: 3 [1815/96 (1891%)]\tLoss: 0.134397\n",
      "Train Epoche: 3 [1816/96 (1892%)]\tLoss: 13.627706\n",
      "Train Epoche: 3 [1817/96 (1893%)]\tLoss: 13.020453\n",
      "Train Epoche: 3 [1818/96 (1894%)]\tLoss: 26.439548\n",
      "Train Epoche: 3 [1819/96 (1895%)]\tLoss: 1.861980\n",
      "Train Epoche: 3 [1820/96 (1896%)]\tLoss: 138.587402\n",
      "Train Epoche: 3 [1821/96 (1897%)]\tLoss: 2.862270\n",
      "Train Epoche: 3 [1822/96 (1898%)]\tLoss: 1.739558\n",
      "Train Epoche: 3 [1823/96 (1899%)]\tLoss: 0.839424\n",
      "Train Epoche: 3 [1824/96 (1900%)]\tLoss: 22.296461\n",
      "Train Epoche: 3 [1825/96 (1901%)]\tLoss: 37.162434\n",
      "Train Epoche: 3 [1826/96 (1902%)]\tLoss: 6.762035\n",
      "Train Epoche: 3 [1827/96 (1903%)]\tLoss: 6.786082\n",
      "Train Epoche: 3 [1828/96 (1904%)]\tLoss: 0.619784\n",
      "Train Epoche: 3 [1829/96 (1905%)]\tLoss: 8.090066\n",
      "Train Epoche: 3 [1830/96 (1906%)]\tLoss: 5.437754\n",
      "Train Epoche: 3 [1831/96 (1907%)]\tLoss: 28.764296\n",
      "Train Epoche: 3 [1832/96 (1908%)]\tLoss: 8.973829\n",
      "Train Epoche: 3 [1833/96 (1909%)]\tLoss: 17.372154\n",
      "Train Epoche: 3 [1834/96 (1910%)]\tLoss: 10.258638\n",
      "Train Epoche: 3 [1835/96 (1911%)]\tLoss: 0.301372\n",
      "Train Epoche: 3 [1836/96 (1912%)]\tLoss: 3.017500\n",
      "Train Epoche: 3 [1837/96 (1914%)]\tLoss: 12.617131\n",
      "Train Epoche: 3 [1838/96 (1915%)]\tLoss: 1.861540\n",
      "Train Epoche: 3 [1839/96 (1916%)]\tLoss: 13.132668\n",
      "Train Epoche: 3 [1840/96 (1917%)]\tLoss: 5.029781\n",
      "Train Epoche: 3 [1841/96 (1918%)]\tLoss: 72.723885\n",
      "Train Epoche: 3 [1842/96 (1919%)]\tLoss: 3.493697\n",
      "Train Epoche: 3 [1843/96 (1920%)]\tLoss: 0.346860\n",
      "Train Epoche: 3 [1844/96 (1921%)]\tLoss: 21.994961\n",
      "Train Epoche: 3 [1845/96 (1922%)]\tLoss: 19.474789\n",
      "Train Epoche: 3 [1846/96 (1923%)]\tLoss: 2.700329\n",
      "Train Epoche: 3 [1847/96 (1924%)]\tLoss: 0.002228\n",
      "Train Epoche: 3 [1848/96 (1925%)]\tLoss: 0.420424\n",
      "Train Epoche: 3 [1849/96 (1926%)]\tLoss: 3.087490\n",
      "Train Epoche: 3 [1850/96 (1927%)]\tLoss: 1.271338\n",
      "Train Epoche: 3 [1851/96 (1928%)]\tLoss: 2.371833\n",
      "Train Epoche: 3 [1852/96 (1929%)]\tLoss: 6.583250\n",
      "Train Epoche: 3 [1853/96 (1930%)]\tLoss: 0.916384\n",
      "Train Epoche: 3 [1854/96 (1931%)]\tLoss: 126.082687\n",
      "Train Epoche: 3 [1855/96 (1932%)]\tLoss: 14.321965\n",
      "Train Epoche: 3 [1856/96 (1933%)]\tLoss: 6.465342\n",
      "Train Epoche: 3 [1857/96 (1934%)]\tLoss: 0.256382\n",
      "Train Epoche: 3 [1858/96 (1935%)]\tLoss: 0.026273\n",
      "Train Epoche: 3 [1859/96 (1936%)]\tLoss: 0.029838\n",
      "Train Epoche: 3 [1860/96 (1938%)]\tLoss: 30.388201\n",
      "Train Epoche: 3 [1861/96 (1939%)]\tLoss: 1.591135\n",
      "Train Epoche: 3 [1862/96 (1940%)]\tLoss: 0.804187\n",
      "Train Epoche: 3 [1863/96 (1941%)]\tLoss: 44.778797\n",
      "Train Epoche: 3 [1864/96 (1942%)]\tLoss: 0.875666\n",
      "Train Epoche: 3 [1865/96 (1943%)]\tLoss: 4.824715\n",
      "Train Epoche: 3 [1866/96 (1944%)]\tLoss: 1.042616\n",
      "Train Epoche: 3 [1867/96 (1945%)]\tLoss: 4.822000\n",
      "Train Epoche: 3 [1868/96 (1946%)]\tLoss: 1.034895\n",
      "Train Epoche: 3 [1869/96 (1947%)]\tLoss: 1.410190\n",
      "Train Epoche: 3 [1870/96 (1948%)]\tLoss: 0.483617\n",
      "Train Epoche: 3 [1871/96 (1949%)]\tLoss: 0.096475\n",
      "Train Epoche: 3 [1872/96 (1950%)]\tLoss: 5.333049\n",
      "Train Epoche: 3 [1873/96 (1951%)]\tLoss: 3.353959\n",
      "Train Epoche: 3 [1874/96 (1952%)]\tLoss: 0.110497\n",
      "Train Epoche: 3 [1875/96 (1953%)]\tLoss: 2.818762\n",
      "Train Epoche: 3 [1876/96 (1954%)]\tLoss: 5.207265\n",
      "Train Epoche: 3 [1877/96 (1955%)]\tLoss: 2.022382\n",
      "Train Epoche: 3 [1878/96 (1956%)]\tLoss: 0.000965\n",
      "Train Epoche: 3 [1879/96 (1957%)]\tLoss: 45.983284\n",
      "Train Epoche: 3 [1880/96 (1958%)]\tLoss: 4.004850\n",
      "Train Epoche: 3 [1881/96 (1959%)]\tLoss: 6.550712\n",
      "Train Epoche: 3 [1882/96 (1960%)]\tLoss: 1.821760\n",
      "Train Epoche: 3 [1883/96 (1961%)]\tLoss: 9.165855\n",
      "Train Epoche: 3 [1884/96 (1962%)]\tLoss: 235.125168\n",
      "Train Epoche: 3 [1885/96 (1964%)]\tLoss: 1.122753\n",
      "Train Epoche: 3 [1886/96 (1965%)]\tLoss: 2.965388\n",
      "Train Epoche: 3 [1887/96 (1966%)]\tLoss: 3.376871\n",
      "Train Epoche: 3 [1888/96 (1967%)]\tLoss: 4.573057\n",
      "Train Epoche: 3 [1889/96 (1968%)]\tLoss: 1.083089\n",
      "Train Epoche: 3 [1890/96 (1969%)]\tLoss: 4.397764\n",
      "Train Epoche: 3 [1891/96 (1970%)]\tLoss: 0.731178\n",
      "Train Epoche: 3 [1892/96 (1971%)]\tLoss: 0.103448\n",
      "Train Epoche: 3 [1893/96 (1972%)]\tLoss: 17.184998\n",
      "Train Epoche: 3 [1894/96 (1973%)]\tLoss: 3.992195\n",
      "Train Epoche: 3 [1895/96 (1974%)]\tLoss: 2.073354\n",
      "Train Epoche: 3 [1896/96 (1975%)]\tLoss: 62.331665\n",
      "Train Epoche: 3 [1897/96 (1976%)]\tLoss: 0.286494\n",
      "Train Epoche: 3 [1898/96 (1977%)]\tLoss: 0.033612\n",
      "Train Epoche: 3 [1899/96 (1978%)]\tLoss: 3.088953\n",
      "Train Epoche: 3 [1900/96 (1979%)]\tLoss: 3.765290\n",
      "Train Epoche: 3 [1901/96 (1980%)]\tLoss: 37.956997\n",
      "Train Epoche: 3 [1902/96 (1981%)]\tLoss: 0.045308\n",
      "Train Epoche: 3 [1903/96 (1982%)]\tLoss: 19.778070\n",
      "Train Epoche: 3 [1904/96 (1983%)]\tLoss: 3.630107\n",
      "Train Epoche: 3 [1905/96 (1984%)]\tLoss: 16.928162\n",
      "Train Epoche: 3 [1906/96 (1985%)]\tLoss: 40.952732\n",
      "Train Epoche: 3 [1907/96 (1986%)]\tLoss: 20.208925\n",
      "Train Epoche: 3 [1908/96 (1988%)]\tLoss: 4.146930\n",
      "Train Epoche: 3 [1909/96 (1989%)]\tLoss: 0.231058\n",
      "Train Epoche: 3 [1910/96 (1990%)]\tLoss: 41.067978\n",
      "Train Epoche: 3 [1911/96 (1991%)]\tLoss: 10.908924\n",
      "Train Epoche: 3 [1912/96 (1992%)]\tLoss: 7.835906\n",
      "Train Epoche: 3 [1913/96 (1993%)]\tLoss: 21.186632\n",
      "Train Epoche: 3 [1914/96 (1994%)]\tLoss: 0.083917\n",
      "Train Epoche: 3 [1915/96 (1995%)]\tLoss: 1.589252\n",
      "Train Epoche: 3 [1916/96 (1996%)]\tLoss: 3.101116\n",
      "Train Epoche: 3 [1917/96 (1997%)]\tLoss: 2.355525\n",
      "Train Epoche: 3 [1918/96 (1998%)]\tLoss: 1.955143\n",
      "Train Epoche: 3 [1919/96 (1999%)]\tLoss: 3.111460\n",
      "Train Epoche: 3 [1920/96 (2000%)]\tLoss: 21.079273\n",
      "Train Epoche: 3 [1921/96 (2001%)]\tLoss: 0.309238\n",
      "Train Epoche: 3 [1922/96 (2002%)]\tLoss: 0.335306\n",
      "Train Epoche: 3 [1923/96 (2003%)]\tLoss: 3.415989\n",
      "Train Epoche: 3 [1924/96 (2004%)]\tLoss: 175.226974\n",
      "Train Epoche: 3 [1925/96 (2005%)]\tLoss: 2.744263\n",
      "Train Epoche: 3 [1926/96 (2006%)]\tLoss: 1.975718\n",
      "Train Epoche: 3 [1927/96 (2007%)]\tLoss: 0.264605\n",
      "Train Epoche: 3 [1928/96 (2008%)]\tLoss: 0.278004\n",
      "Train Epoche: 3 [1929/96 (2009%)]\tLoss: 8.586846\n",
      "Train Epoche: 3 [1930/96 (2010%)]\tLoss: 1.703397\n",
      "Train Epoche: 3 [1931/96 (2011%)]\tLoss: 0.238124\n",
      "Train Epoche: 3 [1932/96 (2012%)]\tLoss: 13.566278\n",
      "Train Epoche: 3 [1933/96 (2014%)]\tLoss: 9.077845\n",
      "Train Epoche: 3 [1934/96 (2015%)]\tLoss: 3.657603\n",
      "Train Epoche: 3 [1935/96 (2016%)]\tLoss: 51.622162\n",
      "Train Epoche: 3 [1936/96 (2017%)]\tLoss: 8.667440\n",
      "Train Epoche: 3 [1937/96 (2018%)]\tLoss: 118.740311\n",
      "Train Epoche: 3 [1938/96 (2019%)]\tLoss: 1.660517\n",
      "Train Epoche: 3 [1939/96 (2020%)]\tLoss: 6.703843\n",
      "Train Epoche: 3 [1940/96 (2021%)]\tLoss: 27.791262\n",
      "Train Epoche: 3 [1941/96 (2022%)]\tLoss: 19.860826\n",
      "Train Epoche: 3 [1942/96 (2023%)]\tLoss: 0.553918\n",
      "Train Epoche: 3 [1943/96 (2024%)]\tLoss: 3.118057\n",
      "Train Epoche: 3 [1944/96 (2025%)]\tLoss: 0.021801\n",
      "Train Epoche: 3 [1945/96 (2026%)]\tLoss: 0.461988\n",
      "Train Epoche: 3 [1946/96 (2027%)]\tLoss: 0.685470\n",
      "Train Epoche: 3 [1947/96 (2028%)]\tLoss: 1.444265\n",
      "Train Epoche: 3 [1948/96 (2029%)]\tLoss: 23.291586\n",
      "Train Epoche: 3 [1949/96 (2030%)]\tLoss: 0.278899\n",
      "Train Epoche: 3 [1950/96 (2031%)]\tLoss: 5.328927\n",
      "Train Epoche: 3 [1951/96 (2032%)]\tLoss: 7.436806\n",
      "Train Epoche: 3 [1952/96 (2033%)]\tLoss: 4.706622\n",
      "Train Epoche: 3 [1953/96 (2034%)]\tLoss: 0.641269\n",
      "Train Epoche: 3 [1954/96 (2035%)]\tLoss: 0.240955\n",
      "Train Epoche: 3 [1955/96 (2036%)]\tLoss: 92.470154\n",
      "Train Epoche: 3 [1956/96 (2038%)]\tLoss: 19.148386\n",
      "Train Epoche: 3 [1957/96 (2039%)]\tLoss: 30.756926\n",
      "Train Epoche: 3 [1958/96 (2040%)]\tLoss: 2.980748\n",
      "Train Epoche: 3 [1959/96 (2041%)]\tLoss: 0.098790\n",
      "Train Epoche: 3 [1960/96 (2042%)]\tLoss: 0.021033\n",
      "Train Epoche: 3 [1961/96 (2043%)]\tLoss: 0.000742\n",
      "Train Epoche: 3 [1962/96 (2044%)]\tLoss: 4.525337\n",
      "Train Epoche: 3 [1963/96 (2045%)]\tLoss: 15.171043\n",
      "Train Epoche: 3 [1964/96 (2046%)]\tLoss: 0.002266\n",
      "Train Epoche: 3 [1965/96 (2047%)]\tLoss: 1.222217\n",
      "Train Epoche: 3 [1966/96 (2048%)]\tLoss: 5.145252\n",
      "Train Epoche: 3 [1967/96 (2049%)]\tLoss: 53.391903\n",
      "Train Epoche: 3 [1968/96 (2050%)]\tLoss: 1.534318\n",
      "Train Epoche: 3 [1969/96 (2051%)]\tLoss: 0.292429\n",
      "Train Epoche: 3 [1970/96 (2052%)]\tLoss: 2.155890\n",
      "Train Epoche: 3 [1971/96 (2053%)]\tLoss: 3.061735\n",
      "Train Epoche: 3 [1972/96 (2054%)]\tLoss: 3.492585\n",
      "Train Epoche: 3 [1973/96 (2055%)]\tLoss: 0.370518\n",
      "Train Epoche: 3 [1974/96 (2056%)]\tLoss: 33.824989\n",
      "Train Epoche: 3 [1975/96 (2057%)]\tLoss: 0.317221\n",
      "Train Epoche: 3 [1976/96 (2058%)]\tLoss: 0.575256\n",
      "Train Epoche: 3 [1977/96 (2059%)]\tLoss: 17.922575\n",
      "Train Epoche: 3 [1978/96 (2060%)]\tLoss: 3.949806\n",
      "Train Epoche: 3 [1979/96 (2061%)]\tLoss: 1.079274\n",
      "Train Epoche: 3 [1980/96 (2062%)]\tLoss: 0.739988\n",
      "Train Epoche: 3 [1981/96 (2064%)]\tLoss: 3.300780\n",
      "Train Epoche: 3 [1982/96 (2065%)]\tLoss: 24.708117\n",
      "Train Epoche: 3 [1983/96 (2066%)]\tLoss: 31.778147\n",
      "Train Epoche: 3 [1984/96 (2067%)]\tLoss: 0.910601\n",
      "Train Epoche: 3 [1985/96 (2068%)]\tLoss: 15.883620\n",
      "Train Epoche: 3 [1986/96 (2069%)]\tLoss: 15.711903\n",
      "Train Epoche: 3 [1987/96 (2070%)]\tLoss: 12.314023\n",
      "Train Epoche: 3 [1988/96 (2071%)]\tLoss: 1.375937\n",
      "Train Epoche: 3 [1989/96 (2072%)]\tLoss: 63.045971\n",
      "Train Epoche: 3 [1990/96 (2073%)]\tLoss: 2.590266\n",
      "Train Epoche: 3 [1991/96 (2074%)]\tLoss: 5.180150\n",
      "Train Epoche: 3 [1992/96 (2075%)]\tLoss: 12.113869\n",
      "Train Epoche: 3 [1993/96 (2076%)]\tLoss: 0.763452\n",
      "Train Epoche: 3 [1994/96 (2077%)]\tLoss: 10.117112\n",
      "Train Epoche: 3 [1995/96 (2078%)]\tLoss: 0.568728\n",
      "Train Epoche: 3 [1996/96 (2079%)]\tLoss: 6.856806\n",
      "Train Epoche: 3 [1997/96 (2080%)]\tLoss: 0.412732\n",
      "Train Epoche: 3 [1998/96 (2081%)]\tLoss: 0.940502\n",
      "Train Epoche: 3 [1999/96 (2082%)]\tLoss: 11.477818\n",
      "Train Epoche: 3 [2000/96 (2083%)]\tLoss: 0.522370\n",
      "Train Epoche: 3 [2001/96 (2084%)]\tLoss: 8.021498\n",
      "Train Epoche: 3 [2002/96 (2085%)]\tLoss: 6.139542\n",
      "Train Epoche: 3 [2003/96 (2086%)]\tLoss: 8.331813\n",
      "Train Epoche: 3 [2004/96 (2088%)]\tLoss: 15.155840\n",
      "Train Epoche: 3 [2005/96 (2089%)]\tLoss: 9.344917\n",
      "Train Epoche: 3 [2006/96 (2090%)]\tLoss: 2.470196\n",
      "Train Epoche: 3 [2007/96 (2091%)]\tLoss: 7.749490\n",
      "Train Epoche: 3 [2008/96 (2092%)]\tLoss: 32.282188\n",
      "Train Epoche: 3 [2009/96 (2093%)]\tLoss: 1.426737\n",
      "Train Epoche: 3 [2010/96 (2094%)]\tLoss: 3.232280\n",
      "Train Epoche: 3 [2011/96 (2095%)]\tLoss: 28.670530\n",
      "Train Epoche: 3 [2012/96 (2096%)]\tLoss: 2.249462\n",
      "Train Epoche: 3 [2013/96 (2097%)]\tLoss: 2.812755\n",
      "Train Epoche: 3 [2014/96 (2098%)]\tLoss: 24.457756\n",
      "Train Epoche: 3 [2015/96 (2099%)]\tLoss: 0.624361\n",
      "Train Epoche: 3 [2016/96 (2100%)]\tLoss: 0.241894\n",
      "Train Epoche: 3 [2017/96 (2101%)]\tLoss: 0.629836\n",
      "Train Epoche: 3 [2018/96 (2102%)]\tLoss: 19.633438\n",
      "Train Epoche: 3 [2019/96 (2103%)]\tLoss: 10.533132\n",
      "Train Epoche: 3 [2020/96 (2104%)]\tLoss: 33.822216\n",
      "Train Epoche: 3 [2021/96 (2105%)]\tLoss: 0.657207\n",
      "Train Epoche: 3 [2022/96 (2106%)]\tLoss: 18.184980\n",
      "Train Epoche: 3 [2023/96 (2107%)]\tLoss: 3.625075\n",
      "Train Epoche: 3 [2024/96 (2108%)]\tLoss: 4.516492\n",
      "Train Epoche: 3 [2025/96 (2109%)]\tLoss: 0.158616\n",
      "Train Epoche: 3 [2026/96 (2110%)]\tLoss: 20.657000\n",
      "Train Epoche: 1 [0/96 (0%)]\tLoss: 9.400014\n",
      "Train Epoche: 1 [1/96 (1%)]\tLoss: 1.034238\n",
      "Train Epoche: 1 [2/96 (2%)]\tLoss: 80.877357\n",
      "Train Epoche: 1 [3/96 (3%)]\tLoss: 525.226196\n",
      "Train Epoche: 1 [4/96 (4%)]\tLoss: 194.074829\n",
      "Train Epoche: 1 [5/96 (5%)]\tLoss: 352.876648\n",
      "Train Epoche: 1 [6/96 (6%)]\tLoss: 164.309433\n",
      "Train Epoche: 1 [7/96 (7%)]\tLoss: 14.165568\n",
      "Train Epoche: 1 [8/96 (8%)]\tLoss: 2.999698\n",
      "Train Epoche: 1 [9/96 (9%)]\tLoss: 21.174461\n",
      "Train Epoche: 1 [10/96 (10%)]\tLoss: 568.613831\n",
      "Train Epoche: 1 [11/96 (11%)]\tLoss: 417.817719\n",
      "Train Epoche: 1 [12/96 (12%)]\tLoss: 455.445984\n",
      "Train Epoche: 1 [13/96 (14%)]\tLoss: 28.562851\n",
      "Train Epoche: 1 [14/96 (15%)]\tLoss: 89.224358\n",
      "Train Epoche: 1 [15/96 (16%)]\tLoss: 199.189789\n",
      "Train Epoche: 1 [16/96 (17%)]\tLoss: 300.523285\n",
      "Train Epoche: 1 [17/96 (18%)]\tLoss: 35.208694\n",
      "Train Epoche: 1 [18/96 (19%)]\tLoss: 46.309296\n",
      "Train Epoche: 1 [19/96 (20%)]\tLoss: 109.539879\n",
      "Train Epoche: 1 [20/96 (21%)]\tLoss: 97.839722\n",
      "Train Epoche: 1 [21/96 (22%)]\tLoss: 239.140152\n",
      "Train Epoche: 1 [22/96 (23%)]\tLoss: 202.305786\n",
      "Train Epoche: 1 [23/96 (24%)]\tLoss: 331.646729\n",
      "Train Epoche: 1 [24/96 (25%)]\tLoss: 286.989929\n",
      "Train Epoche: 1 [25/96 (26%)]\tLoss: 0.500196\n",
      "Train Epoche: 1 [26/96 (27%)]\tLoss: 312.879089\n",
      "Train Epoche: 1 [27/96 (28%)]\tLoss: 0.742903\n",
      "Train Epoche: 1 [28/96 (29%)]\tLoss: 231.164764\n",
      "Train Epoche: 1 [29/96 (30%)]\tLoss: 237.124100\n",
      "Train Epoche: 1 [30/96 (31%)]\tLoss: 10.018556\n",
      "Train Epoche: 1 [31/96 (32%)]\tLoss: 35.724651\n",
      "Train Epoche: 1 [32/96 (33%)]\tLoss: 240.718109\n",
      "Train Epoche: 1 [33/96 (34%)]\tLoss: 13.643186\n",
      "Train Epoche: 1 [34/96 (35%)]\tLoss: 0.006833\n",
      "Train Epoche: 1 [35/96 (36%)]\tLoss: 21.018387\n",
      "Train Epoche: 1 [36/96 (38%)]\tLoss: 21.191698\n",
      "Train Epoche: 1 [37/96 (39%)]\tLoss: 10.670730\n",
      "Train Epoche: 1 [38/96 (40%)]\tLoss: 0.870634\n",
      "Train Epoche: 1 [39/96 (41%)]\tLoss: 15.320020\n",
      "Train Epoche: 1 [40/96 (42%)]\tLoss: 8.413068\n",
      "Train Epoche: 1 [41/96 (43%)]\tLoss: 6.073283\n",
      "Train Epoche: 1 [42/96 (44%)]\tLoss: 2.334754\n",
      "Train Epoche: 1 [43/96 (45%)]\tLoss: 1.176584\n",
      "Train Epoche: 1 [44/96 (46%)]\tLoss: 226.159866\n",
      "Train Epoche: 1 [45/96 (47%)]\tLoss: 23.150394\n",
      "Train Epoche: 1 [46/96 (48%)]\tLoss: 11.697471\n",
      "Train Epoche: 1 [47/96 (49%)]\tLoss: 19.761423\n",
      "Train Epoche: 1 [48/96 (50%)]\tLoss: 13.663222\n",
      "Train Epoche: 1 [49/96 (51%)]\tLoss: 4.905972\n",
      "Train Epoche: 1 [50/96 (52%)]\tLoss: 291.225769\n",
      "Train Epoche: 1 [51/96 (53%)]\tLoss: 6.774287\n",
      "Train Epoche: 1 [52/96 (54%)]\tLoss: 12.242704\n",
      "Train Epoche: 1 [53/96 (55%)]\tLoss: 53.618355\n",
      "Train Epoche: 1 [54/96 (56%)]\tLoss: 78.168335\n",
      "Train Epoche: 1 [55/96 (57%)]\tLoss: 21.984343\n",
      "Train Epoche: 1 [56/96 (58%)]\tLoss: 131.241623\n",
      "Train Epoche: 1 [57/96 (59%)]\tLoss: 0.072482\n",
      "Train Epoche: 1 [58/96 (60%)]\tLoss: 5.431869\n",
      "Train Epoche: 1 [59/96 (61%)]\tLoss: 87.532043\n",
      "Train Epoche: 1 [60/96 (62%)]\tLoss: 70.245216\n",
      "Train Epoche: 1 [61/96 (64%)]\tLoss: 23.079323\n",
      "Train Epoche: 1 [62/96 (65%)]\tLoss: 11.603182\n",
      "Train Epoche: 1 [63/96 (66%)]\tLoss: 0.136712\n",
      "Train Epoche: 1 [64/96 (67%)]\tLoss: 0.665006\n",
      "Train Epoche: 1 [65/96 (68%)]\tLoss: 58.994389\n",
      "Train Epoche: 1 [66/96 (69%)]\tLoss: 56.161343\n",
      "Train Epoche: 1 [67/96 (70%)]\tLoss: 9.898330\n",
      "Train Epoche: 1 [68/96 (71%)]\tLoss: 62.086212\n",
      "Train Epoche: 1 [69/96 (72%)]\tLoss: 24.665112\n",
      "Train Epoche: 1 [70/96 (73%)]\tLoss: 172.697250\n",
      "Train Epoche: 1 [71/96 (74%)]\tLoss: 34.419369\n",
      "Train Epoche: 1 [72/96 (75%)]\tLoss: 6.810052\n",
      "Train Epoche: 1 [73/96 (76%)]\tLoss: 28.603048\n",
      "Train Epoche: 1 [74/96 (77%)]\tLoss: 6.631985\n",
      "Train Epoche: 1 [75/96 (78%)]\tLoss: 0.052577\n",
      "Train Epoche: 1 [76/96 (79%)]\tLoss: 1.986174\n",
      "Train Epoche: 1 [77/96 (80%)]\tLoss: 43.690922\n",
      "Train Epoche: 1 [78/96 (81%)]\tLoss: 0.402173\n",
      "Train Epoche: 1 [79/96 (82%)]\tLoss: 15.640576\n",
      "Train Epoche: 1 [80/96 (83%)]\tLoss: 1.825958\n",
      "Train Epoche: 1 [81/96 (84%)]\tLoss: 5.069631\n",
      "Train Epoche: 1 [82/96 (85%)]\tLoss: 39.664665\n",
      "Train Epoche: 1 [83/96 (86%)]\tLoss: 144.697739\n",
      "Train Epoche: 1 [84/96 (88%)]\tLoss: 80.237518\n",
      "Train Epoche: 1 [85/96 (89%)]\tLoss: 0.315334\n",
      "Train Epoche: 1 [86/96 (90%)]\tLoss: 0.898586\n",
      "Train Epoche: 1 [87/96 (91%)]\tLoss: 98.876373\n",
      "Train Epoche: 1 [88/96 (92%)]\tLoss: 36.071308\n",
      "Train Epoche: 1 [89/96 (93%)]\tLoss: 88.330338\n",
      "Train Epoche: 1 [90/96 (94%)]\tLoss: 128.216156\n",
      "Train Epoche: 1 [91/96 (95%)]\tLoss: 105.420715\n",
      "Train Epoche: 1 [92/96 (96%)]\tLoss: 1.057337\n",
      "Train Epoche: 1 [93/96 (97%)]\tLoss: 0.150348\n",
      "Train Epoche: 1 [94/96 (98%)]\tLoss: 6.903839\n",
      "Train Epoche: 1 [95/96 (99%)]\tLoss: 236.579788\n",
      "Train Epoche: 1 [96/96 (100%)]\tLoss: 193.391403\n",
      "Train Epoche: 1 [97/96 (101%)]\tLoss: 258.720581\n",
      "Train Epoche: 1 [98/96 (102%)]\tLoss: 1.555161\n",
      "Train Epoche: 1 [99/96 (103%)]\tLoss: 194.561951\n",
      "Train Epoche: 1 [100/96 (104%)]\tLoss: 60.102921\n",
      "Train Epoche: 1 [101/96 (105%)]\tLoss: 1.345920\n",
      "Train Epoche: 1 [102/96 (106%)]\tLoss: 16.941763\n",
      "Train Epoche: 1 [103/96 (107%)]\tLoss: 67.446152\n",
      "Train Epoche: 1 [104/96 (108%)]\tLoss: 10.550360\n",
      "Train Epoche: 1 [105/96 (109%)]\tLoss: 6.379452\n",
      "Train Epoche: 1 [106/96 (110%)]\tLoss: 0.074516\n",
      "Train Epoche: 1 [107/96 (111%)]\tLoss: 1.610061\n",
      "Train Epoche: 1 [108/96 (112%)]\tLoss: 65.011810\n",
      "Train Epoche: 1 [109/96 (114%)]\tLoss: 50.065502\n",
      "Train Epoche: 1 [110/96 (115%)]\tLoss: 26.063810\n",
      "Train Epoche: 1 [111/96 (116%)]\tLoss: 1.528672\n",
      "Train Epoche: 1 [112/96 (117%)]\tLoss: 6.294377\n",
      "Train Epoche: 1 [113/96 (118%)]\tLoss: 17.683027\n",
      "Train Epoche: 1 [114/96 (119%)]\tLoss: 25.794754\n",
      "Train Epoche: 1 [115/96 (120%)]\tLoss: 47.566708\n",
      "Train Epoche: 1 [116/96 (121%)]\tLoss: 15.717203\n",
      "Train Epoche: 1 [117/96 (122%)]\tLoss: 3.742577\n",
      "Train Epoche: 1 [118/96 (123%)]\tLoss: 18.927814\n",
      "Train Epoche: 1 [119/96 (124%)]\tLoss: 22.690252\n",
      "Train Epoche: 1 [120/96 (125%)]\tLoss: 1.567773\n",
      "Train Epoche: 1 [121/96 (126%)]\tLoss: 135.320358\n",
      "Train Epoche: 1 [122/96 (127%)]\tLoss: 50.712708\n",
      "Train Epoche: 1 [123/96 (128%)]\tLoss: 178.563141\n",
      "Train Epoche: 1 [124/96 (129%)]\tLoss: 3.492917\n",
      "Train Epoche: 1 [125/96 (130%)]\tLoss: 81.755539\n",
      "Train Epoche: 1 [126/96 (131%)]\tLoss: 4.356217\n",
      "Train Epoche: 1 [127/96 (132%)]\tLoss: 4.778733\n",
      "Train Epoche: 1 [128/96 (133%)]\tLoss: 119.700768\n",
      "Train Epoche: 1 [129/96 (134%)]\tLoss: 6.602232\n",
      "Train Epoche: 1 [130/96 (135%)]\tLoss: 50.516327\n",
      "Train Epoche: 1 [131/96 (136%)]\tLoss: 70.206169\n",
      "Train Epoche: 1 [132/96 (138%)]\tLoss: 67.495049\n",
      "Train Epoche: 1 [133/96 (139%)]\tLoss: 18.369341\n",
      "Train Epoche: 1 [134/96 (140%)]\tLoss: 76.644135\n",
      "Train Epoche: 1 [135/96 (141%)]\tLoss: 30.438301\n",
      "Train Epoche: 1 [136/96 (142%)]\tLoss: 115.823334\n",
      "Train Epoche: 1 [137/96 (143%)]\tLoss: 62.453865\n",
      "Train Epoche: 1 [138/96 (144%)]\tLoss: 67.572121\n",
      "Train Epoche: 1 [139/96 (145%)]\tLoss: 23.477345\n",
      "Train Epoche: 1 [140/96 (146%)]\tLoss: 109.941597\n",
      "Train Epoche: 1 [141/96 (147%)]\tLoss: 41.374134\n",
      "Train Epoche: 1 [142/96 (148%)]\tLoss: 61.413799\n",
      "Train Epoche: 1 [143/96 (149%)]\tLoss: 43.108185\n",
      "Train Epoche: 1 [144/96 (150%)]\tLoss: 27.911530\n",
      "Train Epoche: 1 [145/96 (151%)]\tLoss: 9.506189\n",
      "Train Epoche: 1 [146/96 (152%)]\tLoss: 7.103709\n",
      "Train Epoche: 1 [147/96 (153%)]\tLoss: 18.638329\n",
      "Train Epoche: 1 [148/96 (154%)]\tLoss: 286.310028\n",
      "Train Epoche: 1 [149/96 (155%)]\tLoss: 11.673304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [150/96 (156%)]\tLoss: 1.066975\n",
      "Train Epoche: 1 [151/96 (157%)]\tLoss: 7.191176\n",
      "Train Epoche: 1 [152/96 (158%)]\tLoss: 24.812906\n",
      "Train Epoche: 1 [153/96 (159%)]\tLoss: 3.826661\n",
      "Train Epoche: 1 [154/96 (160%)]\tLoss: 33.817570\n",
      "Train Epoche: 1 [155/96 (161%)]\tLoss: 13.011044\n",
      "Train Epoche: 1 [156/96 (162%)]\tLoss: 145.211746\n",
      "Train Epoche: 1 [157/96 (164%)]\tLoss: 166.791931\n",
      "Train Epoche: 1 [158/96 (165%)]\tLoss: 0.379081\n",
      "Train Epoche: 1 [159/96 (166%)]\tLoss: 208.965378\n",
      "Train Epoche: 1 [160/96 (167%)]\tLoss: 11.122000\n",
      "Train Epoche: 1 [161/96 (168%)]\tLoss: 33.025555\n",
      "Train Epoche: 1 [162/96 (169%)]\tLoss: 17.926104\n",
      "Train Epoche: 1 [163/96 (170%)]\tLoss: 1.729703\n",
      "Train Epoche: 1 [164/96 (171%)]\tLoss: 2.490229\n",
      "Train Epoche: 1 [165/96 (172%)]\tLoss: 0.869464\n",
      "Train Epoche: 1 [166/96 (173%)]\tLoss: 52.725563\n",
      "Train Epoche: 1 [167/96 (174%)]\tLoss: 62.818008\n",
      "Train Epoche: 1 [168/96 (175%)]\tLoss: 18.448935\n",
      "Train Epoche: 1 [169/96 (176%)]\tLoss: 42.118053\n",
      "Train Epoche: 1 [170/96 (177%)]\tLoss: 17.721291\n",
      "Train Epoche: 1 [171/96 (178%)]\tLoss: 31.718393\n",
      "Train Epoche: 1 [172/96 (179%)]\tLoss: 21.446581\n",
      "Train Epoche: 1 [173/96 (180%)]\tLoss: 28.574951\n",
      "Train Epoche: 1 [174/96 (181%)]\tLoss: 24.534222\n",
      "Train Epoche: 1 [175/96 (182%)]\tLoss: 0.742736\n",
      "Train Epoche: 1 [176/96 (183%)]\tLoss: 0.053943\n",
      "Train Epoche: 1 [177/96 (184%)]\tLoss: 0.033357\n",
      "Train Epoche: 1 [178/96 (185%)]\tLoss: 18.392069\n",
      "Train Epoche: 1 [179/96 (186%)]\tLoss: 0.311489\n",
      "Train Epoche: 1 [180/96 (188%)]\tLoss: 14.817774\n",
      "Train Epoche: 1 [181/96 (189%)]\tLoss: 0.046973\n",
      "Train Epoche: 1 [182/96 (190%)]\tLoss: 2.664567\n",
      "Train Epoche: 1 [183/96 (191%)]\tLoss: 10.997918\n",
      "Train Epoche: 1 [184/96 (192%)]\tLoss: 10.458055\n",
      "Train Epoche: 1 [185/96 (193%)]\tLoss: 0.149147\n",
      "Train Epoche: 1 [186/96 (194%)]\tLoss: 11.912082\n",
      "Train Epoche: 1 [187/96 (195%)]\tLoss: 0.282499\n",
      "Train Epoche: 1 [188/96 (196%)]\tLoss: 31.497461\n",
      "Train Epoche: 1 [189/96 (197%)]\tLoss: 1.548489\n",
      "Train Epoche: 1 [190/96 (198%)]\tLoss: 1.319086\n",
      "Train Epoche: 1 [191/96 (199%)]\tLoss: 28.625832\n",
      "Train Epoche: 1 [192/96 (200%)]\tLoss: 9.391875\n",
      "Train Epoche: 1 [193/96 (201%)]\tLoss: 0.007406\n",
      "Train Epoche: 1 [194/96 (202%)]\tLoss: 12.010000\n",
      "Train Epoche: 1 [195/96 (203%)]\tLoss: 1.438803\n",
      "Train Epoche: 1 [196/96 (204%)]\tLoss: 15.688697\n",
      "Train Epoche: 1 [197/96 (205%)]\tLoss: 31.829540\n",
      "Train Epoche: 1 [198/96 (206%)]\tLoss: 49.136948\n",
      "Train Epoche: 1 [199/96 (207%)]\tLoss: 5.048048\n",
      "Train Epoche: 1 [200/96 (208%)]\tLoss: 4.102972\n",
      "Train Epoche: 1 [201/96 (209%)]\tLoss: 3.506233\n",
      "Train Epoche: 1 [202/96 (210%)]\tLoss: 4.181682\n",
      "Train Epoche: 1 [203/96 (211%)]\tLoss: 13.217769\n",
      "Train Epoche: 1 [204/96 (212%)]\tLoss: 0.008701\n",
      "Train Epoche: 1 [205/96 (214%)]\tLoss: 2.218744\n",
      "Train Epoche: 1 [206/96 (215%)]\tLoss: 0.187815\n",
      "Train Epoche: 1 [207/96 (216%)]\tLoss: 0.071887\n",
      "Train Epoche: 1 [208/96 (217%)]\tLoss: 154.061401\n",
      "Train Epoche: 1 [209/96 (218%)]\tLoss: 215.989243\n",
      "Train Epoche: 1 [210/96 (219%)]\tLoss: 1.489225\n",
      "Train Epoche: 1 [211/96 (220%)]\tLoss: 9.727146\n",
      "Train Epoche: 1 [212/96 (221%)]\tLoss: 0.338270\n",
      "Train Epoche: 1 [213/96 (222%)]\tLoss: 29.920235\n",
      "Train Epoche: 1 [214/96 (223%)]\tLoss: 0.000001\n",
      "Train Epoche: 1 [215/96 (224%)]\tLoss: 0.141805\n",
      "Train Epoche: 1 [216/96 (225%)]\tLoss: 0.046024\n",
      "Train Epoche: 1 [217/96 (226%)]\tLoss: 23.505684\n",
      "Train Epoche: 1 [218/96 (227%)]\tLoss: 1.044248\n",
      "Train Epoche: 1 [219/96 (228%)]\tLoss: 4.954895\n",
      "Train Epoche: 1 [220/96 (229%)]\tLoss: 11.944436\n",
      "Train Epoche: 1 [221/96 (230%)]\tLoss: 31.680464\n",
      "Train Epoche: 1 [222/96 (231%)]\tLoss: 16.326231\n",
      "Train Epoche: 1 [223/96 (232%)]\tLoss: 16.915348\n",
      "Train Epoche: 1 [224/96 (233%)]\tLoss: 52.831028\n",
      "Train Epoche: 1 [225/96 (234%)]\tLoss: 5.417549\n",
      "Train Epoche: 1 [226/96 (235%)]\tLoss: 7.772743\n",
      "Train Epoche: 1 [227/96 (236%)]\tLoss: 1.235640\n",
      "Train Epoche: 1 [228/96 (238%)]\tLoss: 1.951533\n",
      "Train Epoche: 1 [229/96 (239%)]\tLoss: 2.917310\n",
      "Train Epoche: 1 [230/96 (240%)]\tLoss: 14.911299\n",
      "Train Epoche: 1 [231/96 (241%)]\tLoss: 237.666565\n",
      "Train Epoche: 1 [232/96 (242%)]\tLoss: 0.089621\n",
      "Train Epoche: 1 [233/96 (243%)]\tLoss: 0.909367\n",
      "Train Epoche: 1 [234/96 (244%)]\tLoss: 35.193207\n",
      "Train Epoche: 1 [235/96 (245%)]\tLoss: 109.732269\n",
      "Train Epoche: 1 [236/96 (246%)]\tLoss: 172.659012\n",
      "Train Epoche: 1 [237/96 (247%)]\tLoss: 52.709679\n",
      "Train Epoche: 1 [238/96 (248%)]\tLoss: 24.679417\n",
      "Train Epoche: 1 [239/96 (249%)]\tLoss: 38.847004\n",
      "Train Epoche: 1 [240/96 (250%)]\tLoss: 18.495981\n",
      "Train Epoche: 1 [241/96 (251%)]\tLoss: 0.955241\n",
      "Train Epoche: 1 [242/96 (252%)]\tLoss: 19.710691\n",
      "Train Epoche: 1 [243/96 (253%)]\tLoss: 0.064397\n",
      "Train Epoche: 1 [244/96 (254%)]\tLoss: 11.855791\n",
      "Train Epoche: 1 [245/96 (255%)]\tLoss: 0.268288\n",
      "Train Epoche: 1 [246/96 (256%)]\tLoss: 30.323530\n",
      "Train Epoche: 1 [247/96 (257%)]\tLoss: 1.718993\n",
      "Train Epoche: 1 [248/96 (258%)]\tLoss: 0.230921\n",
      "Train Epoche: 1 [249/96 (259%)]\tLoss: 55.875973\n",
      "Train Epoche: 1 [250/96 (260%)]\tLoss: 30.866320\n",
      "Train Epoche: 1 [251/96 (261%)]\tLoss: 0.654125\n",
      "Train Epoche: 1 [252/96 (262%)]\tLoss: 0.206899\n",
      "Train Epoche: 1 [253/96 (264%)]\tLoss: 5.005136\n",
      "Train Epoche: 1 [254/96 (265%)]\tLoss: 81.146873\n",
      "Train Epoche: 1 [255/96 (266%)]\tLoss: 0.012880\n",
      "Train Epoche: 1 [256/96 (267%)]\tLoss: 0.576295\n",
      "Train Epoche: 1 [257/96 (268%)]\tLoss: 1.190661\n",
      "Train Epoche: 1 [258/96 (269%)]\tLoss: 15.699249\n",
      "Train Epoche: 1 [259/96 (270%)]\tLoss: 1.983055\n",
      "Train Epoche: 1 [260/96 (271%)]\tLoss: 20.660849\n",
      "Train Epoche: 1 [261/96 (272%)]\tLoss: 1.616854\n",
      "Train Epoche: 1 [262/96 (273%)]\tLoss: 0.398589\n",
      "Train Epoche: 1 [263/96 (274%)]\tLoss: 0.697131\n",
      "Train Epoche: 1 [264/96 (275%)]\tLoss: 1.429607\n",
      "Train Epoche: 1 [265/96 (276%)]\tLoss: 28.202354\n",
      "Train Epoche: 1 [266/96 (277%)]\tLoss: 9.623920\n",
      "Train Epoche: 1 [267/96 (278%)]\tLoss: 166.276489\n",
      "Train Epoche: 1 [268/96 (279%)]\tLoss: 0.089671\n",
      "Train Epoche: 1 [269/96 (280%)]\tLoss: 0.037720\n",
      "Train Epoche: 1 [270/96 (281%)]\tLoss: 18.479357\n",
      "Train Epoche: 1 [271/96 (282%)]\tLoss: 0.528762\n",
      "Train Epoche: 1 [272/96 (283%)]\tLoss: 14.003097\n",
      "Train Epoche: 1 [273/96 (284%)]\tLoss: 10.334828\n",
      "Train Epoche: 1 [274/96 (285%)]\tLoss: 1.923942\n",
      "Train Epoche: 1 [275/96 (286%)]\tLoss: 1.023691\n",
      "Train Epoche: 1 [276/96 (288%)]\tLoss: 1.437421\n",
      "Train Epoche: 1 [277/96 (289%)]\tLoss: 0.011949\n",
      "Train Epoche: 1 [278/96 (290%)]\tLoss: 5.339024\n",
      "Train Epoche: 1 [279/96 (291%)]\tLoss: 0.791858\n",
      "Train Epoche: 1 [280/96 (292%)]\tLoss: 10.802869\n",
      "Train Epoche: 1 [281/96 (293%)]\tLoss: 16.293470\n",
      "Train Epoche: 1 [282/96 (294%)]\tLoss: 6.503766\n",
      "Train Epoche: 1 [283/96 (295%)]\tLoss: 16.795280\n",
      "Train Epoche: 1 [284/96 (296%)]\tLoss: 0.404731\n",
      "Train Epoche: 1 [285/96 (297%)]\tLoss: 10.497487\n",
      "Train Epoche: 1 [286/96 (298%)]\tLoss: 3.110712\n",
      "Train Epoche: 1 [287/96 (299%)]\tLoss: 0.174063\n",
      "Train Epoche: 1 [288/96 (300%)]\tLoss: 1.862311\n",
      "Train Epoche: 1 [289/96 (301%)]\tLoss: 8.079745\n",
      "Train Epoche: 1 [290/96 (302%)]\tLoss: 0.739270\n",
      "Train Epoche: 1 [291/96 (303%)]\tLoss: 7.910011\n",
      "Train Epoche: 1 [292/96 (304%)]\tLoss: 1.648050\n",
      "Train Epoche: 1 [293/96 (305%)]\tLoss: 6.699893\n",
      "Train Epoche: 1 [294/96 (306%)]\tLoss: 11.371794\n",
      "Train Epoche: 1 [295/96 (307%)]\tLoss: 4.221428\n",
      "Train Epoche: 1 [296/96 (308%)]\tLoss: 0.671307\n",
      "Train Epoche: 1 [297/96 (309%)]\tLoss: 3.721953\n",
      "Train Epoche: 1 [298/96 (310%)]\tLoss: 0.839829\n",
      "Train Epoche: 1 [299/96 (311%)]\tLoss: 13.873553\n",
      "Train Epoche: 1 [300/96 (312%)]\tLoss: 0.069222\n",
      "Train Epoche: 1 [301/96 (314%)]\tLoss: 127.043587\n",
      "Train Epoche: 1 [302/96 (315%)]\tLoss: 36.480183\n",
      "Train Epoche: 1 [303/96 (316%)]\tLoss: 10.566653\n",
      "Train Epoche: 1 [304/96 (317%)]\tLoss: 4.240950\n",
      "Train Epoche: 1 [305/96 (318%)]\tLoss: 0.102820\n",
      "Train Epoche: 1 [306/96 (319%)]\tLoss: 35.278881\n",
      "Train Epoche: 1 [307/96 (320%)]\tLoss: 12.092847\n",
      "Train Epoche: 1 [308/96 (321%)]\tLoss: 66.880096\n",
      "Train Epoche: 1 [309/96 (322%)]\tLoss: 10.235757\n",
      "Train Epoche: 1 [310/96 (323%)]\tLoss: 0.772754\n",
      "Train Epoche: 1 [311/96 (324%)]\tLoss: 0.111646\n",
      "Train Epoche: 1 [312/96 (325%)]\tLoss: 5.837528\n",
      "Train Epoche: 1 [313/96 (326%)]\tLoss: 0.695500\n",
      "Train Epoche: 1 [314/96 (327%)]\tLoss: 110.460289\n",
      "Train Epoche: 1 [315/96 (328%)]\tLoss: 6.627142\n",
      "Train Epoche: 1 [316/96 (329%)]\tLoss: 0.192040\n",
      "Train Epoche: 1 [317/96 (330%)]\tLoss: 2.502684\n",
      "Train Epoche: 1 [318/96 (331%)]\tLoss: 14.759396\n",
      "Train Epoche: 1 [319/96 (332%)]\tLoss: 0.329209\n",
      "Train Epoche: 1 [320/96 (333%)]\tLoss: 0.000399\n",
      "Train Epoche: 1 [321/96 (334%)]\tLoss: 0.703285\n",
      "Train Epoche: 1 [322/96 (335%)]\tLoss: 1.703125\n",
      "Train Epoche: 1 [323/96 (336%)]\tLoss: 0.026150\n",
      "Train Epoche: 1 [324/96 (338%)]\tLoss: 0.003900\n",
      "Train Epoche: 1 [325/96 (339%)]\tLoss: 0.070712\n",
      "Train Epoche: 1 [326/96 (340%)]\tLoss: 0.000126\n",
      "Train Epoche: 1 [327/96 (341%)]\tLoss: 15.344562\n",
      "Train Epoche: 1 [328/96 (342%)]\tLoss: 0.004004\n",
      "Train Epoche: 1 [329/96 (343%)]\tLoss: 12.151278\n",
      "Train Epoche: 1 [330/96 (344%)]\tLoss: 53.835373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [331/96 (345%)]\tLoss: 45.398472\n",
      "Train Epoche: 1 [332/96 (346%)]\tLoss: 0.389940\n",
      "Train Epoche: 1 [333/96 (347%)]\tLoss: 3.040217\n",
      "Train Epoche: 1 [334/96 (348%)]\tLoss: 1.005220\n",
      "Train Epoche: 1 [335/96 (349%)]\tLoss: 9.974524\n",
      "Train Epoche: 1 [336/96 (350%)]\tLoss: 29.788454\n",
      "Train Epoche: 1 [337/96 (351%)]\tLoss: 0.001880\n",
      "Train Epoche: 1 [338/96 (352%)]\tLoss: 0.384455\n",
      "Train Epoche: 1 [339/96 (353%)]\tLoss: 0.133717\n",
      "Train Epoche: 1 [340/96 (354%)]\tLoss: 6.556596\n",
      "Train Epoche: 1 [341/96 (355%)]\tLoss: 5.699787\n",
      "Train Epoche: 1 [342/96 (356%)]\tLoss: 4.998594\n",
      "Train Epoche: 1 [343/96 (357%)]\tLoss: 0.273878\n",
      "Train Epoche: 1 [344/96 (358%)]\tLoss: 173.690659\n",
      "Train Epoche: 1 [345/96 (359%)]\tLoss: 357.823303\n",
      "Train Epoche: 1 [346/96 (360%)]\tLoss: 1.408477\n",
      "Train Epoche: 1 [347/96 (361%)]\tLoss: 2.552646\n",
      "Train Epoche: 1 [348/96 (362%)]\tLoss: 1.322688\n",
      "Train Epoche: 1 [349/96 (364%)]\tLoss: 1.759028\n",
      "Train Epoche: 1 [350/96 (365%)]\tLoss: 0.270876\n",
      "Train Epoche: 1 [351/96 (366%)]\tLoss: 22.802580\n",
      "Train Epoche: 1 [352/96 (367%)]\tLoss: 110.720520\n",
      "Train Epoche: 1 [353/96 (368%)]\tLoss: 14.169117\n",
      "Train Epoche: 1 [354/96 (369%)]\tLoss: 6.490318\n",
      "Train Epoche: 1 [355/96 (370%)]\tLoss: 6.033671\n",
      "Train Epoche: 1 [356/96 (371%)]\tLoss: 174.755753\n",
      "Train Epoche: 1 [357/96 (372%)]\tLoss: 22.549112\n",
      "Train Epoche: 1 [358/96 (373%)]\tLoss: 40.405197\n",
      "Train Epoche: 1 [359/96 (374%)]\tLoss: 1.097013\n",
      "Train Epoche: 1 [360/96 (375%)]\tLoss: 9.360159\n",
      "Train Epoche: 1 [361/96 (376%)]\tLoss: 0.007121\n",
      "Train Epoche: 1 [362/96 (377%)]\tLoss: 14.693133\n",
      "Train Epoche: 1 [363/96 (378%)]\tLoss: 9.497662\n",
      "Train Epoche: 1 [364/96 (379%)]\tLoss: 118.293381\n",
      "Train Epoche: 1 [365/96 (380%)]\tLoss: 13.317847\n",
      "Train Epoche: 1 [366/96 (381%)]\tLoss: 0.000644\n",
      "Train Epoche: 1 [367/96 (382%)]\tLoss: 50.485462\n",
      "Train Epoche: 1 [368/96 (383%)]\tLoss: 5.178388\n",
      "Train Epoche: 1 [369/96 (384%)]\tLoss: 3.723392\n",
      "Train Epoche: 1 [370/96 (385%)]\tLoss: 7.399522\n",
      "Train Epoche: 1 [371/96 (386%)]\tLoss: 0.136234\n",
      "Train Epoche: 1 [372/96 (388%)]\tLoss: 44.125237\n",
      "Train Epoche: 1 [373/96 (389%)]\tLoss: 2.276370\n",
      "Train Epoche: 1 [374/96 (390%)]\tLoss: 2.488980\n",
      "Train Epoche: 1 [375/96 (391%)]\tLoss: 0.169076\n",
      "Train Epoche: 1 [376/96 (392%)]\tLoss: 78.846764\n",
      "Train Epoche: 1 [377/96 (393%)]\tLoss: 1.017078\n",
      "Train Epoche: 1 [378/96 (394%)]\tLoss: 0.006305\n",
      "Train Epoche: 1 [379/96 (395%)]\tLoss: 0.775220\n",
      "Train Epoche: 1 [380/96 (396%)]\tLoss: 0.530328\n",
      "Train Epoche: 1 [381/96 (397%)]\tLoss: 25.405869\n",
      "Train Epoche: 1 [382/96 (398%)]\tLoss: 0.018721\n",
      "Train Epoche: 1 [383/96 (399%)]\tLoss: 1.959727\n",
      "Train Epoche: 1 [384/96 (400%)]\tLoss: 0.394950\n",
      "Train Epoche: 1 [385/96 (401%)]\tLoss: 8.012246\n",
      "Train Epoche: 1 [386/96 (402%)]\tLoss: 10.608480\n",
      "Train Epoche: 1 [387/96 (403%)]\tLoss: 51.201542\n",
      "Train Epoche: 1 [388/96 (404%)]\tLoss: 0.022197\n",
      "Train Epoche: 1 [389/96 (405%)]\tLoss: 0.120084\n",
      "Train Epoche: 1 [390/96 (406%)]\tLoss: 2.819704\n",
      "Train Epoche: 1 [391/96 (407%)]\tLoss: 46.714020\n",
      "Train Epoche: 1 [392/96 (408%)]\tLoss: 2.494598\n",
      "Train Epoche: 1 [393/96 (409%)]\tLoss: 212.214310\n",
      "Train Epoche: 1 [394/96 (410%)]\tLoss: 17.153913\n",
      "Train Epoche: 1 [395/96 (411%)]\tLoss: 13.254477\n",
      "Train Epoche: 1 [396/96 (412%)]\tLoss: 0.225426\n",
      "Train Epoche: 1 [397/96 (414%)]\tLoss: 5.238720\n",
      "Train Epoche: 1 [398/96 (415%)]\tLoss: 3.791784\n",
      "Train Epoche: 1 [399/96 (416%)]\tLoss: 0.218036\n",
      "Train Epoche: 1 [400/96 (417%)]\tLoss: 29.588854\n",
      "Train Epoche: 1 [401/96 (418%)]\tLoss: 3.149985\n",
      "Train Epoche: 1 [402/96 (419%)]\tLoss: 11.818101\n",
      "Train Epoche: 1 [403/96 (420%)]\tLoss: 2.543771\n",
      "Train Epoche: 1 [404/96 (421%)]\tLoss: 0.990859\n",
      "Train Epoche: 1 [405/96 (422%)]\tLoss: 3.111652\n",
      "Train Epoche: 1 [406/96 (423%)]\tLoss: 11.153403\n",
      "Train Epoche: 1 [407/96 (424%)]\tLoss: 0.203888\n",
      "Train Epoche: 1 [408/96 (425%)]\tLoss: 3.992268\n",
      "Train Epoche: 1 [409/96 (426%)]\tLoss: 7.118271\n",
      "Train Epoche: 1 [410/96 (427%)]\tLoss: 45.002411\n",
      "Train Epoche: 1 [411/96 (428%)]\tLoss: 10.152558\n",
      "Train Epoche: 1 [412/96 (429%)]\tLoss: 21.283262\n",
      "Train Epoche: 1 [413/96 (430%)]\tLoss: 0.467201\n",
      "Train Epoche: 1 [414/96 (431%)]\tLoss: 11.797675\n",
      "Train Epoche: 1 [415/96 (432%)]\tLoss: 78.628281\n",
      "Train Epoche: 1 [416/96 (433%)]\tLoss: 10.340421\n",
      "Train Epoche: 1 [417/96 (434%)]\tLoss: 3.152088\n",
      "Train Epoche: 1 [418/96 (435%)]\tLoss: 0.047309\n",
      "Train Epoche: 1 [419/96 (436%)]\tLoss: 1.565908\n",
      "Train Epoche: 1 [420/96 (438%)]\tLoss: 4.105488\n",
      "Train Epoche: 1 [421/96 (439%)]\tLoss: 5.461451\n",
      "Train Epoche: 1 [422/96 (440%)]\tLoss: 25.509554\n",
      "Train Epoche: 1 [423/96 (441%)]\tLoss: 118.265335\n",
      "Train Epoche: 1 [424/96 (442%)]\tLoss: 0.230242\n",
      "Train Epoche: 1 [425/96 (443%)]\tLoss: 60.621784\n",
      "Train Epoche: 1 [426/96 (444%)]\tLoss: 2.417746\n",
      "Train Epoche: 1 [427/96 (445%)]\tLoss: 42.501003\n",
      "Train Epoche: 1 [428/96 (446%)]\tLoss: 17.560284\n",
      "Train Epoche: 1 [429/96 (447%)]\tLoss: 4.827604\n",
      "Train Epoche: 1 [430/96 (448%)]\tLoss: 49.344551\n",
      "Train Epoche: 1 [431/96 (449%)]\tLoss: 1.042414\n",
      "Train Epoche: 1 [432/96 (450%)]\tLoss: 1.627334\n",
      "Train Epoche: 1 [433/96 (451%)]\tLoss: 1.507107\n",
      "Train Epoche: 1 [434/96 (452%)]\tLoss: 11.634305\n",
      "Train Epoche: 1 [435/96 (453%)]\tLoss: 15.395110\n",
      "Train Epoche: 1 [436/96 (454%)]\tLoss: 9.969068\n",
      "Train Epoche: 1 [437/96 (455%)]\tLoss: 15.710822\n",
      "Train Epoche: 1 [438/96 (456%)]\tLoss: 0.005059\n",
      "Train Epoche: 1 [439/96 (457%)]\tLoss: 3.877462\n",
      "Train Epoche: 1 [440/96 (458%)]\tLoss: 2.209027\n",
      "Train Epoche: 1 [441/96 (459%)]\tLoss: 0.973580\n",
      "Train Epoche: 1 [442/96 (460%)]\tLoss: 0.001970\n",
      "Train Epoche: 1 [443/96 (461%)]\tLoss: 0.211856\n",
      "Train Epoche: 1 [444/96 (462%)]\tLoss: 55.787868\n",
      "Train Epoche: 1 [445/96 (464%)]\tLoss: 5.179005\n",
      "Train Epoche: 1 [446/96 (465%)]\tLoss: 200.379913\n",
      "Train Epoche: 1 [447/96 (466%)]\tLoss: 8.463077\n",
      "Train Epoche: 1 [448/96 (467%)]\tLoss: 22.671440\n",
      "Train Epoche: 1 [449/96 (468%)]\tLoss: 0.152474\n",
      "Train Epoche: 1 [450/96 (469%)]\tLoss: 35.160698\n",
      "Train Epoche: 1 [451/96 (470%)]\tLoss: 3.041528\n",
      "Train Epoche: 1 [452/96 (471%)]\tLoss: 2.023986\n",
      "Train Epoche: 1 [453/96 (472%)]\tLoss: 13.300326\n",
      "Train Epoche: 1 [454/96 (473%)]\tLoss: 21.993601\n",
      "Train Epoche: 1 [455/96 (474%)]\tLoss: 8.149541\n",
      "Train Epoche: 1 [456/96 (475%)]\tLoss: 81.375839\n",
      "Train Epoche: 1 [457/96 (476%)]\tLoss: 3.613855\n",
      "Train Epoche: 1 [458/96 (477%)]\tLoss: 27.205120\n",
      "Train Epoche: 1 [459/96 (478%)]\tLoss: 0.001089\n",
      "Train Epoche: 1 [460/96 (479%)]\tLoss: 177.828201\n",
      "Train Epoche: 1 [461/96 (480%)]\tLoss: 2.585895\n",
      "Train Epoche: 1 [462/96 (481%)]\tLoss: 2.690452\n",
      "Train Epoche: 1 [463/96 (482%)]\tLoss: 1.294865\n",
      "Train Epoche: 1 [464/96 (483%)]\tLoss: 293.888916\n",
      "Train Epoche: 1 [465/96 (484%)]\tLoss: 24.553455\n",
      "Train Epoche: 1 [466/96 (485%)]\tLoss: 6.139960\n",
      "Train Epoche: 1 [467/96 (486%)]\tLoss: 189.869751\n",
      "Train Epoche: 1 [468/96 (488%)]\tLoss: 0.094985\n",
      "Train Epoche: 1 [469/96 (489%)]\tLoss: 47.576771\n",
      "Train Epoche: 1 [470/96 (490%)]\tLoss: 13.645138\n",
      "Train Epoche: 1 [471/96 (491%)]\tLoss: 113.051590\n",
      "Train Epoche: 1 [472/96 (492%)]\tLoss: 0.033534\n",
      "Train Epoche: 1 [473/96 (493%)]\tLoss: 0.633813\n",
      "Train Epoche: 1 [474/96 (494%)]\tLoss: 0.022933\n",
      "Train Epoche: 1 [475/96 (495%)]\tLoss: 0.769271\n",
      "Train Epoche: 1 [476/96 (496%)]\tLoss: 18.975292\n",
      "Train Epoche: 1 [477/96 (497%)]\tLoss: 27.478729\n",
      "Train Epoche: 1 [478/96 (498%)]\tLoss: 10.861959\n",
      "Train Epoche: 1 [479/96 (499%)]\tLoss: 71.431885\n",
      "Train Epoche: 1 [480/96 (500%)]\tLoss: 91.458008\n",
      "Train Epoche: 1 [481/96 (501%)]\tLoss: 9.860381\n",
      "Train Epoche: 1 [482/96 (502%)]\tLoss: 7.340219\n",
      "Train Epoche: 1 [483/96 (503%)]\tLoss: 39.493599\n",
      "Train Epoche: 1 [484/96 (504%)]\tLoss: 13.165971\n",
      "Train Epoche: 1 [485/96 (505%)]\tLoss: 2.741705\n",
      "Train Epoche: 1 [486/96 (506%)]\tLoss: 0.897006\n",
      "Train Epoche: 1 [487/96 (507%)]\tLoss: 17.686926\n",
      "Train Epoche: 1 [488/96 (508%)]\tLoss: 2.782370\n",
      "Train Epoche: 1 [489/96 (509%)]\tLoss: 3.801164\n",
      "Train Epoche: 1 [490/96 (510%)]\tLoss: 0.879700\n",
      "Train Epoche: 1 [491/96 (511%)]\tLoss: 0.079171\n",
      "Train Epoche: 1 [492/96 (512%)]\tLoss: 33.213581\n",
      "Train Epoche: 1 [493/96 (514%)]\tLoss: 45.122444\n",
      "Train Epoche: 1 [494/96 (515%)]\tLoss: 0.112287\n",
      "Train Epoche: 1 [495/96 (516%)]\tLoss: 22.018295\n",
      "Train Epoche: 1 [496/96 (517%)]\tLoss: 13.379561\n",
      "Train Epoche: 1 [497/96 (518%)]\tLoss: 24.568216\n",
      "Train Epoche: 1 [498/96 (519%)]\tLoss: 3.387787\n",
      "Train Epoche: 1 [499/96 (520%)]\tLoss: 0.554370\n",
      "Train Epoche: 1 [500/96 (521%)]\tLoss: 5.605234\n",
      "Train Epoche: 1 [501/96 (522%)]\tLoss: 2.528792\n",
      "Train Epoche: 1 [502/96 (523%)]\tLoss: 17.175226\n",
      "Train Epoche: 1 [503/96 (524%)]\tLoss: 0.946497\n",
      "Train Epoche: 1 [504/96 (525%)]\tLoss: 5.007944\n",
      "Train Epoche: 1 [505/96 (526%)]\tLoss: 145.460678\n",
      "Train Epoche: 1 [506/96 (527%)]\tLoss: 0.212183\n",
      "Train Epoche: 1 [507/96 (528%)]\tLoss: 24.848677\n",
      "Train Epoche: 1 [508/96 (529%)]\tLoss: 1.940998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [509/96 (530%)]\tLoss: 3.123839\n",
      "Train Epoche: 1 [510/96 (531%)]\tLoss: 0.353771\n",
      "Train Epoche: 1 [511/96 (532%)]\tLoss: 0.154812\n",
      "Train Epoche: 1 [512/96 (533%)]\tLoss: 3.117735\n",
      "Train Epoche: 1 [513/96 (534%)]\tLoss: 4.848870\n",
      "Train Epoche: 1 [514/96 (535%)]\tLoss: 0.030399\n",
      "Train Epoche: 1 [515/96 (536%)]\tLoss: 1.738351\n",
      "Train Epoche: 1 [516/96 (538%)]\tLoss: 0.002884\n",
      "Train Epoche: 1 [517/96 (539%)]\tLoss: 7.271569\n",
      "Train Epoche: 1 [518/96 (540%)]\tLoss: 8.760364\n",
      "Train Epoche: 1 [519/96 (541%)]\tLoss: 12.289893\n",
      "Train Epoche: 1 [520/96 (542%)]\tLoss: 45.462440\n",
      "Train Epoche: 1 [521/96 (543%)]\tLoss: 228.206512\n",
      "Train Epoche: 1 [522/96 (544%)]\tLoss: 1.080099\n",
      "Train Epoche: 1 [523/96 (545%)]\tLoss: 45.798592\n",
      "Train Epoche: 1 [524/96 (546%)]\tLoss: 35.431229\n",
      "Train Epoche: 1 [525/96 (547%)]\tLoss: 3.263106\n",
      "Train Epoche: 1 [526/96 (548%)]\tLoss: 3.136014\n",
      "Train Epoche: 1 [527/96 (549%)]\tLoss: 0.138515\n",
      "Train Epoche: 1 [528/96 (550%)]\tLoss: 6.411128\n",
      "Train Epoche: 1 [529/96 (551%)]\tLoss: 0.562371\n",
      "Train Epoche: 1 [530/96 (552%)]\tLoss: 77.397491\n",
      "Train Epoche: 1 [531/96 (553%)]\tLoss: 0.402754\n",
      "Train Epoche: 1 [532/96 (554%)]\tLoss: 0.059101\n",
      "Train Epoche: 1 [533/96 (555%)]\tLoss: 57.465405\n",
      "Train Epoche: 1 [534/96 (556%)]\tLoss: 1.157916\n",
      "Train Epoche: 1 [535/96 (557%)]\tLoss: 0.508626\n",
      "Train Epoche: 1 [536/96 (558%)]\tLoss: 64.103401\n",
      "Train Epoche: 1 [537/96 (559%)]\tLoss: 44.506943\n",
      "Train Epoche: 1 [538/96 (560%)]\tLoss: 1.055880\n",
      "Train Epoche: 1 [539/96 (561%)]\tLoss: 12.442127\n",
      "Train Epoche: 1 [540/96 (562%)]\tLoss: 2.746533\n",
      "Train Epoche: 1 [541/96 (564%)]\tLoss: 1.484217\n",
      "Train Epoche: 1 [542/96 (565%)]\tLoss: 0.848362\n",
      "Train Epoche: 1 [543/96 (566%)]\tLoss: 34.539677\n",
      "Train Epoche: 1 [544/96 (567%)]\tLoss: 0.122931\n",
      "Train Epoche: 1 [545/96 (568%)]\tLoss: 0.380884\n",
      "Train Epoche: 1 [546/96 (569%)]\tLoss: 1.329391\n",
      "Train Epoche: 1 [547/96 (570%)]\tLoss: 0.159919\n",
      "Train Epoche: 1 [548/96 (571%)]\tLoss: 0.401000\n",
      "Train Epoche: 1 [549/96 (572%)]\tLoss: 3.443095\n",
      "Train Epoche: 1 [550/96 (573%)]\tLoss: 3.232230\n",
      "Train Epoche: 1 [551/96 (574%)]\tLoss: 29.020130\n",
      "Train Epoche: 1 [552/96 (575%)]\tLoss: 13.262540\n",
      "Train Epoche: 1 [553/96 (576%)]\tLoss: 287.483215\n",
      "Train Epoche: 1 [554/96 (577%)]\tLoss: 22.744717\n",
      "Train Epoche: 1 [555/96 (578%)]\tLoss: 0.063498\n",
      "Train Epoche: 1 [556/96 (579%)]\tLoss: 0.106220\n",
      "Train Epoche: 1 [557/96 (580%)]\tLoss: 6.889308\n",
      "Train Epoche: 1 [558/96 (581%)]\tLoss: 4.957907\n",
      "Train Epoche: 1 [559/96 (582%)]\tLoss: 3.316010\n",
      "Train Epoche: 1 [560/96 (583%)]\tLoss: 3.281327\n",
      "Train Epoche: 1 [561/96 (584%)]\tLoss: 1.041094\n",
      "Train Epoche: 1 [562/96 (585%)]\tLoss: 25.211014\n",
      "Train Epoche: 1 [563/96 (586%)]\tLoss: 0.313194\n",
      "Train Epoche: 1 [564/96 (588%)]\tLoss: 4.034096\n",
      "Train Epoche: 1 [565/96 (589%)]\tLoss: 11.758217\n",
      "Train Epoche: 1 [566/96 (590%)]\tLoss: 4.553162\n",
      "Train Epoche: 1 [567/96 (591%)]\tLoss: 3.815487\n",
      "Train Epoche: 1 [568/96 (592%)]\tLoss: 14.506694\n",
      "Train Epoche: 1 [569/96 (593%)]\tLoss: 87.860977\n",
      "Train Epoche: 1 [570/96 (594%)]\tLoss: 75.734261\n",
      "Train Epoche: 1 [571/96 (595%)]\tLoss: 11.462659\n",
      "Train Epoche: 1 [572/96 (596%)]\tLoss: 1.476740\n",
      "Train Epoche: 1 [573/96 (597%)]\tLoss: 2.249807\n",
      "Train Epoche: 1 [574/96 (598%)]\tLoss: 0.243584\n",
      "Train Epoche: 1 [575/96 (599%)]\tLoss: 4.210806\n",
      "Train Epoche: 1 [576/96 (600%)]\tLoss: 51.231632\n",
      "Train Epoche: 1 [577/96 (601%)]\tLoss: 4.757482\n",
      "Train Epoche: 1 [578/96 (602%)]\tLoss: 29.391920\n",
      "Train Epoche: 1 [579/96 (603%)]\tLoss: 0.466136\n",
      "Train Epoche: 1 [580/96 (604%)]\tLoss: 0.172906\n",
      "Train Epoche: 1 [581/96 (605%)]\tLoss: 0.241957\n",
      "Train Epoche: 1 [582/96 (606%)]\tLoss: 4.344340\n",
      "Train Epoche: 1 [583/96 (607%)]\tLoss: 7.272515\n",
      "Train Epoche: 1 [584/96 (608%)]\tLoss: 45.934036\n",
      "Train Epoche: 1 [585/96 (609%)]\tLoss: 32.591000\n",
      "Train Epoche: 1 [586/96 (610%)]\tLoss: 18.552120\n",
      "Train Epoche: 1 [587/96 (611%)]\tLoss: 72.923149\n",
      "Train Epoche: 1 [588/96 (612%)]\tLoss: 0.878451\n",
      "Train Epoche: 1 [589/96 (614%)]\tLoss: 0.052855\n",
      "Train Epoche: 1 [590/96 (615%)]\tLoss: 0.526272\n",
      "Train Epoche: 1 [591/96 (616%)]\tLoss: 5.045948\n",
      "Train Epoche: 1 [592/96 (617%)]\tLoss: 156.155151\n",
      "Train Epoche: 1 [593/96 (618%)]\tLoss: 3.057882\n",
      "Train Epoche: 1 [594/96 (619%)]\tLoss: 186.627457\n",
      "Train Epoche: 1 [595/96 (620%)]\tLoss: 0.005290\n",
      "Train Epoche: 1 [596/96 (621%)]\tLoss: 2.037238\n",
      "Train Epoche: 1 [597/96 (622%)]\tLoss: 0.338528\n",
      "Train Epoche: 1 [598/96 (623%)]\tLoss: 10.145449\n",
      "Train Epoche: 1 [599/96 (624%)]\tLoss: 1.924957\n",
      "Train Epoche: 1 [600/96 (625%)]\tLoss: 8.804388\n",
      "Train Epoche: 1 [601/96 (626%)]\tLoss: 1.561475\n",
      "Train Epoche: 1 [602/96 (627%)]\tLoss: 2.806798\n",
      "Train Epoche: 1 [603/96 (628%)]\tLoss: 0.268005\n",
      "Train Epoche: 1 [604/96 (629%)]\tLoss: 1.310034\n",
      "Train Epoche: 1 [605/96 (630%)]\tLoss: 38.932980\n",
      "Train Epoche: 1 [606/96 (631%)]\tLoss: 4.282830\n",
      "Train Epoche: 1 [607/96 (632%)]\tLoss: 383.131683\n",
      "Train Epoche: 1 [608/96 (633%)]\tLoss: 5.860669\n",
      "Train Epoche: 1 [609/96 (634%)]\tLoss: 13.263457\n",
      "Train Epoche: 1 [610/96 (635%)]\tLoss: 39.307072\n",
      "Train Epoche: 1 [611/96 (636%)]\tLoss: 14.602666\n",
      "Train Epoche: 1 [612/96 (638%)]\tLoss: 122.078415\n",
      "Train Epoche: 1 [613/96 (639%)]\tLoss: 141.011551\n",
      "Train Epoche: 1 [614/96 (640%)]\tLoss: 0.229324\n",
      "Train Epoche: 1 [615/96 (641%)]\tLoss: 2.482393\n",
      "Train Epoche: 1 [616/96 (642%)]\tLoss: 11.106856\n",
      "Train Epoche: 1 [617/96 (643%)]\tLoss: 283.173157\n",
      "Train Epoche: 1 [618/96 (644%)]\tLoss: 29.435129\n",
      "Train Epoche: 1 [619/96 (645%)]\tLoss: 4.941583\n",
      "Train Epoche: 1 [620/96 (646%)]\tLoss: 1.012287\n",
      "Train Epoche: 1 [621/96 (647%)]\tLoss: 5.474195\n",
      "Train Epoche: 1 [622/96 (648%)]\tLoss: 73.671234\n",
      "Train Epoche: 1 [623/96 (649%)]\tLoss: 78.054192\n",
      "Train Epoche: 1 [624/96 (650%)]\tLoss: 78.173531\n",
      "Train Epoche: 1 [625/96 (651%)]\tLoss: 425.292450\n",
      "Train Epoche: 1 [626/96 (652%)]\tLoss: 1.619894\n",
      "Train Epoche: 1 [627/96 (653%)]\tLoss: 2.512044\n",
      "Train Epoche: 1 [628/96 (654%)]\tLoss: 1.798705\n",
      "Train Epoche: 1 [629/96 (655%)]\tLoss: 91.617592\n",
      "Train Epoche: 1 [630/96 (656%)]\tLoss: 0.299776\n",
      "Train Epoche: 1 [631/96 (657%)]\tLoss: 0.037656\n",
      "Train Epoche: 1 [632/96 (658%)]\tLoss: 0.683679\n",
      "Train Epoche: 1 [633/96 (659%)]\tLoss: 6.739618\n",
      "Train Epoche: 1 [634/96 (660%)]\tLoss: 22.575167\n",
      "Train Epoche: 1 [635/96 (661%)]\tLoss: 0.010058\n",
      "Train Epoche: 1 [636/96 (662%)]\tLoss: 1.264840\n",
      "Train Epoche: 1 [637/96 (664%)]\tLoss: 0.228943\n",
      "Train Epoche: 1 [638/96 (665%)]\tLoss: 10.580739\n",
      "Train Epoche: 1 [639/96 (666%)]\tLoss: 5.638487\n",
      "Train Epoche: 1 [640/96 (667%)]\tLoss: 16.899149\n",
      "Train Epoche: 1 [641/96 (668%)]\tLoss: 28.358965\n",
      "Train Epoche: 1 [642/96 (669%)]\tLoss: 0.854241\n",
      "Train Epoche: 1 [643/96 (670%)]\tLoss: 40.534374\n",
      "Train Epoche: 1 [644/96 (671%)]\tLoss: 1.688820\n",
      "Train Epoche: 1 [645/96 (672%)]\tLoss: 0.156234\n",
      "Train Epoche: 1 [646/96 (673%)]\tLoss: 13.826939\n",
      "Train Epoche: 1 [647/96 (674%)]\tLoss: 1.243167\n",
      "Train Epoche: 1 [648/96 (675%)]\tLoss: 39.314507\n",
      "Train Epoche: 1 [649/96 (676%)]\tLoss: 85.036545\n",
      "Train Epoche: 1 [650/96 (677%)]\tLoss: 14.743023\n",
      "Train Epoche: 1 [651/96 (678%)]\tLoss: 87.660927\n",
      "Train Epoche: 1 [652/96 (679%)]\tLoss: 1.754553\n",
      "Train Epoche: 1 [653/96 (680%)]\tLoss: 0.132185\n",
      "Train Epoche: 1 [654/96 (681%)]\tLoss: 39.911713\n",
      "Train Epoche: 1 [655/96 (682%)]\tLoss: 12.808903\n",
      "Train Epoche: 1 [656/96 (683%)]\tLoss: 15.688920\n",
      "Train Epoche: 1 [657/96 (684%)]\tLoss: 234.420380\n",
      "Train Epoche: 1 [658/96 (685%)]\tLoss: 8.256124\n",
      "Train Epoche: 1 [659/96 (686%)]\tLoss: 0.691304\n",
      "Train Epoche: 1 [660/96 (688%)]\tLoss: 20.558924\n",
      "Train Epoche: 1 [661/96 (689%)]\tLoss: 3.438089\n",
      "Train Epoche: 1 [662/96 (690%)]\tLoss: 17.327421\n",
      "Train Epoche: 1 [663/96 (691%)]\tLoss: 0.262790\n",
      "Train Epoche: 1 [664/96 (692%)]\tLoss: 2.105983\n",
      "Train Epoche: 1 [665/96 (693%)]\tLoss: 3.296647\n",
      "Train Epoche: 1 [666/96 (694%)]\tLoss: 6.929046\n",
      "Train Epoche: 1 [667/96 (695%)]\tLoss: 0.338644\n",
      "Train Epoche: 1 [668/96 (696%)]\tLoss: 4.176052\n",
      "Train Epoche: 1 [669/96 (697%)]\tLoss: 16.715521\n",
      "Train Epoche: 1 [670/96 (698%)]\tLoss: 6.860397\n",
      "Train Epoche: 1 [671/96 (699%)]\tLoss: 0.860836\n",
      "Train Epoche: 1 [672/96 (700%)]\tLoss: 0.330282\n",
      "Train Epoche: 1 [673/96 (701%)]\tLoss: 37.913376\n",
      "Train Epoche: 1 [674/96 (702%)]\tLoss: 1.418906\n",
      "Train Epoche: 1 [675/96 (703%)]\tLoss: 0.000540\n",
      "Train Epoche: 1 [676/96 (704%)]\tLoss: 0.713605\n",
      "Train Epoche: 1 [677/96 (705%)]\tLoss: 4.766178\n",
      "Train Epoche: 1 [678/96 (706%)]\tLoss: 3.873679\n",
      "Train Epoche: 1 [679/96 (707%)]\tLoss: 3.749381\n",
      "Train Epoche: 1 [680/96 (708%)]\tLoss: 2.403164\n",
      "Train Epoche: 1 [681/96 (709%)]\tLoss: 13.853569\n",
      "Train Epoche: 1 [682/96 (710%)]\tLoss: 14.039871\n",
      "Train Epoche: 1 [683/96 (711%)]\tLoss: 1.669706\n",
      "Train Epoche: 1 [684/96 (712%)]\tLoss: 15.122069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [685/96 (714%)]\tLoss: 41.004318\n",
      "Train Epoche: 1 [686/96 (715%)]\tLoss: 0.988555\n",
      "Train Epoche: 1 [687/96 (716%)]\tLoss: 5.306126\n",
      "Train Epoche: 1 [688/96 (717%)]\tLoss: 0.163126\n",
      "Train Epoche: 1 [689/96 (718%)]\tLoss: 1.579358\n",
      "Train Epoche: 1 [690/96 (719%)]\tLoss: 0.825558\n",
      "Train Epoche: 1 [691/96 (720%)]\tLoss: 10.243698\n",
      "Train Epoche: 1 [692/96 (721%)]\tLoss: 23.136608\n",
      "Train Epoche: 1 [693/96 (722%)]\tLoss: 17.260504\n",
      "Train Epoche: 1 [694/96 (723%)]\tLoss: 1.139903\n",
      "Train Epoche: 1 [695/96 (724%)]\tLoss: 11.362936\n",
      "Train Epoche: 1 [696/96 (725%)]\tLoss: 19.158421\n",
      "Train Epoche: 1 [697/96 (726%)]\tLoss: 2.825940\n",
      "Train Epoche: 1 [698/96 (727%)]\tLoss: 5.538980\n",
      "Train Epoche: 1 [699/96 (728%)]\tLoss: 9.222582\n",
      "Train Epoche: 1 [700/96 (729%)]\tLoss: 2.538373\n",
      "Train Epoche: 1 [701/96 (730%)]\tLoss: 0.891202\n",
      "Train Epoche: 1 [702/96 (731%)]\tLoss: 13.647675\n",
      "Train Epoche: 1 [703/96 (732%)]\tLoss: 7.537532\n",
      "Train Epoche: 1 [704/96 (733%)]\tLoss: 0.103953\n",
      "Train Epoche: 1 [705/96 (734%)]\tLoss: 1.035296\n",
      "Train Epoche: 1 [706/96 (735%)]\tLoss: 6.974583\n",
      "Train Epoche: 1 [707/96 (736%)]\tLoss: 6.374448\n",
      "Train Epoche: 1 [708/96 (738%)]\tLoss: 6.115161\n",
      "Train Epoche: 1 [709/96 (739%)]\tLoss: 5.314332\n",
      "Train Epoche: 1 [710/96 (740%)]\tLoss: 0.284614\n",
      "Train Epoche: 1 [711/96 (741%)]\tLoss: 0.000272\n",
      "Train Epoche: 1 [712/96 (742%)]\tLoss: 12.384845\n",
      "Train Epoche: 1 [713/96 (743%)]\tLoss: 0.014494\n",
      "Train Epoche: 1 [714/96 (744%)]\tLoss: 10.611012\n",
      "Train Epoche: 1 [715/96 (745%)]\tLoss: 48.047283\n",
      "Train Epoche: 1 [716/96 (746%)]\tLoss: 1.427181\n",
      "Train Epoche: 1 [717/96 (747%)]\tLoss: 1.688744\n",
      "Train Epoche: 1 [718/96 (748%)]\tLoss: 4.428657\n",
      "Train Epoche: 1 [719/96 (749%)]\tLoss: 0.283020\n",
      "Train Epoche: 1 [720/96 (750%)]\tLoss: 0.583638\n",
      "Train Epoche: 1 [721/96 (751%)]\tLoss: 1.866498\n",
      "Train Epoche: 1 [722/96 (752%)]\tLoss: 3.376193\n",
      "Train Epoche: 1 [723/96 (753%)]\tLoss: 0.121391\n",
      "Train Epoche: 1 [724/96 (754%)]\tLoss: 0.848342\n",
      "Train Epoche: 1 [725/96 (755%)]\tLoss: 5.254435\n",
      "Train Epoche: 1 [726/96 (756%)]\tLoss: 0.019844\n",
      "Train Epoche: 1 [727/96 (757%)]\tLoss: 0.486262\n",
      "Train Epoche: 1 [728/96 (758%)]\tLoss: 68.528946\n",
      "Train Epoche: 1 [729/96 (759%)]\tLoss: 8.147489\n",
      "Train Epoche: 1 [730/96 (760%)]\tLoss: 5.152729\n",
      "Train Epoche: 1 [731/96 (761%)]\tLoss: 238.182831\n",
      "Train Epoche: 1 [732/96 (762%)]\tLoss: 30.018784\n",
      "Train Epoche: 1 [733/96 (764%)]\tLoss: 23.616936\n",
      "Train Epoche: 1 [734/96 (765%)]\tLoss: 0.208671\n",
      "Train Epoche: 1 [735/96 (766%)]\tLoss: 12.898429\n",
      "Train Epoche: 1 [736/96 (767%)]\tLoss: 8.025177\n",
      "Train Epoche: 1 [737/96 (768%)]\tLoss: 4.674383\n",
      "Train Epoche: 1 [738/96 (769%)]\tLoss: 0.587231\n",
      "Train Epoche: 1 [739/96 (770%)]\tLoss: 0.030849\n",
      "Train Epoche: 1 [740/96 (771%)]\tLoss: 6.471806\n",
      "Train Epoche: 1 [741/96 (772%)]\tLoss: 6.029221\n",
      "Train Epoche: 1 [742/96 (773%)]\tLoss: 5.159905\n",
      "Train Epoche: 1 [743/96 (774%)]\tLoss: 9.305858\n",
      "Train Epoche: 1 [744/96 (775%)]\tLoss: 14.356724\n",
      "Train Epoche: 1 [745/96 (776%)]\tLoss: 5.543468\n",
      "Train Epoche: 1 [746/96 (777%)]\tLoss: 0.373067\n",
      "Train Epoche: 1 [747/96 (778%)]\tLoss: 54.331280\n",
      "Train Epoche: 1 [748/96 (779%)]\tLoss: 83.724342\n",
      "Train Epoche: 1 [749/96 (780%)]\tLoss: 257.845490\n",
      "Train Epoche: 1 [750/96 (781%)]\tLoss: 42.285664\n",
      "Train Epoche: 1 [751/96 (782%)]\tLoss: 1.090240\n",
      "Train Epoche: 1 [752/96 (783%)]\tLoss: 45.263939\n",
      "Train Epoche: 1 [753/96 (784%)]\tLoss: 0.141134\n",
      "Train Epoche: 1 [754/96 (785%)]\tLoss: 58.794147\n",
      "Train Epoche: 1 [755/96 (786%)]\tLoss: 4.265440\n",
      "Train Epoche: 1 [756/96 (788%)]\tLoss: 9.179216\n",
      "Train Epoche: 1 [757/96 (789%)]\tLoss: 25.611809\n",
      "Train Epoche: 1 [758/96 (790%)]\tLoss: 10.025931\n",
      "Train Epoche: 1 [759/96 (791%)]\tLoss: 0.359515\n",
      "Train Epoche: 1 [760/96 (792%)]\tLoss: 17.517382\n",
      "Train Epoche: 1 [761/96 (793%)]\tLoss: 40.023335\n",
      "Train Epoche: 1 [762/96 (794%)]\tLoss: 0.018793\n",
      "Train Epoche: 1 [763/96 (795%)]\tLoss: 42.725956\n",
      "Train Epoche: 1 [764/96 (796%)]\tLoss: 51.492786\n",
      "Train Epoche: 1 [765/96 (797%)]\tLoss: 12.267800\n",
      "Train Epoche: 1 [766/96 (798%)]\tLoss: 63.554367\n",
      "Train Epoche: 1 [767/96 (799%)]\tLoss: 22.161333\n",
      "Train Epoche: 1 [768/96 (800%)]\tLoss: 102.374832\n",
      "Train Epoche: 1 [769/96 (801%)]\tLoss: 177.149460\n",
      "Train Epoche: 1 [770/96 (802%)]\tLoss: 13.099587\n",
      "Train Epoche: 1 [771/96 (803%)]\tLoss: 5.410410\n",
      "Train Epoche: 1 [772/96 (804%)]\tLoss: 58.059601\n",
      "Train Epoche: 1 [773/96 (805%)]\tLoss: 5.659758\n",
      "Train Epoche: 1 [774/96 (806%)]\tLoss: 13.559064\n",
      "Train Epoche: 1 [775/96 (807%)]\tLoss: 104.148956\n",
      "Train Epoche: 1 [776/96 (808%)]\tLoss: 86.526993\n",
      "Train Epoche: 1 [777/96 (809%)]\tLoss: 74.636375\n",
      "Train Epoche: 1 [778/96 (810%)]\tLoss: 3.389855\n",
      "Train Epoche: 1 [779/96 (811%)]\tLoss: 11.592247\n",
      "Train Epoche: 1 [780/96 (812%)]\tLoss: 3.757704\n",
      "Train Epoche: 1 [781/96 (814%)]\tLoss: 2.123755\n",
      "Train Epoche: 1 [782/96 (815%)]\tLoss: 187.509964\n",
      "Train Epoche: 1 [783/96 (816%)]\tLoss: 14.777428\n",
      "Train Epoche: 1 [784/96 (817%)]\tLoss: 3.129447\n",
      "Train Epoche: 1 [785/96 (818%)]\tLoss: 0.075015\n",
      "Train Epoche: 1 [786/96 (819%)]\tLoss: 12.349523\n",
      "Train Epoche: 1 [787/96 (820%)]\tLoss: 0.211351\n",
      "Train Epoche: 1 [788/96 (821%)]\tLoss: 0.242494\n",
      "Train Epoche: 1 [789/96 (822%)]\tLoss: 1.124810\n",
      "Train Epoche: 1 [790/96 (823%)]\tLoss: 2.612761\n",
      "Train Epoche: 1 [791/96 (824%)]\tLoss: 35.716080\n",
      "Train Epoche: 1 [792/96 (825%)]\tLoss: 97.153198\n",
      "Train Epoche: 1 [793/96 (826%)]\tLoss: 96.264343\n",
      "Train Epoche: 1 [794/96 (827%)]\tLoss: 119.369087\n",
      "Train Epoche: 1 [795/96 (828%)]\tLoss: 251.111038\n",
      "Train Epoche: 1 [796/96 (829%)]\tLoss: 136.223587\n",
      "Train Epoche: 1 [797/96 (830%)]\tLoss: 0.366468\n",
      "Train Epoche: 1 [798/96 (831%)]\tLoss: 6.110804\n",
      "Train Epoche: 1 [799/96 (832%)]\tLoss: 7.098459\n",
      "Train Epoche: 1 [800/96 (833%)]\tLoss: 43.213959\n",
      "Train Epoche: 1 [801/96 (834%)]\tLoss: 32.793747\n",
      "Train Epoche: 1 [802/96 (835%)]\tLoss: 8.225744\n",
      "Train Epoche: 1 [803/96 (836%)]\tLoss: 7.259523\n",
      "Train Epoche: 1 [804/96 (838%)]\tLoss: 1.083732\n",
      "Train Epoche: 1 [805/96 (839%)]\tLoss: 154.243225\n",
      "Train Epoche: 1 [806/96 (840%)]\tLoss: 0.342150\n",
      "Train Epoche: 1 [807/96 (841%)]\tLoss: 43.799793\n",
      "Train Epoche: 1 [808/96 (842%)]\tLoss: 15.532406\n",
      "Train Epoche: 1 [809/96 (843%)]\tLoss: 354.350220\n",
      "Train Epoche: 1 [810/96 (844%)]\tLoss: 119.132355\n",
      "Train Epoche: 1 [811/96 (845%)]\tLoss: 0.118739\n",
      "Train Epoche: 1 [812/96 (846%)]\tLoss: 21.629522\n",
      "Train Epoche: 1 [813/96 (847%)]\tLoss: 24.944479\n",
      "Train Epoche: 1 [814/96 (848%)]\tLoss: 58.298019\n",
      "Train Epoche: 1 [815/96 (849%)]\tLoss: 2.403695\n",
      "Train Epoche: 1 [816/96 (850%)]\tLoss: 65.764481\n",
      "Train Epoche: 1 [817/96 (851%)]\tLoss: 28.389301\n",
      "Train Epoche: 1 [818/96 (852%)]\tLoss: 71.339767\n",
      "Train Epoche: 1 [819/96 (853%)]\tLoss: 0.016986\n",
      "Train Epoche: 1 [820/96 (854%)]\tLoss: 7.220170\n",
      "Train Epoche: 1 [821/96 (855%)]\tLoss: 0.287107\n",
      "Train Epoche: 1 [822/96 (856%)]\tLoss: 17.499424\n",
      "Train Epoche: 1 [823/96 (857%)]\tLoss: 2.541170\n",
      "Train Epoche: 1 [824/96 (858%)]\tLoss: 5.776927\n",
      "Train Epoche: 1 [825/96 (859%)]\tLoss: 0.159065\n",
      "Train Epoche: 1 [826/96 (860%)]\tLoss: 28.021914\n",
      "Train Epoche: 1 [827/96 (861%)]\tLoss: 2.494887\n",
      "Train Epoche: 1 [828/96 (862%)]\tLoss: 70.879044\n",
      "Train Epoche: 1 [829/96 (864%)]\tLoss: 276.412628\n",
      "Train Epoche: 1 [830/96 (865%)]\tLoss: 1.432609\n",
      "Train Epoche: 1 [831/96 (866%)]\tLoss: 4.388515\n",
      "Train Epoche: 1 [832/96 (867%)]\tLoss: 0.048978\n",
      "Train Epoche: 1 [833/96 (868%)]\tLoss: 10.336109\n",
      "Train Epoche: 1 [834/96 (869%)]\tLoss: 9.151973\n",
      "Train Epoche: 1 [835/96 (870%)]\tLoss: 102.995529\n",
      "Train Epoche: 1 [836/96 (871%)]\tLoss: 4.543755\n",
      "Train Epoche: 1 [837/96 (872%)]\tLoss: 120.007111\n",
      "Train Epoche: 1 [838/96 (873%)]\tLoss: 10.653061\n",
      "Train Epoche: 1 [839/96 (874%)]\tLoss: 3.977856\n",
      "Train Epoche: 1 [840/96 (875%)]\tLoss: 2.222624\n",
      "Train Epoche: 1 [841/96 (876%)]\tLoss: 1.744943\n",
      "Train Epoche: 1 [842/96 (877%)]\tLoss: 30.661629\n",
      "Train Epoche: 1 [843/96 (878%)]\tLoss: 0.216147\n",
      "Train Epoche: 1 [844/96 (879%)]\tLoss: 0.330801\n",
      "Train Epoche: 1 [845/96 (880%)]\tLoss: 2.449973\n",
      "Train Epoche: 1 [846/96 (881%)]\tLoss: 19.340103\n",
      "Train Epoche: 1 [847/96 (882%)]\tLoss: 1.395168\n",
      "Train Epoche: 1 [848/96 (883%)]\tLoss: 59.785671\n",
      "Train Epoche: 1 [849/96 (884%)]\tLoss: 4.571572\n",
      "Train Epoche: 1 [850/96 (885%)]\tLoss: 2.343153\n",
      "Train Epoche: 1 [851/96 (886%)]\tLoss: 146.023346\n",
      "Train Epoche: 1 [852/96 (888%)]\tLoss: 52.241016\n",
      "Train Epoche: 1 [853/96 (889%)]\tLoss: 55.538239\n",
      "Train Epoche: 1 [854/96 (890%)]\tLoss: 77.230537\n",
      "Train Epoche: 1 [855/96 (891%)]\tLoss: 15.550745\n",
      "Train Epoche: 1 [856/96 (892%)]\tLoss: 32.829048\n",
      "Train Epoche: 1 [857/96 (893%)]\tLoss: 0.101410\n",
      "Train Epoche: 1 [858/96 (894%)]\tLoss: 168.931519\n",
      "Train Epoche: 1 [859/96 (895%)]\tLoss: 27.925276\n",
      "Train Epoche: 1 [860/96 (896%)]\tLoss: 19.078791\n",
      "Train Epoche: 1 [861/96 (897%)]\tLoss: 83.653084\n",
      "Train Epoche: 1 [862/96 (898%)]\tLoss: 22.717672\n",
      "Train Epoche: 1 [863/96 (899%)]\tLoss: 3.304185\n",
      "Train Epoche: 1 [864/96 (900%)]\tLoss: 15.199243\n",
      "Train Epoche: 1 [865/96 (901%)]\tLoss: 179.566147\n",
      "Train Epoche: 1 [866/96 (902%)]\tLoss: 14.710170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [867/96 (903%)]\tLoss: 8.381815\n",
      "Train Epoche: 1 [868/96 (904%)]\tLoss: 0.585871\n",
      "Train Epoche: 1 [869/96 (905%)]\tLoss: 6.537840\n",
      "Train Epoche: 1 [870/96 (906%)]\tLoss: 0.536122\n",
      "Train Epoche: 1 [871/96 (907%)]\tLoss: 2.292691\n",
      "Train Epoche: 1 [872/96 (908%)]\tLoss: 2.401874\n",
      "Train Epoche: 1 [873/96 (909%)]\tLoss: 120.240852\n",
      "Train Epoche: 1 [874/96 (910%)]\tLoss: 0.391146\n",
      "Train Epoche: 1 [875/96 (911%)]\tLoss: 40.727089\n",
      "Train Epoche: 1 [876/96 (912%)]\tLoss: 0.031444\n",
      "Train Epoche: 1 [877/96 (914%)]\tLoss: 1.376065\n",
      "Train Epoche: 1 [878/96 (915%)]\tLoss: 4.423851\n",
      "Train Epoche: 1 [879/96 (916%)]\tLoss: 0.053153\n",
      "Train Epoche: 1 [880/96 (917%)]\tLoss: 1.467830\n",
      "Train Epoche: 1 [881/96 (918%)]\tLoss: 0.458163\n",
      "Train Epoche: 1 [882/96 (919%)]\tLoss: 2.726151\n",
      "Train Epoche: 1 [883/96 (920%)]\tLoss: 44.164265\n",
      "Train Epoche: 1 [884/96 (921%)]\tLoss: 0.803358\n",
      "Train Epoche: 1 [885/96 (922%)]\tLoss: 0.940107\n",
      "Train Epoche: 1 [886/96 (923%)]\tLoss: 1.269342\n",
      "Train Epoche: 1 [887/96 (924%)]\tLoss: 19.132294\n",
      "Train Epoche: 1 [888/96 (925%)]\tLoss: 9.306771\n",
      "Train Epoche: 1 [889/96 (926%)]\tLoss: 11.535983\n",
      "Train Epoche: 1 [890/96 (927%)]\tLoss: 0.623769\n",
      "Train Epoche: 1 [891/96 (928%)]\tLoss: 10.689158\n",
      "Train Epoche: 1 [892/96 (929%)]\tLoss: 4.961285\n",
      "Train Epoche: 1 [893/96 (930%)]\tLoss: 5.569360\n",
      "Train Epoche: 1 [894/96 (931%)]\tLoss: 3.066517\n",
      "Train Epoche: 1 [895/96 (932%)]\tLoss: 0.922331\n",
      "Train Epoche: 1 [896/96 (933%)]\tLoss: 2.618339\n",
      "Train Epoche: 1 [897/96 (934%)]\tLoss: 3.213720\n",
      "Train Epoche: 1 [898/96 (935%)]\tLoss: 0.194985\n",
      "Train Epoche: 1 [899/96 (936%)]\tLoss: 0.096320\n",
      "Train Epoche: 1 [900/96 (938%)]\tLoss: 94.387970\n",
      "Train Epoche: 1 [901/96 (939%)]\tLoss: 7.214457\n",
      "Train Epoche: 1 [902/96 (940%)]\tLoss: 0.000588\n",
      "Train Epoche: 1 [903/96 (941%)]\tLoss: 0.103108\n",
      "Train Epoche: 1 [904/96 (942%)]\tLoss: 18.009266\n",
      "Train Epoche: 1 [905/96 (943%)]\tLoss: 139.396652\n",
      "Train Epoche: 1 [906/96 (944%)]\tLoss: 0.214224\n",
      "Train Epoche: 1 [907/96 (945%)]\tLoss: 4.694458\n",
      "Train Epoche: 1 [908/96 (946%)]\tLoss: 2.555070\n",
      "Train Epoche: 1 [909/96 (947%)]\tLoss: 14.852258\n",
      "Train Epoche: 1 [910/96 (948%)]\tLoss: 3.988273\n",
      "Train Epoche: 1 [911/96 (949%)]\tLoss: 13.117964\n",
      "Train Epoche: 1 [912/96 (950%)]\tLoss: 10.283937\n",
      "Train Epoche: 1 [913/96 (951%)]\tLoss: 4.696748\n",
      "Train Epoche: 1 [914/96 (952%)]\tLoss: 58.437267\n",
      "Train Epoche: 1 [915/96 (953%)]\tLoss: 1.060545\n",
      "Train Epoche: 1 [916/96 (954%)]\tLoss: 1.478817\n",
      "Train Epoche: 1 [917/96 (955%)]\tLoss: 7.495620\n",
      "Train Epoche: 1 [918/96 (956%)]\tLoss: 17.071371\n",
      "Train Epoche: 1 [919/96 (957%)]\tLoss: 48.321018\n",
      "Train Epoche: 1 [920/96 (958%)]\tLoss: 4.680476\n",
      "Train Epoche: 1 [921/96 (959%)]\tLoss: 6.940297\n",
      "Train Epoche: 1 [922/96 (960%)]\tLoss: 107.502304\n",
      "Train Epoche: 1 [923/96 (961%)]\tLoss: 52.283844\n",
      "Train Epoche: 1 [924/96 (962%)]\tLoss: 0.499103\n",
      "Train Epoche: 1 [925/96 (964%)]\tLoss: 2.748373\n",
      "Train Epoche: 1 [926/96 (965%)]\tLoss: 0.223628\n",
      "Train Epoche: 1 [927/96 (966%)]\tLoss: 8.610785\n",
      "Train Epoche: 1 [928/96 (967%)]\tLoss: 4.908000\n",
      "Train Epoche: 1 [929/96 (968%)]\tLoss: 2.897605\n",
      "Train Epoche: 1 [930/96 (969%)]\tLoss: 1.665453\n",
      "Train Epoche: 1 [931/96 (970%)]\tLoss: 3.090036\n",
      "Train Epoche: 1 [932/96 (971%)]\tLoss: 167.175308\n",
      "Train Epoche: 1 [933/96 (972%)]\tLoss: 358.928040\n",
      "Train Epoche: 1 [934/96 (973%)]\tLoss: 0.566897\n",
      "Train Epoche: 1 [935/96 (974%)]\tLoss: 44.347019\n",
      "Train Epoche: 1 [936/96 (975%)]\tLoss: 0.629231\n",
      "Train Epoche: 1 [937/96 (976%)]\tLoss: 0.920820\n",
      "Train Epoche: 1 [938/96 (977%)]\tLoss: 1.138641\n",
      "Train Epoche: 1 [939/96 (978%)]\tLoss: 40.556442\n",
      "Train Epoche: 1 [940/96 (979%)]\tLoss: 2.093836\n",
      "Train Epoche: 1 [941/96 (980%)]\tLoss: 64.369751\n",
      "Train Epoche: 1 [942/96 (981%)]\tLoss: 1.197432\n",
      "Train Epoche: 1 [943/96 (982%)]\tLoss: 8.694483\n",
      "Train Epoche: 1 [944/96 (983%)]\tLoss: 77.282509\n",
      "Train Epoche: 1 [945/96 (984%)]\tLoss: 38.003696\n",
      "Train Epoche: 1 [946/96 (985%)]\tLoss: 0.545890\n",
      "Train Epoche: 1 [947/96 (986%)]\tLoss: 10.166884\n",
      "Train Epoche: 1 [948/96 (988%)]\tLoss: 2.175142\n",
      "Train Epoche: 1 [949/96 (989%)]\tLoss: 6.625149\n",
      "Train Epoche: 1 [950/96 (990%)]\tLoss: 2.425821\n",
      "Train Epoche: 1 [951/96 (991%)]\tLoss: 1.020688\n",
      "Train Epoche: 1 [952/96 (992%)]\tLoss: 9.677220\n",
      "Train Epoche: 1 [953/96 (993%)]\tLoss: 108.413689\n",
      "Train Epoche: 1 [954/96 (994%)]\tLoss: 180.111099\n",
      "Train Epoche: 1 [955/96 (995%)]\tLoss: 0.992524\n",
      "Train Epoche: 1 [956/96 (996%)]\tLoss: 42.145863\n",
      "Train Epoche: 1 [957/96 (997%)]\tLoss: 13.495545\n",
      "Train Epoche: 1 [958/96 (998%)]\tLoss: 125.168648\n",
      "Train Epoche: 1 [959/96 (999%)]\tLoss: 75.686615\n",
      "Train Epoche: 1 [960/96 (1000%)]\tLoss: 30.400452\n",
      "Train Epoche: 1 [961/96 (1001%)]\tLoss: 28.399353\n",
      "Train Epoche: 1 [962/96 (1002%)]\tLoss: 14.859290\n",
      "Train Epoche: 1 [963/96 (1003%)]\tLoss: 39.032246\n",
      "Train Epoche: 1 [964/96 (1004%)]\tLoss: 164.308105\n",
      "Train Epoche: 1 [965/96 (1005%)]\tLoss: 59.515842\n",
      "Train Epoche: 1 [966/96 (1006%)]\tLoss: 5.044963\n",
      "Train Epoche: 1 [967/96 (1007%)]\tLoss: 24.100224\n",
      "Train Epoche: 1 [968/96 (1008%)]\tLoss: 67.746109\n",
      "Train Epoche: 1 [969/96 (1009%)]\tLoss: 91.885643\n",
      "Train Epoche: 1 [970/96 (1010%)]\tLoss: 18.850075\n",
      "Train Epoche: 1 [971/96 (1011%)]\tLoss: 2.150434\n",
      "Train Epoche: 1 [972/96 (1012%)]\tLoss: 0.735650\n",
      "Train Epoche: 1 [973/96 (1014%)]\tLoss: 3.790733\n",
      "Train Epoche: 1 [974/96 (1015%)]\tLoss: 0.017553\n",
      "Train Epoche: 1 [975/96 (1016%)]\tLoss: 1.161993\n",
      "Train Epoche: 1 [976/96 (1017%)]\tLoss: 0.084041\n",
      "Train Epoche: 1 [977/96 (1018%)]\tLoss: 82.840485\n",
      "Train Epoche: 1 [978/96 (1019%)]\tLoss: 2.154118\n",
      "Train Epoche: 1 [979/96 (1020%)]\tLoss: 18.543186\n",
      "Train Epoche: 1 [980/96 (1021%)]\tLoss: 0.062352\n",
      "Train Epoche: 1 [981/96 (1022%)]\tLoss: 280.195801\n",
      "Train Epoche: 1 [982/96 (1023%)]\tLoss: 11.021945\n",
      "Train Epoche: 1 [983/96 (1024%)]\tLoss: 0.065600\n",
      "Train Epoche: 1 [984/96 (1025%)]\tLoss: 10.673278\n",
      "Train Epoche: 1 [985/96 (1026%)]\tLoss: 12.657020\n",
      "Train Epoche: 1 [986/96 (1027%)]\tLoss: 0.845709\n",
      "Train Epoche: 1 [987/96 (1028%)]\tLoss: 0.000001\n",
      "Train Epoche: 1 [988/96 (1029%)]\tLoss: 51.848305\n",
      "Train Epoche: 1 [989/96 (1030%)]\tLoss: 16.679855\n",
      "Train Epoche: 1 [990/96 (1031%)]\tLoss: 0.273169\n",
      "Train Epoche: 1 [991/96 (1032%)]\tLoss: 3.562531\n",
      "Train Epoche: 1 [992/96 (1033%)]\tLoss: 0.023499\n",
      "Train Epoche: 1 [993/96 (1034%)]\tLoss: 0.074482\n",
      "Train Epoche: 1 [994/96 (1035%)]\tLoss: 172.499039\n",
      "Train Epoche: 1 [995/96 (1036%)]\tLoss: 45.830265\n",
      "Train Epoche: 1 [996/96 (1038%)]\tLoss: 12.854078\n",
      "Train Epoche: 1 [997/96 (1039%)]\tLoss: 0.479033\n",
      "Train Epoche: 1 [998/96 (1040%)]\tLoss: 11.409217\n",
      "Train Epoche: 1 [999/96 (1041%)]\tLoss: 0.018655\n",
      "Train Epoche: 1 [1000/96 (1042%)]\tLoss: 11.577461\n",
      "Train Epoche: 1 [1001/96 (1043%)]\tLoss: 40.111725\n",
      "Train Epoche: 1 [1002/96 (1044%)]\tLoss: 2.374306\n",
      "Train Epoche: 1 [1003/96 (1045%)]\tLoss: 6.359384\n",
      "Train Epoche: 1 [1004/96 (1046%)]\tLoss: 0.000003\n",
      "Train Epoche: 1 [1005/96 (1047%)]\tLoss: 21.417469\n",
      "Train Epoche: 1 [1006/96 (1048%)]\tLoss: 31.896315\n",
      "Train Epoche: 1 [1007/96 (1049%)]\tLoss: 0.702393\n",
      "Train Epoche: 1 [1008/96 (1050%)]\tLoss: 1.823552\n",
      "Train Epoche: 1 [1009/96 (1051%)]\tLoss: 10.892286\n",
      "Train Epoche: 1 [1010/96 (1052%)]\tLoss: 1.623820\n",
      "Train Epoche: 1 [1011/96 (1053%)]\tLoss: 14.893348\n",
      "Train Epoche: 1 [1012/96 (1054%)]\tLoss: 273.172577\n",
      "Train Epoche: 1 [1013/96 (1055%)]\tLoss: 60.746773\n",
      "Train Epoche: 1 [1014/96 (1056%)]\tLoss: 13.603614\n",
      "Train Epoche: 1 [1015/96 (1057%)]\tLoss: 64.592003\n",
      "Train Epoche: 1 [1016/96 (1058%)]\tLoss: 29.036531\n",
      "Train Epoche: 1 [1017/96 (1059%)]\tLoss: 50.550968\n",
      "Train Epoche: 1 [1018/96 (1060%)]\tLoss: 162.906555\n",
      "Train Epoche: 1 [1019/96 (1061%)]\tLoss: 1.263150\n",
      "Train Epoche: 1 [1020/96 (1062%)]\tLoss: 68.348114\n",
      "Train Epoche: 1 [1021/96 (1064%)]\tLoss: 113.784927\n",
      "Train Epoche: 1 [1022/96 (1065%)]\tLoss: 241.155029\n",
      "Train Epoche: 1 [1023/96 (1066%)]\tLoss: 54.612755\n",
      "Train Epoche: 1 [1024/96 (1067%)]\tLoss: 14.162283\n",
      "Train Epoche: 1 [1025/96 (1068%)]\tLoss: 0.075406\n",
      "Train Epoche: 1 [1026/96 (1069%)]\tLoss: 15.690635\n",
      "Train Epoche: 1 [1027/96 (1070%)]\tLoss: 28.189678\n",
      "Train Epoche: 1 [1028/96 (1071%)]\tLoss: 27.047695\n",
      "Train Epoche: 1 [1029/96 (1072%)]\tLoss: 3.797718\n",
      "Train Epoche: 1 [1030/96 (1073%)]\tLoss: 7.044107\n",
      "Train Epoche: 1 [1031/96 (1074%)]\tLoss: 2.066930\n",
      "Train Epoche: 1 [1032/96 (1075%)]\tLoss: 0.006716\n",
      "Train Epoche: 1 [1033/96 (1076%)]\tLoss: 354.658325\n",
      "Train Epoche: 1 [1034/96 (1077%)]\tLoss: 205.625778\n",
      "Train Epoche: 1 [1035/96 (1078%)]\tLoss: 101.275597\n",
      "Train Epoche: 1 [1036/96 (1079%)]\tLoss: 0.951184\n",
      "Train Epoche: 1 [1037/96 (1080%)]\tLoss: 0.700567\n",
      "Train Epoche: 1 [1038/96 (1081%)]\tLoss: 22.986750\n",
      "Train Epoche: 1 [1039/96 (1082%)]\tLoss: 101.665718\n",
      "Train Epoche: 1 [1040/96 (1083%)]\tLoss: 152.226440\n",
      "Train Epoche: 1 [1041/96 (1084%)]\tLoss: 0.920354\n",
      "Train Epoche: 1 [1042/96 (1085%)]\tLoss: 0.216029\n",
      "Train Epoche: 1 [1043/96 (1086%)]\tLoss: 22.715500\n",
      "Train Epoche: 1 [1044/96 (1088%)]\tLoss: 266.216156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1045/96 (1089%)]\tLoss: 0.733856\n",
      "Train Epoche: 1 [1046/96 (1090%)]\tLoss: 0.678394\n",
      "Train Epoche: 1 [1047/96 (1091%)]\tLoss: 58.136734\n",
      "Train Epoche: 1 [1048/96 (1092%)]\tLoss: 1.345246\n",
      "Train Epoche: 1 [1049/96 (1093%)]\tLoss: 254.795059\n",
      "Train Epoche: 1 [1050/96 (1094%)]\tLoss: 67.782913\n",
      "Train Epoche: 1 [1051/96 (1095%)]\tLoss: 101.859222\n",
      "Train Epoche: 1 [1052/96 (1096%)]\tLoss: 13.299790\n",
      "Train Epoche: 1 [1053/96 (1097%)]\tLoss: 25.027702\n",
      "Train Epoche: 1 [1054/96 (1098%)]\tLoss: 27.296703\n",
      "Train Epoche: 1 [1055/96 (1099%)]\tLoss: 10.393911\n",
      "Train Epoche: 1 [1056/96 (1100%)]\tLoss: 12.347190\n",
      "Train Epoche: 1 [1057/96 (1101%)]\tLoss: 0.343495\n",
      "Train Epoche: 1 [1058/96 (1102%)]\tLoss: 1.110953\n",
      "Train Epoche: 1 [1059/96 (1103%)]\tLoss: 22.247660\n",
      "Train Epoche: 1 [1060/96 (1104%)]\tLoss: 0.237325\n",
      "Train Epoche: 1 [1061/96 (1105%)]\tLoss: 2.289865\n",
      "Train Epoche: 1 [1062/96 (1106%)]\tLoss: 19.622503\n",
      "Train Epoche: 1 [1063/96 (1107%)]\tLoss: 0.010897\n",
      "Train Epoche: 1 [1064/96 (1108%)]\tLoss: 1.852644\n",
      "Train Epoche: 1 [1065/96 (1109%)]\tLoss: 0.295016\n",
      "Train Epoche: 1 [1066/96 (1110%)]\tLoss: 142.881958\n",
      "Train Epoche: 1 [1067/96 (1111%)]\tLoss: 10.267776\n",
      "Train Epoche: 1 [1068/96 (1112%)]\tLoss: 2.103658\n",
      "Train Epoche: 1 [1069/96 (1114%)]\tLoss: 1.080644\n",
      "Train Epoche: 1 [1070/96 (1115%)]\tLoss: 86.679230\n",
      "Train Epoche: 1 [1071/96 (1116%)]\tLoss: 0.011764\n",
      "Train Epoche: 1 [1072/96 (1117%)]\tLoss: 4.703753\n",
      "Train Epoche: 1 [1073/96 (1118%)]\tLoss: 6.895442\n",
      "Train Epoche: 1 [1074/96 (1119%)]\tLoss: 30.079321\n",
      "Train Epoche: 1 [1075/96 (1120%)]\tLoss: 1.768179\n",
      "Train Epoche: 1 [1076/96 (1121%)]\tLoss: 2.211262\n",
      "Train Epoche: 1 [1077/96 (1122%)]\tLoss: 7.252916\n",
      "Train Epoche: 1 [1078/96 (1123%)]\tLoss: 51.883003\n",
      "Train Epoche: 1 [1079/96 (1124%)]\tLoss: 19.810537\n",
      "Train Epoche: 1 [1080/96 (1125%)]\tLoss: 0.074524\n",
      "Train Epoche: 1 [1081/96 (1126%)]\tLoss: 4.231724\n",
      "Train Epoche: 1 [1082/96 (1127%)]\tLoss: 10.756872\n",
      "Train Epoche: 1 [1083/96 (1128%)]\tLoss: 1.454716\n",
      "Train Epoche: 1 [1084/96 (1129%)]\tLoss: 0.025470\n",
      "Train Epoche: 1 [1085/96 (1130%)]\tLoss: 25.568544\n",
      "Train Epoche: 1 [1086/96 (1131%)]\tLoss: 4.751503\n",
      "Train Epoche: 1 [1087/96 (1132%)]\tLoss: 3.846895\n",
      "Train Epoche: 1 [1088/96 (1133%)]\tLoss: 1.863429\n",
      "Train Epoche: 1 [1089/96 (1134%)]\tLoss: 1.895607\n",
      "Train Epoche: 1 [1090/96 (1135%)]\tLoss: 3.272531\n",
      "Train Epoche: 1 [1091/96 (1136%)]\tLoss: 0.000466\n",
      "Train Epoche: 1 [1092/96 (1138%)]\tLoss: 14.045868\n",
      "Train Epoche: 1 [1093/96 (1139%)]\tLoss: 3.887765\n",
      "Train Epoche: 1 [1094/96 (1140%)]\tLoss: 47.315151\n",
      "Train Epoche: 1 [1095/96 (1141%)]\tLoss: 9.116267\n",
      "Train Epoche: 1 [1096/96 (1142%)]\tLoss: 40.173306\n",
      "Train Epoche: 1 [1097/96 (1143%)]\tLoss: 41.213219\n",
      "Train Epoche: 1 [1098/96 (1144%)]\tLoss: 10.275516\n",
      "Train Epoche: 1 [1099/96 (1145%)]\tLoss: 39.125565\n",
      "Train Epoche: 1 [1100/96 (1146%)]\tLoss: 94.172417\n",
      "Train Epoche: 1 [1101/96 (1147%)]\tLoss: 0.019563\n",
      "Train Epoche: 1 [1102/96 (1148%)]\tLoss: 25.245829\n",
      "Train Epoche: 1 [1103/96 (1149%)]\tLoss: 9.348618\n",
      "Train Epoche: 1 [1104/96 (1150%)]\tLoss: 31.691650\n",
      "Train Epoche: 1 [1105/96 (1151%)]\tLoss: 0.060650\n",
      "Train Epoche: 1 [1106/96 (1152%)]\tLoss: 15.559396\n",
      "Train Epoche: 1 [1107/96 (1153%)]\tLoss: 1.379054\n",
      "Train Epoche: 1 [1108/96 (1154%)]\tLoss: 19.289356\n",
      "Train Epoche: 1 [1109/96 (1155%)]\tLoss: 0.194094\n",
      "Train Epoche: 1 [1110/96 (1156%)]\tLoss: 431.516449\n",
      "Train Epoche: 1 [1111/96 (1157%)]\tLoss: 1.674167\n",
      "Train Epoche: 1 [1112/96 (1158%)]\tLoss: 0.361179\n",
      "Train Epoche: 1 [1113/96 (1159%)]\tLoss: 60.544067\n",
      "Train Epoche: 1 [1114/96 (1160%)]\tLoss: 3.060845\n",
      "Train Epoche: 1 [1115/96 (1161%)]\tLoss: 24.168036\n",
      "Train Epoche: 1 [1116/96 (1162%)]\tLoss: 6.647432\n",
      "Train Epoche: 1 [1117/96 (1164%)]\tLoss: 0.485311\n",
      "Train Epoche: 1 [1118/96 (1165%)]\tLoss: 1.611673\n",
      "Train Epoche: 1 [1119/96 (1166%)]\tLoss: 1.032081\n",
      "Train Epoche: 1 [1120/96 (1167%)]\tLoss: 4.296154\n",
      "Train Epoche: 1 [1121/96 (1168%)]\tLoss: 1.555070\n",
      "Train Epoche: 1 [1122/96 (1169%)]\tLoss: 8.727324\n",
      "Train Epoche: 1 [1123/96 (1170%)]\tLoss: 15.128505\n",
      "Train Epoche: 1 [1124/96 (1171%)]\tLoss: 9.350889\n",
      "Train Epoche: 1 [1125/96 (1172%)]\tLoss: 97.898148\n",
      "Train Epoche: 1 [1126/96 (1173%)]\tLoss: 16.409363\n",
      "Train Epoche: 1 [1127/96 (1174%)]\tLoss: 5.699211\n",
      "Train Epoche: 1 [1128/96 (1175%)]\tLoss: 3.621699\n",
      "Train Epoche: 1 [1129/96 (1176%)]\tLoss: 8.832152\n",
      "Train Epoche: 1 [1130/96 (1177%)]\tLoss: 5.735159\n",
      "Train Epoche: 1 [1131/96 (1178%)]\tLoss: 1.401707\n",
      "Train Epoche: 1 [1132/96 (1179%)]\tLoss: 32.857555\n",
      "Train Epoche: 1 [1133/96 (1180%)]\tLoss: 0.007029\n",
      "Train Epoche: 1 [1134/96 (1181%)]\tLoss: 89.193497\n",
      "Train Epoche: 1 [1135/96 (1182%)]\tLoss: 388.851807\n",
      "Train Epoche: 1 [1136/96 (1183%)]\tLoss: 262.628052\n",
      "Train Epoche: 1 [1137/96 (1184%)]\tLoss: 15.427022\n",
      "Train Epoche: 1 [1138/96 (1185%)]\tLoss: 18.277775\n",
      "Train Epoche: 1 [1139/96 (1186%)]\tLoss: 25.742567\n",
      "Train Epoche: 1 [1140/96 (1188%)]\tLoss: 4.310699\n",
      "Train Epoche: 1 [1141/96 (1189%)]\tLoss: 65.158424\n",
      "Train Epoche: 1 [1142/96 (1190%)]\tLoss: 59.269043\n",
      "Train Epoche: 1 [1143/96 (1191%)]\tLoss: 45.446419\n",
      "Train Epoche: 1 [1144/96 (1192%)]\tLoss: 2.056744\n",
      "Train Epoche: 1 [1145/96 (1193%)]\tLoss: 9.884431\n",
      "Train Epoche: 1 [1146/96 (1194%)]\tLoss: 10.703297\n",
      "Train Epoche: 1 [1147/96 (1195%)]\tLoss: 0.206060\n",
      "Train Epoche: 1 [1148/96 (1196%)]\tLoss: 0.148362\n",
      "Train Epoche: 1 [1149/96 (1197%)]\tLoss: 39.880173\n",
      "Train Epoche: 1 [1150/96 (1198%)]\tLoss: 1.642481\n",
      "Train Epoche: 1 [1151/96 (1199%)]\tLoss: 2.258994\n",
      "Train Epoche: 1 [1152/96 (1200%)]\tLoss: 3.286069\n",
      "Train Epoche: 1 [1153/96 (1201%)]\tLoss: 1.568631\n",
      "Train Epoche: 1 [1154/96 (1202%)]\tLoss: 46.020836\n",
      "Train Epoche: 1 [1155/96 (1203%)]\tLoss: 11.388450\n",
      "Train Epoche: 1 [1156/96 (1204%)]\tLoss: 0.182313\n",
      "Train Epoche: 1 [1157/96 (1205%)]\tLoss: 0.929941\n",
      "Train Epoche: 1 [1158/96 (1206%)]\tLoss: 3.342893\n",
      "Train Epoche: 1 [1159/96 (1207%)]\tLoss: 1.352500\n",
      "Train Epoche: 1 [1160/96 (1208%)]\tLoss: 1.629145\n",
      "Train Epoche: 1 [1161/96 (1209%)]\tLoss: 30.378128\n",
      "Train Epoche: 1 [1162/96 (1210%)]\tLoss: 20.709774\n",
      "Train Epoche: 1 [1163/96 (1211%)]\tLoss: 0.005148\n",
      "Train Epoche: 1 [1164/96 (1212%)]\tLoss: 0.042840\n",
      "Train Epoche: 1 [1165/96 (1214%)]\tLoss: 3.970426\n",
      "Train Epoche: 1 [1166/96 (1215%)]\tLoss: 12.821589\n",
      "Train Epoche: 1 [1167/96 (1216%)]\tLoss: 23.175966\n",
      "Train Epoche: 1 [1168/96 (1217%)]\tLoss: 14.855368\n",
      "Train Epoche: 1 [1169/96 (1218%)]\tLoss: 23.417284\n",
      "Train Epoche: 1 [1170/96 (1219%)]\tLoss: 27.122282\n",
      "Train Epoche: 1 [1171/96 (1220%)]\tLoss: 22.502943\n",
      "Train Epoche: 1 [1172/96 (1221%)]\tLoss: 10.379794\n",
      "Train Epoche: 1 [1173/96 (1222%)]\tLoss: 67.094498\n",
      "Train Epoche: 1 [1174/96 (1223%)]\tLoss: 93.795670\n",
      "Train Epoche: 1 [1175/96 (1224%)]\tLoss: 9.873559\n",
      "Train Epoche: 1 [1176/96 (1225%)]\tLoss: 47.406918\n",
      "Train Epoche: 1 [1177/96 (1226%)]\tLoss: 66.316956\n",
      "Train Epoche: 1 [1178/96 (1227%)]\tLoss: 10.903075\n",
      "Train Epoche: 1 [1179/96 (1228%)]\tLoss: 39.289982\n",
      "Train Epoche: 1 [1180/96 (1229%)]\tLoss: 12.835348\n",
      "Train Epoche: 1 [1181/96 (1230%)]\tLoss: 57.038567\n",
      "Train Epoche: 1 [1182/96 (1231%)]\tLoss: 27.825319\n",
      "Train Epoche: 1 [1183/96 (1232%)]\tLoss: 42.300873\n",
      "Train Epoche: 1 [1184/96 (1233%)]\tLoss: 6.896983\n",
      "Train Epoche: 1 [1185/96 (1234%)]\tLoss: 2.901291\n",
      "Train Epoche: 1 [1186/96 (1235%)]\tLoss: 10.020286\n",
      "Train Epoche: 1 [1187/96 (1236%)]\tLoss: 4.809633\n",
      "Train Epoche: 1 [1188/96 (1238%)]\tLoss: 36.182983\n",
      "Train Epoche: 1 [1189/96 (1239%)]\tLoss: 6.517626\n",
      "Train Epoche: 1 [1190/96 (1240%)]\tLoss: 4.752513\n",
      "Train Epoche: 1 [1191/96 (1241%)]\tLoss: 0.070127\n",
      "Train Epoche: 1 [1192/96 (1242%)]\tLoss: 0.227126\n",
      "Train Epoche: 1 [1193/96 (1243%)]\tLoss: 0.895005\n",
      "Train Epoche: 1 [1194/96 (1244%)]\tLoss: 1.760780\n",
      "Train Epoche: 1 [1195/96 (1245%)]\tLoss: 4.203244\n",
      "Train Epoche: 1 [1196/96 (1246%)]\tLoss: 0.843137\n",
      "Train Epoche: 1 [1197/96 (1247%)]\tLoss: 0.003785\n",
      "Train Epoche: 1 [1198/96 (1248%)]\tLoss: 236.432587\n",
      "Train Epoche: 1 [1199/96 (1249%)]\tLoss: 3.148365\n",
      "Train Epoche: 1 [1200/96 (1250%)]\tLoss: 23.149454\n",
      "Train Epoche: 1 [1201/96 (1251%)]\tLoss: 8.931700\n",
      "Train Epoche: 1 [1202/96 (1252%)]\tLoss: 0.027612\n",
      "Train Epoche: 1 [1203/96 (1253%)]\tLoss: 186.698730\n",
      "Train Epoche: 1 [1204/96 (1254%)]\tLoss: 0.074920\n",
      "Train Epoche: 1 [1205/96 (1255%)]\tLoss: 4.655186\n",
      "Train Epoche: 1 [1206/96 (1256%)]\tLoss: 2.423565\n",
      "Train Epoche: 1 [1207/96 (1257%)]\tLoss: 13.368345\n",
      "Train Epoche: 1 [1208/96 (1258%)]\tLoss: 0.558334\n",
      "Train Epoche: 1 [1209/96 (1259%)]\tLoss: 48.068295\n",
      "Train Epoche: 1 [1210/96 (1260%)]\tLoss: 6.186495\n",
      "Train Epoche: 1 [1211/96 (1261%)]\tLoss: 0.315656\n",
      "Train Epoche: 1 [1212/96 (1262%)]\tLoss: 0.202835\n",
      "Train Epoche: 1 [1213/96 (1264%)]\tLoss: 4.026713\n",
      "Train Epoche: 1 [1214/96 (1265%)]\tLoss: 0.296003\n",
      "Train Epoche: 1 [1215/96 (1266%)]\tLoss: 0.912520\n",
      "Train Epoche: 1 [1216/96 (1267%)]\tLoss: 0.479012\n",
      "Train Epoche: 1 [1217/96 (1268%)]\tLoss: 56.350525\n",
      "Train Epoche: 1 [1218/96 (1269%)]\tLoss: 4.117667\n",
      "Train Epoche: 1 [1219/96 (1270%)]\tLoss: 0.559666\n",
      "Train Epoche: 1 [1220/96 (1271%)]\tLoss: 1.493850\n",
      "Train Epoche: 1 [1221/96 (1272%)]\tLoss: 2.506430\n",
      "Train Epoche: 1 [1222/96 (1273%)]\tLoss: 20.511559\n",
      "Train Epoche: 1 [1223/96 (1274%)]\tLoss: 3.257234\n",
      "Train Epoche: 1 [1224/96 (1275%)]\tLoss: 1.517475\n",
      "Train Epoche: 1 [1225/96 (1276%)]\tLoss: 1.178722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1226/96 (1277%)]\tLoss: 3.010096\n",
      "Train Epoche: 1 [1227/96 (1278%)]\tLoss: 1.464199\n",
      "Train Epoche: 1 [1228/96 (1279%)]\tLoss: 0.167523\n",
      "Train Epoche: 1 [1229/96 (1280%)]\tLoss: 23.377438\n",
      "Train Epoche: 1 [1230/96 (1281%)]\tLoss: 2.942174\n",
      "Train Epoche: 1 [1231/96 (1282%)]\tLoss: 24.546675\n",
      "Train Epoche: 1 [1232/96 (1283%)]\tLoss: 1.154662\n",
      "Train Epoche: 1 [1233/96 (1284%)]\tLoss: 5.468163\n",
      "Train Epoche: 1 [1234/96 (1285%)]\tLoss: 2.077748\n",
      "Train Epoche: 1 [1235/96 (1286%)]\tLoss: 4.546589\n",
      "Train Epoche: 1 [1236/96 (1288%)]\tLoss: 21.393766\n",
      "Train Epoche: 1 [1237/96 (1289%)]\tLoss: 11.509004\n",
      "Train Epoche: 1 [1238/96 (1290%)]\tLoss: 0.085188\n",
      "Train Epoche: 1 [1239/96 (1291%)]\tLoss: 39.210674\n",
      "Train Epoche: 1 [1240/96 (1292%)]\tLoss: 1.732403\n",
      "Train Epoche: 1 [1241/96 (1293%)]\tLoss: 1.240405\n",
      "Train Epoche: 1 [1242/96 (1294%)]\tLoss: 0.393511\n",
      "Train Epoche: 1 [1243/96 (1295%)]\tLoss: 4.689218\n",
      "Train Epoche: 1 [1244/96 (1296%)]\tLoss: 16.040167\n",
      "Train Epoche: 1 [1245/96 (1297%)]\tLoss: 1.883907\n",
      "Train Epoche: 1 [1246/96 (1298%)]\tLoss: 15.334021\n",
      "Train Epoche: 1 [1247/96 (1299%)]\tLoss: 5.696527\n",
      "Train Epoche: 1 [1248/96 (1300%)]\tLoss: 2.822141\n",
      "Train Epoche: 1 [1249/96 (1301%)]\tLoss: 26.982870\n",
      "Train Epoche: 1 [1250/96 (1302%)]\tLoss: 27.671474\n",
      "Train Epoche: 1 [1251/96 (1303%)]\tLoss: 0.115403\n",
      "Train Epoche: 1 [1252/96 (1304%)]\tLoss: 54.266037\n",
      "Train Epoche: 1 [1253/96 (1305%)]\tLoss: 14.660230\n",
      "Train Epoche: 1 [1254/96 (1306%)]\tLoss: 9.342364\n",
      "Train Epoche: 1 [1255/96 (1307%)]\tLoss: 20.216248\n",
      "Train Epoche: 1 [1256/96 (1308%)]\tLoss: 1.326713\n",
      "Train Epoche: 1 [1257/96 (1309%)]\tLoss: 7.348880\n",
      "Train Epoche: 1 [1258/96 (1310%)]\tLoss: 6.113348\n",
      "Train Epoche: 1 [1259/96 (1311%)]\tLoss: 33.123154\n",
      "Train Epoche: 1 [1260/96 (1312%)]\tLoss: 0.218230\n",
      "Train Epoche: 1 [1261/96 (1314%)]\tLoss: 16.605803\n",
      "Train Epoche: 1 [1262/96 (1315%)]\tLoss: 5.605329\n",
      "Train Epoche: 1 [1263/96 (1316%)]\tLoss: 9.050906\n",
      "Train Epoche: 1 [1264/96 (1317%)]\tLoss: 5.956456\n",
      "Train Epoche: 1 [1265/96 (1318%)]\tLoss: 0.031957\n",
      "Train Epoche: 1 [1266/96 (1319%)]\tLoss: 244.418701\n",
      "Train Epoche: 1 [1267/96 (1320%)]\tLoss: 14.089636\n",
      "Train Epoche: 1 [1268/96 (1321%)]\tLoss: 6.991851\n",
      "Train Epoche: 1 [1269/96 (1322%)]\tLoss: 0.763769\n",
      "Train Epoche: 1 [1270/96 (1323%)]\tLoss: 0.000149\n",
      "Train Epoche: 1 [1271/96 (1324%)]\tLoss: 1.937447\n",
      "Train Epoche: 1 [1272/96 (1325%)]\tLoss: 0.219010\n",
      "Train Epoche: 1 [1273/96 (1326%)]\tLoss: 2.325702\n",
      "Train Epoche: 1 [1274/96 (1327%)]\tLoss: 164.240738\n",
      "Train Epoche: 1 [1275/96 (1328%)]\tLoss: 0.036254\n",
      "Train Epoche: 1 [1276/96 (1329%)]\tLoss: 32.246326\n",
      "Train Epoche: 1 [1277/96 (1330%)]\tLoss: 0.923549\n",
      "Train Epoche: 1 [1278/96 (1331%)]\tLoss: 2.118573\n",
      "Train Epoche: 1 [1279/96 (1332%)]\tLoss: 5.447163\n",
      "Train Epoche: 1 [1280/96 (1333%)]\tLoss: 5.226279\n",
      "Train Epoche: 1 [1281/96 (1334%)]\tLoss: 18.132442\n",
      "Train Epoche: 1 [1282/96 (1335%)]\tLoss: 21.124081\n",
      "Train Epoche: 1 [1283/96 (1336%)]\tLoss: 0.530722\n",
      "Train Epoche: 1 [1284/96 (1338%)]\tLoss: 1.055633\n",
      "Train Epoche: 1 [1285/96 (1339%)]\tLoss: 2.783065\n",
      "Train Epoche: 1 [1286/96 (1340%)]\tLoss: 7.855705\n",
      "Train Epoche: 1 [1287/96 (1341%)]\tLoss: 0.606968\n",
      "Train Epoche: 1 [1288/96 (1342%)]\tLoss: 63.323059\n",
      "Train Epoche: 1 [1289/96 (1343%)]\tLoss: 2.531052\n",
      "Train Epoche: 1 [1290/96 (1344%)]\tLoss: 20.181547\n",
      "Train Epoche: 1 [1291/96 (1345%)]\tLoss: 28.210073\n",
      "Train Epoche: 1 [1292/96 (1346%)]\tLoss: 1.868065\n",
      "Train Epoche: 1 [1293/96 (1347%)]\tLoss: 53.835949\n",
      "Train Epoche: 1 [1294/96 (1348%)]\tLoss: 0.007972\n",
      "Train Epoche: 1 [1295/96 (1349%)]\tLoss: 0.567731\n",
      "Train Epoche: 1 [1296/96 (1350%)]\tLoss: 4.549054\n",
      "Train Epoche: 1 [1297/96 (1351%)]\tLoss: 0.721988\n",
      "Train Epoche: 1 [1298/96 (1352%)]\tLoss: 0.268232\n",
      "Train Epoche: 1 [1299/96 (1353%)]\tLoss: 5.773765\n",
      "Train Epoche: 1 [1300/96 (1354%)]\tLoss: 3.234993\n",
      "Train Epoche: 1 [1301/96 (1355%)]\tLoss: 6.127870\n",
      "Train Epoche: 1 [1302/96 (1356%)]\tLoss: 0.309561\n",
      "Train Epoche: 1 [1303/96 (1357%)]\tLoss: 6.621715\n",
      "Train Epoche: 1 [1304/96 (1358%)]\tLoss: 1.404169\n",
      "Train Epoche: 1 [1305/96 (1359%)]\tLoss: 8.487280\n",
      "Train Epoche: 1 [1306/96 (1360%)]\tLoss: 24.473257\n",
      "Train Epoche: 1 [1307/96 (1361%)]\tLoss: 14.292688\n",
      "Train Epoche: 1 [1308/96 (1362%)]\tLoss: 1.000433\n",
      "Train Epoche: 1 [1309/96 (1364%)]\tLoss: 3.961405\n",
      "Train Epoche: 1 [1310/96 (1365%)]\tLoss: 2.563622\n",
      "Train Epoche: 1 [1311/96 (1366%)]\tLoss: 106.245773\n",
      "Train Epoche: 1 [1312/96 (1367%)]\tLoss: 4.808156\n",
      "Train Epoche: 1 [1313/96 (1368%)]\tLoss: 1.464444\n",
      "Train Epoche: 1 [1314/96 (1369%)]\tLoss: 11.843513\n",
      "Train Epoche: 1 [1315/96 (1370%)]\tLoss: 0.498290\n",
      "Train Epoche: 1 [1316/96 (1371%)]\tLoss: 0.032731\n",
      "Train Epoche: 1 [1317/96 (1372%)]\tLoss: 1.616658\n",
      "Train Epoche: 1 [1318/96 (1373%)]\tLoss: 277.454224\n",
      "Train Epoche: 1 [1319/96 (1374%)]\tLoss: 2.162861\n",
      "Train Epoche: 1 [1320/96 (1375%)]\tLoss: 1.774630\n",
      "Train Epoche: 1 [1321/96 (1376%)]\tLoss: 11.552554\n",
      "Train Epoche: 1 [1322/96 (1377%)]\tLoss: 0.038599\n",
      "Train Epoche: 1 [1323/96 (1378%)]\tLoss: 17.348925\n",
      "Train Epoche: 1 [1324/96 (1379%)]\tLoss: 60.231533\n",
      "Train Epoche: 1 [1325/96 (1380%)]\tLoss: 11.981667\n",
      "Train Epoche: 1 [1326/96 (1381%)]\tLoss: 0.763404\n",
      "Train Epoche: 1 [1327/96 (1382%)]\tLoss: 16.194895\n",
      "Train Epoche: 1 [1328/96 (1383%)]\tLoss: 8.526930\n",
      "Train Epoche: 1 [1329/96 (1384%)]\tLoss: 297.201202\n",
      "Train Epoche: 1 [1330/96 (1385%)]\tLoss: 1.259513\n",
      "Train Epoche: 1 [1331/96 (1386%)]\tLoss: 18.923780\n",
      "Train Epoche: 1 [1332/96 (1388%)]\tLoss: 15.564452\n",
      "Train Epoche: 1 [1333/96 (1389%)]\tLoss: 12.291451\n",
      "Train Epoche: 1 [1334/96 (1390%)]\tLoss: 44.369282\n",
      "Train Epoche: 1 [1335/96 (1391%)]\tLoss: 14.930443\n",
      "Train Epoche: 1 [1336/96 (1392%)]\tLoss: 1.956124\n",
      "Train Epoche: 1 [1337/96 (1393%)]\tLoss: 3.150766\n",
      "Train Epoche: 1 [1338/96 (1394%)]\tLoss: 5.285479\n",
      "Train Epoche: 1 [1339/96 (1395%)]\tLoss: 11.990726\n",
      "Train Epoche: 1 [1340/96 (1396%)]\tLoss: 116.220627\n",
      "Train Epoche: 1 [1341/96 (1397%)]\tLoss: 132.636520\n",
      "Train Epoche: 1 [1342/96 (1398%)]\tLoss: 1.664331\n",
      "Train Epoche: 1 [1343/96 (1399%)]\tLoss: 19.499880\n",
      "Train Epoche: 1 [1344/96 (1400%)]\tLoss: 53.833080\n",
      "Train Epoche: 1 [1345/96 (1401%)]\tLoss: 0.239646\n",
      "Train Epoche: 1 [1346/96 (1402%)]\tLoss: 3.852606\n",
      "Train Epoche: 1 [1347/96 (1403%)]\tLoss: 2.223167\n",
      "Train Epoche: 1 [1348/96 (1404%)]\tLoss: 0.775908\n",
      "Train Epoche: 1 [1349/96 (1405%)]\tLoss: 8.347552\n",
      "Train Epoche: 1 [1350/96 (1406%)]\tLoss: 10.097584\n",
      "Train Epoche: 1 [1351/96 (1407%)]\tLoss: 2.443717\n",
      "Train Epoche: 1 [1352/96 (1408%)]\tLoss: 2.265674\n",
      "Train Epoche: 1 [1353/96 (1409%)]\tLoss: 10.651965\n",
      "Train Epoche: 1 [1354/96 (1410%)]\tLoss: 0.006712\n",
      "Train Epoche: 1 [1355/96 (1411%)]\tLoss: 3.621909\n",
      "Train Epoche: 1 [1356/96 (1412%)]\tLoss: 0.013117\n",
      "Train Epoche: 1 [1357/96 (1414%)]\tLoss: 0.008927\n",
      "Train Epoche: 1 [1358/96 (1415%)]\tLoss: 9.105565\n",
      "Train Epoche: 1 [1359/96 (1416%)]\tLoss: 34.108253\n",
      "Train Epoche: 1 [1360/96 (1417%)]\tLoss: 0.000424\n",
      "Train Epoche: 1 [1361/96 (1418%)]\tLoss: 2.234149\n",
      "Train Epoche: 1 [1362/96 (1419%)]\tLoss: 0.363463\n",
      "Train Epoche: 1 [1363/96 (1420%)]\tLoss: 49.063988\n",
      "Train Epoche: 1 [1364/96 (1421%)]\tLoss: 252.188187\n",
      "Train Epoche: 1 [1365/96 (1422%)]\tLoss: 7.256507\n",
      "Train Epoche: 1 [1366/96 (1423%)]\tLoss: 1.882679\n",
      "Train Epoche: 1 [1367/96 (1424%)]\tLoss: 15.692819\n",
      "Train Epoche: 1 [1368/96 (1425%)]\tLoss: 6.816594\n",
      "Train Epoche: 1 [1369/96 (1426%)]\tLoss: 36.029408\n",
      "Train Epoche: 1 [1370/96 (1427%)]\tLoss: 1.732481\n",
      "Train Epoche: 1 [1371/96 (1428%)]\tLoss: 21.212433\n",
      "Train Epoche: 1 [1372/96 (1429%)]\tLoss: 17.677422\n",
      "Train Epoche: 1 [1373/96 (1430%)]\tLoss: 4.373053\n",
      "Train Epoche: 1 [1374/96 (1431%)]\tLoss: 10.523820\n",
      "Train Epoche: 1 [1375/96 (1432%)]\tLoss: 0.040008\n",
      "Train Epoche: 1 [1376/96 (1433%)]\tLoss: 0.000126\n",
      "Train Epoche: 1 [1377/96 (1434%)]\tLoss: 10.060900\n",
      "Train Epoche: 1 [1378/96 (1435%)]\tLoss: 34.203362\n",
      "Train Epoche: 1 [1379/96 (1436%)]\tLoss: 2.499499\n",
      "Train Epoche: 1 [1380/96 (1438%)]\tLoss: 0.046153\n",
      "Train Epoche: 1 [1381/96 (1439%)]\tLoss: 0.336340\n",
      "Train Epoche: 1 [1382/96 (1440%)]\tLoss: 3.106382\n",
      "Train Epoche: 1 [1383/96 (1441%)]\tLoss: 10.692897\n",
      "Train Epoche: 1 [1384/96 (1442%)]\tLoss: 0.335841\n",
      "Train Epoche: 1 [1385/96 (1443%)]\tLoss: 3.544824\n",
      "Train Epoche: 1 [1386/96 (1444%)]\tLoss: 5.342854\n",
      "Train Epoche: 1 [1387/96 (1445%)]\tLoss: 1.115980\n",
      "Train Epoche: 1 [1388/96 (1446%)]\tLoss: 19.633657\n",
      "Train Epoche: 1 [1389/96 (1447%)]\tLoss: 57.443878\n",
      "Train Epoche: 1 [1390/96 (1448%)]\tLoss: 35.456947\n",
      "Train Epoche: 1 [1391/96 (1449%)]\tLoss: 0.884709\n",
      "Train Epoche: 1 [1392/96 (1450%)]\tLoss: 12.914587\n",
      "Train Epoche: 1 [1393/96 (1451%)]\tLoss: 24.973877\n",
      "Train Epoche: 1 [1394/96 (1452%)]\tLoss: 47.806477\n",
      "Train Epoche: 1 [1395/96 (1453%)]\tLoss: 4.119664\n",
      "Train Epoche: 1 [1396/96 (1454%)]\tLoss: 0.149590\n",
      "Train Epoche: 1 [1397/96 (1455%)]\tLoss: 1.787801\n",
      "Train Epoche: 1 [1398/96 (1456%)]\tLoss: 8.871749\n",
      "Train Epoche: 1 [1399/96 (1457%)]\tLoss: 2.115533\n",
      "Train Epoche: 1 [1400/96 (1458%)]\tLoss: 4.139080\n",
      "Train Epoche: 1 [1401/96 (1459%)]\tLoss: 2.663231\n",
      "Train Epoche: 1 [1402/96 (1460%)]\tLoss: 0.165020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1403/96 (1461%)]\tLoss: 1.730270\n",
      "Train Epoche: 1 [1404/96 (1462%)]\tLoss: 0.010484\n",
      "Train Epoche: 1 [1405/96 (1464%)]\tLoss: 1.891142\n",
      "Train Epoche: 1 [1406/96 (1465%)]\tLoss: 1.453739\n",
      "Train Epoche: 1 [1407/96 (1466%)]\tLoss: 3.696170\n",
      "Train Epoche: 1 [1408/96 (1467%)]\tLoss: 0.842509\n",
      "Train Epoche: 1 [1409/96 (1468%)]\tLoss: 49.497501\n",
      "Train Epoche: 1 [1410/96 (1469%)]\tLoss: 24.915024\n",
      "Train Epoche: 1 [1411/96 (1470%)]\tLoss: 120.475609\n",
      "Train Epoche: 1 [1412/96 (1471%)]\tLoss: 10.718827\n",
      "Train Epoche: 1 [1413/96 (1472%)]\tLoss: 0.308108\n",
      "Train Epoche: 1 [1414/96 (1473%)]\tLoss: 2.563802\n",
      "Train Epoche: 1 [1415/96 (1474%)]\tLoss: 4.043172\n",
      "Train Epoche: 1 [1416/96 (1475%)]\tLoss: 3.006490\n",
      "Train Epoche: 1 [1417/96 (1476%)]\tLoss: 0.995764\n",
      "Train Epoche: 1 [1418/96 (1477%)]\tLoss: 154.489120\n",
      "Train Epoche: 1 [1419/96 (1478%)]\tLoss: 16.693840\n",
      "Train Epoche: 1 [1420/96 (1479%)]\tLoss: 1.344082\n",
      "Train Epoche: 1 [1421/96 (1480%)]\tLoss: 0.093708\n",
      "Train Epoche: 1 [1422/96 (1481%)]\tLoss: 0.371390\n",
      "Train Epoche: 1 [1423/96 (1482%)]\tLoss: 0.123700\n",
      "Train Epoche: 1 [1424/96 (1483%)]\tLoss: 0.925668\n",
      "Train Epoche: 1 [1425/96 (1484%)]\tLoss: 4.489391\n",
      "Train Epoche: 1 [1426/96 (1485%)]\tLoss: 0.005608\n",
      "Train Epoche: 1 [1427/96 (1486%)]\tLoss: 8.461972\n",
      "Train Epoche: 1 [1428/96 (1488%)]\tLoss: 2.316999\n",
      "Train Epoche: 1 [1429/96 (1489%)]\tLoss: 0.591705\n",
      "Train Epoche: 1 [1430/96 (1490%)]\tLoss: 1.891039\n",
      "Train Epoche: 1 [1431/96 (1491%)]\tLoss: 24.501734\n",
      "Train Epoche: 1 [1432/96 (1492%)]\tLoss: 61.545437\n",
      "Train Epoche: 1 [1433/96 (1493%)]\tLoss: 0.005162\n",
      "Train Epoche: 1 [1434/96 (1494%)]\tLoss: 77.266296\n",
      "Train Epoche: 1 [1435/96 (1495%)]\tLoss: 14.971585\n",
      "Train Epoche: 1 [1436/96 (1496%)]\tLoss: 4.389626\n",
      "Train Epoche: 1 [1437/96 (1497%)]\tLoss: 2.400576\n",
      "Train Epoche: 1 [1438/96 (1498%)]\tLoss: 8.441707\n",
      "Train Epoche: 1 [1439/96 (1499%)]\tLoss: 0.326483\n",
      "Train Epoche: 1 [1440/96 (1500%)]\tLoss: 0.509894\n",
      "Train Epoche: 1 [1441/96 (1501%)]\tLoss: 0.419521\n",
      "Train Epoche: 1 [1442/96 (1502%)]\tLoss: 230.725708\n",
      "Train Epoche: 1 [1443/96 (1503%)]\tLoss: 45.655037\n",
      "Train Epoche: 1 [1444/96 (1504%)]\tLoss: 2.499386\n",
      "Train Epoche: 1 [1445/96 (1505%)]\tLoss: 9.554426\n",
      "Train Epoche: 1 [1446/96 (1506%)]\tLoss: 7.373121\n",
      "Train Epoche: 1 [1447/96 (1507%)]\tLoss: 3.813423\n",
      "Train Epoche: 1 [1448/96 (1508%)]\tLoss: 0.977356\n",
      "Train Epoche: 1 [1449/96 (1509%)]\tLoss: 0.452696\n",
      "Train Epoche: 1 [1450/96 (1510%)]\tLoss: 21.436485\n",
      "Train Epoche: 1 [1451/96 (1511%)]\tLoss: 47.118820\n",
      "Train Epoche: 1 [1452/96 (1512%)]\tLoss: 1.196404\n",
      "Train Epoche: 1 [1453/96 (1514%)]\tLoss: 9.734382\n",
      "Train Epoche: 1 [1454/96 (1515%)]\tLoss: 0.059685\n",
      "Train Epoche: 1 [1455/96 (1516%)]\tLoss: 5.034515\n",
      "Train Epoche: 1 [1456/96 (1517%)]\tLoss: 27.847670\n",
      "Train Epoche: 1 [1457/96 (1518%)]\tLoss: 85.598145\n",
      "Train Epoche: 1 [1458/96 (1519%)]\tLoss: 1.880583\n",
      "Train Epoche: 1 [1459/96 (1520%)]\tLoss: 25.095047\n",
      "Train Epoche: 1 [1460/96 (1521%)]\tLoss: 1.414047\n",
      "Train Epoche: 1 [1461/96 (1522%)]\tLoss: 1.743168\n",
      "Train Epoche: 1 [1462/96 (1523%)]\tLoss: 0.226382\n",
      "Train Epoche: 1 [1463/96 (1524%)]\tLoss: 0.146128\n",
      "Train Epoche: 1 [1464/96 (1525%)]\tLoss: 8.552658\n",
      "Train Epoche: 1 [1465/96 (1526%)]\tLoss: 0.241138\n",
      "Train Epoche: 1 [1466/96 (1527%)]\tLoss: 0.265207\n",
      "Train Epoche: 1 [1467/96 (1528%)]\tLoss: 0.001125\n",
      "Train Epoche: 1 [1468/96 (1529%)]\tLoss: 162.660858\n",
      "Train Epoche: 1 [1469/96 (1530%)]\tLoss: 4.191804\n",
      "Train Epoche: 1 [1470/96 (1531%)]\tLoss: 9.119164\n",
      "Train Epoche: 1 [1471/96 (1532%)]\tLoss: 0.892278\n",
      "Train Epoche: 1 [1472/96 (1533%)]\tLoss: 9.171960\n",
      "Train Epoche: 1 [1473/96 (1534%)]\tLoss: 1.668947\n",
      "Train Epoche: 1 [1474/96 (1535%)]\tLoss: 25.020714\n",
      "Train Epoche: 1 [1475/96 (1536%)]\tLoss: 2.493685\n",
      "Train Epoche: 1 [1476/96 (1538%)]\tLoss: 3.968777\n",
      "Train Epoche: 1 [1477/96 (1539%)]\tLoss: 122.550064\n",
      "Train Epoche: 1 [1478/96 (1540%)]\tLoss: 17.594774\n",
      "Train Epoche: 1 [1479/96 (1541%)]\tLoss: 0.182043\n",
      "Train Epoche: 1 [1480/96 (1542%)]\tLoss: 0.983596\n",
      "Train Epoche: 1 [1481/96 (1543%)]\tLoss: 1.503794\n",
      "Train Epoche: 1 [1482/96 (1544%)]\tLoss: 10.049590\n",
      "Train Epoche: 1 [1483/96 (1545%)]\tLoss: 13.152478\n",
      "Train Epoche: 1 [1484/96 (1546%)]\tLoss: 2.997952\n",
      "Train Epoche: 1 [1485/96 (1547%)]\tLoss: 6.178411\n",
      "Train Epoche: 1 [1486/96 (1548%)]\tLoss: 188.156876\n",
      "Train Epoche: 1 [1487/96 (1549%)]\tLoss: 0.023322\n",
      "Train Epoche: 1 [1488/96 (1550%)]\tLoss: 7.254514\n",
      "Train Epoche: 1 [1489/96 (1551%)]\tLoss: 0.017853\n",
      "Train Epoche: 1 [1490/96 (1552%)]\tLoss: 6.467190\n",
      "Train Epoche: 1 [1491/96 (1553%)]\tLoss: 0.000014\n",
      "Train Epoche: 1 [1492/96 (1554%)]\tLoss: 2.429616\n",
      "Train Epoche: 1 [1493/96 (1555%)]\tLoss: 12.241877\n",
      "Train Epoche: 1 [1494/96 (1556%)]\tLoss: 108.493141\n",
      "Train Epoche: 1 [1495/96 (1557%)]\tLoss: 5.596429\n",
      "Train Epoche: 1 [1496/96 (1558%)]\tLoss: 18.040482\n",
      "Train Epoche: 1 [1497/96 (1559%)]\tLoss: 17.125120\n",
      "Train Epoche: 1 [1498/96 (1560%)]\tLoss: 8.755002\n",
      "Train Epoche: 1 [1499/96 (1561%)]\tLoss: 10.358142\n",
      "Train Epoche: 1 [1500/96 (1562%)]\tLoss: 0.246330\n",
      "Train Epoche: 1 [1501/96 (1564%)]\tLoss: 6.795034\n",
      "Train Epoche: 1 [1502/96 (1565%)]\tLoss: 0.004230\n",
      "Train Epoche: 1 [1503/96 (1566%)]\tLoss: 12.094392\n",
      "Train Epoche: 1 [1504/96 (1567%)]\tLoss: 0.567665\n",
      "Train Epoche: 1 [1505/96 (1568%)]\tLoss: 6.942121\n",
      "Train Epoche: 1 [1506/96 (1569%)]\tLoss: 2.355861\n",
      "Train Epoche: 1 [1507/96 (1570%)]\tLoss: 1.107202\n",
      "Train Epoche: 1 [1508/96 (1571%)]\tLoss: 1.722907\n",
      "Train Epoche: 1 [1509/96 (1572%)]\tLoss: 5.789630\n",
      "Train Epoche: 1 [1510/96 (1573%)]\tLoss: 0.540321\n",
      "Train Epoche: 1 [1511/96 (1574%)]\tLoss: 6.188118\n",
      "Train Epoche: 1 [1512/96 (1575%)]\tLoss: 0.420031\n",
      "Train Epoche: 1 [1513/96 (1576%)]\tLoss: 0.032088\n",
      "Train Epoche: 1 [1514/96 (1577%)]\tLoss: 0.165740\n",
      "Train Epoche: 1 [1515/96 (1578%)]\tLoss: 13.438615\n",
      "Train Epoche: 1 [1516/96 (1579%)]\tLoss: 0.216766\n",
      "Train Epoche: 1 [1517/96 (1580%)]\tLoss: 21.141548\n",
      "Train Epoche: 1 [1518/96 (1581%)]\tLoss: 1.369994\n",
      "Train Epoche: 1 [1519/96 (1582%)]\tLoss: 35.689564\n",
      "Train Epoche: 1 [1520/96 (1583%)]\tLoss: 0.047882\n",
      "Train Epoche: 1 [1521/96 (1584%)]\tLoss: 5.238428\n",
      "Train Epoche: 1 [1522/96 (1585%)]\tLoss: 13.562605\n",
      "Train Epoche: 1 [1523/96 (1586%)]\tLoss: 1.996405\n",
      "Train Epoche: 1 [1524/96 (1588%)]\tLoss: 19.016642\n",
      "Train Epoche: 1 [1525/96 (1589%)]\tLoss: 2.257941\n",
      "Train Epoche: 1 [1526/96 (1590%)]\tLoss: 11.155881\n",
      "Train Epoche: 1 [1527/96 (1591%)]\tLoss: 3.675270\n",
      "Train Epoche: 1 [1528/96 (1592%)]\tLoss: 6.623465\n",
      "Train Epoche: 1 [1529/96 (1593%)]\tLoss: 14.410090\n",
      "Train Epoche: 1 [1530/96 (1594%)]\tLoss: 5.194455\n",
      "Train Epoche: 1 [1531/96 (1595%)]\tLoss: 3.630677\n",
      "Train Epoche: 1 [1532/96 (1596%)]\tLoss: 5.715489\n",
      "Train Epoche: 1 [1533/96 (1597%)]\tLoss: 4.241468\n",
      "Train Epoche: 1 [1534/96 (1598%)]\tLoss: 0.524832\n",
      "Train Epoche: 1 [1535/96 (1599%)]\tLoss: 9.984756\n",
      "Train Epoche: 1 [1536/96 (1600%)]\tLoss: 0.773298\n",
      "Train Epoche: 1 [1537/96 (1601%)]\tLoss: 2.222329\n",
      "Train Epoche: 1 [1538/96 (1602%)]\tLoss: 16.741865\n",
      "Train Epoche: 1 [1539/96 (1603%)]\tLoss: 4.659809\n",
      "Train Epoche: 1 [1540/96 (1604%)]\tLoss: 3.450776\n",
      "Train Epoche: 1 [1541/96 (1605%)]\tLoss: 6.487536\n",
      "Train Epoche: 1 [1542/96 (1606%)]\tLoss: 105.181519\n",
      "Train Epoche: 1 [1543/96 (1607%)]\tLoss: 29.890942\n",
      "Train Epoche: 1 [1544/96 (1608%)]\tLoss: 0.627215\n",
      "Train Epoche: 1 [1545/96 (1609%)]\tLoss: 2.596329\n",
      "Train Epoche: 1 [1546/96 (1610%)]\tLoss: 2.047184\n",
      "Train Epoche: 1 [1547/96 (1611%)]\tLoss: 11.927537\n",
      "Train Epoche: 1 [1548/96 (1612%)]\tLoss: 2.079918\n",
      "Train Epoche: 1 [1549/96 (1614%)]\tLoss: 0.775306\n",
      "Train Epoche: 1 [1550/96 (1615%)]\tLoss: 12.596368\n",
      "Train Epoche: 1 [1551/96 (1616%)]\tLoss: 0.129307\n",
      "Train Epoche: 1 [1552/96 (1617%)]\tLoss: 0.047732\n",
      "Train Epoche: 1 [1553/96 (1618%)]\tLoss: 61.450874\n",
      "Train Epoche: 1 [1554/96 (1619%)]\tLoss: 11.537020\n",
      "Train Epoche: 1 [1555/96 (1620%)]\tLoss: 1.160171\n",
      "Train Epoche: 1 [1556/96 (1621%)]\tLoss: 0.911525\n",
      "Train Epoche: 1 [1557/96 (1622%)]\tLoss: 1.413303\n",
      "Train Epoche: 1 [1558/96 (1623%)]\tLoss: 19.444012\n",
      "Train Epoche: 1 [1559/96 (1624%)]\tLoss: 0.090198\n",
      "Train Epoche: 1 [1560/96 (1625%)]\tLoss: 6.533130\n",
      "Train Epoche: 1 [1561/96 (1626%)]\tLoss: 0.278184\n",
      "Train Epoche: 1 [1562/96 (1627%)]\tLoss: 0.226495\n",
      "Train Epoche: 1 [1563/96 (1628%)]\tLoss: 1.977359\n",
      "Train Epoche: 1 [1564/96 (1629%)]\tLoss: 0.013467\n",
      "Train Epoche: 1 [1565/96 (1630%)]\tLoss: 1.654768\n",
      "Train Epoche: 1 [1566/96 (1631%)]\tLoss: 5.762116\n",
      "Train Epoche: 1 [1567/96 (1632%)]\tLoss: 44.762577\n",
      "Train Epoche: 1 [1568/96 (1633%)]\tLoss: 1.817777\n",
      "Train Epoche: 1 [1569/96 (1634%)]\tLoss: 60.268147\n",
      "Train Epoche: 1 [1570/96 (1635%)]\tLoss: 34.474312\n",
      "Train Epoche: 1 [1571/96 (1636%)]\tLoss: 11.480706\n",
      "Train Epoche: 1 [1572/96 (1638%)]\tLoss: 0.647043\n",
      "Train Epoche: 1 [1573/96 (1639%)]\tLoss: 77.563469\n",
      "Train Epoche: 1 [1574/96 (1640%)]\tLoss: 22.578783\n",
      "Train Epoche: 1 [1575/96 (1641%)]\tLoss: 1.293760\n",
      "Train Epoche: 1 [1576/96 (1642%)]\tLoss: 1.351456\n",
      "Train Epoche: 1 [1577/96 (1643%)]\tLoss: 9.093408\n",
      "Train Epoche: 1 [1578/96 (1644%)]\tLoss: 1.006665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1579/96 (1645%)]\tLoss: 1.264189\n",
      "Train Epoche: 1 [1580/96 (1646%)]\tLoss: 50.903996\n",
      "Train Epoche: 1 [1581/96 (1647%)]\tLoss: 0.365962\n",
      "Train Epoche: 1 [1582/96 (1648%)]\tLoss: 0.712994\n",
      "Train Epoche: 1 [1583/96 (1649%)]\tLoss: 3.545787\n",
      "Train Epoche: 1 [1584/96 (1650%)]\tLoss: 0.012986\n",
      "Train Epoche: 1 [1585/96 (1651%)]\tLoss: 4.249968\n",
      "Train Epoche: 1 [1586/96 (1652%)]\tLoss: 15.141215\n",
      "Train Epoche: 1 [1587/96 (1653%)]\tLoss: 64.030380\n",
      "Train Epoche: 1 [1588/96 (1654%)]\tLoss: 0.789200\n",
      "Train Epoche: 1 [1589/96 (1655%)]\tLoss: 1.831866\n",
      "Train Epoche: 1 [1590/96 (1656%)]\tLoss: 0.006035\n",
      "Train Epoche: 1 [1591/96 (1657%)]\tLoss: 10.576365\n",
      "Train Epoche: 1 [1592/96 (1658%)]\tLoss: 3.879988\n",
      "Train Epoche: 1 [1593/96 (1659%)]\tLoss: 0.989594\n",
      "Train Epoche: 1 [1594/96 (1660%)]\tLoss: 5.205925\n",
      "Train Epoche: 1 [1595/96 (1661%)]\tLoss: 0.010566\n",
      "Train Epoche: 1 [1596/96 (1662%)]\tLoss: 0.037206\n",
      "Train Epoche: 1 [1597/96 (1664%)]\tLoss: 2.833454\n",
      "Train Epoche: 1 [1598/96 (1665%)]\tLoss: 1.136903\n",
      "Train Epoche: 1 [1599/96 (1666%)]\tLoss: 0.980700\n",
      "Train Epoche: 1 [1600/96 (1667%)]\tLoss: 313.503235\n",
      "Train Epoche: 1 [1601/96 (1668%)]\tLoss: 1.010514\n",
      "Train Epoche: 1 [1602/96 (1669%)]\tLoss: 34.704002\n",
      "Train Epoche: 1 [1603/96 (1670%)]\tLoss: 28.890995\n",
      "Train Epoche: 1 [1604/96 (1671%)]\tLoss: 2.873882\n",
      "Train Epoche: 1 [1605/96 (1672%)]\tLoss: 4.699534\n",
      "Train Epoche: 1 [1606/96 (1673%)]\tLoss: 0.320249\n",
      "Train Epoche: 1 [1607/96 (1674%)]\tLoss: 3.406896\n",
      "Train Epoche: 1 [1608/96 (1675%)]\tLoss: 0.107739\n",
      "Train Epoche: 1 [1609/96 (1676%)]\tLoss: 8.114229\n",
      "Train Epoche: 1 [1610/96 (1677%)]\tLoss: 34.501820\n",
      "Train Epoche: 1 [1611/96 (1678%)]\tLoss: 3.092749\n",
      "Train Epoche: 1 [1612/96 (1679%)]\tLoss: 7.473495\n",
      "Train Epoche: 1 [1613/96 (1680%)]\tLoss: 33.771255\n",
      "Train Epoche: 1 [1614/96 (1681%)]\tLoss: 126.812172\n",
      "Train Epoche: 1 [1615/96 (1682%)]\tLoss: 12.246041\n",
      "Train Epoche: 1 [1616/96 (1683%)]\tLoss: 11.343372\n",
      "Train Epoche: 1 [1617/96 (1684%)]\tLoss: 45.149368\n",
      "Train Epoche: 1 [1618/96 (1685%)]\tLoss: 70.498871\n",
      "Train Epoche: 1 [1619/96 (1686%)]\tLoss: 310.941376\n",
      "Train Epoche: 1 [1620/96 (1688%)]\tLoss: 81.093140\n",
      "Train Epoche: 1 [1621/96 (1689%)]\tLoss: 53.635197\n",
      "Train Epoche: 1 [1622/96 (1690%)]\tLoss: 10.459393\n",
      "Train Epoche: 1 [1623/96 (1691%)]\tLoss: 281.928162\n",
      "Train Epoche: 1 [1624/96 (1692%)]\tLoss: 8.602750\n",
      "Train Epoche: 1 [1625/96 (1693%)]\tLoss: 20.092840\n",
      "Train Epoche: 1 [1626/96 (1694%)]\tLoss: 27.887432\n",
      "Train Epoche: 1 [1627/96 (1695%)]\tLoss: 40.257977\n",
      "Train Epoche: 1 [1628/96 (1696%)]\tLoss: 11.934546\n",
      "Train Epoche: 1 [1629/96 (1697%)]\tLoss: 7.899678\n",
      "Train Epoche: 1 [1630/96 (1698%)]\tLoss: 79.859406\n",
      "Train Epoche: 1 [1631/96 (1699%)]\tLoss: 2.398966\n",
      "Train Epoche: 1 [1632/96 (1700%)]\tLoss: 1.453558\n",
      "Train Epoche: 1 [1633/96 (1701%)]\tLoss: 3.400504\n",
      "Train Epoche: 1 [1634/96 (1702%)]\tLoss: 15.447556\n",
      "Train Epoche: 1 [1635/96 (1703%)]\tLoss: 0.065967\n",
      "Train Epoche: 1 [1636/96 (1704%)]\tLoss: 0.101402\n",
      "Train Epoche: 1 [1637/96 (1705%)]\tLoss: 0.009524\n",
      "Train Epoche: 1 [1638/96 (1706%)]\tLoss: 2.727486\n",
      "Train Epoche: 1 [1639/96 (1707%)]\tLoss: 21.907545\n",
      "Train Epoche: 1 [1640/96 (1708%)]\tLoss: 10.838531\n",
      "Train Epoche: 1 [1641/96 (1709%)]\tLoss: 0.298804\n",
      "Train Epoche: 1 [1642/96 (1710%)]\tLoss: 0.001880\n",
      "Train Epoche: 1 [1643/96 (1711%)]\tLoss: 1.857164\n",
      "Train Epoche: 1 [1644/96 (1712%)]\tLoss: 184.120377\n",
      "Train Epoche: 1 [1645/96 (1714%)]\tLoss: 0.042154\n",
      "Train Epoche: 1 [1646/96 (1715%)]\tLoss: 0.185781\n",
      "Train Epoche: 1 [1647/96 (1716%)]\tLoss: 1.562340\n",
      "Train Epoche: 1 [1648/96 (1717%)]\tLoss: 0.035790\n",
      "Train Epoche: 1 [1649/96 (1718%)]\tLoss: 3.030551\n",
      "Train Epoche: 1 [1650/96 (1719%)]\tLoss: 1.039113\n",
      "Train Epoche: 1 [1651/96 (1720%)]\tLoss: 0.050270\n",
      "Train Epoche: 1 [1652/96 (1721%)]\tLoss: 1.105154\n",
      "Train Epoche: 1 [1653/96 (1722%)]\tLoss: 93.365753\n",
      "Train Epoche: 1 [1654/96 (1723%)]\tLoss: 6.920046\n",
      "Train Epoche: 1 [1655/96 (1724%)]\tLoss: 15.156107\n",
      "Train Epoche: 1 [1656/96 (1725%)]\tLoss: 7.287522\n",
      "Train Epoche: 1 [1657/96 (1726%)]\tLoss: 18.842936\n",
      "Train Epoche: 1 [1658/96 (1727%)]\tLoss: 28.209707\n",
      "Train Epoche: 1 [1659/96 (1728%)]\tLoss: 31.885155\n",
      "Train Epoche: 1 [1660/96 (1729%)]\tLoss: 17.550766\n",
      "Train Epoche: 1 [1661/96 (1730%)]\tLoss: 11.006345\n",
      "Train Epoche: 1 [1662/96 (1731%)]\tLoss: 41.096634\n",
      "Train Epoche: 1 [1663/96 (1732%)]\tLoss: 13.533083\n",
      "Train Epoche: 1 [1664/96 (1733%)]\tLoss: 1.866943\n",
      "Train Epoche: 1 [1665/96 (1734%)]\tLoss: 4.354764\n",
      "Train Epoche: 1 [1666/96 (1735%)]\tLoss: 0.802221\n",
      "Train Epoche: 1 [1667/96 (1736%)]\tLoss: 1.318898\n",
      "Train Epoche: 1 [1668/96 (1738%)]\tLoss: 0.071890\n",
      "Train Epoche: 1 [1669/96 (1739%)]\tLoss: 0.012485\n",
      "Train Epoche: 1 [1670/96 (1740%)]\tLoss: 2.753355\n",
      "Train Epoche: 1 [1671/96 (1741%)]\tLoss: 1.873547\n",
      "Train Epoche: 1 [1672/96 (1742%)]\tLoss: 73.046761\n",
      "Train Epoche: 1 [1673/96 (1743%)]\tLoss: 72.068047\n",
      "Train Epoche: 1 [1674/96 (1744%)]\tLoss: 40.218674\n",
      "Train Epoche: 1 [1675/96 (1745%)]\tLoss: 1.873850\n",
      "Train Epoche: 1 [1676/96 (1746%)]\tLoss: 0.020280\n",
      "Train Epoche: 1 [1677/96 (1747%)]\tLoss: 0.887070\n",
      "Train Epoche: 1 [1678/96 (1748%)]\tLoss: 99.751167\n",
      "Train Epoche: 1 [1679/96 (1749%)]\tLoss: 14.368177\n",
      "Train Epoche: 1 [1680/96 (1750%)]\tLoss: 47.668102\n",
      "Train Epoche: 1 [1681/96 (1751%)]\tLoss: 0.852930\n",
      "Train Epoche: 1 [1682/96 (1752%)]\tLoss: 12.055533\n",
      "Train Epoche: 1 [1683/96 (1753%)]\tLoss: 2.826196\n",
      "Train Epoche: 1 [1684/96 (1754%)]\tLoss: 1.896137\n",
      "Train Epoche: 1 [1685/96 (1755%)]\tLoss: 352.151550\n",
      "Train Epoche: 1 [1686/96 (1756%)]\tLoss: 2.715636\n",
      "Train Epoche: 1 [1687/96 (1757%)]\tLoss: 0.091380\n",
      "Train Epoche: 1 [1688/96 (1758%)]\tLoss: 2.596157\n",
      "Train Epoche: 1 [1689/96 (1759%)]\tLoss: 12.540845\n",
      "Train Epoche: 1 [1690/96 (1760%)]\tLoss: 3.037276\n",
      "Train Epoche: 1 [1691/96 (1761%)]\tLoss: 82.368515\n",
      "Train Epoche: 1 [1692/96 (1762%)]\tLoss: 11.629608\n",
      "Train Epoche: 1 [1693/96 (1764%)]\tLoss: 6.817498\n",
      "Train Epoche: 1 [1694/96 (1765%)]\tLoss: 74.279099\n",
      "Train Epoche: 1 [1695/96 (1766%)]\tLoss: 2.943190\n",
      "Train Epoche: 1 [1696/96 (1767%)]\tLoss: 2.153413\n",
      "Train Epoche: 1 [1697/96 (1768%)]\tLoss: 0.075399\n",
      "Train Epoche: 1 [1698/96 (1769%)]\tLoss: 69.140839\n",
      "Train Epoche: 1 [1699/96 (1770%)]\tLoss: 23.905081\n",
      "Train Epoche: 1 [1700/96 (1771%)]\tLoss: 3.609933\n",
      "Train Epoche: 1 [1701/96 (1772%)]\tLoss: 3.197613\n",
      "Train Epoche: 1 [1702/96 (1773%)]\tLoss: 7.766560\n",
      "Train Epoche: 1 [1703/96 (1774%)]\tLoss: 13.087605\n",
      "Train Epoche: 1 [1704/96 (1775%)]\tLoss: 0.123837\n",
      "Train Epoche: 1 [1705/96 (1776%)]\tLoss: 197.929688\n",
      "Train Epoche: 1 [1706/96 (1777%)]\tLoss: 19.859619\n",
      "Train Epoche: 1 [1707/96 (1778%)]\tLoss: 8.152416\n",
      "Train Epoche: 1 [1708/96 (1779%)]\tLoss: 31.350573\n",
      "Train Epoche: 1 [1709/96 (1780%)]\tLoss: 25.615824\n",
      "Train Epoche: 1 [1710/96 (1781%)]\tLoss: 0.098914\n",
      "Train Epoche: 1 [1711/96 (1782%)]\tLoss: 2.378577\n",
      "Train Epoche: 1 [1712/96 (1783%)]\tLoss: 4.635039\n",
      "Train Epoche: 1 [1713/96 (1784%)]\tLoss: 4.176442\n",
      "Train Epoche: 1 [1714/96 (1785%)]\tLoss: 2.261822\n",
      "Train Epoche: 1 [1715/96 (1786%)]\tLoss: 16.251791\n",
      "Train Epoche: 1 [1716/96 (1788%)]\tLoss: 5.609326\n",
      "Train Epoche: 1 [1717/96 (1789%)]\tLoss: 3.141767\n",
      "Train Epoche: 1 [1718/96 (1790%)]\tLoss: 6.496391\n",
      "Train Epoche: 1 [1719/96 (1791%)]\tLoss: 3.172267\n",
      "Train Epoche: 1 [1720/96 (1792%)]\tLoss: 7.774976\n",
      "Train Epoche: 1 [1721/96 (1793%)]\tLoss: 49.252934\n",
      "Train Epoche: 1 [1722/96 (1794%)]\tLoss: 2.135701\n",
      "Train Epoche: 1 [1723/96 (1795%)]\tLoss: 37.382259\n",
      "Train Epoche: 1 [1724/96 (1796%)]\tLoss: 3.753677\n",
      "Train Epoche: 1 [1725/96 (1797%)]\tLoss: 2.113147\n",
      "Train Epoche: 1 [1726/96 (1798%)]\tLoss: 17.926798\n",
      "Train Epoche: 1 [1727/96 (1799%)]\tLoss: 90.042900\n",
      "Train Epoche: 1 [1728/96 (1800%)]\tLoss: 4.917588\n",
      "Train Epoche: 1 [1729/96 (1801%)]\tLoss: 3.169519\n",
      "Train Epoche: 1 [1730/96 (1802%)]\tLoss: 24.324598\n",
      "Train Epoche: 1 [1731/96 (1803%)]\tLoss: 7.420919\n",
      "Train Epoche: 1 [1732/96 (1804%)]\tLoss: 0.045352\n",
      "Train Epoche: 1 [1733/96 (1805%)]\tLoss: 7.945477\n",
      "Train Epoche: 1 [1734/96 (1806%)]\tLoss: 24.600655\n",
      "Train Epoche: 1 [1735/96 (1807%)]\tLoss: 0.052674\n",
      "Train Epoche: 1 [1736/96 (1808%)]\tLoss: 0.007316\n",
      "Train Epoche: 1 [1737/96 (1809%)]\tLoss: 6.562753\n",
      "Train Epoche: 1 [1738/96 (1810%)]\tLoss: 80.649673\n",
      "Train Epoche: 1 [1739/96 (1811%)]\tLoss: 5.520514\n",
      "Train Epoche: 1 [1740/96 (1812%)]\tLoss: 5.000571\n",
      "Train Epoche: 1 [1741/96 (1814%)]\tLoss: 26.997843\n",
      "Train Epoche: 1 [1742/96 (1815%)]\tLoss: 1.253035\n",
      "Train Epoche: 1 [1743/96 (1816%)]\tLoss: 48.015202\n",
      "Train Epoche: 1 [1744/96 (1817%)]\tLoss: 2.492613\n",
      "Train Epoche: 1 [1745/96 (1818%)]\tLoss: 0.430425\n",
      "Train Epoche: 1 [1746/96 (1819%)]\tLoss: 1.293843\n",
      "Train Epoche: 1 [1747/96 (1820%)]\tLoss: 0.432315\n",
      "Train Epoche: 1 [1748/96 (1821%)]\tLoss: 0.905515\n",
      "Train Epoche: 1 [1749/96 (1822%)]\tLoss: 5.104416\n",
      "Train Epoche: 1 [1750/96 (1823%)]\tLoss: 3.159710\n",
      "Train Epoche: 1 [1751/96 (1824%)]\tLoss: 1.314256\n",
      "Train Epoche: 1 [1752/96 (1825%)]\tLoss: 3.119156\n",
      "Train Epoche: 1 [1753/96 (1826%)]\tLoss: 0.613265\n",
      "Train Epoche: 1 [1754/96 (1827%)]\tLoss: 47.615864\n",
      "Train Epoche: 1 [1755/96 (1828%)]\tLoss: 1.353610\n",
      "Train Epoche: 1 [1756/96 (1829%)]\tLoss: 133.876877\n",
      "Train Epoche: 1 [1757/96 (1830%)]\tLoss: 1.159380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1758/96 (1831%)]\tLoss: 2.237663\n",
      "Train Epoche: 1 [1759/96 (1832%)]\tLoss: 158.627686\n",
      "Train Epoche: 1 [1760/96 (1833%)]\tLoss: 0.003208\n",
      "Train Epoche: 1 [1761/96 (1834%)]\tLoss: 3.084180\n",
      "Train Epoche: 1 [1762/96 (1835%)]\tLoss: 2.735741\n",
      "Train Epoche: 1 [1763/96 (1836%)]\tLoss: 411.102905\n",
      "Train Epoche: 1 [1764/96 (1838%)]\tLoss: 249.023026\n",
      "Train Epoche: 1 [1765/96 (1839%)]\tLoss: 2.676241\n",
      "Train Epoche: 1 [1766/96 (1840%)]\tLoss: 24.568546\n",
      "Train Epoche: 1 [1767/96 (1841%)]\tLoss: 3.862500\n",
      "Train Epoche: 1 [1768/96 (1842%)]\tLoss: 2.669463\n",
      "Train Epoche: 1 [1769/96 (1843%)]\tLoss: 10.194931\n",
      "Train Epoche: 1 [1770/96 (1844%)]\tLoss: 75.936798\n",
      "Train Epoche: 1 [1771/96 (1845%)]\tLoss: 63.187714\n",
      "Train Epoche: 1 [1772/96 (1846%)]\tLoss: 5.765172\n",
      "Train Epoche: 1 [1773/96 (1847%)]\tLoss: 103.720535\n",
      "Train Epoche: 1 [1774/96 (1848%)]\tLoss: 7.848712\n",
      "Train Epoche: 1 [1775/96 (1849%)]\tLoss: 1.675552\n",
      "Train Epoche: 1 [1776/96 (1850%)]\tLoss: 2.684392\n",
      "Train Epoche: 1 [1777/96 (1851%)]\tLoss: 1.703336\n",
      "Train Epoche: 1 [1778/96 (1852%)]\tLoss: 2.202610\n",
      "Train Epoche: 1 [1779/96 (1853%)]\tLoss: 19.119986\n",
      "Train Epoche: 1 [1780/96 (1854%)]\tLoss: 2.529623\n",
      "Train Epoche: 1 [1781/96 (1855%)]\tLoss: 10.195455\n",
      "Train Epoche: 1 [1782/96 (1856%)]\tLoss: 2.371391\n",
      "Train Epoche: 1 [1783/96 (1857%)]\tLoss: 6.539747\n",
      "Train Epoche: 1 [1784/96 (1858%)]\tLoss: 2.797909\n",
      "Train Epoche: 1 [1785/96 (1859%)]\tLoss: 47.959023\n",
      "Train Epoche: 1 [1786/96 (1860%)]\tLoss: 0.203446\n",
      "Train Epoche: 1 [1787/96 (1861%)]\tLoss: 0.010404\n",
      "Train Epoche: 1 [1788/96 (1862%)]\tLoss: 5.765502\n",
      "Train Epoche: 1 [1789/96 (1864%)]\tLoss: 5.583962\n",
      "Train Epoche: 1 [1790/96 (1865%)]\tLoss: 2.450319\n",
      "Train Epoche: 1 [1791/96 (1866%)]\tLoss: 4.985866\n",
      "Train Epoche: 1 [1792/96 (1867%)]\tLoss: 15.004578\n",
      "Train Epoche: 1 [1793/96 (1868%)]\tLoss: 0.343248\n",
      "Train Epoche: 1 [1794/96 (1869%)]\tLoss: 29.552729\n",
      "Train Epoche: 1 [1795/96 (1870%)]\tLoss: 2.961633\n",
      "Train Epoche: 1 [1796/96 (1871%)]\tLoss: 7.381430\n",
      "Train Epoche: 1 [1797/96 (1872%)]\tLoss: 7.378006\n",
      "Train Epoche: 1 [1798/96 (1873%)]\tLoss: 0.837164\n",
      "Train Epoche: 1 [1799/96 (1874%)]\tLoss: 22.080511\n",
      "Train Epoche: 1 [1800/96 (1875%)]\tLoss: 7.525755\n",
      "Train Epoche: 1 [1801/96 (1876%)]\tLoss: 44.945938\n",
      "Train Epoche: 1 [1802/96 (1877%)]\tLoss: 12.190776\n",
      "Train Epoche: 1 [1803/96 (1878%)]\tLoss: 0.426165\n",
      "Train Epoche: 1 [1804/96 (1879%)]\tLoss: 78.548828\n",
      "Train Epoche: 1 [1805/96 (1880%)]\tLoss: 76.637688\n",
      "Train Epoche: 1 [1806/96 (1881%)]\tLoss: 0.198632\n",
      "Train Epoche: 1 [1807/96 (1882%)]\tLoss: 0.208362\n",
      "Train Epoche: 1 [1808/96 (1883%)]\tLoss: 7.140195\n",
      "Train Epoche: 1 [1809/96 (1884%)]\tLoss: 9.613149\n",
      "Train Epoche: 1 [1810/96 (1885%)]\tLoss: 35.340958\n",
      "Train Epoche: 1 [1811/96 (1886%)]\tLoss: 0.320194\n",
      "Train Epoche: 1 [1812/96 (1888%)]\tLoss: 0.199590\n",
      "Train Epoche: 1 [1813/96 (1889%)]\tLoss: 15.895283\n",
      "Train Epoche: 1 [1814/96 (1890%)]\tLoss: 0.362770\n",
      "Train Epoche: 1 [1815/96 (1891%)]\tLoss: 1.881894\n",
      "Train Epoche: 1 [1816/96 (1892%)]\tLoss: 8.094363\n",
      "Train Epoche: 1 [1817/96 (1893%)]\tLoss: 22.478262\n",
      "Train Epoche: 1 [1818/96 (1894%)]\tLoss: 27.686657\n",
      "Train Epoche: 1 [1819/96 (1895%)]\tLoss: 0.556815\n",
      "Train Epoche: 1 [1820/96 (1896%)]\tLoss: 211.158218\n",
      "Train Epoche: 1 [1821/96 (1897%)]\tLoss: 0.532970\n",
      "Train Epoche: 1 [1822/96 (1898%)]\tLoss: 7.443913\n",
      "Train Epoche: 1 [1823/96 (1899%)]\tLoss: 0.800609\n",
      "Train Epoche: 1 [1824/96 (1900%)]\tLoss: 21.615225\n",
      "Train Epoche: 1 [1825/96 (1901%)]\tLoss: 65.954315\n",
      "Train Epoche: 1 [1826/96 (1902%)]\tLoss: 112.418709\n",
      "Train Epoche: 1 [1827/96 (1903%)]\tLoss: 2.656279\n",
      "Train Epoche: 1 [1828/96 (1904%)]\tLoss: 2.064401\n",
      "Train Epoche: 1 [1829/96 (1905%)]\tLoss: 10.096632\n",
      "Train Epoche: 1 [1830/96 (1906%)]\tLoss: 7.509674\n",
      "Train Epoche: 1 [1831/96 (1907%)]\tLoss: 59.573624\n",
      "Train Epoche: 1 [1832/96 (1908%)]\tLoss: 87.735558\n",
      "Train Epoche: 1 [1833/96 (1909%)]\tLoss: 164.670425\n",
      "Train Epoche: 1 [1834/96 (1910%)]\tLoss: 10.853639\n",
      "Train Epoche: 1 [1835/96 (1911%)]\tLoss: 9.052169\n",
      "Train Epoche: 1 [1836/96 (1912%)]\tLoss: 2.465085\n",
      "Train Epoche: 1 [1837/96 (1914%)]\tLoss: 5.328205\n",
      "Train Epoche: 1 [1838/96 (1915%)]\tLoss: 1.937266\n",
      "Train Epoche: 1 [1839/96 (1916%)]\tLoss: 3.883626\n",
      "Train Epoche: 1 [1840/96 (1917%)]\tLoss: 4.658236\n",
      "Train Epoche: 1 [1841/96 (1918%)]\tLoss: 60.802326\n",
      "Train Epoche: 1 [1842/96 (1919%)]\tLoss: 27.937338\n",
      "Train Epoche: 1 [1843/96 (1920%)]\tLoss: 0.018469\n",
      "Train Epoche: 1 [1844/96 (1921%)]\tLoss: 67.671745\n",
      "Train Epoche: 1 [1845/96 (1922%)]\tLoss: 3.245610\n",
      "Train Epoche: 1 [1846/96 (1923%)]\tLoss: 12.135491\n",
      "Train Epoche: 1 [1847/96 (1924%)]\tLoss: 17.728155\n",
      "Train Epoche: 1 [1848/96 (1925%)]\tLoss: 20.340954\n",
      "Train Epoche: 1 [1849/96 (1926%)]\tLoss: 5.643543\n",
      "Train Epoche: 1 [1850/96 (1927%)]\tLoss: 1.830470\n",
      "Train Epoche: 1 [1851/96 (1928%)]\tLoss: 0.050338\n",
      "Train Epoche: 1 [1852/96 (1929%)]\tLoss: 23.975344\n",
      "Train Epoche: 1 [1853/96 (1930%)]\tLoss: 0.119548\n",
      "Train Epoche: 1 [1854/96 (1931%)]\tLoss: 212.964508\n",
      "Train Epoche: 1 [1855/96 (1932%)]\tLoss: 1.547138\n",
      "Train Epoche: 1 [1856/96 (1933%)]\tLoss: 0.000001\n",
      "Train Epoche: 1 [1857/96 (1934%)]\tLoss: 5.326571\n",
      "Train Epoche: 1 [1858/96 (1935%)]\tLoss: 0.777894\n",
      "Train Epoche: 1 [1859/96 (1936%)]\tLoss: 0.769870\n",
      "Train Epoche: 1 [1860/96 (1938%)]\tLoss: 1.737619\n",
      "Train Epoche: 1 [1861/96 (1939%)]\tLoss: 1.092655\n",
      "Train Epoche: 1 [1862/96 (1940%)]\tLoss: 7.437222\n",
      "Train Epoche: 1 [1863/96 (1941%)]\tLoss: 4.983946\n",
      "Train Epoche: 1 [1864/96 (1942%)]\tLoss: 1.208823\n",
      "Train Epoche: 1 [1865/96 (1943%)]\tLoss: 4.132691\n",
      "Train Epoche: 1 [1866/96 (1944%)]\tLoss: 4.352834\n",
      "Train Epoche: 1 [1867/96 (1945%)]\tLoss: 2.196587\n",
      "Train Epoche: 1 [1868/96 (1946%)]\tLoss: 22.752705\n",
      "Train Epoche: 1 [1869/96 (1947%)]\tLoss: 9.101237\n",
      "Train Epoche: 1 [1870/96 (1948%)]\tLoss: 9.207948\n",
      "Train Epoche: 1 [1871/96 (1949%)]\tLoss: 6.587386\n",
      "Train Epoche: 1 [1872/96 (1950%)]\tLoss: 7.302188\n",
      "Train Epoche: 1 [1873/96 (1951%)]\tLoss: 0.278856\n",
      "Train Epoche: 1 [1874/96 (1952%)]\tLoss: 0.014177\n",
      "Train Epoche: 1 [1875/96 (1953%)]\tLoss: 7.886442\n",
      "Train Epoche: 1 [1876/96 (1954%)]\tLoss: 3.965529\n",
      "Train Epoche: 1 [1877/96 (1955%)]\tLoss: 0.944580\n",
      "Train Epoche: 1 [1878/96 (1956%)]\tLoss: 0.097347\n",
      "Train Epoche: 1 [1879/96 (1957%)]\tLoss: 8.082847\n",
      "Train Epoche: 1 [1880/96 (1958%)]\tLoss: 2.209472\n",
      "Train Epoche: 1 [1881/96 (1959%)]\tLoss: 8.941645\n",
      "Train Epoche: 1 [1882/96 (1960%)]\tLoss: 1.697500\n",
      "Train Epoche: 1 [1883/96 (1961%)]\tLoss: 3.235082\n",
      "Train Epoche: 1 [1884/96 (1962%)]\tLoss: 277.261536\n",
      "Train Epoche: 1 [1885/96 (1964%)]\tLoss: 0.667141\n",
      "Train Epoche: 1 [1886/96 (1965%)]\tLoss: 3.283230\n",
      "Train Epoche: 1 [1887/96 (1966%)]\tLoss: 0.514586\n",
      "Train Epoche: 1 [1888/96 (1967%)]\tLoss: 3.027737\n",
      "Train Epoche: 1 [1889/96 (1968%)]\tLoss: 1.771321\n",
      "Train Epoche: 1 [1890/96 (1969%)]\tLoss: 0.018500\n",
      "Train Epoche: 1 [1891/96 (1970%)]\tLoss: 0.049695\n",
      "Train Epoche: 1 [1892/96 (1971%)]\tLoss: 5.788383\n",
      "Train Epoche: 1 [1893/96 (1972%)]\tLoss: 66.719688\n",
      "Train Epoche: 1 [1894/96 (1973%)]\tLoss: 1.145468\n",
      "Train Epoche: 1 [1895/96 (1974%)]\tLoss: 1.312693\n",
      "Train Epoche: 1 [1896/96 (1975%)]\tLoss: 179.156448\n",
      "Train Epoche: 1 [1897/96 (1976%)]\tLoss: 8.110547\n",
      "Train Epoche: 1 [1898/96 (1977%)]\tLoss: 0.000307\n",
      "Train Epoche: 1 [1899/96 (1978%)]\tLoss: 6.319467\n",
      "Train Epoche: 1 [1900/96 (1979%)]\tLoss: 25.451679\n",
      "Train Epoche: 1 [1901/96 (1980%)]\tLoss: 2.524941\n",
      "Train Epoche: 1 [1902/96 (1981%)]\tLoss: 4.291297\n",
      "Train Epoche: 1 [1903/96 (1982%)]\tLoss: 0.827419\n",
      "Train Epoche: 1 [1904/96 (1983%)]\tLoss: 8.307178\n",
      "Train Epoche: 1 [1905/96 (1984%)]\tLoss: 1.126019\n",
      "Train Epoche: 1 [1906/96 (1985%)]\tLoss: 3.406625\n",
      "Train Epoche: 1 [1907/96 (1986%)]\tLoss: 1.674966\n",
      "Train Epoche: 1 [1908/96 (1988%)]\tLoss: 0.007827\n",
      "Train Epoche: 1 [1909/96 (1989%)]\tLoss: 0.011262\n",
      "Train Epoche: 1 [1910/96 (1990%)]\tLoss: 70.801308\n",
      "Train Epoche: 1 [1911/96 (1991%)]\tLoss: 6.480006\n",
      "Train Epoche: 1 [1912/96 (1992%)]\tLoss: 6.926490\n",
      "Train Epoche: 1 [1913/96 (1993%)]\tLoss: 62.886749\n",
      "Train Epoche: 1 [1914/96 (1994%)]\tLoss: 6.271992\n",
      "Train Epoche: 1 [1915/96 (1995%)]\tLoss: 3.696588\n",
      "Train Epoche: 1 [1916/96 (1996%)]\tLoss: 10.358399\n",
      "Train Epoche: 1 [1917/96 (1997%)]\tLoss: 7.201716\n",
      "Train Epoche: 1 [1918/96 (1998%)]\tLoss: 0.091327\n",
      "Train Epoche: 1 [1919/96 (1999%)]\tLoss: 0.004457\n",
      "Train Epoche: 1 [1920/96 (2000%)]\tLoss: 0.922479\n",
      "Train Epoche: 1 [1921/96 (2001%)]\tLoss: 3.161470\n",
      "Train Epoche: 1 [1922/96 (2002%)]\tLoss: 0.041456\n",
      "Train Epoche: 1 [1923/96 (2003%)]\tLoss: 17.435143\n",
      "Train Epoche: 1 [1924/96 (2004%)]\tLoss: 265.927063\n",
      "Train Epoche: 1 [1925/96 (2005%)]\tLoss: 0.000348\n",
      "Train Epoche: 1 [1926/96 (2006%)]\tLoss: 1.044198\n",
      "Train Epoche: 1 [1927/96 (2007%)]\tLoss: 0.673297\n",
      "Train Epoche: 1 [1928/96 (2008%)]\tLoss: 2.952735\n",
      "Train Epoche: 1 [1929/96 (2009%)]\tLoss: 35.749058\n",
      "Train Epoche: 1 [1930/96 (2010%)]\tLoss: 3.562130\n",
      "Train Epoche: 1 [1931/96 (2011%)]\tLoss: 0.297279\n",
      "Train Epoche: 1 [1932/96 (2012%)]\tLoss: 0.000000\n",
      "Train Epoche: 1 [1933/96 (2014%)]\tLoss: 0.864030\n",
      "Train Epoche: 1 [1934/96 (2015%)]\tLoss: 0.201619\n",
      "Train Epoche: 1 [1935/96 (2016%)]\tLoss: 0.265259\n",
      "Train Epoche: 1 [1936/96 (2017%)]\tLoss: 10.771671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1937/96 (2018%)]\tLoss: 243.342010\n",
      "Train Epoche: 1 [1938/96 (2019%)]\tLoss: 1.208438\n",
      "Train Epoche: 1 [1939/96 (2020%)]\tLoss: 1.493735\n",
      "Train Epoche: 1 [1940/96 (2021%)]\tLoss: 0.076099\n",
      "Train Epoche: 1 [1941/96 (2022%)]\tLoss: 5.937483\n",
      "Train Epoche: 1 [1942/96 (2023%)]\tLoss: 1.127593\n",
      "Train Epoche: 1 [1943/96 (2024%)]\tLoss: 0.668838\n",
      "Train Epoche: 1 [1944/96 (2025%)]\tLoss: 0.740785\n",
      "Train Epoche: 1 [1945/96 (2026%)]\tLoss: 3.718822\n",
      "Train Epoche: 1 [1946/96 (2027%)]\tLoss: 0.921892\n",
      "Train Epoche: 1 [1947/96 (2028%)]\tLoss: 2.422182\n",
      "Train Epoche: 1 [1948/96 (2029%)]\tLoss: 4.257369\n",
      "Train Epoche: 1 [1949/96 (2030%)]\tLoss: 0.001172\n",
      "Train Epoche: 1 [1950/96 (2031%)]\tLoss: 0.000468\n",
      "Train Epoche: 1 [1951/96 (2032%)]\tLoss: 0.174768\n",
      "Train Epoche: 1 [1952/96 (2033%)]\tLoss: 0.370807\n",
      "Train Epoche: 1 [1953/96 (2034%)]\tLoss: 0.000148\n",
      "Train Epoche: 1 [1954/96 (2035%)]\tLoss: 0.066039\n",
      "Train Epoche: 1 [1955/96 (2036%)]\tLoss: 108.878510\n",
      "Train Epoche: 1 [1956/96 (2038%)]\tLoss: 0.000161\n",
      "Train Epoche: 1 [1957/96 (2039%)]\tLoss: 10.668717\n",
      "Train Epoche: 1 [1958/96 (2040%)]\tLoss: 0.808400\n",
      "Train Epoche: 1 [1959/96 (2041%)]\tLoss: 1.273410\n",
      "Train Epoche: 1 [1960/96 (2042%)]\tLoss: 0.134593\n",
      "Train Epoche: 1 [1961/96 (2043%)]\tLoss: 15.002679\n",
      "Train Epoche: 1 [1962/96 (2044%)]\tLoss: 30.152538\n",
      "Train Epoche: 1 [1963/96 (2045%)]\tLoss: 97.491257\n",
      "Train Epoche: 1 [1964/96 (2046%)]\tLoss: 12.747908\n",
      "Train Epoche: 1 [1965/96 (2047%)]\tLoss: 0.019277\n",
      "Train Epoche: 1 [1966/96 (2048%)]\tLoss: 5.608294\n",
      "Train Epoche: 1 [1967/96 (2049%)]\tLoss: 23.684761\n",
      "Train Epoche: 1 [1968/96 (2050%)]\tLoss: 0.146173\n",
      "Train Epoche: 1 [1969/96 (2051%)]\tLoss: 3.862478\n",
      "Train Epoche: 1 [1970/96 (2052%)]\tLoss: 5.110020\n",
      "Train Epoche: 1 [1971/96 (2053%)]\tLoss: 0.904797\n",
      "Train Epoche: 1 [1972/96 (2054%)]\tLoss: 7.146935\n",
      "Train Epoche: 1 [1973/96 (2055%)]\tLoss: 0.565961\n",
      "Train Epoche: 1 [1974/96 (2056%)]\tLoss: 33.032063\n",
      "Train Epoche: 1 [1975/96 (2057%)]\tLoss: 0.977315\n",
      "Train Epoche: 1 [1976/96 (2058%)]\tLoss: 0.045300\n",
      "Train Epoche: 1 [1977/96 (2059%)]\tLoss: 9.834799\n",
      "Train Epoche: 1 [1978/96 (2060%)]\tLoss: 1.359344\n",
      "Train Epoche: 1 [1979/96 (2061%)]\tLoss: 10.470981\n",
      "Train Epoche: 1 [1980/96 (2062%)]\tLoss: 3.719632\n",
      "Train Epoche: 1 [1981/96 (2064%)]\tLoss: 14.135823\n",
      "Train Epoche: 1 [1982/96 (2065%)]\tLoss: 0.580224\n",
      "Train Epoche: 1 [1983/96 (2066%)]\tLoss: 3.448178\n",
      "Train Epoche: 1 [1984/96 (2067%)]\tLoss: 25.816011\n",
      "Train Epoche: 1 [1985/96 (2068%)]\tLoss: 14.845202\n",
      "Train Epoche: 1 [1986/96 (2069%)]\tLoss: 8.863941\n",
      "Train Epoche: 1 [1987/96 (2070%)]\tLoss: 0.030224\n",
      "Train Epoche: 1 [1988/96 (2071%)]\tLoss: 0.776083\n",
      "Train Epoche: 1 [1989/96 (2072%)]\tLoss: 65.315285\n",
      "Train Epoche: 1 [1990/96 (2073%)]\tLoss: 0.099465\n",
      "Train Epoche: 1 [1991/96 (2074%)]\tLoss: 3.198938\n",
      "Train Epoche: 1 [1992/96 (2075%)]\tLoss: 14.217308\n",
      "Train Epoche: 1 [1993/96 (2076%)]\tLoss: 2.464358\n",
      "Train Epoche: 1 [1994/96 (2077%)]\tLoss: 28.058218\n",
      "Train Epoche: 1 [1995/96 (2078%)]\tLoss: 0.601889\n",
      "Train Epoche: 1 [1996/96 (2079%)]\tLoss: 2.016419\n",
      "Train Epoche: 1 [1997/96 (2080%)]\tLoss: 0.011797\n",
      "Train Epoche: 1 [1998/96 (2081%)]\tLoss: 0.903868\n",
      "Train Epoche: 1 [1999/96 (2082%)]\tLoss: 2.112792\n",
      "Train Epoche: 1 [2000/96 (2083%)]\tLoss: 1.254752\n",
      "Train Epoche: 1 [2001/96 (2084%)]\tLoss: 34.936382\n",
      "Train Epoche: 1 [2002/96 (2085%)]\tLoss: 49.920807\n",
      "Train Epoche: 1 [2003/96 (2086%)]\tLoss: 2.095886\n",
      "Train Epoche: 1 [2004/96 (2088%)]\tLoss: 6.272727\n",
      "Train Epoche: 1 [2005/96 (2089%)]\tLoss: 47.615337\n",
      "Train Epoche: 1 [2006/96 (2090%)]\tLoss: 18.103159\n",
      "Train Epoche: 1 [2007/96 (2091%)]\tLoss: 1.938822\n",
      "Train Epoche: 1 [2008/96 (2092%)]\tLoss: 73.420021\n",
      "Train Epoche: 1 [2009/96 (2093%)]\tLoss: 2.536605\n",
      "Train Epoche: 1 [2010/96 (2094%)]\tLoss: 11.804687\n",
      "Train Epoche: 1 [2011/96 (2095%)]\tLoss: 5.946086\n",
      "Train Epoche: 1 [2012/96 (2096%)]\tLoss: 20.035378\n",
      "Train Epoche: 1 [2013/96 (2097%)]\tLoss: 8.938154\n",
      "Train Epoche: 1 [2014/96 (2098%)]\tLoss: 48.214252\n",
      "Train Epoche: 1 [2015/96 (2099%)]\tLoss: 5.266436\n",
      "Train Epoche: 1 [2016/96 (2100%)]\tLoss: 0.266423\n",
      "Train Epoche: 1 [2017/96 (2101%)]\tLoss: 0.347722\n",
      "Train Epoche: 1 [2018/96 (2102%)]\tLoss: 45.750317\n",
      "Train Epoche: 1 [2019/96 (2103%)]\tLoss: 0.038738\n",
      "Train Epoche: 1 [2020/96 (2104%)]\tLoss: 42.098213\n",
      "Train Epoche: 1 [2021/96 (2105%)]\tLoss: 0.003616\n",
      "Train Epoche: 1 [2022/96 (2106%)]\tLoss: 6.541552\n",
      "Train Epoche: 1 [2023/96 (2107%)]\tLoss: 0.256459\n",
      "Train Epoche: 1 [2024/96 (2108%)]\tLoss: 0.020136\n",
      "Train Epoche: 1 [2025/96 (2109%)]\tLoss: 12.475338\n",
      "Train Epoche: 1 [2026/96 (2110%)]\tLoss: 24.177488\n",
      "Train Epoche: 2 [0/96 (0%)]\tLoss: 21.292582\n",
      "Train Epoche: 2 [1/96 (1%)]\tLoss: 4.490277\n",
      "Train Epoche: 2 [2/96 (2%)]\tLoss: 10.205546\n",
      "Train Epoche: 2 [3/96 (3%)]\tLoss: 16.788271\n",
      "Train Epoche: 2 [4/96 (4%)]\tLoss: 124.691643\n",
      "Train Epoche: 2 [5/96 (5%)]\tLoss: 0.447151\n",
      "Train Epoche: 2 [6/96 (6%)]\tLoss: 0.339238\n",
      "Train Epoche: 2 [7/96 (7%)]\tLoss: 55.071560\n",
      "Train Epoche: 2 [8/96 (8%)]\tLoss: 17.636309\n",
      "Train Epoche: 2 [9/96 (9%)]\tLoss: 1.762014\n",
      "Train Epoche: 2 [10/96 (10%)]\tLoss: 385.384430\n",
      "Train Epoche: 2 [11/96 (11%)]\tLoss: 0.661753\n",
      "Train Epoche: 2 [12/96 (12%)]\tLoss: 3.481199\n",
      "Train Epoche: 2 [13/96 (14%)]\tLoss: 0.338307\n",
      "Train Epoche: 2 [14/96 (15%)]\tLoss: 4.719254\n",
      "Train Epoche: 2 [15/96 (16%)]\tLoss: 0.185151\n",
      "Train Epoche: 2 [16/96 (17%)]\tLoss: 3.463726\n",
      "Train Epoche: 2 [17/96 (18%)]\tLoss: 9.914917\n",
      "Train Epoche: 2 [18/96 (19%)]\tLoss: 8.198598\n",
      "Train Epoche: 2 [19/96 (20%)]\tLoss: 0.468849\n",
      "Train Epoche: 2 [20/96 (21%)]\tLoss: 17.097853\n",
      "Train Epoche: 2 [21/96 (22%)]\tLoss: 0.315825\n",
      "Train Epoche: 2 [22/96 (23%)]\tLoss: 20.868225\n",
      "Train Epoche: 2 [23/96 (24%)]\tLoss: 44.354225\n",
      "Train Epoche: 2 [24/96 (25%)]\tLoss: 106.138611\n",
      "Train Epoche: 2 [25/96 (26%)]\tLoss: 2.898222\n",
      "Train Epoche: 2 [26/96 (27%)]\tLoss: 120.462860\n",
      "Train Epoche: 2 [27/96 (28%)]\tLoss: 1.859520\n",
      "Train Epoche: 2 [28/96 (29%)]\tLoss: 18.586874\n",
      "Train Epoche: 2 [29/96 (30%)]\tLoss: 69.041031\n",
      "Train Epoche: 2 [30/96 (31%)]\tLoss: 3.261305\n",
      "Train Epoche: 2 [31/96 (32%)]\tLoss: 7.520168\n",
      "Train Epoche: 2 [32/96 (33%)]\tLoss: 0.657323\n",
      "Train Epoche: 2 [33/96 (34%)]\tLoss: 8.573013\n",
      "Train Epoche: 2 [34/96 (35%)]\tLoss: 14.122653\n",
      "Train Epoche: 2 [35/96 (36%)]\tLoss: 7.550234\n",
      "Train Epoche: 2 [36/96 (38%)]\tLoss: 5.482431\n",
      "Train Epoche: 2 [37/96 (39%)]\tLoss: 0.003890\n",
      "Train Epoche: 2 [38/96 (40%)]\tLoss: 25.589970\n",
      "Train Epoche: 2 [39/96 (41%)]\tLoss: 23.260500\n",
      "Train Epoche: 2 [40/96 (42%)]\tLoss: 14.504892\n",
      "Train Epoche: 2 [41/96 (43%)]\tLoss: 4.664598\n",
      "Train Epoche: 2 [42/96 (44%)]\tLoss: 23.621758\n",
      "Train Epoche: 2 [43/96 (45%)]\tLoss: 0.794992\n",
      "Train Epoche: 2 [44/96 (46%)]\tLoss: 391.097076\n",
      "Train Epoche: 2 [45/96 (47%)]\tLoss: 0.006089\n",
      "Train Epoche: 2 [46/96 (48%)]\tLoss: 30.971640\n",
      "Train Epoche: 2 [47/96 (49%)]\tLoss: 0.043017\n",
      "Train Epoche: 2 [48/96 (50%)]\tLoss: 5.316905\n",
      "Train Epoche: 2 [49/96 (51%)]\tLoss: 0.298817\n",
      "Train Epoche: 2 [50/96 (52%)]\tLoss: 136.087067\n",
      "Train Epoche: 2 [51/96 (53%)]\tLoss: 6.258884\n",
      "Train Epoche: 2 [52/96 (54%)]\tLoss: 0.337541\n",
      "Train Epoche: 2 [53/96 (55%)]\tLoss: 1.531922\n",
      "Train Epoche: 2 [54/96 (56%)]\tLoss: 44.390232\n",
      "Train Epoche: 2 [55/96 (57%)]\tLoss: 0.006653\n",
      "Train Epoche: 2 [56/96 (58%)]\tLoss: 24.911749\n",
      "Train Epoche: 2 [57/96 (59%)]\tLoss: 10.663846\n",
      "Train Epoche: 2 [58/96 (60%)]\tLoss: 1.094496\n",
      "Train Epoche: 2 [59/96 (61%)]\tLoss: 1.311089\n",
      "Train Epoche: 2 [60/96 (62%)]\tLoss: 41.604095\n",
      "Train Epoche: 2 [61/96 (64%)]\tLoss: 2.107697\n",
      "Train Epoche: 2 [62/96 (65%)]\tLoss: 18.985422\n",
      "Train Epoche: 2 [63/96 (66%)]\tLoss: 0.011705\n",
      "Train Epoche: 2 [64/96 (67%)]\tLoss: 2.070109\n",
      "Train Epoche: 2 [65/96 (68%)]\tLoss: 6.458960\n",
      "Train Epoche: 2 [66/96 (69%)]\tLoss: 51.568169\n",
      "Train Epoche: 2 [67/96 (70%)]\tLoss: 0.152440\n",
      "Train Epoche: 2 [68/96 (71%)]\tLoss: 12.035366\n",
      "Train Epoche: 2 [69/96 (72%)]\tLoss: 1.286239\n",
      "Train Epoche: 2 [70/96 (73%)]\tLoss: 71.346664\n",
      "Train Epoche: 2 [71/96 (74%)]\tLoss: 3.031166\n",
      "Train Epoche: 2 [72/96 (75%)]\tLoss: 3.791190\n",
      "Train Epoche: 2 [73/96 (76%)]\tLoss: 0.472618\n",
      "Train Epoche: 2 [74/96 (77%)]\tLoss: 5.701207\n",
      "Train Epoche: 2 [75/96 (78%)]\tLoss: 2.455756\n",
      "Train Epoche: 2 [76/96 (79%)]\tLoss: 17.468464\n",
      "Train Epoche: 2 [77/96 (80%)]\tLoss: 4.703538\n",
      "Train Epoche: 2 [78/96 (81%)]\tLoss: 3.765212\n",
      "Train Epoche: 2 [79/96 (82%)]\tLoss: 0.070060\n",
      "Train Epoche: 2 [80/96 (83%)]\tLoss: 30.982946\n",
      "Train Epoche: 2 [81/96 (84%)]\tLoss: 8.460320\n",
      "Train Epoche: 2 [82/96 (85%)]\tLoss: 7.116520\n",
      "Train Epoche: 2 [83/96 (86%)]\tLoss: 0.024795\n",
      "Train Epoche: 2 [84/96 (88%)]\tLoss: 0.117487\n",
      "Train Epoche: 2 [85/96 (89%)]\tLoss: 33.147396\n",
      "Train Epoche: 2 [86/96 (90%)]\tLoss: 1.625262\n",
      "Train Epoche: 2 [87/96 (91%)]\tLoss: 10.946134\n",
      "Train Epoche: 2 [88/96 (92%)]\tLoss: 6.046982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [89/96 (93%)]\tLoss: 0.004556\n",
      "Train Epoche: 2 [90/96 (94%)]\tLoss: 3.921639\n",
      "Train Epoche: 2 [91/96 (95%)]\tLoss: 0.123292\n",
      "Train Epoche: 2 [92/96 (96%)]\tLoss: 16.145487\n",
      "Train Epoche: 2 [93/96 (97%)]\tLoss: 30.818495\n",
      "Train Epoche: 2 [94/96 (98%)]\tLoss: 8.500539\n",
      "Train Epoche: 2 [95/96 (99%)]\tLoss: 13.377762\n",
      "Train Epoche: 2 [96/96 (100%)]\tLoss: 61.906746\n",
      "Train Epoche: 2 [97/96 (101%)]\tLoss: 187.430237\n",
      "Train Epoche: 2 [98/96 (102%)]\tLoss: 1.170849\n",
      "Train Epoche: 2 [99/96 (103%)]\tLoss: 156.391937\n",
      "Train Epoche: 2 [100/96 (104%)]\tLoss: 4.053744\n",
      "Train Epoche: 2 [101/96 (105%)]\tLoss: 18.360222\n",
      "Train Epoche: 2 [102/96 (106%)]\tLoss: 1.573845\n",
      "Train Epoche: 2 [103/96 (107%)]\tLoss: 26.391876\n",
      "Train Epoche: 2 [104/96 (108%)]\tLoss: 2.282691\n",
      "Train Epoche: 2 [105/96 (109%)]\tLoss: 11.082740\n",
      "Train Epoche: 2 [106/96 (110%)]\tLoss: 0.593845\n",
      "Train Epoche: 2 [107/96 (111%)]\tLoss: 2.570174\n",
      "Train Epoche: 2 [108/96 (112%)]\tLoss: 131.907776\n",
      "Train Epoche: 2 [109/96 (114%)]\tLoss: 2.083103\n",
      "Train Epoche: 2 [110/96 (115%)]\tLoss: 0.701968\n",
      "Train Epoche: 2 [111/96 (116%)]\tLoss: 0.352121\n",
      "Train Epoche: 2 [112/96 (117%)]\tLoss: 0.317734\n",
      "Train Epoche: 2 [113/96 (118%)]\tLoss: 2.836139\n",
      "Train Epoche: 2 [114/96 (119%)]\tLoss: 1.417191\n",
      "Train Epoche: 2 [115/96 (120%)]\tLoss: 109.078537\n",
      "Train Epoche: 2 [116/96 (121%)]\tLoss: 1.319538\n",
      "Train Epoche: 2 [117/96 (122%)]\tLoss: 6.330797\n",
      "Train Epoche: 2 [118/96 (123%)]\tLoss: 8.165994\n",
      "Train Epoche: 2 [119/96 (124%)]\tLoss: 0.004176\n",
      "Train Epoche: 2 [120/96 (125%)]\tLoss: 31.680355\n",
      "Train Epoche: 2 [121/96 (126%)]\tLoss: 14.880587\n",
      "Train Epoche: 2 [122/96 (127%)]\tLoss: 20.003809\n",
      "Train Epoche: 2 [123/96 (128%)]\tLoss: 30.596710\n",
      "Train Epoche: 2 [124/96 (129%)]\tLoss: 37.471245\n",
      "Train Epoche: 2 [125/96 (130%)]\tLoss: 9.221554\n",
      "Train Epoche: 2 [126/96 (131%)]\tLoss: 6.945966\n",
      "Train Epoche: 2 [127/96 (132%)]\tLoss: 0.457311\n",
      "Train Epoche: 2 [128/96 (133%)]\tLoss: 92.936958\n",
      "Train Epoche: 2 [129/96 (134%)]\tLoss: 41.489601\n",
      "Train Epoche: 2 [130/96 (135%)]\tLoss: 0.008828\n",
      "Train Epoche: 2 [131/96 (136%)]\tLoss: 3.549337\n",
      "Train Epoche: 2 [132/96 (138%)]\tLoss: 14.162757\n",
      "Train Epoche: 2 [133/96 (139%)]\tLoss: 10.058540\n",
      "Train Epoche: 2 [134/96 (140%)]\tLoss: 0.086313\n",
      "Train Epoche: 2 [135/96 (141%)]\tLoss: 173.514374\n",
      "Train Epoche: 2 [136/96 (142%)]\tLoss: 1.981838\n",
      "Train Epoche: 2 [137/96 (143%)]\tLoss: 3.624188\n",
      "Train Epoche: 2 [138/96 (144%)]\tLoss: 1.377224\n",
      "Train Epoche: 2 [139/96 (145%)]\tLoss: 87.167831\n",
      "Train Epoche: 2 [140/96 (146%)]\tLoss: 26.966089\n",
      "Train Epoche: 2 [141/96 (147%)]\tLoss: 57.030502\n",
      "Train Epoche: 2 [142/96 (148%)]\tLoss: 20.617142\n",
      "Train Epoche: 2 [143/96 (149%)]\tLoss: 5.307233\n",
      "Train Epoche: 2 [144/96 (150%)]\tLoss: 19.139874\n",
      "Train Epoche: 2 [145/96 (151%)]\tLoss: 6.874228\n",
      "Train Epoche: 2 [146/96 (152%)]\tLoss: 4.451382\n",
      "Train Epoche: 2 [147/96 (153%)]\tLoss: 0.612895\n",
      "Train Epoche: 2 [148/96 (154%)]\tLoss: 370.457062\n",
      "Train Epoche: 2 [149/96 (155%)]\tLoss: 1.872097\n",
      "Train Epoche: 2 [150/96 (156%)]\tLoss: 0.393284\n",
      "Train Epoche: 2 [151/96 (157%)]\tLoss: 4.598679\n",
      "Train Epoche: 2 [152/96 (158%)]\tLoss: 1.146059\n",
      "Train Epoche: 2 [153/96 (159%)]\tLoss: 0.761909\n",
      "Train Epoche: 2 [154/96 (160%)]\tLoss: 67.760849\n",
      "Train Epoche: 2 [155/96 (161%)]\tLoss: 2.087491\n",
      "Train Epoche: 2 [156/96 (162%)]\tLoss: 26.155769\n",
      "Train Epoche: 2 [157/96 (164%)]\tLoss: 0.975754\n",
      "Train Epoche: 2 [158/96 (165%)]\tLoss: 0.178326\n",
      "Train Epoche: 2 [159/96 (166%)]\tLoss: 70.591179\n",
      "Train Epoche: 2 [160/96 (167%)]\tLoss: 0.055998\n",
      "Train Epoche: 2 [161/96 (168%)]\tLoss: 46.439709\n",
      "Train Epoche: 2 [162/96 (169%)]\tLoss: 2.192861\n",
      "Train Epoche: 2 [163/96 (170%)]\tLoss: 71.293991\n",
      "Train Epoche: 2 [164/96 (171%)]\tLoss: 1.098997\n",
      "Train Epoche: 2 [165/96 (172%)]\tLoss: 70.623932\n",
      "Train Epoche: 2 [166/96 (173%)]\tLoss: 25.851707\n",
      "Train Epoche: 2 [167/96 (174%)]\tLoss: 71.351723\n",
      "Train Epoche: 2 [168/96 (175%)]\tLoss: 1.674238\n",
      "Train Epoche: 2 [169/96 (176%)]\tLoss: 0.065003\n",
      "Train Epoche: 2 [170/96 (177%)]\tLoss: 0.004147\n",
      "Train Epoche: 2 [171/96 (178%)]\tLoss: 0.002344\n",
      "Train Epoche: 2 [172/96 (179%)]\tLoss: 8.454206\n",
      "Train Epoche: 2 [173/96 (180%)]\tLoss: 107.819351\n",
      "Train Epoche: 2 [174/96 (181%)]\tLoss: 1.506906\n",
      "Train Epoche: 2 [175/96 (182%)]\tLoss: 7.145747\n",
      "Train Epoche: 2 [176/96 (183%)]\tLoss: 31.529089\n",
      "Train Epoche: 2 [177/96 (184%)]\tLoss: 15.101479\n",
      "Train Epoche: 2 [178/96 (185%)]\tLoss: 0.261484\n",
      "Train Epoche: 2 [179/96 (186%)]\tLoss: 3.096715\n",
      "Train Epoche: 2 [180/96 (188%)]\tLoss: 0.417414\n",
      "Train Epoche: 2 [181/96 (189%)]\tLoss: 0.024140\n",
      "Train Epoche: 2 [182/96 (190%)]\tLoss: 5.422848\n",
      "Train Epoche: 2 [183/96 (191%)]\tLoss: 11.988191\n",
      "Train Epoche: 2 [184/96 (192%)]\tLoss: 2.451691\n",
      "Train Epoche: 2 [185/96 (193%)]\tLoss: 12.701675\n",
      "Train Epoche: 2 [186/96 (194%)]\tLoss: 0.044083\n",
      "Train Epoche: 2 [187/96 (195%)]\tLoss: 3.642684\n",
      "Train Epoche: 2 [188/96 (196%)]\tLoss: 9.065228\n",
      "Train Epoche: 2 [189/96 (197%)]\tLoss: 29.707125\n",
      "Train Epoche: 2 [190/96 (198%)]\tLoss: 12.437926\n",
      "Train Epoche: 2 [191/96 (199%)]\tLoss: 11.664472\n",
      "Train Epoche: 2 [192/96 (200%)]\tLoss: 0.358081\n",
      "Train Epoche: 2 [193/96 (201%)]\tLoss: 0.672944\n",
      "Train Epoche: 2 [194/96 (202%)]\tLoss: 0.059902\n",
      "Train Epoche: 2 [195/96 (203%)]\tLoss: 78.895210\n",
      "Train Epoche: 2 [196/96 (204%)]\tLoss: 0.261947\n",
      "Train Epoche: 2 [197/96 (205%)]\tLoss: 7.080557\n",
      "Train Epoche: 2 [198/96 (206%)]\tLoss: 7.083882\n",
      "Train Epoche: 2 [199/96 (207%)]\tLoss: 0.360827\n",
      "Train Epoche: 2 [200/96 (208%)]\tLoss: 0.064269\n",
      "Train Epoche: 2 [201/96 (209%)]\tLoss: 18.484417\n",
      "Train Epoche: 2 [202/96 (210%)]\tLoss: 48.707802\n",
      "Train Epoche: 2 [203/96 (211%)]\tLoss: 1.230032\n",
      "Train Epoche: 2 [204/96 (212%)]\tLoss: 10.097908\n",
      "Train Epoche: 2 [205/96 (214%)]\tLoss: 19.308929\n",
      "Train Epoche: 2 [206/96 (215%)]\tLoss: 3.915505\n",
      "Train Epoche: 2 [207/96 (216%)]\tLoss: 2.034294\n",
      "Train Epoche: 2 [208/96 (217%)]\tLoss: 10.945076\n",
      "Train Epoche: 2 [209/96 (218%)]\tLoss: 175.288559\n",
      "Train Epoche: 2 [210/96 (219%)]\tLoss: 4.960660\n",
      "Train Epoche: 2 [211/96 (220%)]\tLoss: 0.067316\n",
      "Train Epoche: 2 [212/96 (221%)]\tLoss: 0.004829\n",
      "Train Epoche: 2 [213/96 (222%)]\tLoss: 0.012676\n",
      "Train Epoche: 2 [214/96 (223%)]\tLoss: 0.088431\n",
      "Train Epoche: 2 [215/96 (224%)]\tLoss: 0.525581\n",
      "Train Epoche: 2 [216/96 (225%)]\tLoss: 6.192953\n",
      "Train Epoche: 2 [217/96 (226%)]\tLoss: 7.385473\n",
      "Train Epoche: 2 [218/96 (227%)]\tLoss: 0.571939\n",
      "Train Epoche: 2 [219/96 (228%)]\tLoss: 0.000056\n",
      "Train Epoche: 2 [220/96 (229%)]\tLoss: 2.667312\n",
      "Train Epoche: 2 [221/96 (230%)]\tLoss: 6.276344\n",
      "Train Epoche: 2 [222/96 (231%)]\tLoss: 2.460930\n",
      "Train Epoche: 2 [223/96 (232%)]\tLoss: 38.078056\n",
      "Train Epoche: 2 [224/96 (233%)]\tLoss: 3.704058\n",
      "Train Epoche: 2 [225/96 (234%)]\tLoss: 0.982455\n",
      "Train Epoche: 2 [226/96 (235%)]\tLoss: 1.679800\n",
      "Train Epoche: 2 [227/96 (236%)]\tLoss: 0.138228\n",
      "Train Epoche: 2 [228/96 (238%)]\tLoss: 0.014077\n",
      "Train Epoche: 2 [229/96 (239%)]\tLoss: 0.123194\n",
      "Train Epoche: 2 [230/96 (240%)]\tLoss: 4.782649\n",
      "Train Epoche: 2 [231/96 (241%)]\tLoss: 222.174973\n",
      "Train Epoche: 2 [232/96 (242%)]\tLoss: 0.088072\n",
      "Train Epoche: 2 [233/96 (243%)]\tLoss: 0.052227\n",
      "Train Epoche: 2 [234/96 (244%)]\tLoss: 1.137080\n",
      "Train Epoche: 2 [235/96 (245%)]\tLoss: 15.440742\n",
      "Train Epoche: 2 [236/96 (246%)]\tLoss: 238.670349\n",
      "Train Epoche: 2 [237/96 (247%)]\tLoss: 3.141521\n",
      "Train Epoche: 2 [238/96 (248%)]\tLoss: 21.718887\n",
      "Train Epoche: 2 [239/96 (249%)]\tLoss: 22.993608\n",
      "Train Epoche: 2 [240/96 (250%)]\tLoss: 25.566402\n",
      "Train Epoche: 2 [241/96 (251%)]\tLoss: 32.934418\n",
      "Train Epoche: 2 [242/96 (252%)]\tLoss: 1.008353\n",
      "Train Epoche: 2 [243/96 (253%)]\tLoss: 2.130111\n",
      "Train Epoche: 2 [244/96 (254%)]\tLoss: 1.423465\n",
      "Train Epoche: 2 [245/96 (255%)]\tLoss: 20.986612\n",
      "Train Epoche: 2 [246/96 (256%)]\tLoss: 19.110683\n",
      "Train Epoche: 2 [247/96 (257%)]\tLoss: 0.156526\n",
      "Train Epoche: 2 [248/96 (258%)]\tLoss: 1.855100\n",
      "Train Epoche: 2 [249/96 (259%)]\tLoss: 60.291897\n",
      "Train Epoche: 2 [250/96 (260%)]\tLoss: 14.636680\n",
      "Train Epoche: 2 [251/96 (261%)]\tLoss: 0.435190\n",
      "Train Epoche: 2 [252/96 (262%)]\tLoss: 2.940014\n",
      "Train Epoche: 2 [253/96 (264%)]\tLoss: 0.241026\n",
      "Train Epoche: 2 [254/96 (265%)]\tLoss: 0.248463\n",
      "Train Epoche: 2 [255/96 (266%)]\tLoss: 0.153797\n",
      "Train Epoche: 2 [256/96 (267%)]\tLoss: 33.426136\n",
      "Train Epoche: 2 [257/96 (268%)]\tLoss: 1.897876\n",
      "Train Epoche: 2 [258/96 (269%)]\tLoss: 20.356709\n",
      "Train Epoche: 2 [259/96 (270%)]\tLoss: 0.480500\n",
      "Train Epoche: 2 [260/96 (271%)]\tLoss: 27.418493\n",
      "Train Epoche: 2 [261/96 (272%)]\tLoss: 1.370608\n",
      "Train Epoche: 2 [262/96 (273%)]\tLoss: 1.025897\n",
      "Train Epoche: 2 [263/96 (274%)]\tLoss: 4.969141\n",
      "Train Epoche: 2 [264/96 (275%)]\tLoss: 7.729804\n",
      "Train Epoche: 2 [265/96 (276%)]\tLoss: 1.581804\n",
      "Train Epoche: 2 [266/96 (277%)]\tLoss: 2.976758\n",
      "Train Epoche: 2 [267/96 (278%)]\tLoss: 145.344925\n",
      "Train Epoche: 2 [268/96 (279%)]\tLoss: 11.274652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [269/96 (280%)]\tLoss: 0.840336\n",
      "Train Epoche: 2 [270/96 (281%)]\tLoss: 0.211934\n",
      "Train Epoche: 2 [271/96 (282%)]\tLoss: 11.572092\n",
      "Train Epoche: 2 [272/96 (283%)]\tLoss: 4.607518\n",
      "Train Epoche: 2 [273/96 (284%)]\tLoss: 0.561215\n",
      "Train Epoche: 2 [274/96 (285%)]\tLoss: 0.270971\n",
      "Train Epoche: 2 [275/96 (286%)]\tLoss: 0.036176\n",
      "Train Epoche: 2 [276/96 (288%)]\tLoss: 1.842806\n",
      "Train Epoche: 2 [277/96 (289%)]\tLoss: 3.401087\n",
      "Train Epoche: 2 [278/96 (290%)]\tLoss: 0.946053\n",
      "Train Epoche: 2 [279/96 (291%)]\tLoss: 26.838470\n",
      "Train Epoche: 2 [280/96 (292%)]\tLoss: 4.982566\n",
      "Train Epoche: 2 [281/96 (293%)]\tLoss: 28.423033\n",
      "Train Epoche: 2 [282/96 (294%)]\tLoss: 0.019662\n",
      "Train Epoche: 2 [283/96 (295%)]\tLoss: 0.928977\n",
      "Train Epoche: 2 [284/96 (296%)]\tLoss: 1.872846\n",
      "Train Epoche: 2 [285/96 (297%)]\tLoss: 1.275284\n",
      "Train Epoche: 2 [286/96 (298%)]\tLoss: 0.011758\n",
      "Train Epoche: 2 [287/96 (299%)]\tLoss: 0.164694\n",
      "Train Epoche: 2 [288/96 (300%)]\tLoss: 1.661859\n",
      "Train Epoche: 2 [289/96 (301%)]\tLoss: 40.933048\n",
      "Train Epoche: 2 [290/96 (302%)]\tLoss: 2.478393\n",
      "Train Epoche: 2 [291/96 (303%)]\tLoss: 2.042575\n",
      "Train Epoche: 2 [292/96 (304%)]\tLoss: 3.510684\n",
      "Train Epoche: 2 [293/96 (305%)]\tLoss: 12.912695\n",
      "Train Epoche: 2 [294/96 (306%)]\tLoss: 2.106659\n",
      "Train Epoche: 2 [295/96 (307%)]\tLoss: 0.887852\n",
      "Train Epoche: 2 [296/96 (308%)]\tLoss: 0.000313\n",
      "Train Epoche: 2 [297/96 (309%)]\tLoss: 5.824261\n",
      "Train Epoche: 2 [298/96 (310%)]\tLoss: 0.082454\n",
      "Train Epoche: 2 [299/96 (311%)]\tLoss: 0.603869\n",
      "Train Epoche: 2 [300/96 (312%)]\tLoss: 1.363224\n",
      "Train Epoche: 2 [301/96 (314%)]\tLoss: 16.157015\n",
      "Train Epoche: 2 [302/96 (315%)]\tLoss: 0.029600\n",
      "Train Epoche: 2 [303/96 (316%)]\tLoss: 2.231704\n",
      "Train Epoche: 2 [304/96 (317%)]\tLoss: 5.811055\n",
      "Train Epoche: 2 [305/96 (318%)]\tLoss: 5.643117\n",
      "Train Epoche: 2 [306/96 (319%)]\tLoss: 6.937056\n",
      "Train Epoche: 2 [307/96 (320%)]\tLoss: 0.188175\n",
      "Train Epoche: 2 [308/96 (321%)]\tLoss: 96.362488\n",
      "Train Epoche: 2 [309/96 (322%)]\tLoss: 12.723743\n",
      "Train Epoche: 2 [310/96 (323%)]\tLoss: 0.244550\n",
      "Train Epoche: 2 [311/96 (324%)]\tLoss: 0.000411\n",
      "Train Epoche: 2 [312/96 (325%)]\tLoss: 3.419376\n",
      "Train Epoche: 2 [313/96 (326%)]\tLoss: 0.152502\n",
      "Train Epoche: 2 [314/96 (327%)]\tLoss: 76.917259\n",
      "Train Epoche: 2 [315/96 (328%)]\tLoss: 0.010247\n",
      "Train Epoche: 2 [316/96 (329%)]\tLoss: 2.713056\n",
      "Train Epoche: 2 [317/96 (330%)]\tLoss: 1.802839\n",
      "Train Epoche: 2 [318/96 (331%)]\tLoss: 0.506842\n",
      "Train Epoche: 2 [319/96 (332%)]\tLoss: 0.007090\n",
      "Train Epoche: 2 [320/96 (333%)]\tLoss: 7.912340\n",
      "Train Epoche: 2 [321/96 (334%)]\tLoss: 6.047749\n",
      "Train Epoche: 2 [322/96 (335%)]\tLoss: 0.023656\n",
      "Train Epoche: 2 [323/96 (336%)]\tLoss: 0.810940\n",
      "Train Epoche: 2 [324/96 (338%)]\tLoss: 1.304546\n",
      "Train Epoche: 2 [325/96 (339%)]\tLoss: 1.821626\n",
      "Train Epoche: 2 [326/96 (340%)]\tLoss: 0.147520\n",
      "Train Epoche: 2 [327/96 (341%)]\tLoss: 0.569259\n",
      "Train Epoche: 2 [328/96 (342%)]\tLoss: 0.010985\n",
      "Train Epoche: 2 [329/96 (343%)]\tLoss: 63.244099\n",
      "Train Epoche: 2 [330/96 (344%)]\tLoss: 4.392875\n",
      "Train Epoche: 2 [331/96 (345%)]\tLoss: 28.048803\n",
      "Train Epoche: 2 [332/96 (346%)]\tLoss: 0.149741\n",
      "Train Epoche: 2 [333/96 (347%)]\tLoss: 52.172245\n",
      "Train Epoche: 2 [334/96 (348%)]\tLoss: 0.125021\n",
      "Train Epoche: 2 [335/96 (349%)]\tLoss: 1.732639\n",
      "Train Epoche: 2 [336/96 (350%)]\tLoss: 31.102478\n",
      "Train Epoche: 2 [337/96 (351%)]\tLoss: 1.695353\n",
      "Train Epoche: 2 [338/96 (352%)]\tLoss: 0.005724\n",
      "Train Epoche: 2 [339/96 (353%)]\tLoss: 1.962008\n",
      "Train Epoche: 2 [340/96 (354%)]\tLoss: 1.042281\n",
      "Train Epoche: 2 [341/96 (355%)]\tLoss: 3.263626\n",
      "Train Epoche: 2 [342/96 (356%)]\tLoss: 6.684258\n",
      "Train Epoche: 2 [343/96 (357%)]\tLoss: 2.534737\n",
      "Train Epoche: 2 [344/96 (358%)]\tLoss: 111.089546\n",
      "Train Epoche: 2 [345/96 (359%)]\tLoss: 309.518829\n",
      "Train Epoche: 2 [346/96 (360%)]\tLoss: 0.518529\n",
      "Train Epoche: 2 [347/96 (361%)]\tLoss: 0.643693\n",
      "Train Epoche: 2 [348/96 (362%)]\tLoss: 4.535296\n",
      "Train Epoche: 2 [349/96 (364%)]\tLoss: 2.132846\n",
      "Train Epoche: 2 [350/96 (365%)]\tLoss: 2.846613\n",
      "Train Epoche: 2 [351/96 (366%)]\tLoss: 1.690040\n",
      "Train Epoche: 2 [352/96 (367%)]\tLoss: 165.761917\n",
      "Train Epoche: 2 [353/96 (368%)]\tLoss: 1.055606\n",
      "Train Epoche: 2 [354/96 (369%)]\tLoss: 6.534368\n",
      "Train Epoche: 2 [355/96 (370%)]\tLoss: 0.428641\n",
      "Train Epoche: 2 [356/96 (371%)]\tLoss: 158.618195\n",
      "Train Epoche: 2 [357/96 (372%)]\tLoss: 35.001022\n",
      "Train Epoche: 2 [358/96 (373%)]\tLoss: 9.600403\n",
      "Train Epoche: 2 [359/96 (374%)]\tLoss: 1.166128\n",
      "Train Epoche: 2 [360/96 (375%)]\tLoss: 12.259134\n",
      "Train Epoche: 2 [361/96 (376%)]\tLoss: 10.327955\n",
      "Train Epoche: 2 [362/96 (377%)]\tLoss: 0.100029\n",
      "Train Epoche: 2 [363/96 (378%)]\tLoss: 0.001255\n",
      "Train Epoche: 2 [364/96 (379%)]\tLoss: 7.323826\n",
      "Train Epoche: 2 [365/96 (380%)]\tLoss: 6.003485\n",
      "Train Epoche: 2 [366/96 (381%)]\tLoss: 4.793818\n",
      "Train Epoche: 2 [367/96 (382%)]\tLoss: 16.731394\n",
      "Train Epoche: 2 [368/96 (383%)]\tLoss: 0.487484\n",
      "Train Epoche: 2 [369/96 (384%)]\tLoss: 3.508019\n",
      "Train Epoche: 2 [370/96 (385%)]\tLoss: 10.173785\n",
      "Train Epoche: 2 [371/96 (386%)]\tLoss: 2.018562\n",
      "Train Epoche: 2 [372/96 (388%)]\tLoss: 38.405273\n",
      "Train Epoche: 2 [373/96 (389%)]\tLoss: 13.485958\n",
      "Train Epoche: 2 [374/96 (390%)]\tLoss: 8.322907\n",
      "Train Epoche: 2 [375/96 (391%)]\tLoss: 13.060350\n",
      "Train Epoche: 2 [376/96 (392%)]\tLoss: 70.545540\n",
      "Train Epoche: 2 [377/96 (393%)]\tLoss: 0.045327\n",
      "Train Epoche: 2 [378/96 (394%)]\tLoss: 0.977495\n",
      "Train Epoche: 2 [379/96 (395%)]\tLoss: 3.880238\n",
      "Train Epoche: 2 [380/96 (396%)]\tLoss: 2.677151\n",
      "Train Epoche: 2 [381/96 (397%)]\tLoss: 21.054928\n",
      "Train Epoche: 2 [382/96 (398%)]\tLoss: 0.000491\n",
      "Train Epoche: 2 [383/96 (399%)]\tLoss: 1.062489\n",
      "Train Epoche: 2 [384/96 (400%)]\tLoss: 16.439972\n",
      "Train Epoche: 2 [385/96 (401%)]\tLoss: 12.203328\n",
      "Train Epoche: 2 [386/96 (402%)]\tLoss: 17.160254\n",
      "Train Epoche: 2 [387/96 (403%)]\tLoss: 13.985737\n",
      "Train Epoche: 2 [388/96 (404%)]\tLoss: 13.373911\n",
      "Train Epoche: 2 [389/96 (405%)]\tLoss: 0.546779\n",
      "Train Epoche: 2 [390/96 (406%)]\tLoss: 7.922344\n",
      "Train Epoche: 2 [391/96 (407%)]\tLoss: 14.293323\n",
      "Train Epoche: 2 [392/96 (408%)]\tLoss: 39.461811\n",
      "Train Epoche: 2 [393/96 (409%)]\tLoss: 180.813614\n",
      "Train Epoche: 2 [394/96 (410%)]\tLoss: 9.292597\n",
      "Train Epoche: 2 [395/96 (411%)]\tLoss: 4.505644\n",
      "Train Epoche: 2 [396/96 (412%)]\tLoss: 1.693640\n",
      "Train Epoche: 2 [397/96 (414%)]\tLoss: 6.633866\n",
      "Train Epoche: 2 [398/96 (415%)]\tLoss: 1.586206\n",
      "Train Epoche: 2 [399/96 (416%)]\tLoss: 0.663495\n",
      "Train Epoche: 2 [400/96 (417%)]\tLoss: 1.950057\n",
      "Train Epoche: 2 [401/96 (418%)]\tLoss: 1.704399\n",
      "Train Epoche: 2 [402/96 (419%)]\tLoss: 0.490531\n",
      "Train Epoche: 2 [403/96 (420%)]\tLoss: 5.476963\n",
      "Train Epoche: 2 [404/96 (421%)]\tLoss: 0.467039\n",
      "Train Epoche: 2 [405/96 (422%)]\tLoss: 0.360902\n",
      "Train Epoche: 2 [406/96 (423%)]\tLoss: 25.369991\n",
      "Train Epoche: 2 [407/96 (424%)]\tLoss: 4.494383\n",
      "Train Epoche: 2 [408/96 (425%)]\tLoss: 0.056959\n",
      "Train Epoche: 2 [409/96 (426%)]\tLoss: 4.779840\n",
      "Train Epoche: 2 [410/96 (427%)]\tLoss: 0.072752\n",
      "Train Epoche: 2 [411/96 (428%)]\tLoss: 16.351519\n",
      "Train Epoche: 2 [412/96 (429%)]\tLoss: 28.073557\n",
      "Train Epoche: 2 [413/96 (430%)]\tLoss: 13.892642\n",
      "Train Epoche: 2 [414/96 (431%)]\tLoss: 0.424580\n",
      "Train Epoche: 2 [415/96 (432%)]\tLoss: 17.241785\n",
      "Train Epoche: 2 [416/96 (433%)]\tLoss: 16.593523\n",
      "Train Epoche: 2 [417/96 (434%)]\tLoss: 5.226861\n",
      "Train Epoche: 2 [418/96 (435%)]\tLoss: 3.842067\n",
      "Train Epoche: 2 [419/96 (436%)]\tLoss: 4.461011\n",
      "Train Epoche: 2 [420/96 (438%)]\tLoss: 4.738582\n",
      "Train Epoche: 2 [421/96 (439%)]\tLoss: 1.459078\n",
      "Train Epoche: 2 [422/96 (440%)]\tLoss: 5.408871\n",
      "Train Epoche: 2 [423/96 (441%)]\tLoss: 121.643219\n",
      "Train Epoche: 2 [424/96 (442%)]\tLoss: 6.439024\n",
      "Train Epoche: 2 [425/96 (443%)]\tLoss: 4.954710\n",
      "Train Epoche: 2 [426/96 (444%)]\tLoss: 1.642027\n",
      "Train Epoche: 2 [427/96 (445%)]\tLoss: 0.399567\n",
      "Train Epoche: 2 [428/96 (446%)]\tLoss: 29.739702\n",
      "Train Epoche: 2 [429/96 (447%)]\tLoss: 2.219369\n",
      "Train Epoche: 2 [430/96 (448%)]\tLoss: 8.917711\n",
      "Train Epoche: 2 [431/96 (449%)]\tLoss: 0.061819\n",
      "Train Epoche: 2 [432/96 (450%)]\tLoss: 1.173906\n",
      "Train Epoche: 2 [433/96 (451%)]\tLoss: 0.004669\n",
      "Train Epoche: 2 [434/96 (452%)]\tLoss: 0.943473\n",
      "Train Epoche: 2 [435/96 (453%)]\tLoss: 4.425524\n",
      "Train Epoche: 2 [436/96 (454%)]\tLoss: 18.900581\n",
      "Train Epoche: 2 [437/96 (455%)]\tLoss: 28.906137\n",
      "Train Epoche: 2 [438/96 (456%)]\tLoss: 1.488643\n",
      "Train Epoche: 2 [439/96 (457%)]\tLoss: 0.073069\n",
      "Train Epoche: 2 [440/96 (458%)]\tLoss: 5.816027\n",
      "Train Epoche: 2 [441/96 (459%)]\tLoss: 1.373877\n",
      "Train Epoche: 2 [442/96 (460%)]\tLoss: 0.479202\n",
      "Train Epoche: 2 [443/96 (461%)]\tLoss: 4.062431\n",
      "Train Epoche: 2 [444/96 (462%)]\tLoss: 9.233840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [445/96 (464%)]\tLoss: 1.495203\n",
      "Train Epoche: 2 [446/96 (465%)]\tLoss: 116.682625\n",
      "Train Epoche: 2 [447/96 (466%)]\tLoss: 1.381984\n",
      "Train Epoche: 2 [448/96 (467%)]\tLoss: 3.862646\n",
      "Train Epoche: 2 [449/96 (468%)]\tLoss: 0.582448\n",
      "Train Epoche: 2 [450/96 (469%)]\tLoss: 27.227589\n",
      "Train Epoche: 2 [451/96 (470%)]\tLoss: 0.763760\n",
      "Train Epoche: 2 [452/96 (471%)]\tLoss: 29.070354\n",
      "Train Epoche: 2 [453/96 (472%)]\tLoss: 4.717406\n",
      "Train Epoche: 2 [454/96 (473%)]\tLoss: 49.240059\n",
      "Train Epoche: 2 [455/96 (474%)]\tLoss: 22.306581\n",
      "Train Epoche: 2 [456/96 (475%)]\tLoss: 15.719426\n",
      "Train Epoche: 2 [457/96 (476%)]\tLoss: 2.880393\n",
      "Train Epoche: 2 [458/96 (477%)]\tLoss: 7.561346\n",
      "Train Epoche: 2 [459/96 (478%)]\tLoss: 0.898876\n",
      "Train Epoche: 2 [460/96 (479%)]\tLoss: 130.737732\n",
      "Train Epoche: 2 [461/96 (480%)]\tLoss: 0.547646\n",
      "Train Epoche: 2 [462/96 (481%)]\tLoss: 0.748470\n",
      "Train Epoche: 2 [463/96 (482%)]\tLoss: 1.752193\n",
      "Train Epoche: 2 [464/96 (483%)]\tLoss: 166.515366\n",
      "Train Epoche: 2 [465/96 (484%)]\tLoss: 4.704328\n",
      "Train Epoche: 2 [466/96 (485%)]\tLoss: 0.145539\n",
      "Train Epoche: 2 [467/96 (486%)]\tLoss: 77.226280\n",
      "Train Epoche: 2 [468/96 (488%)]\tLoss: 0.016054\n",
      "Train Epoche: 2 [469/96 (489%)]\tLoss: 8.782846\n",
      "Train Epoche: 2 [470/96 (490%)]\tLoss: 5.422287\n",
      "Train Epoche: 2 [471/96 (491%)]\tLoss: 2.008288\n",
      "Train Epoche: 2 [472/96 (492%)]\tLoss: 0.064572\n",
      "Train Epoche: 2 [473/96 (493%)]\tLoss: 0.722718\n",
      "Train Epoche: 2 [474/96 (494%)]\tLoss: 26.074333\n",
      "Train Epoche: 2 [475/96 (495%)]\tLoss: 10.340776\n",
      "Train Epoche: 2 [476/96 (496%)]\tLoss: 2.723169\n",
      "Train Epoche: 2 [477/96 (497%)]\tLoss: 11.690531\n",
      "Train Epoche: 2 [478/96 (498%)]\tLoss: 9.642154\n",
      "Train Epoche: 2 [479/96 (499%)]\tLoss: 106.887581\n",
      "Train Epoche: 2 [480/96 (500%)]\tLoss: 10.456161\n",
      "Train Epoche: 2 [481/96 (501%)]\tLoss: 3.666710\n",
      "Train Epoche: 2 [482/96 (502%)]\tLoss: 4.850550\n",
      "Train Epoche: 2 [483/96 (503%)]\tLoss: 22.612919\n",
      "Train Epoche: 2 [484/96 (504%)]\tLoss: 10.072725\n",
      "Train Epoche: 2 [485/96 (505%)]\tLoss: 20.883120\n",
      "Train Epoche: 2 [486/96 (506%)]\tLoss: 0.548337\n",
      "Train Epoche: 2 [487/96 (507%)]\tLoss: 5.136294\n",
      "Train Epoche: 2 [488/96 (508%)]\tLoss: 6.579720\n",
      "Train Epoche: 2 [489/96 (509%)]\tLoss: 3.008191\n",
      "Train Epoche: 2 [490/96 (510%)]\tLoss: 48.293072\n",
      "Train Epoche: 2 [491/96 (511%)]\tLoss: 78.739456\n",
      "Train Epoche: 2 [492/96 (512%)]\tLoss: 23.134554\n",
      "Train Epoche: 2 [493/96 (514%)]\tLoss: 3.518374\n",
      "Train Epoche: 2 [494/96 (515%)]\tLoss: 4.046364\n",
      "Train Epoche: 2 [495/96 (516%)]\tLoss: 1.064622\n",
      "Train Epoche: 2 [496/96 (517%)]\tLoss: 4.648777\n",
      "Train Epoche: 2 [497/96 (518%)]\tLoss: 46.871655\n",
      "Train Epoche: 2 [498/96 (519%)]\tLoss: 32.300491\n",
      "Train Epoche: 2 [499/96 (520%)]\tLoss: 0.027218\n",
      "Train Epoche: 2 [500/96 (521%)]\tLoss: 23.820545\n",
      "Train Epoche: 2 [501/96 (522%)]\tLoss: 8.984214\n",
      "Train Epoche: 2 [502/96 (523%)]\tLoss: 51.258564\n",
      "Train Epoche: 2 [503/96 (524%)]\tLoss: 1.602255\n",
      "Train Epoche: 2 [504/96 (525%)]\tLoss: 0.348090\n",
      "Train Epoche: 2 [505/96 (526%)]\tLoss: 64.245430\n",
      "Train Epoche: 2 [506/96 (527%)]\tLoss: 4.619808\n",
      "Train Epoche: 2 [507/96 (528%)]\tLoss: 33.198299\n",
      "Train Epoche: 2 [508/96 (529%)]\tLoss: 5.338882\n",
      "Train Epoche: 2 [509/96 (530%)]\tLoss: 0.001084\n",
      "Train Epoche: 2 [510/96 (531%)]\tLoss: 21.834997\n",
      "Train Epoche: 2 [511/96 (532%)]\tLoss: 9.524552\n",
      "Train Epoche: 2 [512/96 (533%)]\tLoss: 0.182431\n",
      "Train Epoche: 2 [513/96 (534%)]\tLoss: 0.067000\n",
      "Train Epoche: 2 [514/96 (535%)]\tLoss: 32.140736\n",
      "Train Epoche: 2 [515/96 (536%)]\tLoss: 0.196875\n",
      "Train Epoche: 2 [516/96 (538%)]\tLoss: 3.177396\n",
      "Train Epoche: 2 [517/96 (539%)]\tLoss: 18.964344\n",
      "Train Epoche: 2 [518/96 (540%)]\tLoss: 8.634280\n",
      "Train Epoche: 2 [519/96 (541%)]\tLoss: 26.251266\n",
      "Train Epoche: 2 [520/96 (542%)]\tLoss: 5.870709\n",
      "Train Epoche: 2 [521/96 (543%)]\tLoss: 133.608139\n",
      "Train Epoche: 2 [522/96 (544%)]\tLoss: 4.417976\n",
      "Train Epoche: 2 [523/96 (545%)]\tLoss: 70.091095\n",
      "Train Epoche: 2 [524/96 (546%)]\tLoss: 3.330273\n",
      "Train Epoche: 2 [525/96 (547%)]\tLoss: 4.051588\n",
      "Train Epoche: 2 [526/96 (548%)]\tLoss: 2.058562\n",
      "Train Epoche: 2 [527/96 (549%)]\tLoss: 0.128779\n",
      "Train Epoche: 2 [528/96 (550%)]\tLoss: 0.278836\n",
      "Train Epoche: 2 [529/96 (551%)]\tLoss: 156.678452\n",
      "Train Epoche: 2 [530/96 (552%)]\tLoss: 15.933708\n",
      "Train Epoche: 2 [531/96 (553%)]\tLoss: 1.123160\n",
      "Train Epoche: 2 [532/96 (554%)]\tLoss: 1.378968\n",
      "Train Epoche: 2 [533/96 (555%)]\tLoss: 76.433449\n",
      "Train Epoche: 2 [534/96 (556%)]\tLoss: 3.907538\n",
      "Train Epoche: 2 [535/96 (557%)]\tLoss: 0.021481\n",
      "Train Epoche: 2 [536/96 (558%)]\tLoss: 2.899504\n",
      "Train Epoche: 2 [537/96 (559%)]\tLoss: 0.072597\n",
      "Train Epoche: 2 [538/96 (560%)]\tLoss: 1.572580\n",
      "Train Epoche: 2 [539/96 (561%)]\tLoss: 0.000339\n",
      "Train Epoche: 2 [540/96 (562%)]\tLoss: 10.353778\n",
      "Train Epoche: 2 [541/96 (564%)]\tLoss: 2.690189\n",
      "Train Epoche: 2 [542/96 (565%)]\tLoss: 0.107767\n",
      "Train Epoche: 2 [543/96 (566%)]\tLoss: 11.464490\n",
      "Train Epoche: 2 [544/96 (567%)]\tLoss: 0.578428\n",
      "Train Epoche: 2 [545/96 (568%)]\tLoss: 1.560348\n",
      "Train Epoche: 2 [546/96 (569%)]\tLoss: 3.453716\n",
      "Train Epoche: 2 [547/96 (570%)]\tLoss: 1.627703\n",
      "Train Epoche: 2 [548/96 (571%)]\tLoss: 2.883625\n",
      "Train Epoche: 2 [549/96 (572%)]\tLoss: 0.016037\n",
      "Train Epoche: 2 [550/96 (573%)]\tLoss: 0.196148\n",
      "Train Epoche: 2 [551/96 (574%)]\tLoss: 26.128736\n",
      "Train Epoche: 2 [552/96 (575%)]\tLoss: 0.148457\n",
      "Train Epoche: 2 [553/96 (576%)]\tLoss: 285.658844\n",
      "Train Epoche: 2 [554/96 (577%)]\tLoss: 0.291255\n",
      "Train Epoche: 2 [555/96 (578%)]\tLoss: 79.248634\n",
      "Train Epoche: 2 [556/96 (579%)]\tLoss: 0.131926\n",
      "Train Epoche: 2 [557/96 (580%)]\tLoss: 3.738137\n",
      "Train Epoche: 2 [558/96 (581%)]\tLoss: 4.041621\n",
      "Train Epoche: 2 [559/96 (582%)]\tLoss: 3.324584\n",
      "Train Epoche: 2 [560/96 (583%)]\tLoss: 28.529169\n",
      "Train Epoche: 2 [561/96 (584%)]\tLoss: 8.679265\n",
      "Train Epoche: 2 [562/96 (585%)]\tLoss: 0.252131\n",
      "Train Epoche: 2 [563/96 (586%)]\tLoss: 14.207439\n",
      "Train Epoche: 2 [564/96 (588%)]\tLoss: 8.809655\n",
      "Train Epoche: 2 [565/96 (589%)]\tLoss: 25.128395\n",
      "Train Epoche: 2 [566/96 (590%)]\tLoss: 25.780041\n",
      "Train Epoche: 2 [567/96 (591%)]\tLoss: 6.354907\n",
      "Train Epoche: 2 [568/96 (592%)]\tLoss: 1.702359\n",
      "Train Epoche: 2 [569/96 (593%)]\tLoss: 25.586437\n",
      "Train Epoche: 2 [570/96 (594%)]\tLoss: 13.204201\n",
      "Train Epoche: 2 [571/96 (595%)]\tLoss: 0.994946\n",
      "Train Epoche: 2 [572/96 (596%)]\tLoss: 1.264487\n",
      "Train Epoche: 2 [573/96 (597%)]\tLoss: 0.094224\n",
      "Train Epoche: 2 [574/96 (598%)]\tLoss: 1.193733\n",
      "Train Epoche: 2 [575/96 (599%)]\tLoss: 2.370594\n",
      "Train Epoche: 2 [576/96 (600%)]\tLoss: 2.716526\n",
      "Train Epoche: 2 [577/96 (601%)]\tLoss: 1.163785\n",
      "Train Epoche: 2 [578/96 (602%)]\tLoss: 14.704720\n",
      "Train Epoche: 2 [579/96 (603%)]\tLoss: 0.352350\n",
      "Train Epoche: 2 [580/96 (604%)]\tLoss: 0.723571\n",
      "Train Epoche: 2 [581/96 (605%)]\tLoss: 0.057583\n",
      "Train Epoche: 2 [582/96 (606%)]\tLoss: 1.497607\n",
      "Train Epoche: 2 [583/96 (607%)]\tLoss: 17.280924\n",
      "Train Epoche: 2 [584/96 (608%)]\tLoss: 2.408680\n",
      "Train Epoche: 2 [585/96 (609%)]\tLoss: 22.934151\n",
      "Train Epoche: 2 [586/96 (610%)]\tLoss: 0.036438\n",
      "Train Epoche: 2 [587/96 (611%)]\tLoss: 21.876480\n",
      "Train Epoche: 2 [588/96 (612%)]\tLoss: 1.802526\n",
      "Train Epoche: 2 [589/96 (614%)]\tLoss: 4.250870\n",
      "Train Epoche: 2 [590/96 (615%)]\tLoss: 0.828057\n",
      "Train Epoche: 2 [591/96 (616%)]\tLoss: 2.761802\n",
      "Train Epoche: 2 [592/96 (617%)]\tLoss: 18.458725\n",
      "Train Epoche: 2 [593/96 (618%)]\tLoss: 0.301775\n",
      "Train Epoche: 2 [594/96 (619%)]\tLoss: 151.353607\n",
      "Train Epoche: 2 [595/96 (620%)]\tLoss: 0.847220\n",
      "Train Epoche: 2 [596/96 (621%)]\tLoss: 2.555657\n",
      "Train Epoche: 2 [597/96 (622%)]\tLoss: 1.858072\n",
      "Train Epoche: 2 [598/96 (623%)]\tLoss: 24.442064\n",
      "Train Epoche: 2 [599/96 (624%)]\tLoss: 0.098518\n",
      "Train Epoche: 2 [600/96 (625%)]\tLoss: 10.432075\n",
      "Train Epoche: 2 [601/96 (626%)]\tLoss: 0.265972\n",
      "Train Epoche: 2 [602/96 (627%)]\tLoss: 0.379218\n",
      "Train Epoche: 2 [603/96 (628%)]\tLoss: 0.517383\n",
      "Train Epoche: 2 [604/96 (629%)]\tLoss: 3.244871\n",
      "Train Epoche: 2 [605/96 (630%)]\tLoss: 27.200584\n",
      "Train Epoche: 2 [606/96 (631%)]\tLoss: 0.753133\n",
      "Train Epoche: 2 [607/96 (632%)]\tLoss: 327.282410\n",
      "Train Epoche: 2 [608/96 (633%)]\tLoss: 21.055260\n",
      "Train Epoche: 2 [609/96 (634%)]\tLoss: 6.349076\n",
      "Train Epoche: 2 [610/96 (635%)]\tLoss: 4.096963\n",
      "Train Epoche: 2 [611/96 (636%)]\tLoss: 7.187310\n",
      "Train Epoche: 2 [612/96 (638%)]\tLoss: 64.782654\n",
      "Train Epoche: 2 [613/96 (639%)]\tLoss: 64.176147\n",
      "Train Epoche: 2 [614/96 (640%)]\tLoss: 2.247886\n",
      "Train Epoche: 2 [615/96 (641%)]\tLoss: 6.975111\n",
      "Train Epoche: 2 [616/96 (642%)]\tLoss: 6.065762\n",
      "Train Epoche: 2 [617/96 (643%)]\tLoss: 218.056442\n",
      "Train Epoche: 2 [618/96 (644%)]\tLoss: 27.103939\n",
      "Train Epoche: 2 [619/96 (645%)]\tLoss: 1.208663\n",
      "Train Epoche: 2 [620/96 (646%)]\tLoss: 1.545149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [621/96 (647%)]\tLoss: 11.162622\n",
      "Train Epoche: 2 [622/96 (648%)]\tLoss: 21.633362\n",
      "Train Epoche: 2 [623/96 (649%)]\tLoss: 0.131975\n",
      "Train Epoche: 2 [624/96 (650%)]\tLoss: 10.284560\n",
      "Train Epoche: 2 [625/96 (651%)]\tLoss: 370.867737\n",
      "Train Epoche: 2 [626/96 (652%)]\tLoss: 1.164918\n",
      "Train Epoche: 2 [627/96 (653%)]\tLoss: 4.025305\n",
      "Train Epoche: 2 [628/96 (654%)]\tLoss: 0.370459\n",
      "Train Epoche: 2 [629/96 (655%)]\tLoss: 133.973770\n",
      "Train Epoche: 2 [630/96 (656%)]\tLoss: 4.342268\n",
      "Train Epoche: 2 [631/96 (657%)]\tLoss: 2.200499\n",
      "Train Epoche: 2 [632/96 (658%)]\tLoss: 3.605796\n",
      "Train Epoche: 2 [633/96 (659%)]\tLoss: 10.495408\n",
      "Train Epoche: 2 [634/96 (660%)]\tLoss: 15.171980\n",
      "Train Epoche: 2 [635/96 (661%)]\tLoss: 3.052961\n",
      "Train Epoche: 2 [636/96 (662%)]\tLoss: 5.140337\n",
      "Train Epoche: 2 [637/96 (664%)]\tLoss: 5.614287\n",
      "Train Epoche: 2 [638/96 (665%)]\tLoss: 1.232301\n",
      "Train Epoche: 2 [639/96 (666%)]\tLoss: 12.350917\n",
      "Train Epoche: 2 [640/96 (667%)]\tLoss: 7.500186\n",
      "Train Epoche: 2 [641/96 (668%)]\tLoss: 25.687202\n",
      "Train Epoche: 2 [642/96 (669%)]\tLoss: 0.000866\n",
      "Train Epoche: 2 [643/96 (670%)]\tLoss: 40.077255\n",
      "Train Epoche: 2 [644/96 (671%)]\tLoss: 3.552724\n",
      "Train Epoche: 2 [645/96 (672%)]\tLoss: 0.036372\n",
      "Train Epoche: 2 [646/96 (673%)]\tLoss: 25.134821\n",
      "Train Epoche: 2 [647/96 (674%)]\tLoss: 0.073316\n",
      "Train Epoche: 2 [648/96 (675%)]\tLoss: 26.291077\n",
      "Train Epoche: 2 [649/96 (676%)]\tLoss: 21.017084\n",
      "Train Epoche: 2 [650/96 (677%)]\tLoss: 26.078569\n",
      "Train Epoche: 2 [651/96 (678%)]\tLoss: 12.204328\n",
      "Train Epoche: 2 [652/96 (679%)]\tLoss: 0.134803\n",
      "Train Epoche: 2 [653/96 (680%)]\tLoss: 12.687303\n",
      "Train Epoche: 2 [654/96 (681%)]\tLoss: 18.204344\n",
      "Train Epoche: 2 [655/96 (682%)]\tLoss: 21.818218\n",
      "Train Epoche: 2 [656/96 (683%)]\tLoss: 5.557198\n",
      "Train Epoche: 2 [657/96 (684%)]\tLoss: 217.599915\n",
      "Train Epoche: 2 [658/96 (685%)]\tLoss: 17.599754\n",
      "Train Epoche: 2 [659/96 (686%)]\tLoss: 0.503950\n",
      "Train Epoche: 2 [660/96 (688%)]\tLoss: 48.834568\n",
      "Train Epoche: 2 [661/96 (689%)]\tLoss: 1.683540\n",
      "Train Epoche: 2 [662/96 (690%)]\tLoss: 9.285171\n",
      "Train Epoche: 2 [663/96 (691%)]\tLoss: 0.256703\n",
      "Train Epoche: 2 [664/96 (692%)]\tLoss: 2.175019\n",
      "Train Epoche: 2 [665/96 (693%)]\tLoss: 5.321115\n",
      "Train Epoche: 2 [666/96 (694%)]\tLoss: 3.920081\n",
      "Train Epoche: 2 [667/96 (695%)]\tLoss: 5.390880\n",
      "Train Epoche: 2 [668/96 (696%)]\tLoss: 17.094732\n",
      "Train Epoche: 2 [669/96 (697%)]\tLoss: 83.351280\n",
      "Train Epoche: 2 [670/96 (698%)]\tLoss: 7.769000\n",
      "Train Epoche: 2 [671/96 (699%)]\tLoss: 0.605216\n",
      "Train Epoche: 2 [672/96 (700%)]\tLoss: 2.247424\n",
      "Train Epoche: 2 [673/96 (701%)]\tLoss: 26.992531\n",
      "Train Epoche: 2 [674/96 (702%)]\tLoss: 0.608779\n",
      "Train Epoche: 2 [675/96 (703%)]\tLoss: 5.538291\n",
      "Train Epoche: 2 [676/96 (704%)]\tLoss: 31.415796\n",
      "Train Epoche: 2 [677/96 (705%)]\tLoss: 1.222095\n",
      "Train Epoche: 2 [678/96 (706%)]\tLoss: 4.054258\n",
      "Train Epoche: 2 [679/96 (707%)]\tLoss: 0.040310\n",
      "Train Epoche: 2 [680/96 (708%)]\tLoss: 16.154768\n",
      "Train Epoche: 2 [681/96 (709%)]\tLoss: 17.135632\n",
      "Train Epoche: 2 [682/96 (710%)]\tLoss: 23.969629\n",
      "Train Epoche: 2 [683/96 (711%)]\tLoss: 2.159848\n",
      "Train Epoche: 2 [684/96 (712%)]\tLoss: 14.881171\n",
      "Train Epoche: 2 [685/96 (714%)]\tLoss: 28.186119\n",
      "Train Epoche: 2 [686/96 (715%)]\tLoss: 0.847071\n",
      "Train Epoche: 2 [687/96 (716%)]\tLoss: 1.823261\n",
      "Train Epoche: 2 [688/96 (717%)]\tLoss: 0.117115\n",
      "Train Epoche: 2 [689/96 (718%)]\tLoss: 2.582475\n",
      "Train Epoche: 2 [690/96 (719%)]\tLoss: 0.036372\n",
      "Train Epoche: 2 [691/96 (720%)]\tLoss: 23.305285\n",
      "Train Epoche: 2 [692/96 (721%)]\tLoss: 2.084580\n",
      "Train Epoche: 2 [693/96 (722%)]\tLoss: 9.943062\n",
      "Train Epoche: 2 [694/96 (723%)]\tLoss: 0.135778\n",
      "Train Epoche: 2 [695/96 (724%)]\tLoss: 0.008193\n",
      "Train Epoche: 2 [696/96 (725%)]\tLoss: 8.315001\n",
      "Train Epoche: 2 [697/96 (726%)]\tLoss: 12.139832\n",
      "Train Epoche: 2 [698/96 (727%)]\tLoss: 0.861383\n",
      "Train Epoche: 2 [699/96 (728%)]\tLoss: 1.272903\n",
      "Train Epoche: 2 [700/96 (729%)]\tLoss: 0.032501\n",
      "Train Epoche: 2 [701/96 (730%)]\tLoss: 1.933620\n",
      "Train Epoche: 2 [702/96 (731%)]\tLoss: 0.224869\n",
      "Train Epoche: 2 [703/96 (732%)]\tLoss: 20.610006\n",
      "Train Epoche: 2 [704/96 (733%)]\tLoss: 0.015699\n",
      "Train Epoche: 2 [705/96 (734%)]\tLoss: 7.293326\n",
      "Train Epoche: 2 [706/96 (735%)]\tLoss: 6.437814\n",
      "Train Epoche: 2 [707/96 (736%)]\tLoss: 0.178187\n",
      "Train Epoche: 2 [708/96 (738%)]\tLoss: 0.842166\n",
      "Train Epoche: 2 [709/96 (739%)]\tLoss: 2.102248\n",
      "Train Epoche: 2 [710/96 (740%)]\tLoss: 1.923744\n",
      "Train Epoche: 2 [711/96 (741%)]\tLoss: 0.662537\n",
      "Train Epoche: 2 [712/96 (742%)]\tLoss: 0.012767\n",
      "Train Epoche: 2 [713/96 (743%)]\tLoss: 3.080745\n",
      "Train Epoche: 2 [714/96 (744%)]\tLoss: 2.122454\n",
      "Train Epoche: 2 [715/96 (745%)]\tLoss: 3.388676\n",
      "Train Epoche: 2 [716/96 (746%)]\tLoss: 0.005459\n",
      "Train Epoche: 2 [717/96 (747%)]\tLoss: 6.184607\n",
      "Train Epoche: 2 [718/96 (748%)]\tLoss: 1.883928\n",
      "Train Epoche: 2 [719/96 (749%)]\tLoss: 3.963588\n",
      "Train Epoche: 2 [720/96 (750%)]\tLoss: 0.662921\n",
      "Train Epoche: 2 [721/96 (751%)]\tLoss: 0.028029\n",
      "Train Epoche: 2 [722/96 (752%)]\tLoss: 3.297684\n",
      "Train Epoche: 2 [723/96 (753%)]\tLoss: 2.543264\n",
      "Train Epoche: 2 [724/96 (754%)]\tLoss: 0.120107\n",
      "Train Epoche: 2 [725/96 (755%)]\tLoss: 3.593514\n",
      "Train Epoche: 2 [726/96 (756%)]\tLoss: 0.006492\n",
      "Train Epoche: 2 [727/96 (757%)]\tLoss: 0.677940\n",
      "Train Epoche: 2 [728/96 (758%)]\tLoss: 5.112534\n",
      "Train Epoche: 2 [729/96 (759%)]\tLoss: 0.721645\n",
      "Train Epoche: 2 [730/96 (760%)]\tLoss: 49.856022\n",
      "Train Epoche: 2 [731/96 (761%)]\tLoss: 192.939819\n",
      "Train Epoche: 2 [732/96 (762%)]\tLoss: 36.881504\n",
      "Train Epoche: 2 [733/96 (764%)]\tLoss: 5.115656\n",
      "Train Epoche: 2 [734/96 (765%)]\tLoss: 1.919993\n",
      "Train Epoche: 2 [735/96 (766%)]\tLoss: 2.059919\n",
      "Train Epoche: 2 [736/96 (767%)]\tLoss: 10.411173\n",
      "Train Epoche: 2 [737/96 (768%)]\tLoss: 3.699421\n",
      "Train Epoche: 2 [738/96 (769%)]\tLoss: 0.532936\n",
      "Train Epoche: 2 [739/96 (770%)]\tLoss: 0.086307\n",
      "Train Epoche: 2 [740/96 (771%)]\tLoss: 7.730506\n",
      "Train Epoche: 2 [741/96 (772%)]\tLoss: 19.746401\n",
      "Train Epoche: 2 [742/96 (773%)]\tLoss: 1.977274\n",
      "Train Epoche: 2 [743/96 (774%)]\tLoss: 0.823063\n",
      "Train Epoche: 2 [744/96 (775%)]\tLoss: 3.233186\n",
      "Train Epoche: 2 [745/96 (776%)]\tLoss: 1.291337\n",
      "Train Epoche: 2 [746/96 (777%)]\tLoss: 4.381361\n",
      "Train Epoche: 2 [747/96 (778%)]\tLoss: 73.233253\n",
      "Train Epoche: 2 [748/96 (779%)]\tLoss: 20.173769\n",
      "Train Epoche: 2 [749/96 (780%)]\tLoss: 209.520004\n",
      "Train Epoche: 2 [750/96 (781%)]\tLoss: 10.626271\n",
      "Train Epoche: 2 [751/96 (782%)]\tLoss: 14.015104\n",
      "Train Epoche: 2 [752/96 (783%)]\tLoss: 0.354895\n",
      "Train Epoche: 2 [753/96 (784%)]\tLoss: 0.508865\n",
      "Train Epoche: 2 [754/96 (785%)]\tLoss: 34.382854\n",
      "Train Epoche: 2 [755/96 (786%)]\tLoss: 1.779695\n",
      "Train Epoche: 2 [756/96 (788%)]\tLoss: 6.855722\n",
      "Train Epoche: 2 [757/96 (789%)]\tLoss: 20.220104\n",
      "Train Epoche: 2 [758/96 (790%)]\tLoss: 0.011412\n",
      "Train Epoche: 2 [759/96 (791%)]\tLoss: 2.836465\n",
      "Train Epoche: 2 [760/96 (792%)]\tLoss: 2.231963\n",
      "Train Epoche: 2 [761/96 (793%)]\tLoss: 12.968216\n",
      "Train Epoche: 2 [762/96 (794%)]\tLoss: 0.941264\n",
      "Train Epoche: 2 [763/96 (795%)]\tLoss: 0.087844\n",
      "Train Epoche: 2 [764/96 (796%)]\tLoss: 45.827824\n",
      "Train Epoche: 2 [765/96 (797%)]\tLoss: 1.343886\n",
      "Train Epoche: 2 [766/96 (798%)]\tLoss: 79.891968\n",
      "Train Epoche: 2 [767/96 (799%)]\tLoss: 33.273022\n",
      "Train Epoche: 2 [768/96 (800%)]\tLoss: 117.769409\n",
      "Train Epoche: 2 [769/96 (801%)]\tLoss: 83.272484\n",
      "Train Epoche: 2 [770/96 (802%)]\tLoss: 3.217829\n",
      "Train Epoche: 2 [771/96 (803%)]\tLoss: 22.046265\n",
      "Train Epoche: 2 [772/96 (804%)]\tLoss: 20.584400\n",
      "Train Epoche: 2 [773/96 (805%)]\tLoss: 2.007997\n",
      "Train Epoche: 2 [774/96 (806%)]\tLoss: 42.464245\n",
      "Train Epoche: 2 [775/96 (807%)]\tLoss: 12.762243\n",
      "Train Epoche: 2 [776/96 (808%)]\tLoss: 15.996971\n",
      "Train Epoche: 2 [777/96 (809%)]\tLoss: 11.619294\n",
      "Train Epoche: 2 [778/96 (810%)]\tLoss: 2.687928\n",
      "Train Epoche: 2 [779/96 (811%)]\tLoss: 9.971669\n",
      "Train Epoche: 2 [780/96 (812%)]\tLoss: 0.380288\n",
      "Train Epoche: 2 [781/96 (814%)]\tLoss: 3.526543\n",
      "Train Epoche: 2 [782/96 (815%)]\tLoss: 54.082439\n",
      "Train Epoche: 2 [783/96 (816%)]\tLoss: 0.238804\n",
      "Train Epoche: 2 [784/96 (817%)]\tLoss: 0.001080\n",
      "Train Epoche: 2 [785/96 (818%)]\tLoss: 3.086818\n",
      "Train Epoche: 2 [786/96 (819%)]\tLoss: 0.310057\n",
      "Train Epoche: 2 [787/96 (820%)]\tLoss: 3.742883\n",
      "Train Epoche: 2 [788/96 (821%)]\tLoss: 0.136150\n",
      "Train Epoche: 2 [789/96 (822%)]\tLoss: 0.453519\n",
      "Train Epoche: 2 [790/96 (823%)]\tLoss: 0.530538\n",
      "Train Epoche: 2 [791/96 (824%)]\tLoss: 16.772011\n",
      "Train Epoche: 2 [792/96 (825%)]\tLoss: 22.610506\n",
      "Train Epoche: 2 [793/96 (826%)]\tLoss: 8.083269\n",
      "Train Epoche: 2 [794/96 (827%)]\tLoss: 35.637432\n",
      "Train Epoche: 2 [795/96 (828%)]\tLoss: 113.637772\n",
      "Train Epoche: 2 [796/96 (829%)]\tLoss: 4.378032\n",
      "Train Epoche: 2 [797/96 (830%)]\tLoss: 0.772371\n",
      "Train Epoche: 2 [798/96 (831%)]\tLoss: 0.222345\n",
      "Train Epoche: 2 [799/96 (832%)]\tLoss: 31.497086\n",
      "Train Epoche: 2 [800/96 (833%)]\tLoss: 37.126373\n",
      "Train Epoche: 2 [801/96 (834%)]\tLoss: 47.168194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [802/96 (835%)]\tLoss: 1.262789\n",
      "Train Epoche: 2 [803/96 (836%)]\tLoss: 2.022883\n",
      "Train Epoche: 2 [804/96 (838%)]\tLoss: 0.243992\n",
      "Train Epoche: 2 [805/96 (839%)]\tLoss: 78.634239\n",
      "Train Epoche: 2 [806/96 (840%)]\tLoss: 16.138895\n",
      "Train Epoche: 2 [807/96 (841%)]\tLoss: 14.948277\n",
      "Train Epoche: 2 [808/96 (842%)]\tLoss: 15.941681\n",
      "Train Epoche: 2 [809/96 (843%)]\tLoss: 390.549500\n",
      "Train Epoche: 2 [810/96 (844%)]\tLoss: 138.087463\n",
      "Train Epoche: 2 [811/96 (845%)]\tLoss: 1.647291\n",
      "Train Epoche: 2 [812/96 (846%)]\tLoss: 2.630735\n",
      "Train Epoche: 2 [813/96 (847%)]\tLoss: 6.495275\n",
      "Train Epoche: 2 [814/96 (848%)]\tLoss: 109.201576\n",
      "Train Epoche: 2 [815/96 (849%)]\tLoss: 0.006546\n",
      "Train Epoche: 2 [816/96 (850%)]\tLoss: 0.859973\n",
      "Train Epoche: 2 [817/96 (851%)]\tLoss: 10.823127\n",
      "Train Epoche: 2 [818/96 (852%)]\tLoss: 0.290903\n",
      "Train Epoche: 2 [819/96 (853%)]\tLoss: 0.234806\n",
      "Train Epoche: 2 [820/96 (854%)]\tLoss: 69.357605\n",
      "Train Epoche: 2 [821/96 (855%)]\tLoss: 31.866806\n",
      "Train Epoche: 2 [822/96 (856%)]\tLoss: 0.000003\n",
      "Train Epoche: 2 [823/96 (857%)]\tLoss: 3.160740\n",
      "Train Epoche: 2 [824/96 (858%)]\tLoss: 12.835261\n",
      "Train Epoche: 2 [825/96 (859%)]\tLoss: 0.891949\n",
      "Train Epoche: 2 [826/96 (860%)]\tLoss: 15.998306\n",
      "Train Epoche: 2 [827/96 (861%)]\tLoss: 0.044780\n",
      "Train Epoche: 2 [828/96 (862%)]\tLoss: 110.068069\n",
      "Train Epoche: 2 [829/96 (864%)]\tLoss: 218.065475\n",
      "Train Epoche: 2 [830/96 (865%)]\tLoss: 8.575644\n",
      "Train Epoche: 2 [831/96 (866%)]\tLoss: 2.811827\n",
      "Train Epoche: 2 [832/96 (867%)]\tLoss: 0.004252\n",
      "Train Epoche: 2 [833/96 (868%)]\tLoss: 36.754009\n",
      "Train Epoche: 2 [834/96 (869%)]\tLoss: 9.302053\n",
      "Train Epoche: 2 [835/96 (870%)]\tLoss: 5.876543\n",
      "Train Epoche: 2 [836/96 (871%)]\tLoss: 8.549674\n",
      "Train Epoche: 2 [837/96 (872%)]\tLoss: 68.463547\n",
      "Train Epoche: 2 [838/96 (873%)]\tLoss: 1.825938\n",
      "Train Epoche: 2 [839/96 (874%)]\tLoss: 15.214431\n",
      "Train Epoche: 2 [840/96 (875%)]\tLoss: 0.854816\n",
      "Train Epoche: 2 [841/96 (876%)]\tLoss: 0.191651\n",
      "Train Epoche: 2 [842/96 (877%)]\tLoss: 3.881465\n",
      "Train Epoche: 2 [843/96 (878%)]\tLoss: 4.428293\n",
      "Train Epoche: 2 [844/96 (879%)]\tLoss: 0.338854\n",
      "Train Epoche: 2 [845/96 (880%)]\tLoss: 2.456081\n",
      "Train Epoche: 2 [846/96 (881%)]\tLoss: 1.442297\n",
      "Train Epoche: 2 [847/96 (882%)]\tLoss: 1.873791\n",
      "Train Epoche: 2 [848/96 (883%)]\tLoss: 40.939342\n",
      "Train Epoche: 2 [849/96 (884%)]\tLoss: 0.382911\n",
      "Train Epoche: 2 [850/96 (885%)]\tLoss: 7.160749\n",
      "Train Epoche: 2 [851/96 (886%)]\tLoss: 105.013962\n",
      "Train Epoche: 2 [852/96 (888%)]\tLoss: 17.627415\n",
      "Train Epoche: 2 [853/96 (889%)]\tLoss: 19.688517\n",
      "Train Epoche: 2 [854/96 (890%)]\tLoss: 61.453163\n",
      "Train Epoche: 2 [855/96 (891%)]\tLoss: 0.055637\n",
      "Train Epoche: 2 [856/96 (892%)]\tLoss: 22.048538\n",
      "Train Epoche: 2 [857/96 (893%)]\tLoss: 9.836720\n",
      "Train Epoche: 2 [858/96 (894%)]\tLoss: 79.607986\n",
      "Train Epoche: 2 [859/96 (895%)]\tLoss: 15.403440\n",
      "Train Epoche: 2 [860/96 (896%)]\tLoss: 1.329356\n",
      "Train Epoche: 2 [861/96 (897%)]\tLoss: 31.216923\n",
      "Train Epoche: 2 [862/96 (898%)]\tLoss: 5.392816\n",
      "Train Epoche: 2 [863/96 (899%)]\tLoss: 19.011959\n",
      "Train Epoche: 2 [864/96 (900%)]\tLoss: 23.270555\n",
      "Train Epoche: 2 [865/96 (901%)]\tLoss: 42.950039\n",
      "Train Epoche: 2 [866/96 (902%)]\tLoss: 32.566216\n",
      "Train Epoche: 2 [867/96 (903%)]\tLoss: 1.064757\n",
      "Train Epoche: 2 [868/96 (904%)]\tLoss: 8.173683\n",
      "Train Epoche: 2 [869/96 (905%)]\tLoss: 0.787074\n",
      "Train Epoche: 2 [870/96 (906%)]\tLoss: 0.399880\n",
      "Train Epoche: 2 [871/96 (907%)]\tLoss: 0.604501\n",
      "Train Epoche: 2 [872/96 (908%)]\tLoss: 1.992177\n",
      "Train Epoche: 2 [873/96 (909%)]\tLoss: 6.796063\n",
      "Train Epoche: 2 [874/96 (910%)]\tLoss: 0.233040\n",
      "Train Epoche: 2 [875/96 (911%)]\tLoss: 0.571854\n",
      "Train Epoche: 2 [876/96 (912%)]\tLoss: 9.958796\n",
      "Train Epoche: 2 [877/96 (914%)]\tLoss: 4.170074\n",
      "Train Epoche: 2 [878/96 (915%)]\tLoss: 2.546229\n",
      "Train Epoche: 2 [879/96 (916%)]\tLoss: 8.894226\n",
      "Train Epoche: 2 [880/96 (917%)]\tLoss: 17.068026\n",
      "Train Epoche: 2 [881/96 (918%)]\tLoss: 2.216882\n",
      "Train Epoche: 2 [882/96 (919%)]\tLoss: 0.262055\n",
      "Train Epoche: 2 [883/96 (920%)]\tLoss: 49.109810\n",
      "Train Epoche: 2 [884/96 (921%)]\tLoss: 3.089288\n",
      "Train Epoche: 2 [885/96 (922%)]\tLoss: 29.715857\n",
      "Train Epoche: 2 [886/96 (923%)]\tLoss: 2.480042\n",
      "Train Epoche: 2 [887/96 (924%)]\tLoss: 7.949056\n",
      "Train Epoche: 2 [888/96 (925%)]\tLoss: 3.479748\n",
      "Train Epoche: 2 [889/96 (926%)]\tLoss: 0.896321\n",
      "Train Epoche: 2 [890/96 (927%)]\tLoss: 0.128509\n",
      "Train Epoche: 2 [891/96 (928%)]\tLoss: 12.528970\n",
      "Train Epoche: 2 [892/96 (929%)]\tLoss: 3.216420\n",
      "Train Epoche: 2 [893/96 (930%)]\tLoss: 0.570975\n",
      "Train Epoche: 2 [894/96 (931%)]\tLoss: 5.634095\n",
      "Train Epoche: 2 [895/96 (932%)]\tLoss: 0.424414\n",
      "Train Epoche: 2 [896/96 (933%)]\tLoss: 0.116223\n",
      "Train Epoche: 2 [897/96 (934%)]\tLoss: 0.449515\n",
      "Train Epoche: 2 [898/96 (935%)]\tLoss: 3.503376\n",
      "Train Epoche: 2 [899/96 (936%)]\tLoss: 0.383320\n",
      "Train Epoche: 2 [900/96 (938%)]\tLoss: 99.398468\n",
      "Train Epoche: 2 [901/96 (939%)]\tLoss: 9.341530\n",
      "Train Epoche: 2 [902/96 (940%)]\tLoss: 0.563095\n",
      "Train Epoche: 2 [903/96 (941%)]\tLoss: 0.614666\n",
      "Train Epoche: 2 [904/96 (942%)]\tLoss: 19.475218\n",
      "Train Epoche: 2 [905/96 (943%)]\tLoss: 100.507233\n",
      "Train Epoche: 2 [906/96 (944%)]\tLoss: 11.158844\n",
      "Train Epoche: 2 [907/96 (945%)]\tLoss: 24.730364\n",
      "Train Epoche: 2 [908/96 (946%)]\tLoss: 2.130504\n",
      "Train Epoche: 2 [909/96 (947%)]\tLoss: 2.009440\n",
      "Train Epoche: 2 [910/96 (948%)]\tLoss: 3.201893\n",
      "Train Epoche: 2 [911/96 (949%)]\tLoss: 15.983430\n",
      "Train Epoche: 2 [912/96 (950%)]\tLoss: 4.662872\n",
      "Train Epoche: 2 [913/96 (951%)]\tLoss: 14.907896\n",
      "Train Epoche: 2 [914/96 (952%)]\tLoss: 61.587982\n",
      "Train Epoche: 2 [915/96 (953%)]\tLoss: 0.348376\n",
      "Train Epoche: 2 [916/96 (954%)]\tLoss: 1.910457\n",
      "Train Epoche: 2 [917/96 (955%)]\tLoss: 6.381678\n",
      "Train Epoche: 2 [918/96 (956%)]\tLoss: 23.551888\n",
      "Train Epoche: 2 [919/96 (957%)]\tLoss: 11.670375\n",
      "Train Epoche: 2 [920/96 (958%)]\tLoss: 5.680545\n",
      "Train Epoche: 2 [921/96 (959%)]\tLoss: 2.428757\n",
      "Train Epoche: 2 [922/96 (960%)]\tLoss: 61.339878\n",
      "Train Epoche: 2 [923/96 (961%)]\tLoss: 41.693153\n",
      "Train Epoche: 2 [924/96 (962%)]\tLoss: 5.138884\n",
      "Train Epoche: 2 [925/96 (964%)]\tLoss: 16.374788\n",
      "Train Epoche: 2 [926/96 (965%)]\tLoss: 15.105604\n",
      "Train Epoche: 2 [927/96 (966%)]\tLoss: 31.225758\n",
      "Train Epoche: 2 [928/96 (967%)]\tLoss: 4.861537\n",
      "Train Epoche: 2 [929/96 (968%)]\tLoss: 7.005116\n",
      "Train Epoche: 2 [930/96 (969%)]\tLoss: 9.776667\n",
      "Train Epoche: 2 [931/96 (970%)]\tLoss: 2.230515\n",
      "Train Epoche: 2 [932/96 (971%)]\tLoss: 61.212742\n",
      "Train Epoche: 2 [933/96 (972%)]\tLoss: 293.446747\n",
      "Train Epoche: 2 [934/96 (973%)]\tLoss: 0.306036\n",
      "Train Epoche: 2 [935/96 (974%)]\tLoss: 11.160494\n",
      "Train Epoche: 2 [936/96 (975%)]\tLoss: 0.452065\n",
      "Train Epoche: 2 [937/96 (976%)]\tLoss: 14.612587\n",
      "Train Epoche: 2 [938/96 (977%)]\tLoss: 0.805644\n",
      "Train Epoche: 2 [939/96 (978%)]\tLoss: 2.130595\n",
      "Train Epoche: 2 [940/96 (979%)]\tLoss: 0.053249\n",
      "Train Epoche: 2 [941/96 (980%)]\tLoss: 47.804787\n",
      "Train Epoche: 2 [942/96 (981%)]\tLoss: 15.038938\n",
      "Train Epoche: 2 [943/96 (982%)]\tLoss: 12.744571\n",
      "Train Epoche: 2 [944/96 (983%)]\tLoss: 30.330431\n",
      "Train Epoche: 2 [945/96 (984%)]\tLoss: 23.426168\n",
      "Train Epoche: 2 [946/96 (985%)]\tLoss: 0.424718\n",
      "Train Epoche: 2 [947/96 (986%)]\tLoss: 19.737028\n",
      "Train Epoche: 2 [948/96 (988%)]\tLoss: 0.818821\n",
      "Train Epoche: 2 [949/96 (989%)]\tLoss: 21.207813\n",
      "Train Epoche: 2 [950/96 (990%)]\tLoss: 54.029121\n",
      "Train Epoche: 2 [951/96 (991%)]\tLoss: 3.809500\n",
      "Train Epoche: 2 [952/96 (992%)]\tLoss: 14.000031\n",
      "Train Epoche: 2 [953/96 (993%)]\tLoss: 42.985210\n",
      "Train Epoche: 2 [954/96 (994%)]\tLoss: 194.663879\n",
      "Train Epoche: 2 [955/96 (995%)]\tLoss: 15.371044\n",
      "Train Epoche: 2 [956/96 (996%)]\tLoss: 23.350849\n",
      "Train Epoche: 2 [957/96 (997%)]\tLoss: 3.085553\n",
      "Train Epoche: 2 [958/96 (998%)]\tLoss: 54.691406\n",
      "Train Epoche: 2 [959/96 (999%)]\tLoss: 120.895729\n",
      "Train Epoche: 2 [960/96 (1000%)]\tLoss: 9.799868\n",
      "Train Epoche: 2 [961/96 (1001%)]\tLoss: 37.891651\n",
      "Train Epoche: 2 [962/96 (1002%)]\tLoss: 6.730409\n",
      "Train Epoche: 2 [963/96 (1003%)]\tLoss: 43.211876\n",
      "Train Epoche: 2 [964/96 (1004%)]\tLoss: 75.309280\n",
      "Train Epoche: 2 [965/96 (1005%)]\tLoss: 37.057743\n",
      "Train Epoche: 2 [966/96 (1006%)]\tLoss: 30.555134\n",
      "Train Epoche: 2 [967/96 (1007%)]\tLoss: 29.987577\n",
      "Train Epoche: 2 [968/96 (1008%)]\tLoss: 5.872511\n",
      "Train Epoche: 2 [969/96 (1009%)]\tLoss: 87.955315\n",
      "Train Epoche: 2 [970/96 (1010%)]\tLoss: 15.429030\n",
      "Train Epoche: 2 [971/96 (1011%)]\tLoss: 1.646564\n",
      "Train Epoche: 2 [972/96 (1012%)]\tLoss: 0.249390\n",
      "Train Epoche: 2 [973/96 (1014%)]\tLoss: 4.985036\n",
      "Train Epoche: 2 [974/96 (1015%)]\tLoss: 0.766956\n",
      "Train Epoche: 2 [975/96 (1016%)]\tLoss: 0.645088\n",
      "Train Epoche: 2 [976/96 (1017%)]\tLoss: 6.870667\n",
      "Train Epoche: 2 [977/96 (1018%)]\tLoss: 46.176769\n",
      "Train Epoche: 2 [978/96 (1019%)]\tLoss: 0.947996\n",
      "Train Epoche: 2 [979/96 (1020%)]\tLoss: 2.257066\n",
      "Train Epoche: 2 [980/96 (1021%)]\tLoss: 0.104639\n",
      "Train Epoche: 2 [981/96 (1022%)]\tLoss: 203.216904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [982/96 (1023%)]\tLoss: 0.441808\n",
      "Train Epoche: 2 [983/96 (1024%)]\tLoss: 0.338379\n",
      "Train Epoche: 2 [984/96 (1025%)]\tLoss: 0.178386\n",
      "Train Epoche: 2 [985/96 (1026%)]\tLoss: 12.907713\n",
      "Train Epoche: 2 [986/96 (1027%)]\tLoss: 0.159644\n",
      "Train Epoche: 2 [987/96 (1028%)]\tLoss: 7.085054\n",
      "Train Epoche: 2 [988/96 (1029%)]\tLoss: 16.295666\n",
      "Train Epoche: 2 [989/96 (1030%)]\tLoss: 6.510310\n",
      "Train Epoche: 2 [990/96 (1031%)]\tLoss: 0.180403\n",
      "Train Epoche: 2 [991/96 (1032%)]\tLoss: 0.141422\n",
      "Train Epoche: 2 [992/96 (1033%)]\tLoss: 0.117505\n",
      "Train Epoche: 2 [993/96 (1034%)]\tLoss: 0.713624\n",
      "Train Epoche: 2 [994/96 (1035%)]\tLoss: 127.513161\n",
      "Train Epoche: 2 [995/96 (1036%)]\tLoss: 12.465895\n",
      "Train Epoche: 2 [996/96 (1038%)]\tLoss: 114.084610\n",
      "Train Epoche: 2 [997/96 (1039%)]\tLoss: 0.207951\n",
      "Train Epoche: 2 [998/96 (1040%)]\tLoss: 4.786489\n",
      "Train Epoche: 2 [999/96 (1041%)]\tLoss: 1.666918\n",
      "Train Epoche: 2 [1000/96 (1042%)]\tLoss: 9.902390\n",
      "Train Epoche: 2 [1001/96 (1043%)]\tLoss: 63.854210\n",
      "Train Epoche: 2 [1002/96 (1044%)]\tLoss: 0.961235\n",
      "Train Epoche: 2 [1003/96 (1045%)]\tLoss: 1.782475\n",
      "Train Epoche: 2 [1004/96 (1046%)]\tLoss: 0.038629\n",
      "Train Epoche: 2 [1005/96 (1047%)]\tLoss: 26.503464\n",
      "Train Epoche: 2 [1006/96 (1048%)]\tLoss: 4.017838\n",
      "Train Epoche: 2 [1007/96 (1049%)]\tLoss: 8.933005\n",
      "Train Epoche: 2 [1008/96 (1050%)]\tLoss: 0.051088\n",
      "Train Epoche: 2 [1009/96 (1051%)]\tLoss: 7.415464\n",
      "Train Epoche: 2 [1010/96 (1052%)]\tLoss: 1.193566\n",
      "Train Epoche: 2 [1011/96 (1053%)]\tLoss: 32.746677\n",
      "Train Epoche: 2 [1012/96 (1054%)]\tLoss: 306.713348\n",
      "Train Epoche: 2 [1013/96 (1055%)]\tLoss: 160.704834\n",
      "Train Epoche: 2 [1014/96 (1056%)]\tLoss: 47.238800\n",
      "Train Epoche: 2 [1015/96 (1057%)]\tLoss: 77.914001\n",
      "Train Epoche: 2 [1016/96 (1058%)]\tLoss: 130.367050\n",
      "Train Epoche: 2 [1017/96 (1059%)]\tLoss: 83.151596\n",
      "Train Epoche: 2 [1018/96 (1060%)]\tLoss: 66.548698\n",
      "Train Epoche: 2 [1019/96 (1061%)]\tLoss: 0.847123\n",
      "Train Epoche: 2 [1020/96 (1062%)]\tLoss: 36.449776\n",
      "Train Epoche: 2 [1021/96 (1064%)]\tLoss: 76.341629\n",
      "Train Epoche: 2 [1022/96 (1065%)]\tLoss: 91.639847\n",
      "Train Epoche: 2 [1023/96 (1066%)]\tLoss: 10.012399\n",
      "Train Epoche: 2 [1024/96 (1067%)]\tLoss: 6.370413\n",
      "Train Epoche: 2 [1025/96 (1068%)]\tLoss: 11.663651\n",
      "Train Epoche: 2 [1026/96 (1069%)]\tLoss: 48.518707\n",
      "Train Epoche: 2 [1027/96 (1070%)]\tLoss: 29.944366\n",
      "Train Epoche: 2 [1028/96 (1071%)]\tLoss: 26.080099\n",
      "Train Epoche: 2 [1029/96 (1072%)]\tLoss: 8.903716\n",
      "Train Epoche: 2 [1030/96 (1073%)]\tLoss: 5.120198\n",
      "Train Epoche: 2 [1031/96 (1074%)]\tLoss: 19.247761\n",
      "Train Epoche: 2 [1032/96 (1075%)]\tLoss: 2.329931\n",
      "Train Epoche: 2 [1033/96 (1076%)]\tLoss: 373.462585\n",
      "Train Epoche: 2 [1034/96 (1077%)]\tLoss: 217.742706\n",
      "Train Epoche: 2 [1035/96 (1078%)]\tLoss: 48.222118\n",
      "Train Epoche: 2 [1036/96 (1079%)]\tLoss: 0.235716\n",
      "Train Epoche: 2 [1037/96 (1080%)]\tLoss: 0.101763\n",
      "Train Epoche: 2 [1038/96 (1081%)]\tLoss: 10.083348\n",
      "Train Epoche: 2 [1039/96 (1082%)]\tLoss: 62.167858\n",
      "Train Epoche: 2 [1040/96 (1083%)]\tLoss: 120.560600\n",
      "Train Epoche: 2 [1041/96 (1084%)]\tLoss: 1.445214\n",
      "Train Epoche: 2 [1042/96 (1085%)]\tLoss: 3.012463\n",
      "Train Epoche: 2 [1043/96 (1086%)]\tLoss: 19.046480\n",
      "Train Epoche: 2 [1044/96 (1088%)]\tLoss: 122.168900\n",
      "Train Epoche: 2 [1045/96 (1089%)]\tLoss: 3.536534\n",
      "Train Epoche: 2 [1046/96 (1090%)]\tLoss: 2.933416\n",
      "Train Epoche: 2 [1047/96 (1091%)]\tLoss: 36.989574\n",
      "Train Epoche: 2 [1048/96 (1092%)]\tLoss: 2.154449\n",
      "Train Epoche: 2 [1049/96 (1093%)]\tLoss: 197.918625\n",
      "Train Epoche: 2 [1050/96 (1094%)]\tLoss: 0.654750\n",
      "Train Epoche: 2 [1051/96 (1095%)]\tLoss: 16.927368\n",
      "Train Epoche: 2 [1052/96 (1096%)]\tLoss: 4.519018\n",
      "Train Epoche: 2 [1053/96 (1097%)]\tLoss: 15.540916\n",
      "Train Epoche: 2 [1054/96 (1098%)]\tLoss: 61.281483\n",
      "Train Epoche: 2 [1055/96 (1099%)]\tLoss: 6.709835\n",
      "Train Epoche: 2 [1056/96 (1100%)]\tLoss: 6.430445\n",
      "Train Epoche: 2 [1057/96 (1101%)]\tLoss: 0.343343\n",
      "Train Epoche: 2 [1058/96 (1102%)]\tLoss: 2.053138\n",
      "Train Epoche: 2 [1059/96 (1103%)]\tLoss: 7.362466\n",
      "Train Epoche: 2 [1060/96 (1104%)]\tLoss: 4.421561\n",
      "Train Epoche: 2 [1061/96 (1105%)]\tLoss: 0.353281\n",
      "Train Epoche: 2 [1062/96 (1106%)]\tLoss: 10.407752\n",
      "Train Epoche: 2 [1063/96 (1107%)]\tLoss: 0.682831\n",
      "Train Epoche: 2 [1064/96 (1108%)]\tLoss: 0.267488\n",
      "Train Epoche: 2 [1065/96 (1109%)]\tLoss: 0.788585\n",
      "Train Epoche: 2 [1066/96 (1110%)]\tLoss: 3.737752\n",
      "Train Epoche: 2 [1067/96 (1111%)]\tLoss: 3.386818\n",
      "Train Epoche: 2 [1068/96 (1112%)]\tLoss: 0.542404\n",
      "Train Epoche: 2 [1069/96 (1114%)]\tLoss: 0.079232\n",
      "Train Epoche: 2 [1070/96 (1115%)]\tLoss: 28.148310\n",
      "Train Epoche: 2 [1071/96 (1116%)]\tLoss: 3.411931\n",
      "Train Epoche: 2 [1072/96 (1117%)]\tLoss: 12.289371\n",
      "Train Epoche: 2 [1073/96 (1118%)]\tLoss: 11.312007\n",
      "Train Epoche: 2 [1074/96 (1119%)]\tLoss: 7.568853\n",
      "Train Epoche: 2 [1075/96 (1120%)]\tLoss: 0.012305\n",
      "Train Epoche: 2 [1076/96 (1121%)]\tLoss: 0.126908\n",
      "Train Epoche: 2 [1077/96 (1122%)]\tLoss: 0.404452\n",
      "Train Epoche: 2 [1078/96 (1123%)]\tLoss: 2.942639\n",
      "Train Epoche: 2 [1079/96 (1124%)]\tLoss: 20.810350\n",
      "Train Epoche: 2 [1080/96 (1125%)]\tLoss: 0.402567\n",
      "Train Epoche: 2 [1081/96 (1126%)]\tLoss: 0.000157\n",
      "Train Epoche: 2 [1082/96 (1127%)]\tLoss: 7.256505\n",
      "Train Epoche: 2 [1083/96 (1128%)]\tLoss: 1.268552\n",
      "Train Epoche: 2 [1084/96 (1129%)]\tLoss: 0.361797\n",
      "Train Epoche: 2 [1085/96 (1130%)]\tLoss: 11.574856\n",
      "Train Epoche: 2 [1086/96 (1131%)]\tLoss: 4.359168\n",
      "Train Epoche: 2 [1087/96 (1132%)]\tLoss: 6.436338\n",
      "Train Epoche: 2 [1088/96 (1133%)]\tLoss: 6.517312\n",
      "Train Epoche: 2 [1089/96 (1134%)]\tLoss: 0.123541\n",
      "Train Epoche: 2 [1090/96 (1135%)]\tLoss: 7.121939\n",
      "Train Epoche: 2 [1091/96 (1136%)]\tLoss: 0.095353\n",
      "Train Epoche: 2 [1092/96 (1138%)]\tLoss: 3.977076\n",
      "Train Epoche: 2 [1093/96 (1139%)]\tLoss: 1.850647\n",
      "Train Epoche: 2 [1094/96 (1140%)]\tLoss: 13.066058\n",
      "Train Epoche: 2 [1095/96 (1141%)]\tLoss: 3.948819\n",
      "Train Epoche: 2 [1096/96 (1142%)]\tLoss: 21.931200\n",
      "Train Epoche: 2 [1097/96 (1143%)]\tLoss: 17.480177\n",
      "Train Epoche: 2 [1098/96 (1144%)]\tLoss: 2.957940\n",
      "Train Epoche: 2 [1099/96 (1145%)]\tLoss: 0.011950\n",
      "Train Epoche: 2 [1100/96 (1146%)]\tLoss: 18.276747\n",
      "Train Epoche: 2 [1101/96 (1147%)]\tLoss: 25.093126\n",
      "Train Epoche: 2 [1102/96 (1148%)]\tLoss: 0.252428\n",
      "Train Epoche: 2 [1103/96 (1149%)]\tLoss: 12.174068\n",
      "Train Epoche: 2 [1104/96 (1150%)]\tLoss: 12.353002\n",
      "Train Epoche: 2 [1105/96 (1151%)]\tLoss: 9.780386\n",
      "Train Epoche: 2 [1106/96 (1152%)]\tLoss: 1.932751\n",
      "Train Epoche: 2 [1107/96 (1153%)]\tLoss: 1.468439\n",
      "Train Epoche: 2 [1108/96 (1154%)]\tLoss: 1.765472\n",
      "Train Epoche: 2 [1109/96 (1155%)]\tLoss: 4.793367\n",
      "Train Epoche: 2 [1110/96 (1156%)]\tLoss: 306.526367\n",
      "Train Epoche: 2 [1111/96 (1157%)]\tLoss: 1.045826\n",
      "Train Epoche: 2 [1112/96 (1158%)]\tLoss: 1.497289\n",
      "Train Epoche: 2 [1113/96 (1159%)]\tLoss: 26.504259\n",
      "Train Epoche: 2 [1114/96 (1160%)]\tLoss: 35.795670\n",
      "Train Epoche: 2 [1115/96 (1161%)]\tLoss: 5.818612\n",
      "Train Epoche: 2 [1116/96 (1162%)]\tLoss: 15.160570\n",
      "Train Epoche: 2 [1117/96 (1164%)]\tLoss: 3.403308\n",
      "Train Epoche: 2 [1118/96 (1165%)]\tLoss: 0.236355\n",
      "Train Epoche: 2 [1119/96 (1166%)]\tLoss: 2.635857\n",
      "Train Epoche: 2 [1120/96 (1167%)]\tLoss: 2.858552\n",
      "Train Epoche: 2 [1121/96 (1168%)]\tLoss: 25.472960\n",
      "Train Epoche: 2 [1122/96 (1169%)]\tLoss: 7.919230\n",
      "Train Epoche: 2 [1123/96 (1170%)]\tLoss: 0.228607\n",
      "Train Epoche: 2 [1124/96 (1171%)]\tLoss: 17.109362\n",
      "Train Epoche: 2 [1125/96 (1172%)]\tLoss: 33.609974\n",
      "Train Epoche: 2 [1126/96 (1173%)]\tLoss: 0.156891\n",
      "Train Epoche: 2 [1127/96 (1174%)]\tLoss: 14.186106\n",
      "Train Epoche: 2 [1128/96 (1175%)]\tLoss: 8.899238\n",
      "Train Epoche: 2 [1129/96 (1176%)]\tLoss: 11.092577\n",
      "Train Epoche: 2 [1130/96 (1177%)]\tLoss: 6.425499\n",
      "Train Epoche: 2 [1131/96 (1178%)]\tLoss: 0.794550\n",
      "Train Epoche: 2 [1132/96 (1179%)]\tLoss: 4.691636\n",
      "Train Epoche: 2 [1133/96 (1180%)]\tLoss: 0.255788\n",
      "Train Epoche: 2 [1134/96 (1181%)]\tLoss: 70.923561\n",
      "Train Epoche: 2 [1135/96 (1182%)]\tLoss: 389.085114\n",
      "Train Epoche: 2 [1136/96 (1183%)]\tLoss: 252.650253\n",
      "Train Epoche: 2 [1137/96 (1184%)]\tLoss: 0.575817\n",
      "Train Epoche: 2 [1138/96 (1185%)]\tLoss: 9.048841\n",
      "Train Epoche: 2 [1139/96 (1186%)]\tLoss: 123.452583\n",
      "Train Epoche: 2 [1140/96 (1188%)]\tLoss: 9.638677\n",
      "Train Epoche: 2 [1141/96 (1189%)]\tLoss: 98.032234\n",
      "Train Epoche: 2 [1142/96 (1190%)]\tLoss: 0.437012\n",
      "Train Epoche: 2 [1143/96 (1191%)]\tLoss: 0.008331\n",
      "Train Epoche: 2 [1144/96 (1192%)]\tLoss: 16.614904\n",
      "Train Epoche: 2 [1145/96 (1193%)]\tLoss: 4.191813\n",
      "Train Epoche: 2 [1146/96 (1194%)]\tLoss: 13.706503\n",
      "Train Epoche: 2 [1147/96 (1195%)]\tLoss: 12.739860\n",
      "Train Epoche: 2 [1148/96 (1196%)]\tLoss: 4.777674\n",
      "Train Epoche: 2 [1149/96 (1197%)]\tLoss: 45.353142\n",
      "Train Epoche: 2 [1150/96 (1198%)]\tLoss: 0.224673\n",
      "Train Epoche: 2 [1151/96 (1199%)]\tLoss: 1.838944\n",
      "Train Epoche: 2 [1152/96 (1200%)]\tLoss: 0.736352\n",
      "Train Epoche: 2 [1153/96 (1201%)]\tLoss: 0.656652\n",
      "Train Epoche: 2 [1154/96 (1202%)]\tLoss: 55.033794\n",
      "Train Epoche: 2 [1155/96 (1203%)]\tLoss: 2.020744\n",
      "Train Epoche: 2 [1156/96 (1204%)]\tLoss: 0.304374\n",
      "Train Epoche: 2 [1157/96 (1205%)]\tLoss: 4.609598\n",
      "Train Epoche: 2 [1158/96 (1206%)]\tLoss: 3.927884\n",
      "Train Epoche: 2 [1159/96 (1207%)]\tLoss: 0.951735\n",
      "Train Epoche: 2 [1160/96 (1208%)]\tLoss: 2.051381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1161/96 (1209%)]\tLoss: 20.435171\n",
      "Train Epoche: 2 [1162/96 (1210%)]\tLoss: 53.886929\n",
      "Train Epoche: 2 [1163/96 (1211%)]\tLoss: 0.274802\n",
      "Train Epoche: 2 [1164/96 (1212%)]\tLoss: 0.598548\n",
      "Train Epoche: 2 [1165/96 (1214%)]\tLoss: 0.739376\n",
      "Train Epoche: 2 [1166/96 (1215%)]\tLoss: 10.920191\n",
      "Train Epoche: 2 [1167/96 (1216%)]\tLoss: 55.500935\n",
      "Train Epoche: 2 [1168/96 (1217%)]\tLoss: 31.661797\n",
      "Train Epoche: 2 [1169/96 (1218%)]\tLoss: 3.431477\n",
      "Train Epoche: 2 [1170/96 (1219%)]\tLoss: 0.130366\n",
      "Train Epoche: 2 [1171/96 (1220%)]\tLoss: 0.893345\n",
      "Train Epoche: 2 [1172/96 (1221%)]\tLoss: 13.002895\n",
      "Train Epoche: 2 [1173/96 (1222%)]\tLoss: 22.262489\n",
      "Train Epoche: 2 [1174/96 (1223%)]\tLoss: 22.695814\n",
      "Train Epoche: 2 [1175/96 (1224%)]\tLoss: 14.581929\n",
      "Train Epoche: 2 [1176/96 (1225%)]\tLoss: 35.962692\n",
      "Train Epoche: 2 [1177/96 (1226%)]\tLoss: 23.141214\n",
      "Train Epoche: 2 [1178/96 (1227%)]\tLoss: 3.334431\n",
      "Train Epoche: 2 [1179/96 (1228%)]\tLoss: 24.600067\n",
      "Train Epoche: 2 [1180/96 (1229%)]\tLoss: 20.813595\n",
      "Train Epoche: 2 [1181/96 (1230%)]\tLoss: 47.427486\n",
      "Train Epoche: 2 [1182/96 (1231%)]\tLoss: 0.533420\n",
      "Train Epoche: 2 [1183/96 (1232%)]\tLoss: 17.731592\n",
      "Train Epoche: 2 [1184/96 (1233%)]\tLoss: 3.830602\n",
      "Train Epoche: 2 [1185/96 (1234%)]\tLoss: 2.389003\n",
      "Train Epoche: 2 [1186/96 (1235%)]\tLoss: 6.903142\n",
      "Train Epoche: 2 [1187/96 (1236%)]\tLoss: 3.857844\n",
      "Train Epoche: 2 [1188/96 (1238%)]\tLoss: 1.617779\n",
      "Train Epoche: 2 [1189/96 (1239%)]\tLoss: 24.135172\n",
      "Train Epoche: 2 [1190/96 (1240%)]\tLoss: 2.187639\n",
      "Train Epoche: 2 [1191/96 (1241%)]\tLoss: 3.482241\n",
      "Train Epoche: 2 [1192/96 (1242%)]\tLoss: 0.000566\n",
      "Train Epoche: 2 [1193/96 (1243%)]\tLoss: 0.135554\n",
      "Train Epoche: 2 [1194/96 (1244%)]\tLoss: 0.004639\n",
      "Train Epoche: 2 [1195/96 (1245%)]\tLoss: 2.604224\n",
      "Train Epoche: 2 [1196/96 (1246%)]\tLoss: 1.931898\n",
      "Train Epoche: 2 [1197/96 (1247%)]\tLoss: 3.454733\n",
      "Train Epoche: 2 [1198/96 (1248%)]\tLoss: 203.054993\n",
      "Train Epoche: 2 [1199/96 (1249%)]\tLoss: 2.874139\n",
      "Train Epoche: 2 [1200/96 (1250%)]\tLoss: 1.673524\n",
      "Train Epoche: 2 [1201/96 (1251%)]\tLoss: 57.220375\n",
      "Train Epoche: 2 [1202/96 (1252%)]\tLoss: 0.000096\n",
      "Train Epoche: 2 [1203/96 (1253%)]\tLoss: 121.835007\n",
      "Train Epoche: 2 [1204/96 (1254%)]\tLoss: 11.873865\n",
      "Train Epoche: 2 [1205/96 (1255%)]\tLoss: 1.621293\n",
      "Train Epoche: 2 [1206/96 (1256%)]\tLoss: 4.068092\n",
      "Train Epoche: 2 [1207/96 (1257%)]\tLoss: 1.636635\n",
      "Train Epoche: 2 [1208/96 (1258%)]\tLoss: 3.058466\n",
      "Train Epoche: 2 [1209/96 (1259%)]\tLoss: 13.740327\n",
      "Train Epoche: 2 [1210/96 (1260%)]\tLoss: 5.892730\n",
      "Train Epoche: 2 [1211/96 (1261%)]\tLoss: 1.784581\n",
      "Train Epoche: 2 [1212/96 (1262%)]\tLoss: 6.722370\n",
      "Train Epoche: 2 [1213/96 (1264%)]\tLoss: 1.235386\n",
      "Train Epoche: 2 [1214/96 (1265%)]\tLoss: 2.014174\n",
      "Train Epoche: 2 [1215/96 (1266%)]\tLoss: 0.530717\n",
      "Train Epoche: 2 [1216/96 (1267%)]\tLoss: 10.086447\n",
      "Train Epoche: 2 [1217/96 (1268%)]\tLoss: 0.178507\n",
      "Train Epoche: 2 [1218/96 (1269%)]\tLoss: 1.017500\n",
      "Train Epoche: 2 [1219/96 (1270%)]\tLoss: 4.379844\n",
      "Train Epoche: 2 [1220/96 (1271%)]\tLoss: 1.270479\n",
      "Train Epoche: 2 [1221/96 (1272%)]\tLoss: 0.991249\n",
      "Train Epoche: 2 [1222/96 (1273%)]\tLoss: 5.930843\n",
      "Train Epoche: 2 [1223/96 (1274%)]\tLoss: 1.672562\n",
      "Train Epoche: 2 [1224/96 (1275%)]\tLoss: 1.116197\n",
      "Train Epoche: 2 [1225/96 (1276%)]\tLoss: 20.750652\n",
      "Train Epoche: 2 [1226/96 (1277%)]\tLoss: 4.816073\n",
      "Train Epoche: 2 [1227/96 (1278%)]\tLoss: 5.384253\n",
      "Train Epoche: 2 [1228/96 (1279%)]\tLoss: 3.005771\n",
      "Train Epoche: 2 [1229/96 (1280%)]\tLoss: 5.597998\n",
      "Train Epoche: 2 [1230/96 (1281%)]\tLoss: 0.048500\n",
      "Train Epoche: 2 [1231/96 (1282%)]\tLoss: 3.045740\n",
      "Train Epoche: 2 [1232/96 (1283%)]\tLoss: 1.419535\n",
      "Train Epoche: 2 [1233/96 (1284%)]\tLoss: 0.045173\n",
      "Train Epoche: 2 [1234/96 (1285%)]\tLoss: 0.158991\n",
      "Train Epoche: 2 [1235/96 (1286%)]\tLoss: 0.593210\n",
      "Train Epoche: 2 [1236/96 (1288%)]\tLoss: 2.161571\n",
      "Train Epoche: 2 [1237/96 (1289%)]\tLoss: 8.296559\n",
      "Train Epoche: 2 [1238/96 (1290%)]\tLoss: 0.128458\n",
      "Train Epoche: 2 [1239/96 (1291%)]\tLoss: 2.898679\n",
      "Train Epoche: 2 [1240/96 (1292%)]\tLoss: 0.779718\n",
      "Train Epoche: 2 [1241/96 (1293%)]\tLoss: 0.817911\n",
      "Train Epoche: 2 [1242/96 (1294%)]\tLoss: 1.830301\n",
      "Train Epoche: 2 [1243/96 (1295%)]\tLoss: 5.792408\n",
      "Train Epoche: 2 [1244/96 (1296%)]\tLoss: 2.902107\n",
      "Train Epoche: 2 [1245/96 (1297%)]\tLoss: 31.862003\n",
      "Train Epoche: 2 [1246/96 (1298%)]\tLoss: 6.221232\n",
      "Train Epoche: 2 [1247/96 (1299%)]\tLoss: 0.012321\n",
      "Train Epoche: 2 [1248/96 (1300%)]\tLoss: 13.544044\n",
      "Train Epoche: 2 [1249/96 (1301%)]\tLoss: 20.429792\n",
      "Train Epoche: 2 [1250/96 (1302%)]\tLoss: 33.213627\n",
      "Train Epoche: 2 [1251/96 (1303%)]\tLoss: 6.342873\n",
      "Train Epoche: 2 [1252/96 (1304%)]\tLoss: 0.028741\n",
      "Train Epoche: 2 [1253/96 (1305%)]\tLoss: 2.308674\n",
      "Train Epoche: 2 [1254/96 (1306%)]\tLoss: 47.416439\n",
      "Train Epoche: 2 [1255/96 (1307%)]\tLoss: 4.795008\n",
      "Train Epoche: 2 [1256/96 (1308%)]\tLoss: 4.386757\n",
      "Train Epoche: 2 [1257/96 (1309%)]\tLoss: 6.612695\n",
      "Train Epoche: 2 [1258/96 (1310%)]\tLoss: 5.049224\n",
      "Train Epoche: 2 [1259/96 (1311%)]\tLoss: 12.950405\n",
      "Train Epoche: 2 [1260/96 (1312%)]\tLoss: 2.806198\n",
      "Train Epoche: 2 [1261/96 (1314%)]\tLoss: 3.101496\n",
      "Train Epoche: 2 [1262/96 (1315%)]\tLoss: 0.528930\n",
      "Train Epoche: 2 [1263/96 (1316%)]\tLoss: 6.979935\n",
      "Train Epoche: 2 [1264/96 (1317%)]\tLoss: 9.436771\n",
      "Train Epoche: 2 [1265/96 (1318%)]\tLoss: 0.232825\n",
      "Train Epoche: 2 [1266/96 (1319%)]\tLoss: 220.964249\n",
      "Train Epoche: 2 [1267/96 (1320%)]\tLoss: 5.054242\n",
      "Train Epoche: 2 [1268/96 (1321%)]\tLoss: 54.454506\n",
      "Train Epoche: 2 [1269/96 (1322%)]\tLoss: 17.543400\n",
      "Train Epoche: 2 [1270/96 (1323%)]\tLoss: 59.027500\n",
      "Train Epoche: 2 [1271/96 (1324%)]\tLoss: 0.078600\n",
      "Train Epoche: 2 [1272/96 (1325%)]\tLoss: 0.065235\n",
      "Train Epoche: 2 [1273/96 (1326%)]\tLoss: 21.849340\n",
      "Train Epoche: 2 [1274/96 (1327%)]\tLoss: 178.417816\n",
      "Train Epoche: 2 [1275/96 (1328%)]\tLoss: 18.980673\n",
      "Train Epoche: 2 [1276/96 (1329%)]\tLoss: 24.092649\n",
      "Train Epoche: 2 [1277/96 (1330%)]\tLoss: 0.028940\n",
      "Train Epoche: 2 [1278/96 (1331%)]\tLoss: 3.076247\n",
      "Train Epoche: 2 [1279/96 (1332%)]\tLoss: 2.109835\n",
      "Train Epoche: 2 [1280/96 (1333%)]\tLoss: 13.446440\n",
      "Train Epoche: 2 [1281/96 (1334%)]\tLoss: 47.033508\n",
      "Train Epoche: 2 [1282/96 (1335%)]\tLoss: 18.577206\n",
      "Train Epoche: 2 [1283/96 (1336%)]\tLoss: 5.366529\n",
      "Train Epoche: 2 [1284/96 (1338%)]\tLoss: 1.449672\n",
      "Train Epoche: 2 [1285/96 (1339%)]\tLoss: 1.850353\n",
      "Train Epoche: 2 [1286/96 (1340%)]\tLoss: 4.848870\n",
      "Train Epoche: 2 [1287/96 (1341%)]\tLoss: 2.883016\n",
      "Train Epoche: 2 [1288/96 (1342%)]\tLoss: 32.048553\n",
      "Train Epoche: 2 [1289/96 (1343%)]\tLoss: 0.003696\n",
      "Train Epoche: 2 [1290/96 (1344%)]\tLoss: 0.921781\n",
      "Train Epoche: 2 [1291/96 (1345%)]\tLoss: 8.449864\n",
      "Train Epoche: 2 [1292/96 (1346%)]\tLoss: 1.533877\n",
      "Train Epoche: 2 [1293/96 (1347%)]\tLoss: 10.193195\n",
      "Train Epoche: 2 [1294/96 (1348%)]\tLoss: 1.147154\n",
      "Train Epoche: 2 [1295/96 (1349%)]\tLoss: 1.823279\n",
      "Train Epoche: 2 [1296/96 (1350%)]\tLoss: 15.567876\n",
      "Train Epoche: 2 [1297/96 (1351%)]\tLoss: 0.060212\n",
      "Train Epoche: 2 [1298/96 (1352%)]\tLoss: 3.147594\n",
      "Train Epoche: 2 [1299/96 (1353%)]\tLoss: 3.301561\n",
      "Train Epoche: 2 [1300/96 (1354%)]\tLoss: 4.916070\n",
      "Train Epoche: 2 [1301/96 (1355%)]\tLoss: 6.922949\n",
      "Train Epoche: 2 [1302/96 (1356%)]\tLoss: 0.605595\n",
      "Train Epoche: 2 [1303/96 (1357%)]\tLoss: 6.814896\n",
      "Train Epoche: 2 [1304/96 (1358%)]\tLoss: 0.778977\n",
      "Train Epoche: 2 [1305/96 (1359%)]\tLoss: 4.734979\n",
      "Train Epoche: 2 [1306/96 (1360%)]\tLoss: 16.731619\n",
      "Train Epoche: 2 [1307/96 (1361%)]\tLoss: 11.804431\n",
      "Train Epoche: 2 [1308/96 (1362%)]\tLoss: 1.392324\n",
      "Train Epoche: 2 [1309/96 (1364%)]\tLoss: 2.215415\n",
      "Train Epoche: 2 [1310/96 (1365%)]\tLoss: 4.484244\n",
      "Train Epoche: 2 [1311/96 (1366%)]\tLoss: 110.941841\n",
      "Train Epoche: 2 [1312/96 (1367%)]\tLoss: 0.102089\n",
      "Train Epoche: 2 [1313/96 (1368%)]\tLoss: 1.442832\n",
      "Train Epoche: 2 [1314/96 (1369%)]\tLoss: 14.442154\n",
      "Train Epoche: 2 [1315/96 (1370%)]\tLoss: 0.220687\n",
      "Train Epoche: 2 [1316/96 (1371%)]\tLoss: 1.496881\n",
      "Train Epoche: 2 [1317/96 (1372%)]\tLoss: 2.708738\n",
      "Train Epoche: 2 [1318/96 (1373%)]\tLoss: 200.109314\n",
      "Train Epoche: 2 [1319/96 (1374%)]\tLoss: 7.495855\n",
      "Train Epoche: 2 [1320/96 (1375%)]\tLoss: 3.121039\n",
      "Train Epoche: 2 [1321/96 (1376%)]\tLoss: 7.004697\n",
      "Train Epoche: 2 [1322/96 (1377%)]\tLoss: 0.709190\n",
      "Train Epoche: 2 [1323/96 (1378%)]\tLoss: 20.206894\n",
      "Train Epoche: 2 [1324/96 (1379%)]\tLoss: 0.559905\n",
      "Train Epoche: 2 [1325/96 (1380%)]\tLoss: 6.425779\n",
      "Train Epoche: 2 [1326/96 (1381%)]\tLoss: 0.225430\n",
      "Train Epoche: 2 [1327/96 (1382%)]\tLoss: 32.242279\n",
      "Train Epoche: 2 [1328/96 (1383%)]\tLoss: 9.686969\n",
      "Train Epoche: 2 [1329/96 (1384%)]\tLoss: 319.737396\n",
      "Train Epoche: 2 [1330/96 (1385%)]\tLoss: 0.359424\n",
      "Train Epoche: 2 [1331/96 (1386%)]\tLoss: 30.184910\n",
      "Train Epoche: 2 [1332/96 (1388%)]\tLoss: 0.022987\n",
      "Train Epoche: 2 [1333/96 (1389%)]\tLoss: 6.319004\n",
      "Train Epoche: 2 [1334/96 (1390%)]\tLoss: 25.298374\n",
      "Train Epoche: 2 [1335/96 (1391%)]\tLoss: 6.389626\n",
      "Train Epoche: 2 [1336/96 (1392%)]\tLoss: 4.558694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1337/96 (1393%)]\tLoss: 28.701029\n",
      "Train Epoche: 2 [1338/96 (1394%)]\tLoss: 4.806375\n",
      "Train Epoche: 2 [1339/96 (1395%)]\tLoss: 6.042289\n",
      "Train Epoche: 2 [1340/96 (1396%)]\tLoss: 315.947296\n",
      "Train Epoche: 2 [1341/96 (1397%)]\tLoss: 16.650517\n",
      "Train Epoche: 2 [1342/96 (1398%)]\tLoss: 0.026600\n",
      "Train Epoche: 2 [1343/96 (1399%)]\tLoss: 16.531504\n",
      "Train Epoche: 2 [1344/96 (1400%)]\tLoss: 72.064095\n",
      "Train Epoche: 2 [1345/96 (1401%)]\tLoss: 0.038086\n",
      "Train Epoche: 2 [1346/96 (1402%)]\tLoss: 0.282850\n",
      "Train Epoche: 2 [1347/96 (1403%)]\tLoss: 0.015188\n",
      "Train Epoche: 2 [1348/96 (1404%)]\tLoss: 1.686683\n",
      "Train Epoche: 2 [1349/96 (1405%)]\tLoss: 1.056726\n",
      "Train Epoche: 2 [1350/96 (1406%)]\tLoss: 4.305749\n",
      "Train Epoche: 2 [1351/96 (1407%)]\tLoss: 0.341928\n",
      "Train Epoche: 2 [1352/96 (1408%)]\tLoss: 12.321970\n",
      "Train Epoche: 2 [1353/96 (1409%)]\tLoss: 0.032001\n",
      "Train Epoche: 2 [1354/96 (1410%)]\tLoss: 3.459982\n",
      "Train Epoche: 2 [1355/96 (1411%)]\tLoss: 9.899427\n",
      "Train Epoche: 2 [1356/96 (1412%)]\tLoss: 7.613743\n",
      "Train Epoche: 2 [1357/96 (1414%)]\tLoss: 11.355851\n",
      "Train Epoche: 2 [1358/96 (1415%)]\tLoss: 3.399195\n",
      "Train Epoche: 2 [1359/96 (1416%)]\tLoss: 52.563427\n",
      "Train Epoche: 2 [1360/96 (1417%)]\tLoss: 0.018372\n",
      "Train Epoche: 2 [1361/96 (1418%)]\tLoss: 3.238382\n",
      "Train Epoche: 2 [1362/96 (1419%)]\tLoss: 5.534449\n",
      "Train Epoche: 2 [1363/96 (1420%)]\tLoss: 34.519840\n",
      "Train Epoche: 2 [1364/96 (1421%)]\tLoss: 155.447876\n",
      "Train Epoche: 2 [1365/96 (1422%)]\tLoss: 2.532892\n",
      "Train Epoche: 2 [1366/96 (1423%)]\tLoss: 3.975140\n",
      "Train Epoche: 2 [1367/96 (1424%)]\tLoss: 0.152181\n",
      "Train Epoche: 2 [1368/96 (1425%)]\tLoss: 3.831920\n",
      "Train Epoche: 2 [1369/96 (1426%)]\tLoss: 34.820641\n",
      "Train Epoche: 2 [1370/96 (1427%)]\tLoss: 7.545460\n",
      "Train Epoche: 2 [1371/96 (1428%)]\tLoss: 60.533146\n",
      "Train Epoche: 2 [1372/96 (1429%)]\tLoss: 6.938312\n",
      "Train Epoche: 2 [1373/96 (1430%)]\tLoss: 0.217058\n",
      "Train Epoche: 2 [1374/96 (1431%)]\tLoss: 2.206779\n",
      "Train Epoche: 2 [1375/96 (1432%)]\tLoss: 0.025001\n",
      "Train Epoche: 2 [1376/96 (1433%)]\tLoss: 2.590690\n",
      "Train Epoche: 2 [1377/96 (1434%)]\tLoss: 15.234629\n",
      "Train Epoche: 2 [1378/96 (1435%)]\tLoss: 8.850492\n",
      "Train Epoche: 2 [1379/96 (1436%)]\tLoss: 0.136779\n",
      "Train Epoche: 2 [1380/96 (1438%)]\tLoss: 5.039018\n",
      "Train Epoche: 2 [1381/96 (1439%)]\tLoss: 1.158940\n",
      "Train Epoche: 2 [1382/96 (1440%)]\tLoss: 8.238514\n",
      "Train Epoche: 2 [1383/96 (1441%)]\tLoss: 4.043813\n",
      "Train Epoche: 2 [1384/96 (1442%)]\tLoss: 0.534429\n",
      "Train Epoche: 2 [1385/96 (1443%)]\tLoss: 6.723189\n",
      "Train Epoche: 2 [1386/96 (1444%)]\tLoss: 3.793018\n",
      "Train Epoche: 2 [1387/96 (1445%)]\tLoss: 6.469911\n",
      "Train Epoche: 2 [1388/96 (1446%)]\tLoss: 9.314087\n",
      "Train Epoche: 2 [1389/96 (1447%)]\tLoss: 32.033546\n",
      "Train Epoche: 2 [1390/96 (1448%)]\tLoss: 1.294398\n",
      "Train Epoche: 2 [1391/96 (1449%)]\tLoss: 6.916635\n",
      "Train Epoche: 2 [1392/96 (1450%)]\tLoss: 0.000569\n",
      "Train Epoche: 2 [1393/96 (1451%)]\tLoss: 0.952207\n",
      "Train Epoche: 2 [1394/96 (1452%)]\tLoss: 26.388622\n",
      "Train Epoche: 2 [1395/96 (1453%)]\tLoss: 0.662304\n",
      "Train Epoche: 2 [1396/96 (1454%)]\tLoss: 0.414865\n",
      "Train Epoche: 2 [1397/96 (1455%)]\tLoss: 38.521355\n",
      "Train Epoche: 2 [1398/96 (1456%)]\tLoss: 5.656591\n",
      "Train Epoche: 2 [1399/96 (1457%)]\tLoss: 1.639414\n",
      "Train Epoche: 2 [1400/96 (1458%)]\tLoss: 2.415486\n",
      "Train Epoche: 2 [1401/96 (1459%)]\tLoss: 13.430324\n",
      "Train Epoche: 2 [1402/96 (1460%)]\tLoss: 0.107500\n",
      "Train Epoche: 2 [1403/96 (1461%)]\tLoss: 1.517800\n",
      "Train Epoche: 2 [1404/96 (1462%)]\tLoss: 2.907454\n",
      "Train Epoche: 2 [1405/96 (1464%)]\tLoss: 1.030223\n",
      "Train Epoche: 2 [1406/96 (1465%)]\tLoss: 3.111321\n",
      "Train Epoche: 2 [1407/96 (1466%)]\tLoss: 8.260258\n",
      "Train Epoche: 2 [1408/96 (1467%)]\tLoss: 2.688879\n",
      "Train Epoche: 2 [1409/96 (1468%)]\tLoss: 47.685539\n",
      "Train Epoche: 2 [1410/96 (1469%)]\tLoss: 5.731519\n",
      "Train Epoche: 2 [1411/96 (1470%)]\tLoss: 86.325630\n",
      "Train Epoche: 2 [1412/96 (1471%)]\tLoss: 5.723362\n",
      "Train Epoche: 2 [1413/96 (1472%)]\tLoss: 1.567231\n",
      "Train Epoche: 2 [1414/96 (1473%)]\tLoss: 11.528090\n",
      "Train Epoche: 2 [1415/96 (1474%)]\tLoss: 4.247925\n",
      "Train Epoche: 2 [1416/96 (1475%)]\tLoss: 1.748362\n",
      "Train Epoche: 2 [1417/96 (1476%)]\tLoss: 0.107302\n",
      "Train Epoche: 2 [1418/96 (1477%)]\tLoss: 56.069973\n",
      "Train Epoche: 2 [1419/96 (1478%)]\tLoss: 7.493620\n",
      "Train Epoche: 2 [1420/96 (1479%)]\tLoss: 8.694626\n",
      "Train Epoche: 2 [1421/96 (1480%)]\tLoss: 2.967755\n",
      "Train Epoche: 2 [1422/96 (1481%)]\tLoss: 0.483644\n",
      "Train Epoche: 2 [1423/96 (1482%)]\tLoss: 0.145125\n",
      "Train Epoche: 2 [1424/96 (1483%)]\tLoss: 2.181352\n",
      "Train Epoche: 2 [1425/96 (1484%)]\tLoss: 0.356250\n",
      "Train Epoche: 2 [1426/96 (1485%)]\tLoss: 9.816474\n",
      "Train Epoche: 2 [1427/96 (1486%)]\tLoss: 5.295758\n",
      "Train Epoche: 2 [1428/96 (1488%)]\tLoss: 7.598257\n",
      "Train Epoche: 2 [1429/96 (1489%)]\tLoss: 0.128030\n",
      "Train Epoche: 2 [1430/96 (1490%)]\tLoss: 3.799030\n",
      "Train Epoche: 2 [1431/96 (1491%)]\tLoss: 0.668508\n",
      "Train Epoche: 2 [1432/96 (1492%)]\tLoss: 1.904487\n",
      "Train Epoche: 2 [1433/96 (1493%)]\tLoss: 10.698112\n",
      "Train Epoche: 2 [1434/96 (1494%)]\tLoss: 8.509333\n",
      "Train Epoche: 2 [1435/96 (1495%)]\tLoss: 18.323757\n",
      "Train Epoche: 2 [1436/96 (1496%)]\tLoss: 15.541559\n",
      "Train Epoche: 2 [1437/96 (1497%)]\tLoss: 15.823715\n",
      "Train Epoche: 2 [1438/96 (1498%)]\tLoss: 6.059740\n",
      "Train Epoche: 2 [1439/96 (1499%)]\tLoss: 0.895922\n",
      "Train Epoche: 2 [1440/96 (1500%)]\tLoss: 0.874228\n",
      "Train Epoche: 2 [1441/96 (1501%)]\tLoss: 0.877133\n",
      "Train Epoche: 2 [1442/96 (1502%)]\tLoss: 192.091415\n",
      "Train Epoche: 2 [1443/96 (1503%)]\tLoss: 43.355042\n",
      "Train Epoche: 2 [1444/96 (1504%)]\tLoss: 1.912289\n",
      "Train Epoche: 2 [1445/96 (1505%)]\tLoss: 5.867973\n",
      "Train Epoche: 2 [1446/96 (1506%)]\tLoss: 2.688122\n",
      "Train Epoche: 2 [1447/96 (1507%)]\tLoss: 27.545971\n",
      "Train Epoche: 2 [1448/96 (1508%)]\tLoss: 7.647376\n",
      "Train Epoche: 2 [1449/96 (1509%)]\tLoss: 2.117014\n",
      "Train Epoche: 2 [1450/96 (1510%)]\tLoss: 10.829058\n",
      "Train Epoche: 2 [1451/96 (1511%)]\tLoss: 0.766233\n",
      "Train Epoche: 2 [1452/96 (1512%)]\tLoss: 6.023958\n",
      "Train Epoche: 2 [1453/96 (1514%)]\tLoss: 16.373894\n",
      "Train Epoche: 2 [1454/96 (1515%)]\tLoss: 0.307947\n",
      "Train Epoche: 2 [1455/96 (1516%)]\tLoss: 0.074023\n",
      "Train Epoche: 2 [1456/96 (1517%)]\tLoss: 10.791813\n",
      "Train Epoche: 2 [1457/96 (1518%)]\tLoss: 0.121704\n",
      "Train Epoche: 2 [1458/96 (1519%)]\tLoss: 13.245485\n",
      "Train Epoche: 2 [1459/96 (1520%)]\tLoss: 0.272756\n",
      "Train Epoche: 2 [1460/96 (1521%)]\tLoss: 1.498582\n",
      "Train Epoche: 2 [1461/96 (1522%)]\tLoss: 4.596853\n",
      "Train Epoche: 2 [1462/96 (1523%)]\tLoss: 3.523791\n",
      "Train Epoche: 2 [1463/96 (1524%)]\tLoss: 10.648766\n",
      "Train Epoche: 2 [1464/96 (1525%)]\tLoss: 10.257023\n",
      "Train Epoche: 2 [1465/96 (1526%)]\tLoss: 0.173456\n",
      "Train Epoche: 2 [1466/96 (1527%)]\tLoss: 2.454510\n",
      "Train Epoche: 2 [1467/96 (1528%)]\tLoss: 0.658893\n",
      "Train Epoche: 2 [1468/96 (1529%)]\tLoss: 109.462402\n",
      "Train Epoche: 2 [1469/96 (1530%)]\tLoss: 2.448349\n",
      "Train Epoche: 2 [1470/96 (1531%)]\tLoss: 2.240771\n",
      "Train Epoche: 2 [1471/96 (1532%)]\tLoss: 1.690787\n",
      "Train Epoche: 2 [1472/96 (1533%)]\tLoss: 13.145244\n",
      "Train Epoche: 2 [1473/96 (1534%)]\tLoss: 1.254444\n",
      "Train Epoche: 2 [1474/96 (1535%)]\tLoss: 35.068024\n",
      "Train Epoche: 2 [1475/96 (1536%)]\tLoss: 0.514156\n",
      "Train Epoche: 2 [1476/96 (1538%)]\tLoss: 39.516006\n",
      "Train Epoche: 2 [1477/96 (1539%)]\tLoss: 15.424235\n",
      "Train Epoche: 2 [1478/96 (1540%)]\tLoss: 44.549938\n",
      "Train Epoche: 2 [1479/96 (1541%)]\tLoss: 2.275823\n",
      "Train Epoche: 2 [1480/96 (1542%)]\tLoss: 3.412547\n",
      "Train Epoche: 2 [1481/96 (1543%)]\tLoss: 0.104229\n",
      "Train Epoche: 2 [1482/96 (1544%)]\tLoss: 5.485571\n",
      "Train Epoche: 2 [1483/96 (1545%)]\tLoss: 3.517277\n",
      "Train Epoche: 2 [1484/96 (1546%)]\tLoss: 4.464632\n",
      "Train Epoche: 2 [1485/96 (1547%)]\tLoss: 3.214211\n",
      "Train Epoche: 2 [1486/96 (1548%)]\tLoss: 174.207642\n",
      "Train Epoche: 2 [1487/96 (1549%)]\tLoss: 2.257158\n",
      "Train Epoche: 2 [1488/96 (1550%)]\tLoss: 19.138914\n",
      "Train Epoche: 2 [1489/96 (1551%)]\tLoss: 1.680845\n",
      "Train Epoche: 2 [1490/96 (1552%)]\tLoss: 24.457748\n",
      "Train Epoche: 2 [1491/96 (1553%)]\tLoss: 13.293726\n",
      "Train Epoche: 2 [1492/96 (1554%)]\tLoss: 9.545095\n",
      "Train Epoche: 2 [1493/96 (1555%)]\tLoss: 4.250587\n",
      "Train Epoche: 2 [1494/96 (1556%)]\tLoss: 126.053650\n",
      "Train Epoche: 2 [1495/96 (1557%)]\tLoss: 1.882475\n",
      "Train Epoche: 2 [1496/96 (1558%)]\tLoss: 20.901287\n",
      "Train Epoche: 2 [1497/96 (1559%)]\tLoss: 2.440289\n",
      "Train Epoche: 2 [1498/96 (1560%)]\tLoss: 4.410913\n",
      "Train Epoche: 2 [1499/96 (1561%)]\tLoss: 2.593650\n",
      "Train Epoche: 2 [1500/96 (1562%)]\tLoss: 0.181809\n",
      "Train Epoche: 2 [1501/96 (1564%)]\tLoss: 1.862180\n",
      "Train Epoche: 2 [1502/96 (1565%)]\tLoss: 13.184996\n",
      "Train Epoche: 2 [1503/96 (1566%)]\tLoss: 3.205744\n",
      "Train Epoche: 2 [1504/96 (1567%)]\tLoss: 9.614520\n",
      "Train Epoche: 2 [1505/96 (1568%)]\tLoss: 1.055947\n",
      "Train Epoche: 2 [1506/96 (1569%)]\tLoss: 0.112996\n",
      "Train Epoche: 2 [1507/96 (1570%)]\tLoss: 2.669513\n",
      "Train Epoche: 2 [1508/96 (1571%)]\tLoss: 5.122129\n",
      "Train Epoche: 2 [1509/96 (1572%)]\tLoss: 0.373404\n",
      "Train Epoche: 2 [1510/96 (1573%)]\tLoss: 4.635458\n",
      "Train Epoche: 2 [1511/96 (1574%)]\tLoss: 6.078050\n",
      "Train Epoche: 2 [1512/96 (1575%)]\tLoss: 0.629859\n",
      "Train Epoche: 2 [1513/96 (1576%)]\tLoss: 0.246509\n",
      "Train Epoche: 2 [1514/96 (1577%)]\tLoss: 0.213519\n",
      "Train Epoche: 2 [1515/96 (1578%)]\tLoss: 17.849306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1516/96 (1579%)]\tLoss: 0.115524\n",
      "Train Epoche: 2 [1517/96 (1580%)]\tLoss: 14.090094\n",
      "Train Epoche: 2 [1518/96 (1581%)]\tLoss: 0.695341\n",
      "Train Epoche: 2 [1519/96 (1582%)]\tLoss: 3.591634\n",
      "Train Epoche: 2 [1520/96 (1583%)]\tLoss: 0.216766\n",
      "Train Epoche: 2 [1521/96 (1584%)]\tLoss: 4.354669\n",
      "Train Epoche: 2 [1522/96 (1585%)]\tLoss: 22.095646\n",
      "Train Epoche: 2 [1523/96 (1586%)]\tLoss: 11.700504\n",
      "Train Epoche: 2 [1524/96 (1588%)]\tLoss: 0.184628\n",
      "Train Epoche: 2 [1525/96 (1589%)]\tLoss: 3.217388\n",
      "Train Epoche: 2 [1526/96 (1590%)]\tLoss: 1.252740\n",
      "Train Epoche: 2 [1527/96 (1591%)]\tLoss: 1.457912\n",
      "Train Epoche: 2 [1528/96 (1592%)]\tLoss: 5.470046\n",
      "Train Epoche: 2 [1529/96 (1593%)]\tLoss: 5.955031\n",
      "Train Epoche: 2 [1530/96 (1594%)]\tLoss: 3.430371\n",
      "Train Epoche: 2 [1531/96 (1595%)]\tLoss: 0.031342\n",
      "Train Epoche: 2 [1532/96 (1596%)]\tLoss: 40.072292\n",
      "Train Epoche: 2 [1533/96 (1597%)]\tLoss: 9.363437\n",
      "Train Epoche: 2 [1534/96 (1598%)]\tLoss: 6.579820\n",
      "Train Epoche: 2 [1535/96 (1599%)]\tLoss: 2.302835\n",
      "Train Epoche: 2 [1536/96 (1600%)]\tLoss: 0.222940\n",
      "Train Epoche: 2 [1537/96 (1601%)]\tLoss: 4.353553\n",
      "Train Epoche: 2 [1538/96 (1602%)]\tLoss: 1.362650\n",
      "Train Epoche: 2 [1539/96 (1603%)]\tLoss: 0.220725\n",
      "Train Epoche: 2 [1540/96 (1604%)]\tLoss: 4.054360\n",
      "Train Epoche: 2 [1541/96 (1605%)]\tLoss: 0.012029\n",
      "Train Epoche: 2 [1542/96 (1606%)]\tLoss: 66.788956\n",
      "Train Epoche: 2 [1543/96 (1607%)]\tLoss: 3.354284\n",
      "Train Epoche: 2 [1544/96 (1608%)]\tLoss: 0.123494\n",
      "Train Epoche: 2 [1545/96 (1609%)]\tLoss: 0.045862\n",
      "Train Epoche: 2 [1546/96 (1610%)]\tLoss: 4.640995\n",
      "Train Epoche: 2 [1547/96 (1611%)]\tLoss: 0.188679\n",
      "Train Epoche: 2 [1548/96 (1612%)]\tLoss: 20.380018\n",
      "Train Epoche: 2 [1549/96 (1614%)]\tLoss: 1.737936\n",
      "Train Epoche: 2 [1550/96 (1615%)]\tLoss: 19.413258\n",
      "Train Epoche: 2 [1551/96 (1616%)]\tLoss: 23.963215\n",
      "Train Epoche: 2 [1552/96 (1617%)]\tLoss: 3.193896\n",
      "Train Epoche: 2 [1553/96 (1618%)]\tLoss: 50.709774\n",
      "Train Epoche: 2 [1554/96 (1619%)]\tLoss: 3.659708\n",
      "Train Epoche: 2 [1555/96 (1620%)]\tLoss: 0.626471\n",
      "Train Epoche: 2 [1556/96 (1621%)]\tLoss: 1.136481\n",
      "Train Epoche: 2 [1557/96 (1622%)]\tLoss: 1.280933\n",
      "Train Epoche: 2 [1558/96 (1623%)]\tLoss: 38.904518\n",
      "Train Epoche: 2 [1559/96 (1624%)]\tLoss: 0.039897\n",
      "Train Epoche: 2 [1560/96 (1625%)]\tLoss: 6.757964\n",
      "Train Epoche: 2 [1561/96 (1626%)]\tLoss: 24.412605\n",
      "Train Epoche: 2 [1562/96 (1627%)]\tLoss: 0.222126\n",
      "Train Epoche: 2 [1563/96 (1628%)]\tLoss: 0.406283\n",
      "Train Epoche: 2 [1564/96 (1629%)]\tLoss: 0.270932\n",
      "Train Epoche: 2 [1565/96 (1630%)]\tLoss: 2.406854\n",
      "Train Epoche: 2 [1566/96 (1631%)]\tLoss: 0.046280\n",
      "Train Epoche: 2 [1567/96 (1632%)]\tLoss: 38.643322\n",
      "Train Epoche: 2 [1568/96 (1633%)]\tLoss: 1.220706\n",
      "Train Epoche: 2 [1569/96 (1634%)]\tLoss: 13.276867\n",
      "Train Epoche: 2 [1570/96 (1635%)]\tLoss: 0.190707\n",
      "Train Epoche: 2 [1571/96 (1636%)]\tLoss: 0.939979\n",
      "Train Epoche: 2 [1572/96 (1638%)]\tLoss: 2.116164\n",
      "Train Epoche: 2 [1573/96 (1639%)]\tLoss: 27.690462\n",
      "Train Epoche: 2 [1574/96 (1640%)]\tLoss: 6.269719\n",
      "Train Epoche: 2 [1575/96 (1641%)]\tLoss: 39.809380\n",
      "Train Epoche: 2 [1576/96 (1642%)]\tLoss: 29.687563\n",
      "Train Epoche: 2 [1577/96 (1643%)]\tLoss: 48.204082\n",
      "Train Epoche: 2 [1578/96 (1644%)]\tLoss: 1.514150\n",
      "Train Epoche: 2 [1579/96 (1645%)]\tLoss: 2.730867\n",
      "Train Epoche: 2 [1580/96 (1646%)]\tLoss: 5.197742\n",
      "Train Epoche: 2 [1581/96 (1647%)]\tLoss: 0.001345\n",
      "Train Epoche: 2 [1582/96 (1648%)]\tLoss: 0.797743\n",
      "Train Epoche: 2 [1583/96 (1649%)]\tLoss: 10.527047\n",
      "Train Epoche: 2 [1584/96 (1650%)]\tLoss: 0.023709\n",
      "Train Epoche: 2 [1585/96 (1651%)]\tLoss: 0.038620\n",
      "Train Epoche: 2 [1586/96 (1652%)]\tLoss: 0.000793\n",
      "Train Epoche: 2 [1587/96 (1653%)]\tLoss: 0.337638\n",
      "Train Epoche: 2 [1588/96 (1654%)]\tLoss: 9.287725\n",
      "Train Epoche: 2 [1589/96 (1655%)]\tLoss: 0.162394\n",
      "Train Epoche: 2 [1590/96 (1656%)]\tLoss: 0.002239\n",
      "Train Epoche: 2 [1591/96 (1657%)]\tLoss: 1.515208\n",
      "Train Epoche: 2 [1592/96 (1658%)]\tLoss: 1.405569\n",
      "Train Epoche: 2 [1593/96 (1659%)]\tLoss: 0.747502\n",
      "Train Epoche: 2 [1594/96 (1660%)]\tLoss: 0.461488\n",
      "Train Epoche: 2 [1595/96 (1661%)]\tLoss: 0.653540\n",
      "Train Epoche: 2 [1596/96 (1662%)]\tLoss: 3.453875\n",
      "Train Epoche: 2 [1597/96 (1664%)]\tLoss: 0.741124\n",
      "Train Epoche: 2 [1598/96 (1665%)]\tLoss: 0.416243\n",
      "Train Epoche: 2 [1599/96 (1666%)]\tLoss: 0.975986\n",
      "Train Epoche: 2 [1600/96 (1667%)]\tLoss: 222.259262\n",
      "Train Epoche: 2 [1601/96 (1668%)]\tLoss: 1.636515\n",
      "Train Epoche: 2 [1602/96 (1669%)]\tLoss: 57.708733\n",
      "Train Epoche: 2 [1603/96 (1670%)]\tLoss: 8.617435\n",
      "Train Epoche: 2 [1604/96 (1671%)]\tLoss: 4.688179\n",
      "Train Epoche: 2 [1605/96 (1672%)]\tLoss: 7.045358\n",
      "Train Epoche: 2 [1606/96 (1673%)]\tLoss: 6.499408\n",
      "Train Epoche: 2 [1607/96 (1674%)]\tLoss: 1.768245\n",
      "Train Epoche: 2 [1608/96 (1675%)]\tLoss: 0.046192\n",
      "Train Epoche: 2 [1609/96 (1676%)]\tLoss: 3.873964\n",
      "Train Epoche: 2 [1610/96 (1677%)]\tLoss: 23.576651\n",
      "Train Epoche: 2 [1611/96 (1678%)]\tLoss: 2.457927\n",
      "Train Epoche: 2 [1612/96 (1679%)]\tLoss: 4.570312\n",
      "Train Epoche: 2 [1613/96 (1680%)]\tLoss: 35.462841\n",
      "Train Epoche: 2 [1614/96 (1681%)]\tLoss: 238.616364\n",
      "Train Epoche: 2 [1615/96 (1682%)]\tLoss: 1.759151\n",
      "Train Epoche: 2 [1616/96 (1683%)]\tLoss: 8.627729\n",
      "Train Epoche: 2 [1617/96 (1684%)]\tLoss: 4.577067\n",
      "Train Epoche: 2 [1618/96 (1685%)]\tLoss: 10.204623\n",
      "Train Epoche: 2 [1619/96 (1686%)]\tLoss: 193.718033\n",
      "Train Epoche: 2 [1620/96 (1688%)]\tLoss: 19.818823\n",
      "Train Epoche: 2 [1621/96 (1689%)]\tLoss: 16.449831\n",
      "Train Epoche: 2 [1622/96 (1690%)]\tLoss: 2.606087\n",
      "Train Epoche: 2 [1623/96 (1691%)]\tLoss: 224.293259\n",
      "Train Epoche: 2 [1624/96 (1692%)]\tLoss: 17.985411\n",
      "Train Epoche: 2 [1625/96 (1693%)]\tLoss: 0.277748\n",
      "Train Epoche: 2 [1626/96 (1694%)]\tLoss: 30.165350\n",
      "Train Epoche: 2 [1627/96 (1695%)]\tLoss: 55.612278\n",
      "Train Epoche: 2 [1628/96 (1696%)]\tLoss: 5.538751\n",
      "Train Epoche: 2 [1629/96 (1697%)]\tLoss: 6.269461\n",
      "Train Epoche: 2 [1630/96 (1698%)]\tLoss: 40.457966\n",
      "Train Epoche: 2 [1631/96 (1699%)]\tLoss: 0.010894\n",
      "Train Epoche: 2 [1632/96 (1700%)]\tLoss: 17.213648\n",
      "Train Epoche: 2 [1633/96 (1701%)]\tLoss: 0.041987\n",
      "Train Epoche: 2 [1634/96 (1702%)]\tLoss: 5.235573\n",
      "Train Epoche: 2 [1635/96 (1703%)]\tLoss: 3.596123\n",
      "Train Epoche: 2 [1636/96 (1704%)]\tLoss: 2.363368\n",
      "Train Epoche: 2 [1637/96 (1705%)]\tLoss: 11.964369\n",
      "Train Epoche: 2 [1638/96 (1706%)]\tLoss: 4.221161\n",
      "Train Epoche: 2 [1639/96 (1707%)]\tLoss: 2.446845\n",
      "Train Epoche: 2 [1640/96 (1708%)]\tLoss: 3.532701\n",
      "Train Epoche: 2 [1641/96 (1709%)]\tLoss: 0.003313\n",
      "Train Epoche: 2 [1642/96 (1710%)]\tLoss: 0.000262\n",
      "Train Epoche: 2 [1643/96 (1711%)]\tLoss: 3.160880\n",
      "Train Epoche: 2 [1644/96 (1712%)]\tLoss: 110.205681\n",
      "Train Epoche: 2 [1645/96 (1714%)]\tLoss: 6.730716\n",
      "Train Epoche: 2 [1646/96 (1715%)]\tLoss: 0.306788\n",
      "Train Epoche: 2 [1647/96 (1716%)]\tLoss: 0.505947\n",
      "Train Epoche: 2 [1648/96 (1717%)]\tLoss: 1.600508\n",
      "Train Epoche: 2 [1649/96 (1718%)]\tLoss: 1.928443\n",
      "Train Epoche: 2 [1650/96 (1719%)]\tLoss: 2.248903\n",
      "Train Epoche: 2 [1651/96 (1720%)]\tLoss: 0.019441\n",
      "Train Epoche: 2 [1652/96 (1721%)]\tLoss: 1.312564\n",
      "Train Epoche: 2 [1653/96 (1722%)]\tLoss: 19.759954\n",
      "Train Epoche: 2 [1654/96 (1723%)]\tLoss: 57.995193\n",
      "Train Epoche: 2 [1655/96 (1724%)]\tLoss: 47.694893\n",
      "Train Epoche: 2 [1656/96 (1725%)]\tLoss: 11.428959\n",
      "Train Epoche: 2 [1657/96 (1726%)]\tLoss: 26.151575\n",
      "Train Epoche: 2 [1658/96 (1727%)]\tLoss: 25.001736\n",
      "Train Epoche: 2 [1659/96 (1728%)]\tLoss: 63.822723\n",
      "Train Epoche: 2 [1660/96 (1729%)]\tLoss: 0.548519\n",
      "Train Epoche: 2 [1661/96 (1730%)]\tLoss: 3.493109\n",
      "Train Epoche: 2 [1662/96 (1731%)]\tLoss: 15.550219\n",
      "Train Epoche: 2 [1663/96 (1732%)]\tLoss: 5.119074\n",
      "Train Epoche: 2 [1664/96 (1733%)]\tLoss: 4.559529\n",
      "Train Epoche: 2 [1665/96 (1734%)]\tLoss: 0.269946\n",
      "Train Epoche: 2 [1666/96 (1735%)]\tLoss: 0.078409\n",
      "Train Epoche: 2 [1667/96 (1736%)]\tLoss: 1.519393\n",
      "Train Epoche: 2 [1668/96 (1738%)]\tLoss: 8.630576\n",
      "Train Epoche: 2 [1669/96 (1739%)]\tLoss: 6.077097\n",
      "Train Epoche: 2 [1670/96 (1740%)]\tLoss: 2.022339\n",
      "Train Epoche: 2 [1671/96 (1741%)]\tLoss: 5.405821\n",
      "Train Epoche: 2 [1672/96 (1742%)]\tLoss: 5.504739\n",
      "Train Epoche: 2 [1673/96 (1743%)]\tLoss: 101.911980\n",
      "Train Epoche: 2 [1674/96 (1744%)]\tLoss: 30.358358\n",
      "Train Epoche: 2 [1675/96 (1745%)]\tLoss: 12.279359\n",
      "Train Epoche: 2 [1676/96 (1746%)]\tLoss: 21.620882\n",
      "Train Epoche: 2 [1677/96 (1747%)]\tLoss: 15.835800\n",
      "Train Epoche: 2 [1678/96 (1748%)]\tLoss: 79.199791\n",
      "Train Epoche: 2 [1679/96 (1749%)]\tLoss: 34.528671\n",
      "Train Epoche: 2 [1680/96 (1750%)]\tLoss: 2.667622\n",
      "Train Epoche: 2 [1681/96 (1751%)]\tLoss: 0.000393\n",
      "Train Epoche: 2 [1682/96 (1752%)]\tLoss: 43.293087\n",
      "Train Epoche: 2 [1683/96 (1753%)]\tLoss: 15.213032\n",
      "Train Epoche: 2 [1684/96 (1754%)]\tLoss: 28.532297\n",
      "Train Epoche: 2 [1685/96 (1755%)]\tLoss: 270.539825\n",
      "Train Epoche: 2 [1686/96 (1756%)]\tLoss: 0.591063\n",
      "Train Epoche: 2 [1687/96 (1757%)]\tLoss: 0.135220\n",
      "Train Epoche: 2 [1688/96 (1758%)]\tLoss: 0.078927\n",
      "Train Epoche: 2 [1689/96 (1759%)]\tLoss: 6.150398\n",
      "Train Epoche: 2 [1690/96 (1760%)]\tLoss: 11.838706\n",
      "Train Epoche: 2 [1691/96 (1761%)]\tLoss: 31.302053\n",
      "Train Epoche: 2 [1692/96 (1762%)]\tLoss: 1.158466\n",
      "Train Epoche: 2 [1693/96 (1764%)]\tLoss: 4.586731\n",
      "Train Epoche: 2 [1694/96 (1765%)]\tLoss: 33.016014\n",
      "Train Epoche: 2 [1695/96 (1766%)]\tLoss: 2.640288\n",
      "Train Epoche: 2 [1696/96 (1767%)]\tLoss: 7.056444\n",
      "Train Epoche: 2 [1697/96 (1768%)]\tLoss: 22.215689\n",
      "Train Epoche: 2 [1698/96 (1769%)]\tLoss: 52.445827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1699/96 (1770%)]\tLoss: 61.416145\n",
      "Train Epoche: 2 [1700/96 (1771%)]\tLoss: 11.087376\n",
      "Train Epoche: 2 [1701/96 (1772%)]\tLoss: 0.905348\n",
      "Train Epoche: 2 [1702/96 (1773%)]\tLoss: 17.170025\n",
      "Train Epoche: 2 [1703/96 (1774%)]\tLoss: 32.228947\n",
      "Train Epoche: 2 [1704/96 (1775%)]\tLoss: 4.153246\n",
      "Train Epoche: 2 [1705/96 (1776%)]\tLoss: 175.989227\n",
      "Train Epoche: 2 [1706/96 (1777%)]\tLoss: 1.029496\n",
      "Train Epoche: 2 [1707/96 (1778%)]\tLoss: 18.850256\n",
      "Train Epoche: 2 [1708/96 (1779%)]\tLoss: 27.880114\n",
      "Train Epoche: 2 [1709/96 (1780%)]\tLoss: 12.875502\n",
      "Train Epoche: 2 [1710/96 (1781%)]\tLoss: 0.650169\n",
      "Train Epoche: 2 [1711/96 (1782%)]\tLoss: 4.044662\n",
      "Train Epoche: 2 [1712/96 (1783%)]\tLoss: 3.221799\n",
      "Train Epoche: 2 [1713/96 (1784%)]\tLoss: 4.991562\n",
      "Train Epoche: 2 [1714/96 (1785%)]\tLoss: 1.642163\n",
      "Train Epoche: 2 [1715/96 (1786%)]\tLoss: 19.251869\n",
      "Train Epoche: 2 [1716/96 (1788%)]\tLoss: 17.701809\n",
      "Train Epoche: 2 [1717/96 (1789%)]\tLoss: 10.069178\n",
      "Train Epoche: 2 [1718/96 (1790%)]\tLoss: 4.363571\n",
      "Train Epoche: 2 [1719/96 (1791%)]\tLoss: 2.030420\n",
      "Train Epoche: 2 [1720/96 (1792%)]\tLoss: 76.704124\n",
      "Train Epoche: 2 [1721/96 (1793%)]\tLoss: 7.096010\n",
      "Train Epoche: 2 [1722/96 (1794%)]\tLoss: 1.053347\n",
      "Train Epoche: 2 [1723/96 (1795%)]\tLoss: 32.810230\n",
      "Train Epoche: 2 [1724/96 (1796%)]\tLoss: 0.988738\n",
      "Train Epoche: 2 [1725/96 (1797%)]\tLoss: 2.132904\n",
      "Train Epoche: 2 [1726/96 (1798%)]\tLoss: 14.815022\n",
      "Train Epoche: 2 [1727/96 (1799%)]\tLoss: 51.282600\n",
      "Train Epoche: 2 [1728/96 (1800%)]\tLoss: 1.257262\n",
      "Train Epoche: 2 [1729/96 (1801%)]\tLoss: 0.653734\n",
      "Train Epoche: 2 [1730/96 (1802%)]\tLoss: 69.026695\n",
      "Train Epoche: 2 [1731/96 (1803%)]\tLoss: 0.053589\n",
      "Train Epoche: 2 [1732/96 (1804%)]\tLoss: 0.866263\n",
      "Train Epoche: 2 [1733/96 (1805%)]\tLoss: 1.568218\n",
      "Train Epoche: 2 [1734/96 (1806%)]\tLoss: 25.511374\n",
      "Train Epoche: 2 [1735/96 (1807%)]\tLoss: 4.271085\n",
      "Train Epoche: 2 [1736/96 (1808%)]\tLoss: 4.102702\n",
      "Train Epoche: 2 [1737/96 (1809%)]\tLoss: 2.615764\n",
      "Train Epoche: 2 [1738/96 (1810%)]\tLoss: 7.443637\n",
      "Train Epoche: 2 [1739/96 (1811%)]\tLoss: 3.658891\n",
      "Train Epoche: 2 [1740/96 (1812%)]\tLoss: 0.055826\n",
      "Train Epoche: 2 [1741/96 (1814%)]\tLoss: 26.499163\n",
      "Train Epoche: 2 [1742/96 (1815%)]\tLoss: 29.699327\n",
      "Train Epoche: 2 [1743/96 (1816%)]\tLoss: 40.275536\n",
      "Train Epoche: 2 [1744/96 (1817%)]\tLoss: 0.371015\n",
      "Train Epoche: 2 [1745/96 (1818%)]\tLoss: 40.119743\n",
      "Train Epoche: 2 [1746/96 (1819%)]\tLoss: 8.407746\n",
      "Train Epoche: 2 [1747/96 (1820%)]\tLoss: 6.815230\n",
      "Train Epoche: 2 [1748/96 (1821%)]\tLoss: 6.609076\n",
      "Train Epoche: 2 [1749/96 (1822%)]\tLoss: 3.198455\n",
      "Train Epoche: 2 [1750/96 (1823%)]\tLoss: 0.467289\n",
      "Train Epoche: 2 [1751/96 (1824%)]\tLoss: 0.112901\n",
      "Train Epoche: 2 [1752/96 (1825%)]\tLoss: 10.940042\n",
      "Train Epoche: 2 [1753/96 (1826%)]\tLoss: 0.056226\n",
      "Train Epoche: 2 [1754/96 (1827%)]\tLoss: 61.558453\n",
      "Train Epoche: 2 [1755/96 (1828%)]\tLoss: 0.015703\n",
      "Train Epoche: 2 [1756/96 (1829%)]\tLoss: 47.735710\n",
      "Train Epoche: 2 [1757/96 (1830%)]\tLoss: 0.031676\n",
      "Train Epoche: 2 [1758/96 (1831%)]\tLoss: 22.110504\n",
      "Train Epoche: 2 [1759/96 (1832%)]\tLoss: 104.916336\n",
      "Train Epoche: 2 [1760/96 (1833%)]\tLoss: 0.838422\n",
      "Train Epoche: 2 [1761/96 (1834%)]\tLoss: 2.098897\n",
      "Train Epoche: 2 [1762/96 (1835%)]\tLoss: 4.102103\n",
      "Train Epoche: 2 [1763/96 (1836%)]\tLoss: 297.664307\n",
      "Train Epoche: 2 [1764/96 (1838%)]\tLoss: 148.475586\n",
      "Train Epoche: 2 [1765/96 (1839%)]\tLoss: 18.626564\n",
      "Train Epoche: 2 [1766/96 (1840%)]\tLoss: 27.668304\n",
      "Train Epoche: 2 [1767/96 (1841%)]\tLoss: 38.354698\n",
      "Train Epoche: 2 [1768/96 (1842%)]\tLoss: 7.220529\n",
      "Train Epoche: 2 [1769/96 (1843%)]\tLoss: 3.316607\n",
      "Train Epoche: 2 [1770/96 (1844%)]\tLoss: 43.246063\n",
      "Train Epoche: 2 [1771/96 (1845%)]\tLoss: 152.078247\n",
      "Train Epoche: 2 [1772/96 (1846%)]\tLoss: 1.800695\n",
      "Train Epoche: 2 [1773/96 (1847%)]\tLoss: 25.508945\n",
      "Train Epoche: 2 [1774/96 (1848%)]\tLoss: 1.175471\n",
      "Train Epoche: 2 [1775/96 (1849%)]\tLoss: 0.055877\n",
      "Train Epoche: 2 [1776/96 (1850%)]\tLoss: 0.401550\n",
      "Train Epoche: 2 [1777/96 (1851%)]\tLoss: 4.473795\n",
      "Train Epoche: 2 [1778/96 (1852%)]\tLoss: 2.762195\n",
      "Train Epoche: 2 [1779/96 (1853%)]\tLoss: 0.091587\n",
      "Train Epoche: 2 [1780/96 (1854%)]\tLoss: 5.759014\n",
      "Train Epoche: 2 [1781/96 (1855%)]\tLoss: 9.937704\n",
      "Train Epoche: 2 [1782/96 (1856%)]\tLoss: 0.331800\n",
      "Train Epoche: 2 [1783/96 (1857%)]\tLoss: 34.975956\n",
      "Train Epoche: 2 [1784/96 (1858%)]\tLoss: 11.860004\n",
      "Train Epoche: 2 [1785/96 (1859%)]\tLoss: 2.523314\n",
      "Train Epoche: 2 [1786/96 (1860%)]\tLoss: 3.066572\n",
      "Train Epoche: 2 [1787/96 (1861%)]\tLoss: 0.006976\n",
      "Train Epoche: 2 [1788/96 (1862%)]\tLoss: 25.744831\n",
      "Train Epoche: 2 [1789/96 (1864%)]\tLoss: 4.975306\n",
      "Train Epoche: 2 [1790/96 (1865%)]\tLoss: 1.871881\n",
      "Train Epoche: 2 [1791/96 (1866%)]\tLoss: 1.237247\n",
      "Train Epoche: 2 [1792/96 (1867%)]\tLoss: 5.821134\n",
      "Train Epoche: 2 [1793/96 (1868%)]\tLoss: 3.604496\n",
      "Train Epoche: 2 [1794/96 (1869%)]\tLoss: 1.333328\n",
      "Train Epoche: 2 [1795/96 (1870%)]\tLoss: 6.252127\n",
      "Train Epoche: 2 [1796/96 (1871%)]\tLoss: 10.211710\n",
      "Train Epoche: 2 [1797/96 (1872%)]\tLoss: 0.435104\n",
      "Train Epoche: 2 [1798/96 (1873%)]\tLoss: 0.139852\n",
      "Train Epoche: 2 [1799/96 (1874%)]\tLoss: 15.640723\n",
      "Train Epoche: 2 [1800/96 (1875%)]\tLoss: 0.604854\n",
      "Train Epoche: 2 [1801/96 (1876%)]\tLoss: 30.814201\n",
      "Train Epoche: 2 [1802/96 (1877%)]\tLoss: 4.427181\n",
      "Train Epoche: 2 [1803/96 (1878%)]\tLoss: 0.104682\n",
      "Train Epoche: 2 [1804/96 (1879%)]\tLoss: 44.298798\n",
      "Train Epoche: 2 [1805/96 (1880%)]\tLoss: 100.111649\n",
      "Train Epoche: 2 [1806/96 (1881%)]\tLoss: 0.573389\n",
      "Train Epoche: 2 [1807/96 (1882%)]\tLoss: 0.058125\n",
      "Train Epoche: 2 [1808/96 (1883%)]\tLoss: 0.001685\n",
      "Train Epoche: 2 [1809/96 (1884%)]\tLoss: 4.208184\n",
      "Train Epoche: 2 [1810/96 (1885%)]\tLoss: 65.824310\n",
      "Train Epoche: 2 [1811/96 (1886%)]\tLoss: 4.015037\n",
      "Train Epoche: 2 [1812/96 (1888%)]\tLoss: 0.068539\n",
      "Train Epoche: 2 [1813/96 (1889%)]\tLoss: 0.064753\n",
      "Train Epoche: 2 [1814/96 (1890%)]\tLoss: 0.419201\n",
      "Train Epoche: 2 [1815/96 (1891%)]\tLoss: 1.268331\n",
      "Train Epoche: 2 [1816/96 (1892%)]\tLoss: 0.439903\n",
      "Train Epoche: 2 [1817/96 (1893%)]\tLoss: 20.013340\n",
      "Train Epoche: 2 [1818/96 (1894%)]\tLoss: 15.567515\n",
      "Train Epoche: 2 [1819/96 (1895%)]\tLoss: 0.100043\n",
      "Train Epoche: 2 [1820/96 (1896%)]\tLoss: 192.495850\n",
      "Train Epoche: 2 [1821/96 (1897%)]\tLoss: 2.222619\n",
      "Train Epoche: 2 [1822/96 (1898%)]\tLoss: 1.484817\n",
      "Train Epoche: 2 [1823/96 (1899%)]\tLoss: 0.215221\n",
      "Train Epoche: 2 [1824/96 (1900%)]\tLoss: 8.395863\n",
      "Train Epoche: 2 [1825/96 (1901%)]\tLoss: 23.875397\n",
      "Train Epoche: 2 [1826/96 (1902%)]\tLoss: 58.318207\n",
      "Train Epoche: 2 [1827/96 (1903%)]\tLoss: 5.898512\n",
      "Train Epoche: 2 [1828/96 (1904%)]\tLoss: 0.936691\n",
      "Train Epoche: 2 [1829/96 (1905%)]\tLoss: 53.128735\n",
      "Train Epoche: 2 [1830/96 (1906%)]\tLoss: 2.240461\n",
      "Train Epoche: 2 [1831/96 (1907%)]\tLoss: 10.934768\n",
      "Train Epoche: 2 [1832/96 (1908%)]\tLoss: 25.409916\n",
      "Train Epoche: 2 [1833/96 (1909%)]\tLoss: 52.810345\n",
      "Train Epoche: 2 [1834/96 (1910%)]\tLoss: 5.915402\n",
      "Train Epoche: 2 [1835/96 (1911%)]\tLoss: 1.147154\n",
      "Train Epoche: 2 [1836/96 (1912%)]\tLoss: 0.082988\n",
      "Train Epoche: 2 [1837/96 (1914%)]\tLoss: 0.017618\n",
      "Train Epoche: 2 [1838/96 (1915%)]\tLoss: 0.035103\n",
      "Train Epoche: 2 [1839/96 (1916%)]\tLoss: 2.651031\n",
      "Train Epoche: 2 [1840/96 (1917%)]\tLoss: 2.179662\n",
      "Train Epoche: 2 [1841/96 (1918%)]\tLoss: 70.950874\n",
      "Train Epoche: 2 [1842/96 (1919%)]\tLoss: 5.273843\n",
      "Train Epoche: 2 [1843/96 (1920%)]\tLoss: 0.461552\n",
      "Train Epoche: 2 [1844/96 (1921%)]\tLoss: 43.256725\n",
      "Train Epoche: 2 [1845/96 (1922%)]\tLoss: 10.502221\n",
      "Train Epoche: 2 [1846/96 (1923%)]\tLoss: 1.340960\n",
      "Train Epoche: 2 [1847/96 (1924%)]\tLoss: 20.568369\n",
      "Train Epoche: 2 [1848/96 (1925%)]\tLoss: 12.393633\n",
      "Train Epoche: 2 [1849/96 (1926%)]\tLoss: 3.010484\n",
      "Train Epoche: 2 [1850/96 (1927%)]\tLoss: 2.740088\n",
      "Train Epoche: 2 [1851/96 (1928%)]\tLoss: 0.198131\n",
      "Train Epoche: 2 [1852/96 (1929%)]\tLoss: 14.225037\n",
      "Train Epoche: 2 [1853/96 (1930%)]\tLoss: 77.295303\n",
      "Train Epoche: 2 [1854/96 (1931%)]\tLoss: 174.382477\n",
      "Train Epoche: 2 [1855/96 (1932%)]\tLoss: 3.048067\n",
      "Train Epoche: 2 [1856/96 (1933%)]\tLoss: 6.046027\n",
      "Train Epoche: 2 [1857/96 (1934%)]\tLoss: 9.157868\n",
      "Train Epoche: 2 [1858/96 (1935%)]\tLoss: 8.867505\n",
      "Train Epoche: 2 [1859/96 (1936%)]\tLoss: 34.346458\n",
      "Train Epoche: 2 [1860/96 (1938%)]\tLoss: 6.354344\n",
      "Train Epoche: 2 [1861/96 (1939%)]\tLoss: 5.411603\n",
      "Train Epoche: 2 [1862/96 (1940%)]\tLoss: 3.299304\n",
      "Train Epoche: 2 [1863/96 (1941%)]\tLoss: 13.334934\n",
      "Train Epoche: 2 [1864/96 (1942%)]\tLoss: 0.367830\n",
      "Train Epoche: 2 [1865/96 (1943%)]\tLoss: 1.556987\n",
      "Train Epoche: 2 [1866/96 (1944%)]\tLoss: 21.382475\n",
      "Train Epoche: 2 [1867/96 (1945%)]\tLoss: 2.828919\n",
      "Train Epoche: 2 [1868/96 (1946%)]\tLoss: 1.386838\n",
      "Train Epoche: 2 [1869/96 (1947%)]\tLoss: 12.748963\n",
      "Train Epoche: 2 [1870/96 (1948%)]\tLoss: 0.414821\n",
      "Train Epoche: 2 [1871/96 (1949%)]\tLoss: 0.675944\n",
      "Train Epoche: 2 [1872/96 (1950%)]\tLoss: 1.155756\n",
      "Train Epoche: 2 [1873/96 (1951%)]\tLoss: 3.044619\n",
      "Train Epoche: 2 [1874/96 (1952%)]\tLoss: 0.020664\n",
      "Train Epoche: 2 [1875/96 (1953%)]\tLoss: 2.322315\n",
      "Train Epoche: 2 [1876/96 (1954%)]\tLoss: 0.916303\n",
      "Train Epoche: 2 [1877/96 (1955%)]\tLoss: 10.462064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1878/96 (1956%)]\tLoss: 15.261486\n",
      "Train Epoche: 2 [1879/96 (1957%)]\tLoss: 9.841978\n",
      "Train Epoche: 2 [1880/96 (1958%)]\tLoss: 3.542890\n",
      "Train Epoche: 2 [1881/96 (1959%)]\tLoss: 18.083773\n",
      "Train Epoche: 2 [1882/96 (1960%)]\tLoss: 5.765181\n",
      "Train Epoche: 2 [1883/96 (1961%)]\tLoss: 4.344650\n",
      "Train Epoche: 2 [1884/96 (1962%)]\tLoss: 251.877563\n",
      "Train Epoche: 2 [1885/96 (1964%)]\tLoss: 1.280093\n",
      "Train Epoche: 2 [1886/96 (1965%)]\tLoss: 4.111710\n",
      "Train Epoche: 2 [1887/96 (1966%)]\tLoss: 4.135889\n",
      "Train Epoche: 2 [1888/96 (1967%)]\tLoss: 1.515598\n",
      "Train Epoche: 2 [1889/96 (1968%)]\tLoss: 4.919001\n",
      "Train Epoche: 2 [1890/96 (1969%)]\tLoss: 0.360544\n",
      "Train Epoche: 2 [1891/96 (1970%)]\tLoss: 3.217561\n",
      "Train Epoche: 2 [1892/96 (1971%)]\tLoss: 0.115083\n",
      "Train Epoche: 2 [1893/96 (1972%)]\tLoss: 21.045416\n",
      "Train Epoche: 2 [1894/96 (1973%)]\tLoss: 0.524213\n",
      "Train Epoche: 2 [1895/96 (1974%)]\tLoss: 3.038578\n",
      "Train Epoche: 2 [1896/96 (1975%)]\tLoss: 44.079826\n",
      "Train Epoche: 2 [1897/96 (1976%)]\tLoss: 0.803464\n",
      "Train Epoche: 2 [1898/96 (1977%)]\tLoss: 2.041357\n",
      "Train Epoche: 2 [1899/96 (1978%)]\tLoss: 0.960048\n",
      "Train Epoche: 2 [1900/96 (1979%)]\tLoss: 6.939935\n",
      "Train Epoche: 2 [1901/96 (1980%)]\tLoss: 4.855954\n",
      "Train Epoche: 2 [1902/96 (1981%)]\tLoss: 11.698057\n",
      "Train Epoche: 2 [1903/96 (1982%)]\tLoss: 0.210525\n",
      "Train Epoche: 2 [1904/96 (1983%)]\tLoss: 6.659298\n",
      "Train Epoche: 2 [1905/96 (1984%)]\tLoss: 10.257239\n",
      "Train Epoche: 2 [1906/96 (1985%)]\tLoss: 10.408822\n",
      "Train Epoche: 2 [1907/96 (1986%)]\tLoss: 0.002996\n",
      "Train Epoche: 2 [1908/96 (1988%)]\tLoss: 5.839235\n",
      "Train Epoche: 2 [1909/96 (1989%)]\tLoss: 0.249938\n",
      "Train Epoche: 2 [1910/96 (1990%)]\tLoss: 57.098782\n",
      "Train Epoche: 2 [1911/96 (1991%)]\tLoss: 0.187842\n",
      "Train Epoche: 2 [1912/96 (1992%)]\tLoss: 43.296425\n",
      "Train Epoche: 2 [1913/96 (1993%)]\tLoss: 9.907723\n",
      "Train Epoche: 2 [1914/96 (1994%)]\tLoss: 5.158021\n",
      "Train Epoche: 2 [1915/96 (1995%)]\tLoss: 14.110635\n",
      "Train Epoche: 2 [1916/96 (1996%)]\tLoss: 0.013184\n",
      "Train Epoche: 2 [1917/96 (1997%)]\tLoss: 0.108643\n",
      "Train Epoche: 2 [1918/96 (1998%)]\tLoss: 2.260745\n",
      "Train Epoche: 2 [1919/96 (1999%)]\tLoss: 1.618286\n",
      "Train Epoche: 2 [1920/96 (2000%)]\tLoss: 3.351557\n",
      "Train Epoche: 2 [1921/96 (2001%)]\tLoss: 2.992221\n",
      "Train Epoche: 2 [1922/96 (2002%)]\tLoss: 0.855467\n",
      "Train Epoche: 2 [1923/96 (2003%)]\tLoss: 17.346827\n",
      "Train Epoche: 2 [1924/96 (2004%)]\tLoss: 189.724991\n",
      "Train Epoche: 2 [1925/96 (2005%)]\tLoss: 3.042033\n",
      "Train Epoche: 2 [1926/96 (2006%)]\tLoss: 2.264770\n",
      "Train Epoche: 2 [1927/96 (2007%)]\tLoss: 5.058003\n",
      "Train Epoche: 2 [1928/96 (2008%)]\tLoss: 20.509789\n",
      "Train Epoche: 2 [1929/96 (2009%)]\tLoss: 19.537159\n",
      "Train Epoche: 2 [1930/96 (2010%)]\tLoss: 2.132377\n",
      "Train Epoche: 2 [1931/96 (2011%)]\tLoss: 0.360145\n",
      "Train Epoche: 2 [1932/96 (2012%)]\tLoss: 8.866102\n",
      "Train Epoche: 2 [1933/96 (2014%)]\tLoss: 0.828732\n",
      "Train Epoche: 2 [1934/96 (2015%)]\tLoss: 0.029354\n",
      "Train Epoche: 2 [1935/96 (2016%)]\tLoss: 5.346686\n",
      "Train Epoche: 2 [1936/96 (2017%)]\tLoss: 1.363768\n",
      "Train Epoche: 2 [1937/96 (2018%)]\tLoss: 167.902435\n",
      "Train Epoche: 2 [1938/96 (2019%)]\tLoss: 4.035100\n",
      "Train Epoche: 2 [1939/96 (2020%)]\tLoss: 13.741437\n",
      "Train Epoche: 2 [1940/96 (2021%)]\tLoss: 18.205664\n",
      "Train Epoche: 2 [1941/96 (2022%)]\tLoss: 3.786137\n",
      "Train Epoche: 2 [1942/96 (2023%)]\tLoss: 1.243365\n",
      "Train Epoche: 2 [1943/96 (2024%)]\tLoss: 1.634956\n",
      "Train Epoche: 2 [1944/96 (2025%)]\tLoss: 0.487157\n",
      "Train Epoche: 2 [1945/96 (2026%)]\tLoss: 1.277773\n",
      "Train Epoche: 2 [1946/96 (2027%)]\tLoss: 0.065966\n",
      "Train Epoche: 2 [1947/96 (2028%)]\tLoss: 3.519932\n",
      "Train Epoche: 2 [1948/96 (2029%)]\tLoss: 20.759306\n",
      "Train Epoche: 2 [1949/96 (2030%)]\tLoss: 0.153791\n",
      "Train Epoche: 2 [1950/96 (2031%)]\tLoss: 0.614428\n",
      "Train Epoche: 2 [1951/96 (2032%)]\tLoss: 2.095900\n",
      "Train Epoche: 2 [1952/96 (2033%)]\tLoss: 10.519190\n",
      "Train Epoche: 2 [1953/96 (2034%)]\tLoss: 0.020907\n",
      "Train Epoche: 2 [1954/96 (2035%)]\tLoss: 0.651117\n",
      "Train Epoche: 2 [1955/96 (2036%)]\tLoss: 72.003120\n",
      "Train Epoche: 2 [1956/96 (2038%)]\tLoss: 27.856880\n",
      "Train Epoche: 2 [1957/96 (2039%)]\tLoss: 4.499354\n",
      "Train Epoche: 2 [1958/96 (2040%)]\tLoss: 5.661123\n",
      "Train Epoche: 2 [1959/96 (2041%)]\tLoss: 0.036019\n",
      "Train Epoche: 2 [1960/96 (2042%)]\tLoss: 13.902845\n",
      "Train Epoche: 2 [1961/96 (2043%)]\tLoss: 28.888390\n",
      "Train Epoche: 2 [1962/96 (2044%)]\tLoss: 1.955379\n",
      "Train Epoche: 2 [1963/96 (2045%)]\tLoss: 65.662758\n",
      "Train Epoche: 2 [1964/96 (2046%)]\tLoss: 26.347311\n",
      "Train Epoche: 2 [1965/96 (2047%)]\tLoss: 0.000334\n",
      "Train Epoche: 2 [1966/96 (2048%)]\tLoss: 5.728111\n",
      "Train Epoche: 2 [1967/96 (2049%)]\tLoss: 41.691761\n",
      "Train Epoche: 2 [1968/96 (2050%)]\tLoss: 0.152995\n",
      "Train Epoche: 2 [1969/96 (2051%)]\tLoss: 4.833426\n",
      "Train Epoche: 2 [1970/96 (2052%)]\tLoss: 1.888787\n",
      "Train Epoche: 2 [1971/96 (2053%)]\tLoss: 3.267548\n",
      "Train Epoche: 2 [1972/96 (2054%)]\tLoss: 17.572851\n",
      "Train Epoche: 2 [1973/96 (2055%)]\tLoss: 9.476885\n",
      "Train Epoche: 2 [1974/96 (2056%)]\tLoss: 66.270042\n",
      "Train Epoche: 2 [1975/96 (2057%)]\tLoss: 0.880958\n",
      "Train Epoche: 2 [1976/96 (2058%)]\tLoss: 3.169548\n",
      "Train Epoche: 2 [1977/96 (2059%)]\tLoss: 18.376911\n",
      "Train Epoche: 2 [1978/96 (2060%)]\tLoss: 6.531568\n",
      "Train Epoche: 2 [1979/96 (2061%)]\tLoss: 3.055364\n",
      "Train Epoche: 2 [1980/96 (2062%)]\tLoss: 11.537052\n",
      "Train Epoche: 2 [1981/96 (2064%)]\tLoss: 10.170762\n",
      "Train Epoche: 2 [1982/96 (2065%)]\tLoss: 3.113129\n",
      "Train Epoche: 2 [1983/96 (2066%)]\tLoss: 13.388835\n",
      "Train Epoche: 2 [1984/96 (2067%)]\tLoss: 4.616283\n",
      "Train Epoche: 2 [1985/96 (2068%)]\tLoss: 23.891094\n",
      "Train Epoche: 2 [1986/96 (2069%)]\tLoss: 7.451359\n",
      "Train Epoche: 2 [1987/96 (2070%)]\tLoss: 2.045787\n",
      "Train Epoche: 2 [1988/96 (2071%)]\tLoss: 1.612003\n",
      "Train Epoche: 2 [1989/96 (2072%)]\tLoss: 94.653923\n",
      "Train Epoche: 2 [1990/96 (2073%)]\tLoss: 0.114289\n",
      "Train Epoche: 2 [1991/96 (2074%)]\tLoss: 7.435612\n",
      "Train Epoche: 2 [1992/96 (2075%)]\tLoss: 30.044628\n",
      "Train Epoche: 2 [1993/96 (2076%)]\tLoss: 0.788611\n",
      "Train Epoche: 2 [1994/96 (2077%)]\tLoss: 16.646776\n",
      "Train Epoche: 2 [1995/96 (2078%)]\tLoss: 0.116596\n",
      "Train Epoche: 2 [1996/96 (2079%)]\tLoss: 0.871321\n",
      "Train Epoche: 2 [1997/96 (2080%)]\tLoss: 1.245337\n",
      "Train Epoche: 2 [1998/96 (2081%)]\tLoss: 0.693954\n",
      "Train Epoche: 2 [1999/96 (2082%)]\tLoss: 1.465799\n",
      "Train Epoche: 2 [2000/96 (2083%)]\tLoss: 0.036571\n",
      "Train Epoche: 2 [2001/96 (2084%)]\tLoss: 24.313885\n",
      "Train Epoche: 2 [2002/96 (2085%)]\tLoss: 6.084343\n",
      "Train Epoche: 2 [2003/96 (2086%)]\tLoss: 11.830015\n",
      "Train Epoche: 2 [2004/96 (2088%)]\tLoss: 32.329109\n",
      "Train Epoche: 2 [2005/96 (2089%)]\tLoss: 0.424215\n",
      "Train Epoche: 2 [2006/96 (2090%)]\tLoss: 20.501703\n",
      "Train Epoche: 2 [2007/96 (2091%)]\tLoss: 16.733391\n",
      "Train Epoche: 2 [2008/96 (2092%)]\tLoss: 21.056730\n",
      "Train Epoche: 2 [2009/96 (2093%)]\tLoss: 0.544350\n",
      "Train Epoche: 2 [2010/96 (2094%)]\tLoss: 8.347379\n",
      "Train Epoche: 2 [2011/96 (2095%)]\tLoss: 16.780588\n",
      "Train Epoche: 2 [2012/96 (2096%)]\tLoss: 3.010512\n",
      "Train Epoche: 2 [2013/96 (2097%)]\tLoss: 3.504700\n",
      "Train Epoche: 2 [2014/96 (2098%)]\tLoss: 29.230963\n",
      "Train Epoche: 2 [2015/96 (2099%)]\tLoss: 1.751730\n",
      "Train Epoche: 2 [2016/96 (2100%)]\tLoss: 0.528280\n",
      "Train Epoche: 2 [2017/96 (2101%)]\tLoss: 1.475860\n",
      "Train Epoche: 2 [2018/96 (2102%)]\tLoss: 24.178968\n",
      "Train Epoche: 2 [2019/96 (2103%)]\tLoss: 1.652078\n",
      "Train Epoche: 2 [2020/96 (2104%)]\tLoss: 56.046631\n",
      "Train Epoche: 2 [2021/96 (2105%)]\tLoss: 0.992655\n",
      "Train Epoche: 2 [2022/96 (2106%)]\tLoss: 52.545216\n",
      "Train Epoche: 2 [2023/96 (2107%)]\tLoss: 0.760068\n",
      "Train Epoche: 2 [2024/96 (2108%)]\tLoss: 8.333971\n",
      "Train Epoche: 2 [2025/96 (2109%)]\tLoss: 3.850271\n",
      "Train Epoche: 2 [2026/96 (2110%)]\tLoss: 65.155899\n",
      "Train Epoche: 3 [0/96 (0%)]\tLoss: 16.084784\n",
      "Train Epoche: 3 [1/96 (1%)]\tLoss: 4.951727\n",
      "Train Epoche: 3 [2/96 (2%)]\tLoss: 22.950920\n",
      "Train Epoche: 3 [3/96 (3%)]\tLoss: 23.973635\n",
      "Train Epoche: 3 [4/96 (4%)]\tLoss: 116.706406\n",
      "Train Epoche: 3 [5/96 (5%)]\tLoss: 3.460510\n",
      "Train Epoche: 3 [6/96 (6%)]\tLoss: 5.858889\n",
      "Train Epoche: 3 [7/96 (7%)]\tLoss: 36.337059\n",
      "Train Epoche: 3 [8/96 (8%)]\tLoss: 34.459820\n",
      "Train Epoche: 3 [9/96 (9%)]\tLoss: 3.828001\n",
      "Train Epoche: 3 [10/96 (10%)]\tLoss: 246.954697\n",
      "Train Epoche: 3 [11/96 (11%)]\tLoss: 64.796227\n",
      "Train Epoche: 3 [12/96 (12%)]\tLoss: 9.699790\n",
      "Train Epoche: 3 [13/96 (14%)]\tLoss: 0.084973\n",
      "Train Epoche: 3 [14/96 (15%)]\tLoss: 2.744854\n",
      "Train Epoche: 3 [15/96 (16%)]\tLoss: 2.745866\n",
      "Train Epoche: 3 [16/96 (17%)]\tLoss: 1.543106\n",
      "Train Epoche: 3 [17/96 (18%)]\tLoss: 12.120183\n",
      "Train Epoche: 3 [18/96 (19%)]\tLoss: 0.936526\n",
      "Train Epoche: 3 [19/96 (20%)]\tLoss: 19.510017\n",
      "Train Epoche: 3 [20/96 (21%)]\tLoss: 30.100706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [21/96 (22%)]\tLoss: 20.640602\n",
      "Train Epoche: 3 [22/96 (23%)]\tLoss: 13.625072\n",
      "Train Epoche: 3 [23/96 (24%)]\tLoss: 8.224669\n",
      "Train Epoche: 3 [24/96 (25%)]\tLoss: 153.626419\n",
      "Train Epoche: 3 [25/96 (26%)]\tLoss: 6.282115\n",
      "Train Epoche: 3 [26/96 (27%)]\tLoss: 30.770214\n",
      "Train Epoche: 3 [27/96 (28%)]\tLoss: 7.729494\n",
      "Train Epoche: 3 [28/96 (29%)]\tLoss: 0.505126\n",
      "Train Epoche: 3 [29/96 (30%)]\tLoss: 36.211178\n",
      "Train Epoche: 3 [30/96 (31%)]\tLoss: 13.951006\n",
      "Train Epoche: 3 [31/96 (32%)]\tLoss: 37.379414\n",
      "Train Epoche: 3 [32/96 (33%)]\tLoss: 7.164746\n",
      "Train Epoche: 3 [33/96 (34%)]\tLoss: 1.181179\n",
      "Train Epoche: 3 [34/96 (35%)]\tLoss: 0.502446\n",
      "Train Epoche: 3 [35/96 (36%)]\tLoss: 15.106301\n",
      "Train Epoche: 3 [36/96 (38%)]\tLoss: 0.758628\n",
      "Train Epoche: 3 [37/96 (39%)]\tLoss: 0.049217\n",
      "Train Epoche: 3 [38/96 (40%)]\tLoss: 9.998297\n",
      "Train Epoche: 3 [39/96 (41%)]\tLoss: 61.839771\n",
      "Train Epoche: 3 [40/96 (42%)]\tLoss: 13.173488\n",
      "Train Epoche: 3 [41/96 (43%)]\tLoss: 0.000001\n",
      "Train Epoche: 3 [42/96 (44%)]\tLoss: 6.546348\n",
      "Train Epoche: 3 [43/96 (45%)]\tLoss: 17.183416\n",
      "Train Epoche: 3 [44/96 (46%)]\tLoss: 377.077667\n",
      "Train Epoche: 3 [45/96 (47%)]\tLoss: 1.818297\n",
      "Train Epoche: 3 [46/96 (48%)]\tLoss: 5.142776\n",
      "Train Epoche: 3 [47/96 (49%)]\tLoss: 0.019359\n",
      "Train Epoche: 3 [48/96 (50%)]\tLoss: 0.707550\n",
      "Train Epoche: 3 [49/96 (51%)]\tLoss: 0.235950\n",
      "Train Epoche: 3 [50/96 (52%)]\tLoss: 49.107082\n",
      "Train Epoche: 3 [51/96 (53%)]\tLoss: 2.266915\n",
      "Train Epoche: 3 [52/96 (54%)]\tLoss: 0.498748\n",
      "Train Epoche: 3 [53/96 (55%)]\tLoss: 3.720728\n",
      "Train Epoche: 3 [54/96 (56%)]\tLoss: 13.858638\n",
      "Train Epoche: 3 [55/96 (57%)]\tLoss: 1.126650\n",
      "Train Epoche: 3 [56/96 (58%)]\tLoss: 0.335725\n",
      "Train Epoche: 3 [57/96 (59%)]\tLoss: 10.670555\n",
      "Train Epoche: 3 [58/96 (60%)]\tLoss: 0.265314\n",
      "Train Epoche: 3 [59/96 (61%)]\tLoss: 1.058326\n",
      "Train Epoche: 3 [60/96 (62%)]\tLoss: 27.995594\n",
      "Train Epoche: 3 [61/96 (64%)]\tLoss: 0.690983\n",
      "Train Epoche: 3 [62/96 (65%)]\tLoss: 3.217070\n",
      "Train Epoche: 3 [63/96 (66%)]\tLoss: 4.670949\n",
      "Train Epoche: 3 [64/96 (67%)]\tLoss: 0.443828\n",
      "Train Epoche: 3 [65/96 (68%)]\tLoss: 6.294390\n",
      "Train Epoche: 3 [66/96 (69%)]\tLoss: 54.843796\n",
      "Train Epoche: 3 [67/96 (70%)]\tLoss: 0.033130\n",
      "Train Epoche: 3 [68/96 (71%)]\tLoss: 8.377265\n",
      "Train Epoche: 3 [69/96 (72%)]\tLoss: 0.481216\n",
      "Train Epoche: 3 [70/96 (73%)]\tLoss: 80.875336\n",
      "Train Epoche: 3 [71/96 (74%)]\tLoss: 0.827113\n",
      "Train Epoche: 3 [72/96 (75%)]\tLoss: 9.934373\n",
      "Train Epoche: 3 [73/96 (76%)]\tLoss: 0.000032\n",
      "Train Epoche: 3 [74/96 (77%)]\tLoss: 7.931403\n",
      "Train Epoche: 3 [75/96 (78%)]\tLoss: 1.091091\n",
      "Train Epoche: 3 [76/96 (79%)]\tLoss: 28.818302\n",
      "Train Epoche: 3 [77/96 (80%)]\tLoss: 7.078710\n",
      "Train Epoche: 3 [78/96 (81%)]\tLoss: 1.508700\n",
      "Train Epoche: 3 [79/96 (82%)]\tLoss: 0.005478\n",
      "Train Epoche: 3 [80/96 (83%)]\tLoss: 43.630630\n",
      "Train Epoche: 3 [81/96 (84%)]\tLoss: 5.016257\n",
      "Train Epoche: 3 [82/96 (85%)]\tLoss: 3.113822\n",
      "Train Epoche: 3 [83/96 (86%)]\tLoss: 22.947819\n",
      "Train Epoche: 3 [84/96 (88%)]\tLoss: 8.600624\n",
      "Train Epoche: 3 [85/96 (89%)]\tLoss: 24.814474\n",
      "Train Epoche: 3 [86/96 (90%)]\tLoss: 43.634914\n",
      "Train Epoche: 3 [87/96 (91%)]\tLoss: 14.209431\n",
      "Train Epoche: 3 [88/96 (92%)]\tLoss: 25.545576\n",
      "Train Epoche: 3 [89/96 (93%)]\tLoss: 0.502570\n",
      "Train Epoche: 3 [90/96 (94%)]\tLoss: 0.672211\n",
      "Train Epoche: 3 [91/96 (95%)]\tLoss: 0.148608\n",
      "Train Epoche: 3 [92/96 (96%)]\tLoss: 1.500502\n",
      "Train Epoche: 3 [93/96 (97%)]\tLoss: 27.117212\n",
      "Train Epoche: 3 [94/96 (98%)]\tLoss: 2.181259\n",
      "Train Epoche: 3 [95/96 (99%)]\tLoss: 67.353233\n",
      "Train Epoche: 3 [96/96 (100%)]\tLoss: 80.072083\n",
      "Train Epoche: 3 [97/96 (101%)]\tLoss: 146.779648\n",
      "Train Epoche: 3 [98/96 (102%)]\tLoss: 30.743614\n",
      "Train Epoche: 3 [99/96 (103%)]\tLoss: 122.079491\n",
      "Train Epoche: 3 [100/96 (104%)]\tLoss: 3.426946\n",
      "Train Epoche: 3 [101/96 (105%)]\tLoss: 22.174772\n",
      "Train Epoche: 3 [102/96 (106%)]\tLoss: 1.164141\n",
      "Train Epoche: 3 [103/96 (107%)]\tLoss: 13.983761\n",
      "Train Epoche: 3 [104/96 (108%)]\tLoss: 0.688856\n",
      "Train Epoche: 3 [105/96 (109%)]\tLoss: 2.092340\n",
      "Train Epoche: 3 [106/96 (110%)]\tLoss: 0.019159\n",
      "Train Epoche: 3 [107/96 (111%)]\tLoss: 39.959225\n",
      "Train Epoche: 3 [108/96 (112%)]\tLoss: 8.995000\n",
      "Train Epoche: 3 [109/96 (114%)]\tLoss: 3.642115\n",
      "Train Epoche: 3 [110/96 (115%)]\tLoss: 0.620497\n",
      "Train Epoche: 3 [111/96 (116%)]\tLoss: 2.286854\n",
      "Train Epoche: 3 [112/96 (117%)]\tLoss: 0.540387\n",
      "Train Epoche: 3 [113/96 (118%)]\tLoss: 0.831669\n",
      "Train Epoche: 3 [114/96 (119%)]\tLoss: 123.645042\n",
      "Train Epoche: 3 [115/96 (120%)]\tLoss: 40.861225\n",
      "Train Epoche: 3 [116/96 (121%)]\tLoss: 4.143262\n",
      "Train Epoche: 3 [117/96 (122%)]\tLoss: 8.843060\n",
      "Train Epoche: 3 [118/96 (123%)]\tLoss: 0.976472\n",
      "Train Epoche: 3 [119/96 (124%)]\tLoss: 0.063565\n",
      "Train Epoche: 3 [120/96 (125%)]\tLoss: 17.151869\n",
      "Train Epoche: 3 [121/96 (126%)]\tLoss: 13.764493\n",
      "Train Epoche: 3 [122/96 (127%)]\tLoss: 6.356792\n",
      "Train Epoche: 3 [123/96 (128%)]\tLoss: 14.312770\n",
      "Train Epoche: 3 [124/96 (129%)]\tLoss: 0.099975\n",
      "Train Epoche: 3 [125/96 (130%)]\tLoss: 16.701355\n",
      "Train Epoche: 3 [126/96 (131%)]\tLoss: 0.020132\n",
      "Train Epoche: 3 [127/96 (132%)]\tLoss: 13.065962\n",
      "Train Epoche: 3 [128/96 (133%)]\tLoss: 82.292679\n",
      "Train Epoche: 3 [129/96 (134%)]\tLoss: 4.887701\n",
      "Train Epoche: 3 [130/96 (135%)]\tLoss: 19.446484\n",
      "Train Epoche: 3 [131/96 (136%)]\tLoss: 2.132591\n",
      "Train Epoche: 3 [132/96 (138%)]\tLoss: 10.020032\n",
      "Train Epoche: 3 [133/96 (139%)]\tLoss: 0.173410\n",
      "Train Epoche: 3 [134/96 (140%)]\tLoss: 7.067073\n",
      "Train Epoche: 3 [135/96 (141%)]\tLoss: 143.856659\n",
      "Train Epoche: 3 [136/96 (142%)]\tLoss: 1.161680\n",
      "Train Epoche: 3 [137/96 (143%)]\tLoss: 0.708921\n",
      "Train Epoche: 3 [138/96 (144%)]\tLoss: 0.107225\n",
      "Train Epoche: 3 [139/96 (145%)]\tLoss: 52.984249\n",
      "Train Epoche: 3 [140/96 (146%)]\tLoss: 20.402868\n",
      "Train Epoche: 3 [141/96 (147%)]\tLoss: 0.151321\n",
      "Train Epoche: 3 [142/96 (148%)]\tLoss: 38.709274\n",
      "Train Epoche: 3 [143/96 (149%)]\tLoss: 1.266926\n",
      "Train Epoche: 3 [144/96 (150%)]\tLoss: 75.605736\n",
      "Train Epoche: 3 [145/96 (151%)]\tLoss: 6.108376\n",
      "Train Epoche: 3 [146/96 (152%)]\tLoss: 0.212368\n",
      "Train Epoche: 3 [147/96 (153%)]\tLoss: 5.016603\n",
      "Train Epoche: 3 [148/96 (154%)]\tLoss: 397.564148\n",
      "Train Epoche: 3 [149/96 (155%)]\tLoss: 0.036361\n",
      "Train Epoche: 3 [150/96 (156%)]\tLoss: 1.492518\n",
      "Train Epoche: 3 [151/96 (157%)]\tLoss: 0.015145\n",
      "Train Epoche: 3 [152/96 (158%)]\tLoss: 2.520915\n",
      "Train Epoche: 3 [153/96 (159%)]\tLoss: 11.086435\n",
      "Train Epoche: 3 [154/96 (160%)]\tLoss: 26.931463\n",
      "Train Epoche: 3 [155/96 (161%)]\tLoss: 0.069645\n",
      "Train Epoche: 3 [156/96 (162%)]\tLoss: 58.952656\n",
      "Train Epoche: 3 [157/96 (164%)]\tLoss: 2.368408\n",
      "Train Epoche: 3 [158/96 (165%)]\tLoss: 2.324585\n",
      "Train Epoche: 3 [159/96 (166%)]\tLoss: 92.957520\n",
      "Train Epoche: 3 [160/96 (167%)]\tLoss: 1.389049\n",
      "Train Epoche: 3 [161/96 (168%)]\tLoss: 8.860474\n",
      "Train Epoche: 3 [162/96 (169%)]\tLoss: 4.743757\n",
      "Train Epoche: 3 [163/96 (170%)]\tLoss: 0.656808\n",
      "Train Epoche: 3 [164/96 (171%)]\tLoss: 3.220110\n",
      "Train Epoche: 3 [165/96 (172%)]\tLoss: 0.309108\n",
      "Train Epoche: 3 [166/96 (173%)]\tLoss: 51.586182\n",
      "Train Epoche: 3 [167/96 (174%)]\tLoss: 56.774353\n",
      "Train Epoche: 3 [168/96 (175%)]\tLoss: 1.755685\n",
      "Train Epoche: 3 [169/96 (176%)]\tLoss: 0.150258\n",
      "Train Epoche: 3 [170/96 (177%)]\tLoss: 0.277788\n",
      "Train Epoche: 3 [171/96 (178%)]\tLoss: 0.004975\n",
      "Train Epoche: 3 [172/96 (179%)]\tLoss: 1.463567\n",
      "Train Epoche: 3 [173/96 (180%)]\tLoss: 4.000282\n",
      "Train Epoche: 3 [174/96 (181%)]\tLoss: 0.850752\n",
      "Train Epoche: 3 [175/96 (182%)]\tLoss: 7.543964\n",
      "Train Epoche: 3 [176/96 (183%)]\tLoss: 1.525222\n",
      "Train Epoche: 3 [177/96 (184%)]\tLoss: 6.220143\n",
      "Train Epoche: 3 [178/96 (185%)]\tLoss: 0.125966\n",
      "Train Epoche: 3 [179/96 (186%)]\tLoss: 1.175506\n",
      "Train Epoche: 3 [180/96 (188%)]\tLoss: 3.229096\n",
      "Train Epoche: 3 [181/96 (189%)]\tLoss: 0.363903\n",
      "Train Epoche: 3 [182/96 (190%)]\tLoss: 1.624775\n",
      "Train Epoche: 3 [183/96 (191%)]\tLoss: 25.431246\n",
      "Train Epoche: 3 [184/96 (192%)]\tLoss: 4.025359\n",
      "Train Epoche: 3 [185/96 (193%)]\tLoss: 1.101732\n",
      "Train Epoche: 3 [186/96 (194%)]\tLoss: 3.519174\n",
      "Train Epoche: 3 [187/96 (195%)]\tLoss: 0.180748\n",
      "Train Epoche: 3 [188/96 (196%)]\tLoss: 3.007011\n",
      "Train Epoche: 3 [189/96 (197%)]\tLoss: 2.640216\n",
      "Train Epoche: 3 [190/96 (198%)]\tLoss: 11.026904\n",
      "Train Epoche: 3 [191/96 (199%)]\tLoss: 27.554600\n",
      "Train Epoche: 3 [192/96 (200%)]\tLoss: 0.000207\n",
      "Train Epoche: 3 [193/96 (201%)]\tLoss: 0.267173\n",
      "Train Epoche: 3 [194/96 (202%)]\tLoss: 0.429816\n",
      "Train Epoche: 3 [195/96 (203%)]\tLoss: 1.905867\n",
      "Train Epoche: 3 [196/96 (204%)]\tLoss: 0.233279\n",
      "Train Epoche: 3 [197/96 (205%)]\tLoss: 3.362489\n",
      "Train Epoche: 3 [198/96 (206%)]\tLoss: 11.204204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [199/96 (207%)]\tLoss: 0.092890\n",
      "Train Epoche: 3 [200/96 (208%)]\tLoss: 2.730675\n",
      "Train Epoche: 3 [201/96 (209%)]\tLoss: 3.446616\n",
      "Train Epoche: 3 [202/96 (210%)]\tLoss: 0.205966\n",
      "Train Epoche: 3 [203/96 (211%)]\tLoss: 2.782422\n",
      "Train Epoche: 3 [204/96 (212%)]\tLoss: 0.068024\n",
      "Train Epoche: 3 [205/96 (214%)]\tLoss: 0.940577\n",
      "Train Epoche: 3 [206/96 (215%)]\tLoss: 39.759487\n",
      "Train Epoche: 3 [207/96 (216%)]\tLoss: 0.189861\n",
      "Train Epoche: 3 [208/96 (217%)]\tLoss: 0.300976\n",
      "Train Epoche: 3 [209/96 (218%)]\tLoss: 123.496628\n",
      "Train Epoche: 3 [210/96 (219%)]\tLoss: 0.318609\n",
      "Train Epoche: 3 [211/96 (220%)]\tLoss: 5.978163\n",
      "Train Epoche: 3 [212/96 (221%)]\tLoss: 8.269387\n",
      "Train Epoche: 3 [213/96 (222%)]\tLoss: 5.369357\n",
      "Train Epoche: 3 [214/96 (223%)]\tLoss: 0.792727\n",
      "Train Epoche: 3 [215/96 (224%)]\tLoss: 0.205927\n",
      "Train Epoche: 3 [216/96 (225%)]\tLoss: 0.877907\n",
      "Train Epoche: 3 [217/96 (226%)]\tLoss: 9.111894\n",
      "Train Epoche: 3 [218/96 (227%)]\tLoss: 0.095055\n",
      "Train Epoche: 3 [219/96 (228%)]\tLoss: 0.942637\n",
      "Train Epoche: 3 [220/96 (229%)]\tLoss: 2.171140\n",
      "Train Epoche: 3 [221/96 (230%)]\tLoss: 12.360404\n",
      "Train Epoche: 3 [222/96 (231%)]\tLoss: 5.220736\n",
      "Train Epoche: 3 [223/96 (232%)]\tLoss: 52.569717\n",
      "Train Epoche: 3 [224/96 (233%)]\tLoss: 7.840006\n",
      "Train Epoche: 3 [225/96 (234%)]\tLoss: 9.638585\n",
      "Train Epoche: 3 [226/96 (235%)]\tLoss: 9.693993\n",
      "Train Epoche: 3 [227/96 (236%)]\tLoss: 0.025007\n",
      "Train Epoche: 3 [228/96 (238%)]\tLoss: 0.118414\n",
      "Train Epoche: 3 [229/96 (239%)]\tLoss: 0.328262\n",
      "Train Epoche: 3 [230/96 (240%)]\tLoss: 14.380680\n",
      "Train Epoche: 3 [231/96 (241%)]\tLoss: 265.991058\n",
      "Train Epoche: 3 [232/96 (242%)]\tLoss: 5.217111\n",
      "Train Epoche: 3 [233/96 (243%)]\tLoss: 7.063950\n",
      "Train Epoche: 3 [234/96 (244%)]\tLoss: 34.615067\n",
      "Train Epoche: 3 [235/96 (245%)]\tLoss: 14.097100\n",
      "Train Epoche: 3 [236/96 (246%)]\tLoss: 270.661316\n",
      "Train Epoche: 3 [237/96 (247%)]\tLoss: 0.523071\n",
      "Train Epoche: 3 [238/96 (248%)]\tLoss: 2.346438\n",
      "Train Epoche: 3 [239/96 (249%)]\tLoss: 4.652376\n",
      "Train Epoche: 3 [240/96 (250%)]\tLoss: 4.314549\n",
      "Train Epoche: 3 [241/96 (251%)]\tLoss: 0.953434\n",
      "Train Epoche: 3 [242/96 (252%)]\tLoss: 0.000943\n",
      "Train Epoche: 3 [243/96 (253%)]\tLoss: 0.706429\n",
      "Train Epoche: 3 [244/96 (254%)]\tLoss: 8.673456\n",
      "Train Epoche: 3 [245/96 (255%)]\tLoss: 3.429707\n",
      "Train Epoche: 3 [246/96 (256%)]\tLoss: 17.152401\n",
      "Train Epoche: 3 [247/96 (257%)]\tLoss: 4.663424\n",
      "Train Epoche: 3 [248/96 (258%)]\tLoss: 6.769219\n",
      "Train Epoche: 3 [249/96 (259%)]\tLoss: 0.357305\n",
      "Train Epoche: 3 [250/96 (260%)]\tLoss: 21.686207\n",
      "Train Epoche: 3 [251/96 (261%)]\tLoss: 3.469106\n",
      "Train Epoche: 3 [252/96 (262%)]\tLoss: 5.893366\n",
      "Train Epoche: 3 [253/96 (264%)]\tLoss: 4.586625\n",
      "Train Epoche: 3 [254/96 (265%)]\tLoss: 1.300638\n",
      "Train Epoche: 3 [255/96 (266%)]\tLoss: 1.100827\n",
      "Train Epoche: 3 [256/96 (267%)]\tLoss: 0.185726\n",
      "Train Epoche: 3 [257/96 (268%)]\tLoss: 0.133292\n",
      "Train Epoche: 3 [258/96 (269%)]\tLoss: 11.703368\n",
      "Train Epoche: 3 [259/96 (270%)]\tLoss: 0.728960\n",
      "Train Epoche: 3 [260/96 (271%)]\tLoss: 39.422897\n",
      "Train Epoche: 3 [261/96 (272%)]\tLoss: 2.440493\n",
      "Train Epoche: 3 [262/96 (273%)]\tLoss: 0.954044\n",
      "Train Epoche: 3 [263/96 (274%)]\tLoss: 3.257370\n",
      "Train Epoche: 3 [264/96 (275%)]\tLoss: 1.936831\n",
      "Train Epoche: 3 [265/96 (276%)]\tLoss: 16.992573\n",
      "Train Epoche: 3 [266/96 (277%)]\tLoss: 1.653562\n",
      "Train Epoche: 3 [267/96 (278%)]\tLoss: 64.217804\n",
      "Train Epoche: 3 [268/96 (279%)]\tLoss: 3.914456\n",
      "Train Epoche: 3 [269/96 (280%)]\tLoss: 1.696471\n",
      "Train Epoche: 3 [270/96 (281%)]\tLoss: 1.207705\n",
      "Train Epoche: 3 [271/96 (282%)]\tLoss: 0.891525\n",
      "Train Epoche: 3 [272/96 (283%)]\tLoss: 0.283065\n",
      "Train Epoche: 3 [273/96 (284%)]\tLoss: 2.470748\n",
      "Train Epoche: 3 [274/96 (285%)]\tLoss: 1.885746\n",
      "Train Epoche: 3 [275/96 (286%)]\tLoss: 0.431089\n",
      "Train Epoche: 3 [276/96 (288%)]\tLoss: 0.708625\n",
      "Train Epoche: 3 [277/96 (289%)]\tLoss: 0.260859\n",
      "Train Epoche: 3 [278/96 (290%)]\tLoss: 0.110641\n",
      "Train Epoche: 3 [279/96 (291%)]\tLoss: 3.893382\n",
      "Train Epoche: 3 [280/96 (292%)]\tLoss: 7.351871\n",
      "Train Epoche: 3 [281/96 (293%)]\tLoss: 21.728613\n",
      "Train Epoche: 3 [282/96 (294%)]\tLoss: 0.120565\n",
      "Train Epoche: 3 [283/96 (295%)]\tLoss: 4.087895\n",
      "Train Epoche: 3 [284/96 (296%)]\tLoss: 5.435003\n",
      "Train Epoche: 3 [285/96 (297%)]\tLoss: 7.804798\n",
      "Train Epoche: 3 [286/96 (298%)]\tLoss: 0.581361\n",
      "Train Epoche: 3 [287/96 (299%)]\tLoss: 2.726040\n",
      "Train Epoche: 3 [288/96 (300%)]\tLoss: 13.933422\n",
      "Train Epoche: 3 [289/96 (301%)]\tLoss: 33.706665\n",
      "Train Epoche: 3 [290/96 (302%)]\tLoss: 13.901154\n",
      "Train Epoche: 3 [291/96 (303%)]\tLoss: 1.148169\n",
      "Train Epoche: 3 [292/96 (304%)]\tLoss: 0.621595\n",
      "Train Epoche: 3 [293/96 (305%)]\tLoss: 39.413498\n",
      "Train Epoche: 3 [294/96 (306%)]\tLoss: 1.587748\n",
      "Train Epoche: 3 [295/96 (307%)]\tLoss: 0.970268\n",
      "Train Epoche: 3 [296/96 (308%)]\tLoss: 0.992890\n",
      "Train Epoche: 3 [297/96 (309%)]\tLoss: 1.668125\n",
      "Train Epoche: 3 [298/96 (310%)]\tLoss: 0.356930\n",
      "Train Epoche: 3 [299/96 (311%)]\tLoss: 0.663186\n",
      "Train Epoche: 3 [300/96 (312%)]\tLoss: 5.453374\n",
      "Train Epoche: 3 [301/96 (314%)]\tLoss: 1.561589\n",
      "Train Epoche: 3 [302/96 (315%)]\tLoss: 3.143352\n",
      "Train Epoche: 3 [303/96 (316%)]\tLoss: 0.017399\n",
      "Train Epoche: 3 [304/96 (317%)]\tLoss: 20.639240\n",
      "Train Epoche: 3 [305/96 (318%)]\tLoss: 0.017756\n",
      "Train Epoche: 3 [306/96 (319%)]\tLoss: 0.685650\n",
      "Train Epoche: 3 [307/96 (320%)]\tLoss: 0.769482\n",
      "Train Epoche: 3 [308/96 (321%)]\tLoss: 109.549782\n",
      "Train Epoche: 3 [309/96 (322%)]\tLoss: 5.106579\n",
      "Train Epoche: 3 [310/96 (323%)]\tLoss: 6.463693\n",
      "Train Epoche: 3 [311/96 (324%)]\tLoss: 0.543002\n",
      "Train Epoche: 3 [312/96 (325%)]\tLoss: 1.158509\n",
      "Train Epoche: 3 [313/96 (326%)]\tLoss: 1.991973\n",
      "Train Epoche: 3 [314/96 (327%)]\tLoss: 78.703445\n",
      "Train Epoche: 3 [315/96 (328%)]\tLoss: 5.622963\n",
      "Train Epoche: 3 [316/96 (329%)]\tLoss: 2.710172\n",
      "Train Epoche: 3 [317/96 (330%)]\tLoss: 0.503065\n",
      "Train Epoche: 3 [318/96 (331%)]\tLoss: 2.472127\n",
      "Train Epoche: 3 [319/96 (332%)]\tLoss: 0.706077\n",
      "Train Epoche: 3 [320/96 (333%)]\tLoss: 0.638024\n",
      "Train Epoche: 3 [321/96 (334%)]\tLoss: 6.289769\n",
      "Train Epoche: 3 [322/96 (335%)]\tLoss: 0.287217\n",
      "Train Epoche: 3 [323/96 (336%)]\tLoss: 1.909739\n",
      "Train Epoche: 3 [324/96 (338%)]\tLoss: 5.296416\n",
      "Train Epoche: 3 [325/96 (339%)]\tLoss: 0.689040\n",
      "Train Epoche: 3 [326/96 (340%)]\tLoss: 0.003216\n",
      "Train Epoche: 3 [327/96 (341%)]\tLoss: 2.449895\n",
      "Train Epoche: 3 [328/96 (342%)]\tLoss: 0.264138\n",
      "Train Epoche: 3 [329/96 (343%)]\tLoss: 26.937946\n",
      "Train Epoche: 3 [330/96 (344%)]\tLoss: 4.595743\n",
      "Train Epoche: 3 [331/96 (345%)]\tLoss: 7.264935\n",
      "Train Epoche: 3 [332/96 (346%)]\tLoss: 0.360849\n",
      "Train Epoche: 3 [333/96 (347%)]\tLoss: 0.959155\n",
      "Train Epoche: 3 [334/96 (348%)]\tLoss: 1.724938\n",
      "Train Epoche: 3 [335/96 (349%)]\tLoss: 101.655968\n",
      "Train Epoche: 3 [336/96 (350%)]\tLoss: 31.006287\n",
      "Train Epoche: 3 [337/96 (351%)]\tLoss: 7.015767\n",
      "Train Epoche: 3 [338/96 (352%)]\tLoss: 0.909444\n",
      "Train Epoche: 3 [339/96 (353%)]\tLoss: 1.340915\n",
      "Train Epoche: 3 [340/96 (354%)]\tLoss: 5.879585\n",
      "Train Epoche: 3 [341/96 (355%)]\tLoss: 7.975587\n",
      "Train Epoche: 3 [342/96 (356%)]\tLoss: 3.850914\n",
      "Train Epoche: 3 [343/96 (357%)]\tLoss: 0.190997\n",
      "Train Epoche: 3 [344/96 (358%)]\tLoss: 161.568146\n",
      "Train Epoche: 3 [345/96 (359%)]\tLoss: 280.317963\n",
      "Train Epoche: 3 [346/96 (360%)]\tLoss: 0.376640\n",
      "Train Epoche: 3 [347/96 (361%)]\tLoss: 1.153651\n",
      "Train Epoche: 3 [348/96 (362%)]\tLoss: 0.656238\n",
      "Train Epoche: 3 [349/96 (364%)]\tLoss: 0.123301\n",
      "Train Epoche: 3 [350/96 (365%)]\tLoss: 0.092613\n",
      "Train Epoche: 3 [351/96 (366%)]\tLoss: 9.543711\n",
      "Train Epoche: 3 [352/96 (367%)]\tLoss: 39.461864\n",
      "Train Epoche: 3 [353/96 (368%)]\tLoss: 5.647762\n",
      "Train Epoche: 3 [354/96 (369%)]\tLoss: 0.099898\n",
      "Train Epoche: 3 [355/96 (370%)]\tLoss: 0.748982\n",
      "Train Epoche: 3 [356/96 (371%)]\tLoss: 171.277130\n",
      "Train Epoche: 3 [357/96 (372%)]\tLoss: 1.467363\n",
      "Train Epoche: 3 [358/96 (373%)]\tLoss: 1.917676\n",
      "Train Epoche: 3 [359/96 (374%)]\tLoss: 3.417034\n",
      "Train Epoche: 3 [360/96 (375%)]\tLoss: 0.492801\n",
      "Train Epoche: 3 [361/96 (376%)]\tLoss: 1.311428\n",
      "Train Epoche: 3 [362/96 (377%)]\tLoss: 6.698881\n",
      "Train Epoche: 3 [363/96 (378%)]\tLoss: 1.153126\n",
      "Train Epoche: 3 [364/96 (379%)]\tLoss: 16.075621\n",
      "Train Epoche: 3 [365/96 (380%)]\tLoss: 0.234592\n",
      "Train Epoche: 3 [366/96 (381%)]\tLoss: 2.857849\n",
      "Train Epoche: 3 [367/96 (382%)]\tLoss: 6.752585\n",
      "Train Epoche: 3 [368/96 (383%)]\tLoss: 1.541412\n",
      "Train Epoche: 3 [369/96 (384%)]\tLoss: 16.685690\n",
      "Train Epoche: 3 [370/96 (385%)]\tLoss: 8.464104\n",
      "Train Epoche: 3 [371/96 (386%)]\tLoss: 46.198574\n",
      "Train Epoche: 3 [372/96 (388%)]\tLoss: 35.934937\n",
      "Train Epoche: 3 [373/96 (389%)]\tLoss: 15.270698\n",
      "Train Epoche: 3 [374/96 (390%)]\tLoss: 0.342668\n",
      "Train Epoche: 3 [375/96 (391%)]\tLoss: 3.500399\n",
      "Train Epoche: 3 [376/96 (392%)]\tLoss: 18.417555\n",
      "Train Epoche: 3 [377/96 (393%)]\tLoss: 0.000039\n",
      "Train Epoche: 3 [378/96 (394%)]\tLoss: 0.498731\n",
      "Train Epoche: 3 [379/96 (395%)]\tLoss: 2.269488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [380/96 (396%)]\tLoss: 0.746263\n",
      "Train Epoche: 3 [381/96 (397%)]\tLoss: 10.643631\n",
      "Train Epoche: 3 [382/96 (398%)]\tLoss: 0.648980\n",
      "Train Epoche: 3 [383/96 (399%)]\tLoss: 2.111211\n",
      "Train Epoche: 3 [384/96 (400%)]\tLoss: 0.381419\n",
      "Train Epoche: 3 [385/96 (401%)]\tLoss: 2.333784\n",
      "Train Epoche: 3 [386/96 (402%)]\tLoss: 16.996067\n",
      "Train Epoche: 3 [387/96 (403%)]\tLoss: 20.397243\n",
      "Train Epoche: 3 [388/96 (404%)]\tLoss: 10.528013\n",
      "Train Epoche: 3 [389/96 (405%)]\tLoss: 0.753067\n",
      "Train Epoche: 3 [390/96 (406%)]\tLoss: 7.153442\n",
      "Train Epoche: 3 [391/96 (407%)]\tLoss: 13.994804\n",
      "Train Epoche: 3 [392/96 (408%)]\tLoss: 59.292297\n",
      "Train Epoche: 3 [393/96 (409%)]\tLoss: 214.947754\n",
      "Train Epoche: 3 [394/96 (410%)]\tLoss: 2.316285\n",
      "Train Epoche: 3 [395/96 (411%)]\tLoss: 4.835577\n",
      "Train Epoche: 3 [396/96 (412%)]\tLoss: 0.052528\n",
      "Train Epoche: 3 [397/96 (414%)]\tLoss: 18.679012\n",
      "Train Epoche: 3 [398/96 (415%)]\tLoss: 1.169934\n",
      "Train Epoche: 3 [399/96 (416%)]\tLoss: 0.205275\n",
      "Train Epoche: 3 [400/96 (417%)]\tLoss: 0.386917\n",
      "Train Epoche: 3 [401/96 (418%)]\tLoss: 2.466863\n",
      "Train Epoche: 3 [402/96 (419%)]\tLoss: 1.164849\n",
      "Train Epoche: 3 [403/96 (420%)]\tLoss: 5.213123\n",
      "Train Epoche: 3 [404/96 (421%)]\tLoss: 0.916106\n",
      "Train Epoche: 3 [405/96 (422%)]\tLoss: 1.550785\n",
      "Train Epoche: 3 [406/96 (423%)]\tLoss: 24.275509\n",
      "Train Epoche: 3 [407/96 (424%)]\tLoss: 5.235416\n",
      "Train Epoche: 3 [408/96 (425%)]\tLoss: 0.405964\n",
      "Train Epoche: 3 [409/96 (426%)]\tLoss: 9.563684\n",
      "Train Epoche: 3 [410/96 (427%)]\tLoss: 0.261165\n",
      "Train Epoche: 3 [411/96 (428%)]\tLoss: 0.687920\n",
      "Train Epoche: 3 [412/96 (429%)]\tLoss: 1.365341\n",
      "Train Epoche: 3 [413/96 (430%)]\tLoss: 2.899875\n",
      "Train Epoche: 3 [414/96 (431%)]\tLoss: 0.001242\n",
      "Train Epoche: 3 [415/96 (432%)]\tLoss: 0.478085\n",
      "Train Epoche: 3 [416/96 (433%)]\tLoss: 2.479525\n",
      "Train Epoche: 3 [417/96 (434%)]\tLoss: 32.033794\n",
      "Train Epoche: 3 [418/96 (435%)]\tLoss: 3.475393\n",
      "Train Epoche: 3 [419/96 (436%)]\tLoss: 4.537788\n",
      "Train Epoche: 3 [420/96 (438%)]\tLoss: 2.747244\n",
      "Train Epoche: 3 [421/96 (439%)]\tLoss: 12.244390\n",
      "Train Epoche: 3 [422/96 (440%)]\tLoss: 5.990388\n",
      "Train Epoche: 3 [423/96 (441%)]\tLoss: 58.821705\n",
      "Train Epoche: 3 [424/96 (442%)]\tLoss: 1.106658\n",
      "Train Epoche: 3 [425/96 (443%)]\tLoss: 2.486170\n",
      "Train Epoche: 3 [426/96 (444%)]\tLoss: 1.533888\n",
      "Train Epoche: 3 [427/96 (445%)]\tLoss: 1.126579\n",
      "Train Epoche: 3 [428/96 (446%)]\tLoss: 1.423105\n",
      "Train Epoche: 3 [429/96 (447%)]\tLoss: 2.973745\n",
      "Train Epoche: 3 [430/96 (448%)]\tLoss: 19.248314\n",
      "Train Epoche: 3 [431/96 (449%)]\tLoss: 1.506339\n",
      "Train Epoche: 3 [432/96 (450%)]\tLoss: 1.029707\n",
      "Train Epoche: 3 [433/96 (451%)]\tLoss: 0.838662\n",
      "Train Epoche: 3 [434/96 (452%)]\tLoss: 0.759517\n",
      "Train Epoche: 3 [435/96 (453%)]\tLoss: 9.455173\n",
      "Train Epoche: 3 [436/96 (454%)]\tLoss: 10.592946\n",
      "Train Epoche: 3 [437/96 (455%)]\tLoss: 18.365932\n",
      "Train Epoche: 3 [438/96 (456%)]\tLoss: 0.112789\n",
      "Train Epoche: 3 [439/96 (457%)]\tLoss: 0.141411\n",
      "Train Epoche: 3 [440/96 (458%)]\tLoss: 6.353392\n",
      "Train Epoche: 3 [441/96 (459%)]\tLoss: 0.072519\n",
      "Train Epoche: 3 [442/96 (460%)]\tLoss: 1.829950\n",
      "Train Epoche: 3 [443/96 (461%)]\tLoss: 0.495315\n",
      "Train Epoche: 3 [444/96 (462%)]\tLoss: 6.055962\n",
      "Train Epoche: 3 [445/96 (464%)]\tLoss: 0.993387\n",
      "Train Epoche: 3 [446/96 (465%)]\tLoss: 85.847832\n",
      "Train Epoche: 3 [447/96 (466%)]\tLoss: 0.001643\n",
      "Train Epoche: 3 [448/96 (467%)]\tLoss: 1.172563\n",
      "Train Epoche: 3 [449/96 (468%)]\tLoss: 1.225019\n",
      "Train Epoche: 3 [450/96 (469%)]\tLoss: 9.697925\n",
      "Train Epoche: 3 [451/96 (470%)]\tLoss: 1.159322\n",
      "Train Epoche: 3 [452/96 (471%)]\tLoss: 0.676380\n",
      "Train Epoche: 3 [453/96 (472%)]\tLoss: 3.776164\n",
      "Train Epoche: 3 [454/96 (473%)]\tLoss: 0.215560\n",
      "Train Epoche: 3 [455/96 (474%)]\tLoss: 2.305840\n",
      "Train Epoche: 3 [456/96 (475%)]\tLoss: 16.638386\n",
      "Train Epoche: 3 [457/96 (476%)]\tLoss: 9.854255\n",
      "Train Epoche: 3 [458/96 (477%)]\tLoss: 44.844936\n",
      "Train Epoche: 3 [459/96 (478%)]\tLoss: 2.103761\n",
      "Train Epoche: 3 [460/96 (479%)]\tLoss: 51.415329\n",
      "Train Epoche: 3 [461/96 (480%)]\tLoss: 0.010144\n",
      "Train Epoche: 3 [462/96 (481%)]\tLoss: 0.964427\n",
      "Train Epoche: 3 [463/96 (482%)]\tLoss: 2.180658\n",
      "Train Epoche: 3 [464/96 (483%)]\tLoss: 110.369270\n",
      "Train Epoche: 3 [465/96 (484%)]\tLoss: 0.026246\n",
      "Train Epoche: 3 [466/96 (485%)]\tLoss: 0.004713\n",
      "Train Epoche: 3 [467/96 (486%)]\tLoss: 2.447293\n",
      "Train Epoche: 3 [468/96 (488%)]\tLoss: 1.150107\n",
      "Train Epoche: 3 [469/96 (489%)]\tLoss: 10.004304\n",
      "Train Epoche: 3 [470/96 (490%)]\tLoss: 15.118776\n",
      "Train Epoche: 3 [471/96 (491%)]\tLoss: 14.529325\n",
      "Train Epoche: 3 [472/96 (492%)]\tLoss: 0.144896\n",
      "Train Epoche: 3 [473/96 (493%)]\tLoss: 4.513671\n",
      "Train Epoche: 3 [474/96 (494%)]\tLoss: 4.892429\n",
      "Train Epoche: 3 [475/96 (495%)]\tLoss: 3.148647\n",
      "Train Epoche: 3 [476/96 (496%)]\tLoss: 0.890173\n",
      "Train Epoche: 3 [477/96 (497%)]\tLoss: 37.374866\n",
      "Train Epoche: 3 [478/96 (498%)]\tLoss: 0.125287\n",
      "Train Epoche: 3 [479/96 (499%)]\tLoss: 58.533787\n",
      "Train Epoche: 3 [480/96 (500%)]\tLoss: 6.278490\n",
      "Train Epoche: 3 [481/96 (501%)]\tLoss: 2.253876\n",
      "Train Epoche: 3 [482/96 (502%)]\tLoss: 12.797144\n",
      "Train Epoche: 3 [483/96 (503%)]\tLoss: 8.563134\n",
      "Train Epoche: 3 [484/96 (504%)]\tLoss: 20.145884\n",
      "Train Epoche: 3 [485/96 (505%)]\tLoss: 17.727625\n",
      "Train Epoche: 3 [486/96 (506%)]\tLoss: 0.198515\n",
      "Train Epoche: 3 [487/96 (507%)]\tLoss: 5.827157\n",
      "Train Epoche: 3 [488/96 (508%)]\tLoss: 6.374092\n",
      "Train Epoche: 3 [489/96 (509%)]\tLoss: 1.840272\n",
      "Train Epoche: 3 [490/96 (510%)]\tLoss: 1.097617\n",
      "Train Epoche: 3 [491/96 (511%)]\tLoss: 3.543683\n",
      "Train Epoche: 3 [492/96 (512%)]\tLoss: 3.195846\n",
      "Train Epoche: 3 [493/96 (514%)]\tLoss: 30.430252\n",
      "Train Epoche: 3 [494/96 (515%)]\tLoss: 2.238131\n",
      "Train Epoche: 3 [495/96 (516%)]\tLoss: 8.769896\n",
      "Train Epoche: 3 [496/96 (517%)]\tLoss: 0.602986\n",
      "Train Epoche: 3 [497/96 (518%)]\tLoss: 37.612057\n",
      "Train Epoche: 3 [498/96 (519%)]\tLoss: 0.985966\n",
      "Train Epoche: 3 [499/96 (520%)]\tLoss: 6.229570\n",
      "Train Epoche: 3 [500/96 (521%)]\tLoss: 3.126096\n",
      "Train Epoche: 3 [501/96 (522%)]\tLoss: 0.111730\n",
      "Train Epoche: 3 [502/96 (523%)]\tLoss: 4.824121\n",
      "Train Epoche: 3 [503/96 (524%)]\tLoss: 1.269905\n",
      "Train Epoche: 3 [504/96 (525%)]\tLoss: 4.440533\n",
      "Train Epoche: 3 [505/96 (526%)]\tLoss: 1.164991\n",
      "Train Epoche: 3 [506/96 (527%)]\tLoss: 2.699847\n",
      "Train Epoche: 3 [507/96 (528%)]\tLoss: 6.581498\n",
      "Train Epoche: 3 [508/96 (529%)]\tLoss: 15.932620\n",
      "Train Epoche: 3 [509/96 (530%)]\tLoss: 0.556284\n",
      "Train Epoche: 3 [510/96 (531%)]\tLoss: 2.650959\n",
      "Train Epoche: 3 [511/96 (532%)]\tLoss: 0.162427\n",
      "Train Epoche: 3 [512/96 (533%)]\tLoss: 1.646017\n",
      "Train Epoche: 3 [513/96 (534%)]\tLoss: 3.024052\n",
      "Train Epoche: 3 [514/96 (535%)]\tLoss: 18.727631\n",
      "Train Epoche: 3 [515/96 (536%)]\tLoss: 0.037789\n",
      "Train Epoche: 3 [516/96 (538%)]\tLoss: 0.001873\n",
      "Train Epoche: 3 [517/96 (539%)]\tLoss: 1.205469\n",
      "Train Epoche: 3 [518/96 (540%)]\tLoss: 13.014222\n",
      "Train Epoche: 3 [519/96 (541%)]\tLoss: 46.049618\n",
      "Train Epoche: 3 [520/96 (542%)]\tLoss: 19.302687\n",
      "Train Epoche: 3 [521/96 (543%)]\tLoss: 24.069811\n",
      "Train Epoche: 3 [522/96 (544%)]\tLoss: 5.095512\n",
      "Train Epoche: 3 [523/96 (545%)]\tLoss: 20.261177\n",
      "Train Epoche: 3 [524/96 (546%)]\tLoss: 3.205544\n",
      "Train Epoche: 3 [525/96 (547%)]\tLoss: 0.296112\n",
      "Train Epoche: 3 [526/96 (548%)]\tLoss: 0.012536\n",
      "Train Epoche: 3 [527/96 (549%)]\tLoss: 2.099927\n",
      "Train Epoche: 3 [528/96 (550%)]\tLoss: 10.068793\n",
      "Train Epoche: 3 [529/96 (551%)]\tLoss: 0.746548\n",
      "Train Epoche: 3 [530/96 (552%)]\tLoss: 14.016347\n",
      "Train Epoche: 3 [531/96 (553%)]\tLoss: 7.463471\n",
      "Train Epoche: 3 [532/96 (554%)]\tLoss: 3.832447\n",
      "Train Epoche: 3 [533/96 (555%)]\tLoss: 115.916550\n",
      "Train Epoche: 3 [534/96 (556%)]\tLoss: 29.377558\n",
      "Train Epoche: 3 [535/96 (557%)]\tLoss: 5.712523\n",
      "Train Epoche: 3 [536/96 (558%)]\tLoss: 5.397601\n",
      "Train Epoche: 3 [537/96 (559%)]\tLoss: 3.643704\n",
      "Train Epoche: 3 [538/96 (560%)]\tLoss: 2.339367\n",
      "Train Epoche: 3 [539/96 (561%)]\tLoss: 0.147669\n",
      "Train Epoche: 3 [540/96 (562%)]\tLoss: 25.248762\n",
      "Train Epoche: 3 [541/96 (564%)]\tLoss: 1.049709\n",
      "Train Epoche: 3 [542/96 (565%)]\tLoss: 0.773604\n",
      "Train Epoche: 3 [543/96 (566%)]\tLoss: 20.065311\n",
      "Train Epoche: 3 [544/96 (567%)]\tLoss: 1.879932\n",
      "Train Epoche: 3 [545/96 (568%)]\tLoss: 5.610464\n",
      "Train Epoche: 3 [546/96 (569%)]\tLoss: 0.119519\n",
      "Train Epoche: 3 [547/96 (570%)]\tLoss: 1.777312\n",
      "Train Epoche: 3 [548/96 (571%)]\tLoss: 45.994976\n",
      "Train Epoche: 3 [549/96 (572%)]\tLoss: 9.712679\n",
      "Train Epoche: 3 [550/96 (573%)]\tLoss: 0.626949\n",
      "Train Epoche: 3 [551/96 (574%)]\tLoss: 15.347999\n",
      "Train Epoche: 3 [552/96 (575%)]\tLoss: 6.417466\n",
      "Train Epoche: 3 [553/96 (576%)]\tLoss: 157.122604\n",
      "Train Epoche: 3 [554/96 (577%)]\tLoss: 3.161786\n",
      "Train Epoche: 3 [555/96 (578%)]\tLoss: 126.914047\n",
      "Train Epoche: 3 [556/96 (579%)]\tLoss: 0.133281\n",
      "Train Epoche: 3 [557/96 (580%)]\tLoss: 4.020542\n",
      "Train Epoche: 3 [558/96 (581%)]\tLoss: 1.899427\n",
      "Train Epoche: 3 [559/96 (582%)]\tLoss: 0.258983\n",
      "Train Epoche: 3 [560/96 (583%)]\tLoss: 2.070878\n",
      "Train Epoche: 3 [561/96 (584%)]\tLoss: 3.590035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [562/96 (585%)]\tLoss: 0.040009\n",
      "Train Epoche: 3 [563/96 (586%)]\tLoss: 18.674486\n",
      "Train Epoche: 3 [564/96 (588%)]\tLoss: 11.502379\n",
      "Train Epoche: 3 [565/96 (589%)]\tLoss: 15.067895\n",
      "Train Epoche: 3 [566/96 (590%)]\tLoss: 1.745606\n",
      "Train Epoche: 3 [567/96 (591%)]\tLoss: 12.644504\n",
      "Train Epoche: 3 [568/96 (592%)]\tLoss: 1.925375\n",
      "Train Epoche: 3 [569/96 (593%)]\tLoss: 6.851238\n",
      "Train Epoche: 3 [570/96 (594%)]\tLoss: 5.233548\n",
      "Train Epoche: 3 [571/96 (595%)]\tLoss: 3.840652\n",
      "Train Epoche: 3 [572/96 (596%)]\tLoss: 0.063172\n",
      "Train Epoche: 3 [573/96 (597%)]\tLoss: 1.035647\n",
      "Train Epoche: 3 [574/96 (598%)]\tLoss: 0.116466\n",
      "Train Epoche: 3 [575/96 (599%)]\tLoss: 1.511758\n",
      "Train Epoche: 3 [576/96 (600%)]\tLoss: 0.003854\n",
      "Train Epoche: 3 [577/96 (601%)]\tLoss: 1.470833\n",
      "Train Epoche: 3 [578/96 (602%)]\tLoss: 13.729975\n",
      "Train Epoche: 3 [579/96 (603%)]\tLoss: 0.024773\n",
      "Train Epoche: 3 [580/96 (604%)]\tLoss: 1.305179\n",
      "Train Epoche: 3 [581/96 (605%)]\tLoss: 0.030962\n",
      "Train Epoche: 3 [582/96 (606%)]\tLoss: 1.786590\n",
      "Train Epoche: 3 [583/96 (607%)]\tLoss: 1.833161\n",
      "Train Epoche: 3 [584/96 (608%)]\tLoss: 11.624964\n",
      "Train Epoche: 3 [585/96 (609%)]\tLoss: 33.132542\n",
      "Train Epoche: 3 [586/96 (610%)]\tLoss: 0.140552\n",
      "Train Epoche: 3 [587/96 (611%)]\tLoss: 5.391553\n",
      "Train Epoche: 3 [588/96 (612%)]\tLoss: 4.149763\n",
      "Train Epoche: 3 [589/96 (614%)]\tLoss: 14.337510\n",
      "Train Epoche: 3 [590/96 (615%)]\tLoss: 0.205329\n",
      "Train Epoche: 3 [591/96 (616%)]\tLoss: 1.780214\n",
      "Train Epoche: 3 [592/96 (617%)]\tLoss: 1.048576\n",
      "Train Epoche: 3 [593/96 (618%)]\tLoss: 12.036362\n",
      "Train Epoche: 3 [594/96 (619%)]\tLoss: 77.575630\n",
      "Train Epoche: 3 [595/96 (620%)]\tLoss: 0.048035\n",
      "Train Epoche: 3 [596/96 (621%)]\tLoss: 1.239935\n",
      "Train Epoche: 3 [597/96 (622%)]\tLoss: 1.127533\n",
      "Train Epoche: 3 [598/96 (623%)]\tLoss: 0.638640\n",
      "Train Epoche: 3 [599/96 (624%)]\tLoss: 2.878387\n",
      "Train Epoche: 3 [600/96 (625%)]\tLoss: 5.060886\n",
      "Train Epoche: 3 [601/96 (626%)]\tLoss: 4.433810\n",
      "Train Epoche: 3 [602/96 (627%)]\tLoss: 2.217179\n",
      "Train Epoche: 3 [603/96 (628%)]\tLoss: 0.139408\n",
      "Train Epoche: 3 [604/96 (629%)]\tLoss: 9.368611\n",
      "Train Epoche: 3 [605/96 (630%)]\tLoss: 16.988115\n",
      "Train Epoche: 3 [606/96 (631%)]\tLoss: 0.105993\n",
      "Train Epoche: 3 [607/96 (632%)]\tLoss: 216.839630\n",
      "Train Epoche: 3 [608/96 (633%)]\tLoss: 7.897040\n",
      "Train Epoche: 3 [609/96 (634%)]\tLoss: 12.678985\n",
      "Train Epoche: 3 [610/96 (635%)]\tLoss: 48.005344\n",
      "Train Epoche: 3 [611/96 (636%)]\tLoss: 1.183566\n",
      "Train Epoche: 3 [612/96 (638%)]\tLoss: 118.078316\n",
      "Train Epoche: 3 [613/96 (639%)]\tLoss: 100.410614\n",
      "Train Epoche: 3 [614/96 (640%)]\tLoss: 10.015628\n",
      "Train Epoche: 3 [615/96 (641%)]\tLoss: 1.772095\n",
      "Train Epoche: 3 [616/96 (642%)]\tLoss: 2.161363\n",
      "Train Epoche: 3 [617/96 (643%)]\tLoss: 284.660950\n",
      "Train Epoche: 3 [618/96 (644%)]\tLoss: 9.704269\n",
      "Train Epoche: 3 [619/96 (645%)]\tLoss: 20.754040\n",
      "Train Epoche: 3 [620/96 (646%)]\tLoss: 0.687192\n",
      "Train Epoche: 3 [621/96 (647%)]\tLoss: 7.667409\n",
      "Train Epoche: 3 [622/96 (648%)]\tLoss: 39.786472\n",
      "Train Epoche: 3 [623/96 (649%)]\tLoss: 30.601004\n",
      "Train Epoche: 3 [624/96 (650%)]\tLoss: 28.510326\n",
      "Train Epoche: 3 [625/96 (651%)]\tLoss: 376.311951\n",
      "Train Epoche: 3 [626/96 (652%)]\tLoss: 1.191128\n",
      "Train Epoche: 3 [627/96 (653%)]\tLoss: 9.640407\n",
      "Train Epoche: 3 [628/96 (654%)]\tLoss: 3.440588\n",
      "Train Epoche: 3 [629/96 (655%)]\tLoss: 129.723877\n",
      "Train Epoche: 3 [630/96 (656%)]\tLoss: 3.721626\n",
      "Train Epoche: 3 [631/96 (657%)]\tLoss: 0.711818\n",
      "Train Epoche: 3 [632/96 (658%)]\tLoss: 0.231785\n",
      "Train Epoche: 3 [633/96 (659%)]\tLoss: 8.146395\n",
      "Train Epoche: 3 [634/96 (660%)]\tLoss: 5.257531\n",
      "Train Epoche: 3 [635/96 (661%)]\tLoss: 13.818776\n",
      "Train Epoche: 3 [636/96 (662%)]\tLoss: 0.320334\n",
      "Train Epoche: 3 [637/96 (664%)]\tLoss: 3.478315\n",
      "Train Epoche: 3 [638/96 (665%)]\tLoss: 1.628996\n",
      "Train Epoche: 3 [639/96 (666%)]\tLoss: 6.575126\n",
      "Train Epoche: 3 [640/96 (667%)]\tLoss: 4.797302\n",
      "Train Epoche: 3 [641/96 (668%)]\tLoss: 16.577242\n",
      "Train Epoche: 3 [642/96 (669%)]\tLoss: 19.582813\n",
      "Train Epoche: 3 [643/96 (670%)]\tLoss: 36.370686\n",
      "Train Epoche: 3 [644/96 (671%)]\tLoss: 0.630346\n",
      "Train Epoche: 3 [645/96 (672%)]\tLoss: 7.126848\n",
      "Train Epoche: 3 [646/96 (673%)]\tLoss: 10.309341\n",
      "Train Epoche: 3 [647/96 (674%)]\tLoss: 12.782472\n",
      "Train Epoche: 3 [648/96 (675%)]\tLoss: 2.019877\n",
      "Train Epoche: 3 [649/96 (676%)]\tLoss: 36.880215\n",
      "Train Epoche: 3 [650/96 (677%)]\tLoss: 0.113972\n",
      "Train Epoche: 3 [651/96 (678%)]\tLoss: 28.608158\n",
      "Train Epoche: 3 [652/96 (679%)]\tLoss: 1.372764\n",
      "Train Epoche: 3 [653/96 (680%)]\tLoss: 1.892813\n",
      "Train Epoche: 3 [654/96 (681%)]\tLoss: 60.923851\n",
      "Train Epoche: 3 [655/96 (682%)]\tLoss: 21.977064\n",
      "Train Epoche: 3 [656/96 (683%)]\tLoss: 8.665694\n",
      "Train Epoche: 3 [657/96 (684%)]\tLoss: 171.660339\n",
      "Train Epoche: 3 [658/96 (685%)]\tLoss: 0.403423\n",
      "Train Epoche: 3 [659/96 (686%)]\tLoss: 1.789586\n",
      "Train Epoche: 3 [660/96 (688%)]\tLoss: 53.257580\n",
      "Train Epoche: 3 [661/96 (689%)]\tLoss: 0.353164\n",
      "Train Epoche: 3 [662/96 (690%)]\tLoss: 12.907832\n",
      "Train Epoche: 3 [663/96 (691%)]\tLoss: 0.439230\n",
      "Train Epoche: 3 [664/96 (692%)]\tLoss: 0.032643\n",
      "Train Epoche: 3 [665/96 (693%)]\tLoss: 4.151590\n",
      "Train Epoche: 3 [666/96 (694%)]\tLoss: 14.005637\n",
      "Train Epoche: 3 [667/96 (695%)]\tLoss: 0.005516\n",
      "Train Epoche: 3 [668/96 (696%)]\tLoss: 4.874854\n",
      "Train Epoche: 3 [669/96 (697%)]\tLoss: 88.407509\n",
      "Train Epoche: 3 [670/96 (698%)]\tLoss: 1.313574\n",
      "Train Epoche: 3 [671/96 (699%)]\tLoss: 1.865402\n",
      "Train Epoche: 3 [672/96 (700%)]\tLoss: 1.388499\n",
      "Train Epoche: 3 [673/96 (701%)]\tLoss: 1.475665\n",
      "Train Epoche: 3 [674/96 (702%)]\tLoss: 0.093885\n",
      "Train Epoche: 3 [675/96 (703%)]\tLoss: 0.071470\n",
      "Train Epoche: 3 [676/96 (704%)]\tLoss: 2.832777\n",
      "Train Epoche: 3 [677/96 (705%)]\tLoss: 1.725670\n",
      "Train Epoche: 3 [678/96 (706%)]\tLoss: 3.645450\n",
      "Train Epoche: 3 [679/96 (707%)]\tLoss: 0.406635\n",
      "Train Epoche: 3 [680/96 (708%)]\tLoss: 11.552550\n",
      "Train Epoche: 3 [681/96 (709%)]\tLoss: 14.750878\n",
      "Train Epoche: 3 [682/96 (710%)]\tLoss: 2.840573\n",
      "Train Epoche: 3 [683/96 (711%)]\tLoss: 0.938079\n",
      "Train Epoche: 3 [684/96 (712%)]\tLoss: 24.358202\n",
      "Train Epoche: 3 [685/96 (714%)]\tLoss: 20.513512\n",
      "Train Epoche: 3 [686/96 (715%)]\tLoss: 40.816956\n",
      "Train Epoche: 3 [687/96 (716%)]\tLoss: 1.802259\n",
      "Train Epoche: 3 [688/96 (717%)]\tLoss: 0.763084\n",
      "Train Epoche: 3 [689/96 (718%)]\tLoss: 1.723540\n",
      "Train Epoche: 3 [690/96 (719%)]\tLoss: 1.537200\n",
      "Train Epoche: 3 [691/96 (720%)]\tLoss: 4.460477\n",
      "Train Epoche: 3 [692/96 (721%)]\tLoss: 17.267200\n",
      "Train Epoche: 3 [693/96 (722%)]\tLoss: 22.736204\n",
      "Train Epoche: 3 [694/96 (723%)]\tLoss: 0.004699\n",
      "Train Epoche: 3 [695/96 (724%)]\tLoss: 0.033921\n",
      "Train Epoche: 3 [696/96 (725%)]\tLoss: 0.552710\n",
      "Train Epoche: 3 [697/96 (726%)]\tLoss: 39.065315\n",
      "Train Epoche: 3 [698/96 (727%)]\tLoss: 2.037368\n",
      "Train Epoche: 3 [699/96 (728%)]\tLoss: 6.812917\n",
      "Train Epoche: 3 [700/96 (729%)]\tLoss: 0.851308\n",
      "Train Epoche: 3 [701/96 (730%)]\tLoss: 4.010920\n",
      "Train Epoche: 3 [702/96 (731%)]\tLoss: 23.466108\n",
      "Train Epoche: 3 [703/96 (732%)]\tLoss: 11.241264\n",
      "Train Epoche: 3 [704/96 (733%)]\tLoss: 0.122235\n",
      "Train Epoche: 3 [705/96 (734%)]\tLoss: 0.697098\n",
      "Train Epoche: 3 [706/96 (735%)]\tLoss: 13.540886\n",
      "Train Epoche: 3 [707/96 (736%)]\tLoss: 8.120008\n",
      "Train Epoche: 3 [708/96 (738%)]\tLoss: 3.104705\n",
      "Train Epoche: 3 [709/96 (739%)]\tLoss: 1.495700\n",
      "Train Epoche: 3 [710/96 (740%)]\tLoss: 0.460658\n",
      "Train Epoche: 3 [711/96 (741%)]\tLoss: 1.092968\n",
      "Train Epoche: 3 [712/96 (742%)]\tLoss: 1.918014\n",
      "Train Epoche: 3 [713/96 (743%)]\tLoss: 30.733681\n",
      "Train Epoche: 3 [714/96 (744%)]\tLoss: 9.661299\n",
      "Train Epoche: 3 [715/96 (745%)]\tLoss: 5.754808\n",
      "Train Epoche: 3 [716/96 (746%)]\tLoss: 1.211317\n",
      "Train Epoche: 3 [717/96 (747%)]\tLoss: 0.724818\n",
      "Train Epoche: 3 [718/96 (748%)]\tLoss: 1.023938\n",
      "Train Epoche: 3 [719/96 (749%)]\tLoss: 57.188030\n",
      "Train Epoche: 3 [720/96 (750%)]\tLoss: 0.317918\n",
      "Train Epoche: 3 [721/96 (751%)]\tLoss: 0.088601\n",
      "Train Epoche: 3 [722/96 (752%)]\tLoss: 2.416426\n",
      "Train Epoche: 3 [723/96 (753%)]\tLoss: 6.480283\n",
      "Train Epoche: 3 [724/96 (754%)]\tLoss: 0.792873\n",
      "Train Epoche: 3 [725/96 (755%)]\tLoss: 2.072572\n",
      "Train Epoche: 3 [726/96 (756%)]\tLoss: 1.067534\n",
      "Train Epoche: 3 [727/96 (757%)]\tLoss: 0.000007\n",
      "Train Epoche: 3 [728/96 (758%)]\tLoss: 5.544226\n",
      "Train Epoche: 3 [729/96 (759%)]\tLoss: 1.744321\n",
      "Train Epoche: 3 [730/96 (760%)]\tLoss: 3.238240\n",
      "Train Epoche: 3 [731/96 (761%)]\tLoss: 162.747757\n",
      "Train Epoche: 3 [732/96 (762%)]\tLoss: 107.719772\n",
      "Train Epoche: 3 [733/96 (764%)]\tLoss: 1.670508\n",
      "Train Epoche: 3 [734/96 (765%)]\tLoss: 0.148120\n",
      "Train Epoche: 3 [735/96 (766%)]\tLoss: 0.447423\n",
      "Train Epoche: 3 [736/96 (767%)]\tLoss: 3.707928\n",
      "Train Epoche: 3 [737/96 (768%)]\tLoss: 1.557122\n",
      "Train Epoche: 3 [738/96 (769%)]\tLoss: 2.800296\n",
      "Train Epoche: 3 [739/96 (770%)]\tLoss: 1.188569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [740/96 (771%)]\tLoss: 6.213133\n",
      "Train Epoche: 3 [741/96 (772%)]\tLoss: 1.963766\n",
      "Train Epoche: 3 [742/96 (773%)]\tLoss: 1.151986\n",
      "Train Epoche: 3 [743/96 (774%)]\tLoss: 0.083739\n",
      "Train Epoche: 3 [744/96 (775%)]\tLoss: 0.004264\n",
      "Train Epoche: 3 [745/96 (776%)]\tLoss: 0.223307\n",
      "Train Epoche: 3 [746/96 (777%)]\tLoss: 0.837358\n",
      "Train Epoche: 3 [747/96 (778%)]\tLoss: 29.823400\n",
      "Train Epoche: 3 [748/96 (779%)]\tLoss: 64.010773\n",
      "Train Epoche: 3 [749/96 (780%)]\tLoss: 150.723434\n",
      "Train Epoche: 3 [750/96 (781%)]\tLoss: 52.118504\n",
      "Train Epoche: 3 [751/96 (782%)]\tLoss: 0.907462\n",
      "Train Epoche: 3 [752/96 (783%)]\tLoss: 2.716032\n",
      "Train Epoche: 3 [753/96 (784%)]\tLoss: 0.014608\n",
      "Train Epoche: 3 [754/96 (785%)]\tLoss: 131.047882\n",
      "Train Epoche: 3 [755/96 (786%)]\tLoss: 4.685534\n",
      "Train Epoche: 3 [756/96 (788%)]\tLoss: 2.582361\n",
      "Train Epoche: 3 [757/96 (789%)]\tLoss: 6.742077\n",
      "Train Epoche: 3 [758/96 (790%)]\tLoss: 2.161515\n",
      "Train Epoche: 3 [759/96 (791%)]\tLoss: 0.191752\n",
      "Train Epoche: 3 [760/96 (792%)]\tLoss: 20.845064\n",
      "Train Epoche: 3 [761/96 (793%)]\tLoss: 46.468113\n",
      "Train Epoche: 3 [762/96 (794%)]\tLoss: 0.186013\n",
      "Train Epoche: 3 [763/96 (795%)]\tLoss: 1.096205\n",
      "Train Epoche: 3 [764/96 (796%)]\tLoss: 61.335918\n",
      "Train Epoche: 3 [765/96 (797%)]\tLoss: 17.528488\n",
      "Train Epoche: 3 [766/96 (798%)]\tLoss: 53.926266\n",
      "Train Epoche: 3 [767/96 (799%)]\tLoss: 31.923206\n",
      "Train Epoche: 3 [768/96 (800%)]\tLoss: 0.087859\n",
      "Train Epoche: 3 [769/96 (801%)]\tLoss: 49.601234\n",
      "Train Epoche: 3 [770/96 (802%)]\tLoss: 0.173022\n",
      "Train Epoche: 3 [771/96 (803%)]\tLoss: 8.498264\n",
      "Train Epoche: 3 [772/96 (804%)]\tLoss: 96.500023\n",
      "Train Epoche: 3 [773/96 (805%)]\tLoss: 0.000405\n",
      "Train Epoche: 3 [774/96 (806%)]\tLoss: 17.604208\n",
      "Train Epoche: 3 [775/96 (807%)]\tLoss: 27.495409\n",
      "Train Epoche: 3 [776/96 (808%)]\tLoss: 32.217155\n",
      "Train Epoche: 3 [777/96 (809%)]\tLoss: 0.543722\n",
      "Train Epoche: 3 [778/96 (810%)]\tLoss: 0.762184\n",
      "Train Epoche: 3 [779/96 (811%)]\tLoss: 6.984437\n",
      "Train Epoche: 3 [780/96 (812%)]\tLoss: 1.532535\n",
      "Train Epoche: 3 [781/96 (814%)]\tLoss: 0.220331\n",
      "Train Epoche: 3 [782/96 (815%)]\tLoss: 79.453743\n",
      "Train Epoche: 3 [783/96 (816%)]\tLoss: 1.981905\n",
      "Train Epoche: 3 [784/96 (817%)]\tLoss: 1.966231\n",
      "Train Epoche: 3 [785/96 (818%)]\tLoss: 0.863487\n",
      "Train Epoche: 3 [786/96 (819%)]\tLoss: 2.342904\n",
      "Train Epoche: 3 [787/96 (820%)]\tLoss: 1.554644\n",
      "Train Epoche: 3 [788/96 (821%)]\tLoss: 0.085818\n",
      "Train Epoche: 3 [789/96 (822%)]\tLoss: 0.380383\n",
      "Train Epoche: 3 [790/96 (823%)]\tLoss: 5.891991\n",
      "Train Epoche: 3 [791/96 (824%)]\tLoss: 8.799202\n",
      "Train Epoche: 3 [792/96 (825%)]\tLoss: 2.419990\n",
      "Train Epoche: 3 [793/96 (826%)]\tLoss: 2.908638\n",
      "Train Epoche: 3 [794/96 (827%)]\tLoss: 33.630589\n",
      "Train Epoche: 3 [795/96 (828%)]\tLoss: 127.637611\n",
      "Train Epoche: 3 [796/96 (829%)]\tLoss: 26.430595\n",
      "Train Epoche: 3 [797/96 (830%)]\tLoss: 3.388795\n",
      "Train Epoche: 3 [798/96 (831%)]\tLoss: 6.742507\n",
      "Train Epoche: 3 [799/96 (832%)]\tLoss: 0.424146\n",
      "Train Epoche: 3 [800/96 (833%)]\tLoss: 16.313902\n",
      "Train Epoche: 3 [801/96 (834%)]\tLoss: 25.503185\n",
      "Train Epoche: 3 [802/96 (835%)]\tLoss: 0.806235\n",
      "Train Epoche: 3 [803/96 (836%)]\tLoss: 0.000405\n",
      "Train Epoche: 3 [804/96 (838%)]\tLoss: 0.372757\n",
      "Train Epoche: 3 [805/96 (839%)]\tLoss: 59.078568\n",
      "Train Epoche: 3 [806/96 (840%)]\tLoss: 11.395806\n",
      "Train Epoche: 3 [807/96 (841%)]\tLoss: 15.061365\n",
      "Train Epoche: 3 [808/96 (842%)]\tLoss: 28.993990\n",
      "Train Epoche: 3 [809/96 (843%)]\tLoss: 313.564758\n",
      "Train Epoche: 3 [810/96 (844%)]\tLoss: 51.705624\n",
      "Train Epoche: 3 [811/96 (845%)]\tLoss: 0.332266\n",
      "Train Epoche: 3 [812/96 (846%)]\tLoss: 1.472367\n",
      "Train Epoche: 3 [813/96 (847%)]\tLoss: 3.961059\n",
      "Train Epoche: 3 [814/96 (848%)]\tLoss: 52.968796\n",
      "Train Epoche: 3 [815/96 (849%)]\tLoss: 1.110679\n",
      "Train Epoche: 3 [816/96 (850%)]\tLoss: 67.367523\n",
      "Train Epoche: 3 [817/96 (851%)]\tLoss: 2.408686\n",
      "Train Epoche: 3 [818/96 (852%)]\tLoss: 3.859607\n",
      "Train Epoche: 3 [819/96 (853%)]\tLoss: 1.845586\n",
      "Train Epoche: 3 [820/96 (854%)]\tLoss: 15.183913\n",
      "Train Epoche: 3 [821/96 (855%)]\tLoss: 27.968916\n",
      "Train Epoche: 3 [822/96 (856%)]\tLoss: 69.409584\n",
      "Train Epoche: 3 [823/96 (857%)]\tLoss: 0.000038\n",
      "Train Epoche: 3 [824/96 (858%)]\tLoss: 3.240729\n",
      "Train Epoche: 3 [825/96 (859%)]\tLoss: 0.029325\n",
      "Train Epoche: 3 [826/96 (860%)]\tLoss: 29.275631\n",
      "Train Epoche: 3 [827/96 (861%)]\tLoss: 0.003132\n",
      "Train Epoche: 3 [828/96 (862%)]\tLoss: 16.378841\n",
      "Train Epoche: 3 [829/96 (864%)]\tLoss: 210.465332\n",
      "Train Epoche: 3 [830/96 (865%)]\tLoss: 37.951366\n",
      "Train Epoche: 3 [831/96 (866%)]\tLoss: 3.942426\n",
      "Train Epoche: 3 [832/96 (867%)]\tLoss: 0.681387\n",
      "Train Epoche: 3 [833/96 (868%)]\tLoss: 22.307852\n",
      "Train Epoche: 3 [834/96 (869%)]\tLoss: 0.850761\n",
      "Train Epoche: 3 [835/96 (870%)]\tLoss: 53.235073\n",
      "Train Epoche: 3 [836/96 (871%)]\tLoss: 14.220224\n",
      "Train Epoche: 3 [837/96 (872%)]\tLoss: 103.454079\n",
      "Train Epoche: 3 [838/96 (873%)]\tLoss: 17.088842\n",
      "Train Epoche: 3 [839/96 (874%)]\tLoss: 3.943583\n",
      "Train Epoche: 3 [840/96 (875%)]\tLoss: 0.028407\n",
      "Train Epoche: 3 [841/96 (876%)]\tLoss: 1.123126\n",
      "Train Epoche: 3 [842/96 (877%)]\tLoss: 1.523997\n",
      "Train Epoche: 3 [843/96 (878%)]\tLoss: 12.963862\n",
      "Train Epoche: 3 [844/96 (879%)]\tLoss: 11.350993\n",
      "Train Epoche: 3 [845/96 (880%)]\tLoss: 0.375148\n",
      "Train Epoche: 3 [846/96 (881%)]\tLoss: 2.508868\n",
      "Train Epoche: 3 [847/96 (882%)]\tLoss: 92.367256\n",
      "Train Epoche: 3 [848/96 (883%)]\tLoss: 6.831014\n",
      "Train Epoche: 3 [849/96 (884%)]\tLoss: 0.111917\n",
      "Train Epoche: 3 [850/96 (885%)]\tLoss: 19.495441\n",
      "Train Epoche: 3 [851/96 (886%)]\tLoss: 91.915634\n",
      "Train Epoche: 3 [852/96 (888%)]\tLoss: 5.299714\n",
      "Train Epoche: 3 [853/96 (889%)]\tLoss: 20.046238\n",
      "Train Epoche: 3 [854/96 (890%)]\tLoss: 34.506096\n",
      "Train Epoche: 3 [855/96 (891%)]\tLoss: 0.962993\n",
      "Train Epoche: 3 [856/96 (892%)]\tLoss: 17.083679\n",
      "Train Epoche: 3 [857/96 (893%)]\tLoss: 1.944619\n",
      "Train Epoche: 3 [858/96 (894%)]\tLoss: 98.243896\n",
      "Train Epoche: 3 [859/96 (895%)]\tLoss: 1.583202\n",
      "Train Epoche: 3 [860/96 (896%)]\tLoss: 5.366975\n",
      "Train Epoche: 3 [861/96 (897%)]\tLoss: 0.066883\n",
      "Train Epoche: 3 [862/96 (898%)]\tLoss: 7.861105\n",
      "Train Epoche: 3 [863/96 (899%)]\tLoss: 3.413834\n",
      "Train Epoche: 3 [864/96 (900%)]\tLoss: 18.849125\n",
      "Train Epoche: 3 [865/96 (901%)]\tLoss: 76.785500\n",
      "Train Epoche: 3 [866/96 (902%)]\tLoss: 33.157104\n",
      "Train Epoche: 3 [867/96 (903%)]\tLoss: 2.275203\n",
      "Train Epoche: 3 [868/96 (904%)]\tLoss: 4.360030\n",
      "Train Epoche: 3 [869/96 (905%)]\tLoss: 0.050850\n",
      "Train Epoche: 3 [870/96 (906%)]\tLoss: 2.461226\n",
      "Train Epoche: 3 [871/96 (907%)]\tLoss: 3.083069\n",
      "Train Epoche: 3 [872/96 (908%)]\tLoss: 2.600754\n",
      "Train Epoche: 3 [873/96 (909%)]\tLoss: 0.061147\n",
      "Train Epoche: 3 [874/96 (910%)]\tLoss: 0.612386\n",
      "Train Epoche: 3 [875/96 (911%)]\tLoss: 2.577095\n",
      "Train Epoche: 3 [876/96 (912%)]\tLoss: 0.273996\n",
      "Train Epoche: 3 [877/96 (914%)]\tLoss: 4.535546\n",
      "Train Epoche: 3 [878/96 (915%)]\tLoss: 0.003303\n",
      "Train Epoche: 3 [879/96 (916%)]\tLoss: 9.355509\n",
      "Train Epoche: 3 [880/96 (917%)]\tLoss: 2.936893\n",
      "Train Epoche: 3 [881/96 (918%)]\tLoss: 4.278969\n",
      "Train Epoche: 3 [882/96 (919%)]\tLoss: 49.643368\n",
      "Train Epoche: 3 [883/96 (920%)]\tLoss: 44.509232\n",
      "Train Epoche: 3 [884/96 (921%)]\tLoss: 3.122677\n",
      "Train Epoche: 3 [885/96 (922%)]\tLoss: 18.253906\n",
      "Train Epoche: 3 [886/96 (923%)]\tLoss: 0.263402\n",
      "Train Epoche: 3 [887/96 (924%)]\tLoss: 26.162519\n",
      "Train Epoche: 3 [888/96 (925%)]\tLoss: 5.055948\n",
      "Train Epoche: 3 [889/96 (926%)]\tLoss: 2.618328\n",
      "Train Epoche: 3 [890/96 (927%)]\tLoss: 0.006793\n",
      "Train Epoche: 3 [891/96 (928%)]\tLoss: 14.153771\n",
      "Train Epoche: 3 [892/96 (929%)]\tLoss: 5.444528\n",
      "Train Epoche: 3 [893/96 (930%)]\tLoss: 0.038768\n",
      "Train Epoche: 3 [894/96 (931%)]\tLoss: 3.431329\n",
      "Train Epoche: 3 [895/96 (932%)]\tLoss: 0.345425\n",
      "Train Epoche: 3 [896/96 (933%)]\tLoss: 2.951226\n",
      "Train Epoche: 3 [897/96 (934%)]\tLoss: 0.935022\n",
      "Train Epoche: 3 [898/96 (935%)]\tLoss: 13.558459\n",
      "Train Epoche: 3 [899/96 (936%)]\tLoss: 0.543157\n",
      "Train Epoche: 3 [900/96 (938%)]\tLoss: 54.283562\n",
      "Train Epoche: 3 [901/96 (939%)]\tLoss: 3.136728\n",
      "Train Epoche: 3 [902/96 (940%)]\tLoss: 0.086199\n",
      "Train Epoche: 3 [903/96 (941%)]\tLoss: 1.769920\n",
      "Train Epoche: 3 [904/96 (942%)]\tLoss: 3.727386\n",
      "Train Epoche: 3 [905/96 (943%)]\tLoss: 29.931993\n",
      "Train Epoche: 3 [906/96 (944%)]\tLoss: 0.254813\n",
      "Train Epoche: 3 [907/96 (945%)]\tLoss: 15.689683\n",
      "Train Epoche: 3 [908/96 (946%)]\tLoss: 1.163674\n",
      "Train Epoche: 3 [909/96 (947%)]\tLoss: 28.210337\n",
      "Train Epoche: 3 [910/96 (948%)]\tLoss: 1.859152\n",
      "Train Epoche: 3 [911/96 (949%)]\tLoss: 6.053832\n",
      "Train Epoche: 3 [912/96 (950%)]\tLoss: 2.787066\n",
      "Train Epoche: 3 [913/96 (951%)]\tLoss: 14.707496\n",
      "Train Epoche: 3 [914/96 (952%)]\tLoss: 35.640961\n",
      "Train Epoche: 3 [915/96 (953%)]\tLoss: 0.009944\n",
      "Train Epoche: 3 [916/96 (954%)]\tLoss: 0.446071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [917/96 (955%)]\tLoss: 0.000751\n",
      "Train Epoche: 3 [918/96 (956%)]\tLoss: 23.836452\n",
      "Train Epoche: 3 [919/96 (957%)]\tLoss: 27.613834\n",
      "Train Epoche: 3 [920/96 (958%)]\tLoss: 0.006929\n",
      "Train Epoche: 3 [921/96 (959%)]\tLoss: 0.000055\n",
      "Train Epoche: 3 [922/96 (960%)]\tLoss: 123.935097\n",
      "Train Epoche: 3 [923/96 (961%)]\tLoss: 21.055147\n",
      "Train Epoche: 3 [924/96 (962%)]\tLoss: 0.834698\n",
      "Train Epoche: 3 [925/96 (964%)]\tLoss: 1.672220\n",
      "Train Epoche: 3 [926/96 (965%)]\tLoss: 34.119015\n",
      "Train Epoche: 3 [927/96 (966%)]\tLoss: 3.494856\n",
      "Train Epoche: 3 [928/96 (967%)]\tLoss: 7.231640\n",
      "Train Epoche: 3 [929/96 (968%)]\tLoss: 14.572047\n",
      "Train Epoche: 3 [930/96 (969%)]\tLoss: 7.965840\n",
      "Train Epoche: 3 [931/96 (970%)]\tLoss: 2.666760\n",
      "Train Epoche: 3 [932/96 (971%)]\tLoss: 27.873856\n",
      "Train Epoche: 3 [933/96 (972%)]\tLoss: 258.177216\n",
      "Train Epoche: 3 [934/96 (973%)]\tLoss: 0.402889\n",
      "Train Epoche: 3 [935/96 (974%)]\tLoss: 7.619928\n",
      "Train Epoche: 3 [936/96 (975%)]\tLoss: 0.106183\n",
      "Train Epoche: 3 [937/96 (976%)]\tLoss: 2.180181\n",
      "Train Epoche: 3 [938/96 (977%)]\tLoss: 0.144701\n",
      "Train Epoche: 3 [939/96 (978%)]\tLoss: 19.627319\n",
      "Train Epoche: 3 [940/96 (979%)]\tLoss: 0.068891\n",
      "Train Epoche: 3 [941/96 (980%)]\tLoss: 22.499994\n",
      "Train Epoche: 3 [942/96 (981%)]\tLoss: 4.165297\n",
      "Train Epoche: 3 [943/96 (982%)]\tLoss: 0.850218\n",
      "Train Epoche: 3 [944/96 (983%)]\tLoss: 1.047940\n",
      "Train Epoche: 3 [945/96 (984%)]\tLoss: 22.560520\n",
      "Train Epoche: 3 [946/96 (985%)]\tLoss: 0.787654\n",
      "Train Epoche: 3 [947/96 (986%)]\tLoss: 6.353789\n",
      "Train Epoche: 3 [948/96 (988%)]\tLoss: 1.076470\n",
      "Train Epoche: 3 [949/96 (989%)]\tLoss: 2.814508\n",
      "Train Epoche: 3 [950/96 (990%)]\tLoss: 0.669319\n",
      "Train Epoche: 3 [951/96 (991%)]\tLoss: 2.080885\n",
      "Train Epoche: 3 [952/96 (992%)]\tLoss: 18.168827\n",
      "Train Epoche: 3 [953/96 (993%)]\tLoss: 27.902140\n",
      "Train Epoche: 3 [954/96 (994%)]\tLoss: 112.806519\n",
      "Train Epoche: 3 [955/96 (995%)]\tLoss: 24.525871\n",
      "Train Epoche: 3 [956/96 (996%)]\tLoss: 24.436632\n",
      "Train Epoche: 3 [957/96 (997%)]\tLoss: 6.250032\n",
      "Train Epoche: 3 [958/96 (998%)]\tLoss: 17.449362\n",
      "Train Epoche: 3 [959/96 (999%)]\tLoss: 25.079437\n",
      "Train Epoche: 3 [960/96 (1000%)]\tLoss: 7.736646\n",
      "Train Epoche: 3 [961/96 (1001%)]\tLoss: 4.202505\n",
      "Train Epoche: 3 [962/96 (1002%)]\tLoss: 13.537914\n",
      "Train Epoche: 3 [963/96 (1003%)]\tLoss: 48.562454\n",
      "Train Epoche: 3 [964/96 (1004%)]\tLoss: 152.444504\n",
      "Train Epoche: 3 [965/96 (1005%)]\tLoss: 28.552893\n",
      "Train Epoche: 3 [966/96 (1006%)]\tLoss: 73.960762\n",
      "Train Epoche: 3 [967/96 (1007%)]\tLoss: 26.978252\n",
      "Train Epoche: 3 [968/96 (1008%)]\tLoss: 11.186213\n",
      "Train Epoche: 3 [969/96 (1009%)]\tLoss: 32.863918\n",
      "Train Epoche: 3 [970/96 (1010%)]\tLoss: 0.197710\n",
      "Train Epoche: 3 [971/96 (1011%)]\tLoss: 1.304347\n",
      "Train Epoche: 3 [972/96 (1012%)]\tLoss: 0.345104\n",
      "Train Epoche: 3 [973/96 (1014%)]\tLoss: 12.374919\n",
      "Train Epoche: 3 [974/96 (1015%)]\tLoss: 1.528920\n",
      "Train Epoche: 3 [975/96 (1016%)]\tLoss: 7.324048\n",
      "Train Epoche: 3 [976/96 (1017%)]\tLoss: 15.945329\n",
      "Train Epoche: 3 [977/96 (1018%)]\tLoss: 52.953403\n",
      "Train Epoche: 3 [978/96 (1019%)]\tLoss: 0.102395\n",
      "Train Epoche: 3 [979/96 (1020%)]\tLoss: 19.232895\n",
      "Train Epoche: 3 [980/96 (1021%)]\tLoss: 0.103057\n",
      "Train Epoche: 3 [981/96 (1022%)]\tLoss: 192.182053\n",
      "Train Epoche: 3 [982/96 (1023%)]\tLoss: 0.063469\n",
      "Train Epoche: 3 [983/96 (1024%)]\tLoss: 0.326368\n",
      "Train Epoche: 3 [984/96 (1025%)]\tLoss: 6.536277\n",
      "Train Epoche: 3 [985/96 (1026%)]\tLoss: 39.841530\n",
      "Train Epoche: 3 [986/96 (1027%)]\tLoss: 3.282222\n",
      "Train Epoche: 3 [987/96 (1028%)]\tLoss: 4.422335\n",
      "Train Epoche: 3 [988/96 (1029%)]\tLoss: 11.287528\n",
      "Train Epoche: 3 [989/96 (1030%)]\tLoss: 28.725149\n",
      "Train Epoche: 3 [990/96 (1031%)]\tLoss: 1.630460\n",
      "Train Epoche: 3 [991/96 (1032%)]\tLoss: 5.089439\n",
      "Train Epoche: 3 [992/96 (1033%)]\tLoss: 0.953128\n",
      "Train Epoche: 3 [993/96 (1034%)]\tLoss: 0.953298\n",
      "Train Epoche: 3 [994/96 (1035%)]\tLoss: 128.251099\n",
      "Train Epoche: 3 [995/96 (1036%)]\tLoss: 18.768841\n",
      "Train Epoche: 3 [996/96 (1038%)]\tLoss: 40.937328\n",
      "Train Epoche: 3 [997/96 (1039%)]\tLoss: 4.045441\n",
      "Train Epoche: 3 [998/96 (1040%)]\tLoss: 1.572689\n",
      "Train Epoche: 3 [999/96 (1041%)]\tLoss: 1.265093\n",
      "Train Epoche: 3 [1000/96 (1042%)]\tLoss: 7.173901\n",
      "Train Epoche: 3 [1001/96 (1043%)]\tLoss: 36.581341\n",
      "Train Epoche: 3 [1002/96 (1044%)]\tLoss: 3.449626\n",
      "Train Epoche: 3 [1003/96 (1045%)]\tLoss: 0.316719\n",
      "Train Epoche: 3 [1004/96 (1046%)]\tLoss: 4.978437\n",
      "Train Epoche: 3 [1005/96 (1047%)]\tLoss: 0.026977\n",
      "Train Epoche: 3 [1006/96 (1048%)]\tLoss: 0.113673\n",
      "Train Epoche: 3 [1007/96 (1049%)]\tLoss: 26.420006\n",
      "Train Epoche: 3 [1008/96 (1050%)]\tLoss: 1.636959\n",
      "Train Epoche: 3 [1009/96 (1051%)]\tLoss: 0.013285\n",
      "Train Epoche: 3 [1010/96 (1052%)]\tLoss: 4.080155\n",
      "Train Epoche: 3 [1011/96 (1053%)]\tLoss: 22.614552\n",
      "Train Epoche: 3 [1012/96 (1054%)]\tLoss: 180.494568\n",
      "Train Epoche: 3 [1013/96 (1055%)]\tLoss: 53.868378\n",
      "Train Epoche: 3 [1014/96 (1056%)]\tLoss: 14.961911\n",
      "Train Epoche: 3 [1015/96 (1057%)]\tLoss: 8.701762\n",
      "Train Epoche: 3 [1016/96 (1058%)]\tLoss: 967.006592\n",
      "Train Epoche: 3 [1017/96 (1059%)]\tLoss: 227.240692\n",
      "Train Epoche: 3 [1018/96 (1060%)]\tLoss: 44.487640\n",
      "Train Epoche: 3 [1019/96 (1061%)]\tLoss: 10.451492\n",
      "Train Epoche: 3 [1020/96 (1062%)]\tLoss: 145.264908\n",
      "Train Epoche: 3 [1021/96 (1064%)]\tLoss: 17.566288\n",
      "Train Epoche: 3 [1022/96 (1065%)]\tLoss: 25.245296\n",
      "Train Epoche: 3 [1023/96 (1066%)]\tLoss: 3.397535\n",
      "Train Epoche: 3 [1024/96 (1067%)]\tLoss: 2.625898\n",
      "Train Epoche: 3 [1025/96 (1068%)]\tLoss: 61.985085\n",
      "Train Epoche: 3 [1026/96 (1069%)]\tLoss: 0.690886\n",
      "Train Epoche: 3 [1027/96 (1070%)]\tLoss: 81.266602\n",
      "Train Epoche: 3 [1028/96 (1071%)]\tLoss: 112.329559\n",
      "Train Epoche: 3 [1029/96 (1072%)]\tLoss: 24.431484\n",
      "Train Epoche: 3 [1030/96 (1073%)]\tLoss: 27.324951\n",
      "Train Epoche: 3 [1031/96 (1074%)]\tLoss: 21.284327\n",
      "Train Epoche: 3 [1032/96 (1075%)]\tLoss: 7.049344\n",
      "Train Epoche: 3 [1033/96 (1076%)]\tLoss: 437.613678\n",
      "Train Epoche: 3 [1034/96 (1077%)]\tLoss: 248.115372\n",
      "Train Epoche: 3 [1035/96 (1078%)]\tLoss: 138.940628\n",
      "Train Epoche: 3 [1036/96 (1079%)]\tLoss: 0.237744\n",
      "Train Epoche: 3 [1037/96 (1080%)]\tLoss: 0.394375\n",
      "Train Epoche: 3 [1038/96 (1081%)]\tLoss: 43.515221\n",
      "Train Epoche: 3 [1039/96 (1082%)]\tLoss: 156.465408\n",
      "Train Epoche: 3 [1040/96 (1083%)]\tLoss: 157.150223\n",
      "Train Epoche: 3 [1041/96 (1084%)]\tLoss: 1.456693\n",
      "Train Epoche: 3 [1042/96 (1085%)]\tLoss: 0.006183\n",
      "Train Epoche: 3 [1043/96 (1086%)]\tLoss: 90.204559\n",
      "Train Epoche: 3 [1044/96 (1088%)]\tLoss: 194.148376\n",
      "Train Epoche: 3 [1045/96 (1089%)]\tLoss: 7.394514\n",
      "Train Epoche: 3 [1046/96 (1090%)]\tLoss: 0.144917\n",
      "Train Epoche: 3 [1047/96 (1091%)]\tLoss: 51.509632\n",
      "Train Epoche: 3 [1048/96 (1092%)]\tLoss: 3.160463\n",
      "Train Epoche: 3 [1049/96 (1093%)]\tLoss: 199.237961\n",
      "Train Epoche: 3 [1050/96 (1094%)]\tLoss: 73.318954\n",
      "Train Epoche: 3 [1051/96 (1095%)]\tLoss: 46.760479\n",
      "Train Epoche: 3 [1052/96 (1096%)]\tLoss: 8.174340\n",
      "Train Epoche: 3 [1053/96 (1097%)]\tLoss: 5.588899\n",
      "Train Epoche: 3 [1054/96 (1098%)]\tLoss: 6.077425\n",
      "Train Epoche: 3 [1055/96 (1099%)]\tLoss: 0.112168\n",
      "Train Epoche: 3 [1056/96 (1100%)]\tLoss: 2.586084\n",
      "Train Epoche: 3 [1057/96 (1101%)]\tLoss: 0.318189\n",
      "Train Epoche: 3 [1058/96 (1102%)]\tLoss: 1.083954\n",
      "Train Epoche: 3 [1059/96 (1103%)]\tLoss: 34.552570\n",
      "Train Epoche: 3 [1060/96 (1104%)]\tLoss: 24.973419\n",
      "Train Epoche: 3 [1061/96 (1105%)]\tLoss: 0.040665\n",
      "Train Epoche: 3 [1062/96 (1106%)]\tLoss: 1.818615\n",
      "Train Epoche: 3 [1063/96 (1107%)]\tLoss: 0.108246\n",
      "Train Epoche: 3 [1064/96 (1108%)]\tLoss: 1.189530\n",
      "Train Epoche: 3 [1065/96 (1109%)]\tLoss: 0.938502\n",
      "Train Epoche: 3 [1066/96 (1110%)]\tLoss: 0.460190\n",
      "Train Epoche: 3 [1067/96 (1111%)]\tLoss: 0.843677\n",
      "Train Epoche: 3 [1068/96 (1112%)]\tLoss: 2.447066\n",
      "Train Epoche: 3 [1069/96 (1114%)]\tLoss: 3.796844\n",
      "Train Epoche: 3 [1070/96 (1115%)]\tLoss: 32.701878\n",
      "Train Epoche: 3 [1071/96 (1116%)]\tLoss: 13.353928\n",
      "Train Epoche: 3 [1072/96 (1117%)]\tLoss: 0.100497\n",
      "Train Epoche: 3 [1073/96 (1118%)]\tLoss: 1.355357\n",
      "Train Epoche: 3 [1074/96 (1119%)]\tLoss: 2.068205\n",
      "Train Epoche: 3 [1075/96 (1120%)]\tLoss: 74.066895\n",
      "Train Epoche: 3 [1076/96 (1121%)]\tLoss: 1.068964\n",
      "Train Epoche: 3 [1077/96 (1122%)]\tLoss: 1.229143\n",
      "Train Epoche: 3 [1078/96 (1123%)]\tLoss: 8.976977\n",
      "Train Epoche: 3 [1079/96 (1124%)]\tLoss: 7.906702\n",
      "Train Epoche: 3 [1080/96 (1125%)]\tLoss: 0.207801\n",
      "Train Epoche: 3 [1081/96 (1126%)]\tLoss: 0.660572\n",
      "Train Epoche: 3 [1082/96 (1127%)]\tLoss: 6.132854\n",
      "Train Epoche: 3 [1083/96 (1128%)]\tLoss: 0.007108\n",
      "Train Epoche: 3 [1084/96 (1129%)]\tLoss: 42.120258\n",
      "Train Epoche: 3 [1085/96 (1130%)]\tLoss: 39.816978\n",
      "Train Epoche: 3 [1086/96 (1131%)]\tLoss: 13.889635\n",
      "Train Epoche: 3 [1087/96 (1132%)]\tLoss: 3.208895\n",
      "Train Epoche: 3 [1088/96 (1133%)]\tLoss: 1.613931\n",
      "Train Epoche: 3 [1089/96 (1134%)]\tLoss: 2.128759\n",
      "Train Epoche: 3 [1090/96 (1135%)]\tLoss: 2.461135\n",
      "Train Epoche: 3 [1091/96 (1136%)]\tLoss: 20.889526\n",
      "Train Epoche: 3 [1092/96 (1138%)]\tLoss: 13.738361\n",
      "Train Epoche: 3 [1093/96 (1139%)]\tLoss: 14.841403\n",
      "Train Epoche: 3 [1094/96 (1140%)]\tLoss: 11.658037\n",
      "Train Epoche: 3 [1095/96 (1141%)]\tLoss: 5.908466\n",
      "Train Epoche: 3 [1096/96 (1142%)]\tLoss: 37.477211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1097/96 (1143%)]\tLoss: 14.874671\n",
      "Train Epoche: 3 [1098/96 (1144%)]\tLoss: 4.749302\n",
      "Train Epoche: 3 [1099/96 (1145%)]\tLoss: 3.549107\n",
      "Train Epoche: 3 [1100/96 (1146%)]\tLoss: 26.423044\n",
      "Train Epoche: 3 [1101/96 (1147%)]\tLoss: 5.914495\n",
      "Train Epoche: 3 [1102/96 (1148%)]\tLoss: 2.139887\n",
      "Train Epoche: 3 [1103/96 (1149%)]\tLoss: 13.193381\n",
      "Train Epoche: 3 [1104/96 (1150%)]\tLoss: 0.529642\n",
      "Train Epoche: 3 [1105/96 (1151%)]\tLoss: 9.503479\n",
      "Train Epoche: 3 [1106/96 (1152%)]\tLoss: 0.071059\n",
      "Train Epoche: 3 [1107/96 (1153%)]\tLoss: 2.251654\n",
      "Train Epoche: 3 [1108/96 (1154%)]\tLoss: 3.196678\n",
      "Train Epoche: 3 [1109/96 (1155%)]\tLoss: 15.843752\n",
      "Train Epoche: 3 [1110/96 (1156%)]\tLoss: 260.992371\n",
      "Train Epoche: 3 [1111/96 (1157%)]\tLoss: 8.447068\n",
      "Train Epoche: 3 [1112/96 (1158%)]\tLoss: 2.715491\n",
      "Train Epoche: 3 [1113/96 (1159%)]\tLoss: 6.276689\n",
      "Train Epoche: 3 [1114/96 (1160%)]\tLoss: 13.731265\n",
      "Train Epoche: 3 [1115/96 (1161%)]\tLoss: 1.969367\n",
      "Train Epoche: 3 [1116/96 (1162%)]\tLoss: 1.230169\n",
      "Train Epoche: 3 [1117/96 (1164%)]\tLoss: 35.912254\n",
      "Train Epoche: 3 [1118/96 (1165%)]\tLoss: 2.784230\n",
      "Train Epoche: 3 [1119/96 (1166%)]\tLoss: 3.409592\n",
      "Train Epoche: 3 [1120/96 (1167%)]\tLoss: 2.747326\n",
      "Train Epoche: 3 [1121/96 (1168%)]\tLoss: 1.347545\n",
      "Train Epoche: 3 [1122/96 (1169%)]\tLoss: 11.016820\n",
      "Train Epoche: 3 [1123/96 (1170%)]\tLoss: 5.453334\n",
      "Train Epoche: 3 [1124/96 (1171%)]\tLoss: 5.668019\n",
      "Train Epoche: 3 [1125/96 (1172%)]\tLoss: 40.958824\n",
      "Train Epoche: 3 [1126/96 (1173%)]\tLoss: 11.767048\n",
      "Train Epoche: 3 [1127/96 (1174%)]\tLoss: 19.732185\n",
      "Train Epoche: 3 [1128/96 (1175%)]\tLoss: 24.766411\n",
      "Train Epoche: 3 [1129/96 (1176%)]\tLoss: 28.570414\n",
      "Train Epoche: 3 [1130/96 (1177%)]\tLoss: 1.278629\n",
      "Train Epoche: 3 [1131/96 (1178%)]\tLoss: 1.886185\n",
      "Train Epoche: 3 [1132/96 (1179%)]\tLoss: 0.366216\n",
      "Train Epoche: 3 [1133/96 (1180%)]\tLoss: 0.037818\n",
      "Train Epoche: 3 [1134/96 (1181%)]\tLoss: 39.013294\n",
      "Train Epoche: 3 [1135/96 (1182%)]\tLoss: 308.706421\n",
      "Train Epoche: 3 [1136/96 (1183%)]\tLoss: 243.270462\n",
      "Train Epoche: 3 [1137/96 (1184%)]\tLoss: 0.019538\n",
      "Train Epoche: 3 [1138/96 (1185%)]\tLoss: 0.051010\n",
      "Train Epoche: 3 [1139/96 (1186%)]\tLoss: 14.917943\n",
      "Train Epoche: 3 [1140/96 (1188%)]\tLoss: 3.133996\n",
      "Train Epoche: 3 [1141/96 (1189%)]\tLoss: 50.709435\n",
      "Train Epoche: 3 [1142/96 (1190%)]\tLoss: 12.119558\n",
      "Train Epoche: 3 [1143/96 (1191%)]\tLoss: 2.038483\n",
      "Train Epoche: 3 [1144/96 (1192%)]\tLoss: 1.951074\n",
      "Train Epoche: 3 [1145/96 (1193%)]\tLoss: 36.640408\n",
      "Train Epoche: 3 [1146/96 (1194%)]\tLoss: 49.719650\n",
      "Train Epoche: 3 [1147/96 (1195%)]\tLoss: 0.000556\n",
      "Train Epoche: 3 [1148/96 (1196%)]\tLoss: 21.465231\n",
      "Train Epoche: 3 [1149/96 (1197%)]\tLoss: 0.707126\n",
      "Train Epoche: 3 [1150/96 (1198%)]\tLoss: 12.277934\n",
      "Train Epoche: 3 [1151/96 (1199%)]\tLoss: 5.006639\n",
      "Train Epoche: 3 [1152/96 (1200%)]\tLoss: 6.311630\n",
      "Train Epoche: 3 [1153/96 (1201%)]\tLoss: 6.127638\n",
      "Train Epoche: 3 [1154/96 (1202%)]\tLoss: 57.053967\n",
      "Train Epoche: 3 [1155/96 (1203%)]\tLoss: 4.566668\n",
      "Train Epoche: 3 [1156/96 (1204%)]\tLoss: 1.253149\n",
      "Train Epoche: 3 [1157/96 (1205%)]\tLoss: 1.174966\n",
      "Train Epoche: 3 [1158/96 (1206%)]\tLoss: 2.617426\n",
      "Train Epoche: 3 [1159/96 (1207%)]\tLoss: 1.037649\n",
      "Train Epoche: 3 [1160/96 (1208%)]\tLoss: 1.132211\n",
      "Train Epoche: 3 [1161/96 (1209%)]\tLoss: 20.174643\n",
      "Train Epoche: 3 [1162/96 (1210%)]\tLoss: 40.654766\n",
      "Train Epoche: 3 [1163/96 (1211%)]\tLoss: 0.726520\n",
      "Train Epoche: 3 [1164/96 (1212%)]\tLoss: 0.170388\n",
      "Train Epoche: 3 [1165/96 (1214%)]\tLoss: 0.348449\n",
      "Train Epoche: 3 [1166/96 (1215%)]\tLoss: 15.969314\n",
      "Train Epoche: 3 [1167/96 (1216%)]\tLoss: 10.014176\n",
      "Train Epoche: 3 [1168/96 (1217%)]\tLoss: 20.701687\n",
      "Train Epoche: 3 [1169/96 (1218%)]\tLoss: 0.447469\n",
      "Train Epoche: 3 [1170/96 (1219%)]\tLoss: 8.078097\n",
      "Train Epoche: 3 [1171/96 (1220%)]\tLoss: 9.280391\n",
      "Train Epoche: 3 [1172/96 (1221%)]\tLoss: 7.340432\n",
      "Train Epoche: 3 [1173/96 (1222%)]\tLoss: 4.904004\n",
      "Train Epoche: 3 [1174/96 (1223%)]\tLoss: 28.009785\n",
      "Train Epoche: 3 [1175/96 (1224%)]\tLoss: 4.434641\n",
      "Train Epoche: 3 [1176/96 (1225%)]\tLoss: 41.133739\n",
      "Train Epoche: 3 [1177/96 (1226%)]\tLoss: 28.685295\n",
      "Train Epoche: 3 [1178/96 (1227%)]\tLoss: 5.787499\n",
      "Train Epoche: 3 [1179/96 (1228%)]\tLoss: 7.885820\n",
      "Train Epoche: 3 [1180/96 (1229%)]\tLoss: 15.157941\n",
      "Train Epoche: 3 [1181/96 (1230%)]\tLoss: 25.512558\n",
      "Train Epoche: 3 [1182/96 (1231%)]\tLoss: 10.705699\n",
      "Train Epoche: 3 [1183/96 (1232%)]\tLoss: 7.260248\n",
      "Train Epoche: 3 [1184/96 (1233%)]\tLoss: 4.947022\n",
      "Train Epoche: 3 [1185/96 (1234%)]\tLoss: 1.509438\n",
      "Train Epoche: 3 [1186/96 (1235%)]\tLoss: 3.790180\n",
      "Train Epoche: 3 [1187/96 (1236%)]\tLoss: 5.056789\n",
      "Train Epoche: 3 [1188/96 (1238%)]\tLoss: 3.934165\n",
      "Train Epoche: 3 [1189/96 (1239%)]\tLoss: 15.786453\n",
      "Train Epoche: 3 [1190/96 (1240%)]\tLoss: 2.984240\n",
      "Train Epoche: 3 [1191/96 (1241%)]\tLoss: 0.364410\n",
      "Train Epoche: 3 [1192/96 (1242%)]\tLoss: 0.139397\n",
      "Train Epoche: 3 [1193/96 (1243%)]\tLoss: 1.546688\n",
      "Train Epoche: 3 [1194/96 (1244%)]\tLoss: 0.083855\n",
      "Train Epoche: 3 [1195/96 (1245%)]\tLoss: 5.420504\n",
      "Train Epoche: 3 [1196/96 (1246%)]\tLoss: 2.001556\n",
      "Train Epoche: 3 [1197/96 (1247%)]\tLoss: 0.050348\n",
      "Train Epoche: 3 [1198/96 (1248%)]\tLoss: 161.872314\n",
      "Train Epoche: 3 [1199/96 (1249%)]\tLoss: 2.061402\n",
      "Train Epoche: 3 [1200/96 (1250%)]\tLoss: 0.064416\n",
      "Train Epoche: 3 [1201/96 (1251%)]\tLoss: 0.515404\n",
      "Train Epoche: 3 [1202/96 (1252%)]\tLoss: 0.262721\n",
      "Train Epoche: 3 [1203/96 (1253%)]\tLoss: 101.011360\n",
      "Train Epoche: 3 [1204/96 (1254%)]\tLoss: 2.149988\n",
      "Train Epoche: 3 [1205/96 (1255%)]\tLoss: 0.330450\n",
      "Train Epoche: 3 [1206/96 (1256%)]\tLoss: 2.165037\n",
      "Train Epoche: 3 [1207/96 (1257%)]\tLoss: 35.362755\n",
      "Train Epoche: 3 [1208/96 (1258%)]\tLoss: 0.463433\n",
      "Train Epoche: 3 [1209/96 (1259%)]\tLoss: 7.767564\n",
      "Train Epoche: 3 [1210/96 (1260%)]\tLoss: 4.890501\n",
      "Train Epoche: 3 [1211/96 (1261%)]\tLoss: 1.125825\n",
      "Train Epoche: 3 [1212/96 (1262%)]\tLoss: 0.061190\n",
      "Train Epoche: 3 [1213/96 (1264%)]\tLoss: 1.407727\n",
      "Train Epoche: 3 [1214/96 (1265%)]\tLoss: 3.527103\n",
      "Train Epoche: 3 [1215/96 (1266%)]\tLoss: 1.235813\n",
      "Train Epoche: 3 [1216/96 (1267%)]\tLoss: 0.756077\n",
      "Train Epoche: 3 [1217/96 (1268%)]\tLoss: 16.517159\n",
      "Train Epoche: 3 [1218/96 (1269%)]\tLoss: 0.616840\n",
      "Train Epoche: 3 [1219/96 (1270%)]\tLoss: 1.780810\n",
      "Train Epoche: 3 [1220/96 (1271%)]\tLoss: 0.586277\n",
      "Train Epoche: 3 [1221/96 (1272%)]\tLoss: 0.353103\n",
      "Train Epoche: 3 [1222/96 (1273%)]\tLoss: 0.002744\n",
      "Train Epoche: 3 [1223/96 (1274%)]\tLoss: 42.838150\n",
      "Train Epoche: 3 [1224/96 (1275%)]\tLoss: 0.775928\n",
      "Train Epoche: 3 [1225/96 (1276%)]\tLoss: 2.302557\n",
      "Train Epoche: 3 [1226/96 (1277%)]\tLoss: 4.501635\n",
      "Train Epoche: 3 [1227/96 (1278%)]\tLoss: 3.739744\n",
      "Train Epoche: 3 [1228/96 (1279%)]\tLoss: 0.923797\n",
      "Train Epoche: 3 [1229/96 (1280%)]\tLoss: 5.786836\n",
      "Train Epoche: 3 [1230/96 (1281%)]\tLoss: 33.660702\n",
      "Train Epoche: 3 [1231/96 (1282%)]\tLoss: 12.019461\n",
      "Train Epoche: 3 [1232/96 (1283%)]\tLoss: 0.668727\n",
      "Train Epoche: 3 [1233/96 (1284%)]\tLoss: 6.238135\n",
      "Train Epoche: 3 [1234/96 (1285%)]\tLoss: 0.124647\n",
      "Train Epoche: 3 [1235/96 (1286%)]\tLoss: 2.716670\n",
      "Train Epoche: 3 [1236/96 (1288%)]\tLoss: 33.045788\n",
      "Train Epoche: 3 [1237/96 (1289%)]\tLoss: 51.716858\n",
      "Train Epoche: 3 [1238/96 (1290%)]\tLoss: 0.000001\n",
      "Train Epoche: 3 [1239/96 (1291%)]\tLoss: 33.335564\n",
      "Train Epoche: 3 [1240/96 (1292%)]\tLoss: 0.159359\n",
      "Train Epoche: 3 [1241/96 (1293%)]\tLoss: 0.018975\n",
      "Train Epoche: 3 [1242/96 (1294%)]\tLoss: 3.229226\n",
      "Train Epoche: 3 [1243/96 (1295%)]\tLoss: 3.395056\n",
      "Train Epoche: 3 [1244/96 (1296%)]\tLoss: 6.036623\n",
      "Train Epoche: 3 [1245/96 (1297%)]\tLoss: 1.439640\n",
      "Train Epoche: 3 [1246/96 (1298%)]\tLoss: 30.329443\n",
      "Train Epoche: 3 [1247/96 (1299%)]\tLoss: 13.497321\n",
      "Train Epoche: 3 [1248/96 (1300%)]\tLoss: 4.020599\n",
      "Train Epoche: 3 [1249/96 (1301%)]\tLoss: 3.357055\n",
      "Train Epoche: 3 [1250/96 (1302%)]\tLoss: 12.947612\n",
      "Train Epoche: 3 [1251/96 (1303%)]\tLoss: 0.966891\n",
      "Train Epoche: 3 [1252/96 (1304%)]\tLoss: 3.619318\n",
      "Train Epoche: 3 [1253/96 (1305%)]\tLoss: 0.906697\n",
      "Train Epoche: 3 [1254/96 (1306%)]\tLoss: 66.605766\n",
      "Train Epoche: 3 [1255/96 (1307%)]\tLoss: 6.008720\n",
      "Train Epoche: 3 [1256/96 (1308%)]\tLoss: 1.942530\n",
      "Train Epoche: 3 [1257/96 (1309%)]\tLoss: 11.040592\n",
      "Train Epoche: 3 [1258/96 (1310%)]\tLoss: 5.640038\n",
      "Train Epoche: 3 [1259/96 (1311%)]\tLoss: 8.643390\n",
      "Train Epoche: 3 [1260/96 (1312%)]\tLoss: 0.565140\n",
      "Train Epoche: 3 [1261/96 (1314%)]\tLoss: 1.866957\n",
      "Train Epoche: 3 [1262/96 (1315%)]\tLoss: 10.157396\n",
      "Train Epoche: 3 [1263/96 (1316%)]\tLoss: 7.991161\n",
      "Train Epoche: 3 [1264/96 (1317%)]\tLoss: 0.112465\n",
      "Train Epoche: 3 [1265/96 (1318%)]\tLoss: 0.025462\n",
      "Train Epoche: 3 [1266/96 (1319%)]\tLoss: 184.615250\n",
      "Train Epoche: 3 [1267/96 (1320%)]\tLoss: 3.042426\n",
      "Train Epoche: 3 [1268/96 (1321%)]\tLoss: 1.925545\n",
      "Train Epoche: 3 [1269/96 (1322%)]\tLoss: 10.672910\n",
      "Train Epoche: 3 [1270/96 (1323%)]\tLoss: 14.602264\n",
      "Train Epoche: 3 [1271/96 (1324%)]\tLoss: 0.021357\n",
      "Train Epoche: 3 [1272/96 (1325%)]\tLoss: 0.539492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1273/96 (1326%)]\tLoss: 22.849163\n",
      "Train Epoche: 3 [1274/96 (1327%)]\tLoss: 148.389206\n",
      "Train Epoche: 3 [1275/96 (1328%)]\tLoss: 4.048336\n",
      "Train Epoche: 3 [1276/96 (1329%)]\tLoss: 3.535874\n",
      "Train Epoche: 3 [1277/96 (1330%)]\tLoss: 0.563337\n",
      "Train Epoche: 3 [1278/96 (1331%)]\tLoss: 3.899045\n",
      "Train Epoche: 3 [1279/96 (1332%)]\tLoss: 1.041862\n",
      "Train Epoche: 3 [1280/96 (1333%)]\tLoss: 7.849614\n",
      "Train Epoche: 3 [1281/96 (1334%)]\tLoss: 32.962708\n",
      "Train Epoche: 3 [1282/96 (1335%)]\tLoss: 9.823109\n",
      "Train Epoche: 3 [1283/96 (1336%)]\tLoss: 0.561886\n",
      "Train Epoche: 3 [1284/96 (1338%)]\tLoss: 5.123459\n",
      "Train Epoche: 3 [1285/96 (1339%)]\tLoss: 1.813253\n",
      "Train Epoche: 3 [1286/96 (1340%)]\tLoss: 0.153011\n",
      "Train Epoche: 3 [1287/96 (1341%)]\tLoss: 2.305223\n",
      "Train Epoche: 3 [1288/96 (1342%)]\tLoss: 55.096405\n",
      "Train Epoche: 3 [1289/96 (1343%)]\tLoss: 3.615647\n",
      "Train Epoche: 3 [1290/96 (1344%)]\tLoss: 0.249812\n",
      "Train Epoche: 3 [1291/96 (1345%)]\tLoss: 20.552187\n",
      "Train Epoche: 3 [1292/96 (1346%)]\tLoss: 13.172007\n",
      "Train Epoche: 3 [1293/96 (1347%)]\tLoss: 0.445787\n",
      "Train Epoche: 3 [1294/96 (1348%)]\tLoss: 1.793646\n",
      "Train Epoche: 3 [1295/96 (1349%)]\tLoss: 11.909619\n",
      "Train Epoche: 3 [1296/96 (1350%)]\tLoss: 0.002016\n",
      "Train Epoche: 3 [1297/96 (1351%)]\tLoss: 0.616566\n",
      "Train Epoche: 3 [1298/96 (1352%)]\tLoss: 3.548478\n",
      "Train Epoche: 3 [1299/96 (1353%)]\tLoss: 3.290044\n",
      "Train Epoche: 3 [1300/96 (1354%)]\tLoss: 20.228270\n",
      "Train Epoche: 3 [1301/96 (1355%)]\tLoss: 1.593145\n",
      "Train Epoche: 3 [1302/96 (1356%)]\tLoss: 10.094851\n",
      "Train Epoche: 3 [1303/96 (1357%)]\tLoss: 0.933351\n",
      "Train Epoche: 3 [1304/96 (1358%)]\tLoss: 1.318871\n",
      "Train Epoche: 3 [1305/96 (1359%)]\tLoss: 5.821270\n",
      "Train Epoche: 3 [1306/96 (1360%)]\tLoss: 10.589166\n",
      "Train Epoche: 3 [1307/96 (1361%)]\tLoss: 14.343967\n",
      "Train Epoche: 3 [1308/96 (1362%)]\tLoss: 1.495411\n",
      "Train Epoche: 3 [1309/96 (1364%)]\tLoss: 13.454064\n",
      "Train Epoche: 3 [1310/96 (1365%)]\tLoss: 14.634833\n",
      "Train Epoche: 3 [1311/96 (1366%)]\tLoss: 30.622522\n",
      "Train Epoche: 3 [1312/96 (1367%)]\tLoss: 5.123761\n",
      "Train Epoche: 3 [1313/96 (1368%)]\tLoss: 0.228778\n",
      "Train Epoche: 3 [1314/96 (1369%)]\tLoss: 30.043655\n",
      "Train Epoche: 3 [1315/96 (1370%)]\tLoss: 0.144183\n",
      "Train Epoche: 3 [1316/96 (1371%)]\tLoss: 1.805500\n",
      "Train Epoche: 3 [1317/96 (1372%)]\tLoss: 0.298388\n",
      "Train Epoche: 3 [1318/96 (1373%)]\tLoss: 154.310089\n",
      "Train Epoche: 3 [1319/96 (1374%)]\tLoss: 5.800325\n",
      "Train Epoche: 3 [1320/96 (1375%)]\tLoss: 1.142109\n",
      "Train Epoche: 3 [1321/96 (1376%)]\tLoss: 4.033510\n",
      "Train Epoche: 3 [1322/96 (1377%)]\tLoss: 18.420280\n",
      "Train Epoche: 3 [1323/96 (1378%)]\tLoss: 33.971432\n",
      "Train Epoche: 3 [1324/96 (1379%)]\tLoss: 3.982594\n",
      "Train Epoche: 3 [1325/96 (1380%)]\tLoss: 61.737923\n",
      "Train Epoche: 3 [1326/96 (1381%)]\tLoss: 0.070749\n",
      "Train Epoche: 3 [1327/96 (1382%)]\tLoss: 38.724407\n",
      "Train Epoche: 3 [1328/96 (1383%)]\tLoss: 6.814264\n",
      "Train Epoche: 3 [1329/96 (1384%)]\tLoss: 341.314423\n",
      "Train Epoche: 3 [1330/96 (1385%)]\tLoss: 0.080665\n",
      "Train Epoche: 3 [1331/96 (1386%)]\tLoss: 19.554497\n",
      "Train Epoche: 3 [1332/96 (1388%)]\tLoss: 0.058867\n",
      "Train Epoche: 3 [1333/96 (1389%)]\tLoss: 18.947701\n",
      "Train Epoche: 3 [1334/96 (1390%)]\tLoss: 12.046124\n",
      "Train Epoche: 3 [1335/96 (1391%)]\tLoss: 11.062556\n",
      "Train Epoche: 3 [1336/96 (1392%)]\tLoss: 0.122554\n",
      "Train Epoche: 3 [1337/96 (1393%)]\tLoss: 0.039617\n",
      "Train Epoche: 3 [1338/96 (1394%)]\tLoss: 2.365608\n",
      "Train Epoche: 3 [1339/96 (1395%)]\tLoss: 4.626584\n",
      "Train Epoche: 3 [1340/96 (1396%)]\tLoss: 229.583237\n",
      "Train Epoche: 3 [1341/96 (1397%)]\tLoss: 8.809978\n",
      "Train Epoche: 3 [1342/96 (1398%)]\tLoss: 0.060341\n",
      "Train Epoche: 3 [1343/96 (1399%)]\tLoss: 0.180106\n",
      "Train Epoche: 3 [1344/96 (1400%)]\tLoss: 32.414383\n",
      "Train Epoche: 3 [1345/96 (1401%)]\tLoss: 0.100468\n",
      "Train Epoche: 3 [1346/96 (1402%)]\tLoss: 4.763564\n",
      "Train Epoche: 3 [1347/96 (1403%)]\tLoss: 0.174276\n",
      "Train Epoche: 3 [1348/96 (1404%)]\tLoss: 2.227281\n",
      "Train Epoche: 3 [1349/96 (1405%)]\tLoss: 1.060531\n",
      "Train Epoche: 3 [1350/96 (1406%)]\tLoss: 1.500702\n",
      "Train Epoche: 3 [1351/96 (1407%)]\tLoss: 0.000127\n",
      "Train Epoche: 3 [1352/96 (1408%)]\tLoss: 4.592685\n",
      "Train Epoche: 3 [1353/96 (1409%)]\tLoss: 9.774401\n",
      "Train Epoche: 3 [1354/96 (1410%)]\tLoss: 6.884633\n",
      "Train Epoche: 3 [1355/96 (1411%)]\tLoss: 5.084935\n",
      "Train Epoche: 3 [1356/96 (1412%)]\tLoss: 8.287151\n",
      "Train Epoche: 3 [1357/96 (1414%)]\tLoss: 1.917911\n",
      "Train Epoche: 3 [1358/96 (1415%)]\tLoss: 4.688956\n",
      "Train Epoche: 3 [1359/96 (1416%)]\tLoss: 8.127233\n",
      "Train Epoche: 3 [1360/96 (1417%)]\tLoss: 0.097456\n",
      "Train Epoche: 3 [1361/96 (1418%)]\tLoss: 2.196958\n",
      "Train Epoche: 3 [1362/96 (1419%)]\tLoss: 4.609189\n",
      "Train Epoche: 3 [1363/96 (1420%)]\tLoss: 21.578775\n",
      "Train Epoche: 3 [1364/96 (1421%)]\tLoss: 130.274506\n",
      "Train Epoche: 3 [1365/96 (1422%)]\tLoss: 4.617437\n",
      "Train Epoche: 3 [1366/96 (1423%)]\tLoss: 5.529510\n",
      "Train Epoche: 3 [1367/96 (1424%)]\tLoss: 0.895568\n",
      "Train Epoche: 3 [1368/96 (1425%)]\tLoss: 0.871407\n",
      "Train Epoche: 3 [1369/96 (1426%)]\tLoss: 56.338612\n",
      "Train Epoche: 3 [1370/96 (1427%)]\tLoss: 6.610419\n",
      "Train Epoche: 3 [1371/96 (1428%)]\tLoss: 20.651911\n",
      "Train Epoche: 3 [1372/96 (1429%)]\tLoss: 0.104983\n",
      "Train Epoche: 3 [1373/96 (1430%)]\tLoss: 9.415602\n",
      "Train Epoche: 3 [1374/96 (1431%)]\tLoss: 10.589557\n",
      "Train Epoche: 3 [1375/96 (1432%)]\tLoss: 0.332842\n",
      "Train Epoche: 3 [1376/96 (1433%)]\tLoss: 0.258711\n",
      "Train Epoche: 3 [1377/96 (1434%)]\tLoss: 0.330169\n",
      "Train Epoche: 3 [1378/96 (1435%)]\tLoss: 0.044484\n",
      "Train Epoche: 3 [1379/96 (1436%)]\tLoss: 4.969309\n",
      "Train Epoche: 3 [1380/96 (1438%)]\tLoss: 2.194895\n",
      "Train Epoche: 3 [1381/96 (1439%)]\tLoss: 0.226250\n",
      "Train Epoche: 3 [1382/96 (1440%)]\tLoss: 1.181461\n",
      "Train Epoche: 3 [1383/96 (1441%)]\tLoss: 8.048160\n",
      "Train Epoche: 3 [1384/96 (1442%)]\tLoss: 3.671734\n",
      "Train Epoche: 3 [1385/96 (1443%)]\tLoss: 6.122757\n",
      "Train Epoche: 3 [1386/96 (1444%)]\tLoss: 10.794821\n",
      "Train Epoche: 3 [1387/96 (1445%)]\tLoss: 7.204296\n",
      "Train Epoche: 3 [1388/96 (1446%)]\tLoss: 9.841733\n",
      "Train Epoche: 3 [1389/96 (1447%)]\tLoss: 27.395807\n",
      "Train Epoche: 3 [1390/96 (1448%)]\tLoss: 15.173762\n",
      "Train Epoche: 3 [1391/96 (1449%)]\tLoss: 5.056633\n",
      "Train Epoche: 3 [1392/96 (1450%)]\tLoss: 0.975880\n",
      "Train Epoche: 3 [1393/96 (1451%)]\tLoss: 2.833730\n",
      "Train Epoche: 3 [1394/96 (1452%)]\tLoss: 52.474174\n",
      "Train Epoche: 3 [1395/96 (1453%)]\tLoss: 0.104460\n",
      "Train Epoche: 3 [1396/96 (1454%)]\tLoss: 10.442947\n",
      "Train Epoche: 3 [1397/96 (1455%)]\tLoss: 8.447087\n",
      "Train Epoche: 3 [1398/96 (1456%)]\tLoss: 0.451262\n",
      "Train Epoche: 3 [1399/96 (1457%)]\tLoss: 0.581563\n",
      "Train Epoche: 3 [1400/96 (1458%)]\tLoss: 1.871222\n",
      "Train Epoche: 3 [1401/96 (1459%)]\tLoss: 1.228832\n",
      "Train Epoche: 3 [1402/96 (1460%)]\tLoss: 0.897446\n",
      "Train Epoche: 3 [1403/96 (1461%)]\tLoss: 1.284796\n",
      "Train Epoche: 3 [1404/96 (1462%)]\tLoss: 2.109702\n",
      "Train Epoche: 3 [1405/96 (1464%)]\tLoss: 4.051000\n",
      "Train Epoche: 3 [1406/96 (1465%)]\tLoss: 0.341046\n",
      "Train Epoche: 3 [1407/96 (1466%)]\tLoss: 0.685039\n",
      "Train Epoche: 3 [1408/96 (1467%)]\tLoss: 3.479967\n",
      "Train Epoche: 3 [1409/96 (1468%)]\tLoss: 29.602758\n",
      "Train Epoche: 3 [1410/96 (1469%)]\tLoss: 2.106714\n",
      "Train Epoche: 3 [1411/96 (1470%)]\tLoss: 94.727112\n",
      "Train Epoche: 3 [1412/96 (1471%)]\tLoss: 4.056457\n",
      "Train Epoche: 3 [1413/96 (1472%)]\tLoss: 0.385333\n",
      "Train Epoche: 3 [1414/96 (1473%)]\tLoss: 1.548529\n",
      "Train Epoche: 3 [1415/96 (1474%)]\tLoss: 5.448049\n",
      "Train Epoche: 3 [1416/96 (1475%)]\tLoss: 2.112402\n",
      "Train Epoche: 3 [1417/96 (1476%)]\tLoss: 0.043670\n",
      "Train Epoche: 3 [1418/96 (1477%)]\tLoss: 73.011536\n",
      "Train Epoche: 3 [1419/96 (1478%)]\tLoss: 2.010368\n",
      "Train Epoche: 3 [1420/96 (1479%)]\tLoss: 10.609365\n",
      "Train Epoche: 3 [1421/96 (1480%)]\tLoss: 0.004339\n",
      "Train Epoche: 3 [1422/96 (1481%)]\tLoss: 0.245807\n",
      "Train Epoche: 3 [1423/96 (1482%)]\tLoss: 0.051459\n",
      "Train Epoche: 3 [1424/96 (1483%)]\tLoss: 0.575363\n",
      "Train Epoche: 3 [1425/96 (1484%)]\tLoss: 6.283232\n",
      "Train Epoche: 3 [1426/96 (1485%)]\tLoss: 0.214105\n",
      "Train Epoche: 3 [1427/96 (1486%)]\tLoss: 2.538845\n",
      "Train Epoche: 3 [1428/96 (1488%)]\tLoss: 5.562749\n",
      "Train Epoche: 3 [1429/96 (1489%)]\tLoss: 0.066347\n",
      "Train Epoche: 3 [1430/96 (1490%)]\tLoss: 0.035824\n",
      "Train Epoche: 3 [1431/96 (1491%)]\tLoss: 8.238963\n",
      "Train Epoche: 3 [1432/96 (1492%)]\tLoss: 3.084831\n",
      "Train Epoche: 3 [1433/96 (1493%)]\tLoss: 8.617475\n",
      "Train Epoche: 3 [1434/96 (1494%)]\tLoss: 0.478109\n",
      "Train Epoche: 3 [1435/96 (1495%)]\tLoss: 10.432487\n",
      "Train Epoche: 3 [1436/96 (1496%)]\tLoss: 0.044822\n",
      "Train Epoche: 3 [1437/96 (1497%)]\tLoss: 3.425883\n",
      "Train Epoche: 3 [1438/96 (1498%)]\tLoss: 8.788198\n",
      "Train Epoche: 3 [1439/96 (1499%)]\tLoss: 2.675511\n",
      "Train Epoche: 3 [1440/96 (1500%)]\tLoss: 6.359932\n",
      "Train Epoche: 3 [1441/96 (1501%)]\tLoss: 3.433223\n",
      "Train Epoche: 3 [1442/96 (1502%)]\tLoss: 77.809570\n",
      "Train Epoche: 3 [1443/96 (1503%)]\tLoss: 8.209786\n",
      "Train Epoche: 3 [1444/96 (1504%)]\tLoss: 1.689619\n",
      "Train Epoche: 3 [1445/96 (1505%)]\tLoss: 10.245780\n",
      "Train Epoche: 3 [1446/96 (1506%)]\tLoss: 24.135706\n",
      "Train Epoche: 3 [1447/96 (1507%)]\tLoss: 2.355133\n",
      "Train Epoche: 3 [1448/96 (1508%)]\tLoss: 1.183032\n",
      "Train Epoche: 3 [1449/96 (1509%)]\tLoss: 28.200975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1450/96 (1510%)]\tLoss: 12.170897\n",
      "Train Epoche: 3 [1451/96 (1511%)]\tLoss: 4.227268\n",
      "Train Epoche: 3 [1452/96 (1512%)]\tLoss: 2.994803\n",
      "Train Epoche: 3 [1453/96 (1514%)]\tLoss: 28.225067\n",
      "Train Epoche: 3 [1454/96 (1515%)]\tLoss: 0.023117\n",
      "Train Epoche: 3 [1455/96 (1516%)]\tLoss: 0.087913\n",
      "Train Epoche: 3 [1456/96 (1517%)]\tLoss: 16.413597\n",
      "Train Epoche: 3 [1457/96 (1518%)]\tLoss: 13.991665\n",
      "Train Epoche: 3 [1458/96 (1519%)]\tLoss: 3.430127\n",
      "Train Epoche: 3 [1459/96 (1520%)]\tLoss: 0.478832\n",
      "Train Epoche: 3 [1460/96 (1521%)]\tLoss: 0.467718\n",
      "Train Epoche: 3 [1461/96 (1522%)]\tLoss: 0.174477\n",
      "Train Epoche: 3 [1462/96 (1523%)]\tLoss: 0.125705\n",
      "Train Epoche: 3 [1463/96 (1524%)]\tLoss: 1.161911\n",
      "Train Epoche: 3 [1464/96 (1525%)]\tLoss: 3.939357\n",
      "Train Epoche: 3 [1465/96 (1526%)]\tLoss: 3.851540\n",
      "Train Epoche: 3 [1466/96 (1527%)]\tLoss: 2.357752\n",
      "Train Epoche: 3 [1467/96 (1528%)]\tLoss: 0.159561\n",
      "Train Epoche: 3 [1468/96 (1529%)]\tLoss: 86.627747\n",
      "Train Epoche: 3 [1469/96 (1530%)]\tLoss: 2.548887\n",
      "Train Epoche: 3 [1470/96 (1531%)]\tLoss: 4.223159\n",
      "Train Epoche: 3 [1471/96 (1532%)]\tLoss: 0.740894\n",
      "Train Epoche: 3 [1472/96 (1533%)]\tLoss: 0.432584\n",
      "Train Epoche: 3 [1473/96 (1534%)]\tLoss: 0.396899\n",
      "Train Epoche: 3 [1474/96 (1535%)]\tLoss: 32.001472\n",
      "Train Epoche: 3 [1475/96 (1536%)]\tLoss: 0.006340\n",
      "Train Epoche: 3 [1476/96 (1538%)]\tLoss: 34.818718\n",
      "Train Epoche: 3 [1477/96 (1539%)]\tLoss: 55.148487\n",
      "Train Epoche: 3 [1478/96 (1540%)]\tLoss: 1.016063\n",
      "Train Epoche: 3 [1479/96 (1541%)]\tLoss: 1.557389\n",
      "Train Epoche: 3 [1480/96 (1542%)]\tLoss: 13.139159\n",
      "Train Epoche: 3 [1481/96 (1543%)]\tLoss: 0.001880\n",
      "Train Epoche: 3 [1482/96 (1544%)]\tLoss: 4.025836\n",
      "Train Epoche: 3 [1483/96 (1545%)]\tLoss: 36.898823\n",
      "Train Epoche: 3 [1484/96 (1546%)]\tLoss: 3.328002\n",
      "Train Epoche: 3 [1485/96 (1547%)]\tLoss: 2.312810\n",
      "Train Epoche: 3 [1486/96 (1548%)]\tLoss: 62.176582\n",
      "Train Epoche: 3 [1487/96 (1549%)]\tLoss: 2.520187\n",
      "Train Epoche: 3 [1488/96 (1550%)]\tLoss: 31.227228\n",
      "Train Epoche: 3 [1489/96 (1551%)]\tLoss: 2.099427\n",
      "Train Epoche: 3 [1490/96 (1552%)]\tLoss: 17.457266\n",
      "Train Epoche: 3 [1491/96 (1553%)]\tLoss: 13.068892\n",
      "Train Epoche: 3 [1492/96 (1554%)]\tLoss: 12.193622\n",
      "Train Epoche: 3 [1493/96 (1555%)]\tLoss: 9.913655\n",
      "Train Epoche: 3 [1494/96 (1556%)]\tLoss: 25.504341\n",
      "Train Epoche: 3 [1495/96 (1557%)]\tLoss: 28.404476\n",
      "Train Epoche: 3 [1496/96 (1558%)]\tLoss: 33.171429\n",
      "Train Epoche: 3 [1497/96 (1559%)]\tLoss: 3.277027\n",
      "Train Epoche: 3 [1498/96 (1560%)]\tLoss: 0.711605\n",
      "Train Epoche: 3 [1499/96 (1561%)]\tLoss: 0.008303\n",
      "Train Epoche: 3 [1500/96 (1562%)]\tLoss: 0.684682\n",
      "Train Epoche: 3 [1501/96 (1564%)]\tLoss: 0.000800\n",
      "Train Epoche: 3 [1502/96 (1565%)]\tLoss: 10.172879\n",
      "Train Epoche: 3 [1503/96 (1566%)]\tLoss: 2.367798\n",
      "Train Epoche: 3 [1504/96 (1567%)]\tLoss: 19.666344\n",
      "Train Epoche: 3 [1505/96 (1568%)]\tLoss: 1.448739\n",
      "Train Epoche: 3 [1506/96 (1569%)]\tLoss: 1.033739\n",
      "Train Epoche: 3 [1507/96 (1570%)]\tLoss: 0.223565\n",
      "Train Epoche: 3 [1508/96 (1571%)]\tLoss: 25.474075\n",
      "Train Epoche: 3 [1509/96 (1572%)]\tLoss: 0.382277\n",
      "Train Epoche: 3 [1510/96 (1573%)]\tLoss: 6.836648\n",
      "Train Epoche: 3 [1511/96 (1574%)]\tLoss: 18.162605\n",
      "Train Epoche: 3 [1512/96 (1575%)]\tLoss: 0.021486\n",
      "Train Epoche: 3 [1513/96 (1576%)]\tLoss: 0.447738\n",
      "Train Epoche: 3 [1514/96 (1577%)]\tLoss: 2.906687\n",
      "Train Epoche: 3 [1515/96 (1578%)]\tLoss: 7.324105\n",
      "Train Epoche: 3 [1516/96 (1579%)]\tLoss: 4.873960\n",
      "Train Epoche: 3 [1517/96 (1580%)]\tLoss: 10.559487\n",
      "Train Epoche: 3 [1518/96 (1581%)]\tLoss: 1.369278\n",
      "Train Epoche: 3 [1519/96 (1582%)]\tLoss: 2.740821\n",
      "Train Epoche: 3 [1520/96 (1583%)]\tLoss: 0.000011\n",
      "Train Epoche: 3 [1521/96 (1584%)]\tLoss: 0.000766\n",
      "Train Epoche: 3 [1522/96 (1585%)]\tLoss: 13.415245\n",
      "Train Epoche: 3 [1523/96 (1586%)]\tLoss: 0.922413\n",
      "Train Epoche: 3 [1524/96 (1588%)]\tLoss: 19.050301\n",
      "Train Epoche: 3 [1525/96 (1589%)]\tLoss: 0.020975\n",
      "Train Epoche: 3 [1526/96 (1590%)]\tLoss: 16.071354\n",
      "Train Epoche: 3 [1527/96 (1591%)]\tLoss: 1.801786\n",
      "Train Epoche: 3 [1528/96 (1592%)]\tLoss: 2.441734\n",
      "Train Epoche: 3 [1529/96 (1593%)]\tLoss: 0.270060\n",
      "Train Epoche: 3 [1530/96 (1594%)]\tLoss: 4.484009\n",
      "Train Epoche: 3 [1531/96 (1595%)]\tLoss: 0.700918\n",
      "Train Epoche: 3 [1532/96 (1596%)]\tLoss: 6.363353\n",
      "Train Epoche: 3 [1533/96 (1597%)]\tLoss: 13.583734\n",
      "Train Epoche: 3 [1534/96 (1598%)]\tLoss: 0.466707\n",
      "Train Epoche: 3 [1535/96 (1599%)]\tLoss: 1.221121\n",
      "Train Epoche: 3 [1536/96 (1600%)]\tLoss: 0.749439\n",
      "Train Epoche: 3 [1537/96 (1601%)]\tLoss: 2.095875\n",
      "Train Epoche: 3 [1538/96 (1602%)]\tLoss: 70.207825\n",
      "Train Epoche: 3 [1539/96 (1603%)]\tLoss: 1.941426\n",
      "Train Epoche: 3 [1540/96 (1604%)]\tLoss: 2.134478\n",
      "Train Epoche: 3 [1541/96 (1605%)]\tLoss: 2.019031\n",
      "Train Epoche: 3 [1542/96 (1606%)]\tLoss: 16.565704\n",
      "Train Epoche: 3 [1543/96 (1607%)]\tLoss: 19.436584\n",
      "Train Epoche: 3 [1544/96 (1608%)]\tLoss: 0.106477\n",
      "Train Epoche: 3 [1545/96 (1609%)]\tLoss: 0.055501\n",
      "Train Epoche: 3 [1546/96 (1610%)]\tLoss: 0.100540\n",
      "Train Epoche: 3 [1547/96 (1611%)]\tLoss: 11.861112\n",
      "Train Epoche: 3 [1548/96 (1612%)]\tLoss: 18.490650\n",
      "Train Epoche: 3 [1549/96 (1614%)]\tLoss: 0.969962\n",
      "Train Epoche: 3 [1550/96 (1615%)]\tLoss: 8.544611\n",
      "Train Epoche: 3 [1551/96 (1616%)]\tLoss: 1.291965\n",
      "Train Epoche: 3 [1552/96 (1617%)]\tLoss: 1.417972\n",
      "Train Epoche: 3 [1553/96 (1618%)]\tLoss: 12.541075\n",
      "Train Epoche: 3 [1554/96 (1619%)]\tLoss: 3.734080\n",
      "Train Epoche: 3 [1555/96 (1620%)]\tLoss: 12.644415\n",
      "Train Epoche: 3 [1556/96 (1621%)]\tLoss: 0.168261\n",
      "Train Epoche: 3 [1557/96 (1622%)]\tLoss: 0.519506\n",
      "Train Epoche: 3 [1558/96 (1623%)]\tLoss: 54.937542\n",
      "Train Epoche: 3 [1559/96 (1624%)]\tLoss: 0.027115\n",
      "Train Epoche: 3 [1560/96 (1625%)]\tLoss: 7.023175\n",
      "Train Epoche: 3 [1561/96 (1626%)]\tLoss: 31.587271\n",
      "Train Epoche: 3 [1562/96 (1627%)]\tLoss: 0.287000\n",
      "Train Epoche: 3 [1563/96 (1628%)]\tLoss: 5.596694\n",
      "Train Epoche: 3 [1564/96 (1629%)]\tLoss: 0.598405\n",
      "Train Epoche: 3 [1565/96 (1630%)]\tLoss: 7.927981\n",
      "Train Epoche: 3 [1566/96 (1631%)]\tLoss: 7.204784\n",
      "Train Epoche: 3 [1567/96 (1632%)]\tLoss: 89.802307\n",
      "Train Epoche: 3 [1568/96 (1633%)]\tLoss: 3.177678\n",
      "Train Epoche: 3 [1569/96 (1634%)]\tLoss: 0.509560\n",
      "Train Epoche: 3 [1570/96 (1635%)]\tLoss: 5.639112\n",
      "Train Epoche: 3 [1571/96 (1636%)]\tLoss: 10.972259\n",
      "Train Epoche: 3 [1572/96 (1638%)]\tLoss: 0.980190\n",
      "Train Epoche: 3 [1573/96 (1639%)]\tLoss: 16.149395\n",
      "Train Epoche: 3 [1574/96 (1640%)]\tLoss: 25.070745\n",
      "Train Epoche: 3 [1575/96 (1641%)]\tLoss: 24.246168\n",
      "Train Epoche: 3 [1576/96 (1642%)]\tLoss: 3.173246\n",
      "Train Epoche: 3 [1577/96 (1643%)]\tLoss: 7.425773\n",
      "Train Epoche: 3 [1578/96 (1644%)]\tLoss: 2.764322\n",
      "Train Epoche: 3 [1579/96 (1645%)]\tLoss: 2.052264\n",
      "Train Epoche: 3 [1580/96 (1646%)]\tLoss: 20.112246\n",
      "Train Epoche: 3 [1581/96 (1647%)]\tLoss: 0.000153\n",
      "Train Epoche: 3 [1582/96 (1648%)]\tLoss: 0.005752\n",
      "Train Epoche: 3 [1583/96 (1649%)]\tLoss: 0.274778\n",
      "Train Epoche: 3 [1584/96 (1650%)]\tLoss: 0.377505\n",
      "Train Epoche: 3 [1585/96 (1651%)]\tLoss: 0.924980\n",
      "Train Epoche: 3 [1586/96 (1652%)]\tLoss: 0.654953\n",
      "Train Epoche: 3 [1587/96 (1653%)]\tLoss: 0.385966\n",
      "Train Epoche: 3 [1588/96 (1654%)]\tLoss: 7.987495\n",
      "Train Epoche: 3 [1589/96 (1655%)]\tLoss: 0.295933\n",
      "Train Epoche: 3 [1590/96 (1656%)]\tLoss: 0.119851\n",
      "Train Epoche: 3 [1591/96 (1657%)]\tLoss: 2.225545\n",
      "Train Epoche: 3 [1592/96 (1658%)]\tLoss: 0.001812\n",
      "Train Epoche: 3 [1593/96 (1659%)]\tLoss: 1.868761\n",
      "Train Epoche: 3 [1594/96 (1660%)]\tLoss: 0.689031\n",
      "Train Epoche: 3 [1595/96 (1661%)]\tLoss: 0.396977\n",
      "Train Epoche: 3 [1596/96 (1662%)]\tLoss: 6.839520\n",
      "Train Epoche: 3 [1597/96 (1664%)]\tLoss: 25.193224\n",
      "Train Epoche: 3 [1598/96 (1665%)]\tLoss: 0.867306\n",
      "Train Epoche: 3 [1599/96 (1666%)]\tLoss: 15.440525\n",
      "Train Epoche: 3 [1600/96 (1667%)]\tLoss: 154.049637\n",
      "Train Epoche: 3 [1601/96 (1668%)]\tLoss: 2.662546\n",
      "Train Epoche: 3 [1602/96 (1669%)]\tLoss: 2.957302\n",
      "Train Epoche: 3 [1603/96 (1670%)]\tLoss: 15.908426\n",
      "Train Epoche: 3 [1604/96 (1671%)]\tLoss: 8.985723\n",
      "Train Epoche: 3 [1605/96 (1672%)]\tLoss: 1.475166\n",
      "Train Epoche: 3 [1606/96 (1673%)]\tLoss: 1.396200\n",
      "Train Epoche: 3 [1607/96 (1674%)]\tLoss: 0.024457\n",
      "Train Epoche: 3 [1608/96 (1675%)]\tLoss: 0.050870\n",
      "Train Epoche: 3 [1609/96 (1676%)]\tLoss: 3.557135\n",
      "Train Epoche: 3 [1610/96 (1677%)]\tLoss: 0.728896\n",
      "Train Epoche: 3 [1611/96 (1678%)]\tLoss: 6.896013\n",
      "Train Epoche: 3 [1612/96 (1679%)]\tLoss: 8.668570\n",
      "Train Epoche: 3 [1613/96 (1680%)]\tLoss: 0.404137\n",
      "Train Epoche: 3 [1614/96 (1681%)]\tLoss: 86.054276\n",
      "Train Epoche: 3 [1615/96 (1682%)]\tLoss: 2.303809\n",
      "Train Epoche: 3 [1616/96 (1683%)]\tLoss: 2.109350\n",
      "Train Epoche: 3 [1617/96 (1684%)]\tLoss: 0.271742\n",
      "Train Epoche: 3 [1618/96 (1685%)]\tLoss: 2.014099\n",
      "Train Epoche: 3 [1619/96 (1686%)]\tLoss: 33.191116\n",
      "Train Epoche: 3 [1620/96 (1688%)]\tLoss: 31.708757\n",
      "Train Epoche: 3 [1621/96 (1689%)]\tLoss: 23.194853\n",
      "Train Epoche: 3 [1622/96 (1690%)]\tLoss: 4.114223\n",
      "Train Epoche: 3 [1623/96 (1691%)]\tLoss: 120.248566\n",
      "Train Epoche: 3 [1624/96 (1692%)]\tLoss: 2.426706\n",
      "Train Epoche: 3 [1625/96 (1693%)]\tLoss: 1.882949\n",
      "Train Epoche: 3 [1626/96 (1694%)]\tLoss: 49.216385\n",
      "Train Epoche: 3 [1627/96 (1695%)]\tLoss: 55.383865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1628/96 (1696%)]\tLoss: 4.385445\n",
      "Train Epoche: 3 [1629/96 (1697%)]\tLoss: 8.068058\n",
      "Train Epoche: 3 [1630/96 (1698%)]\tLoss: 0.947304\n",
      "Train Epoche: 3 [1631/96 (1699%)]\tLoss: 0.248534\n",
      "Train Epoche: 3 [1632/96 (1700%)]\tLoss: 5.849442\n",
      "Train Epoche: 3 [1633/96 (1701%)]\tLoss: 0.001713\n",
      "Train Epoche: 3 [1634/96 (1702%)]\tLoss: 50.952496\n",
      "Train Epoche: 3 [1635/96 (1703%)]\tLoss: 0.062290\n",
      "Train Epoche: 3 [1636/96 (1704%)]\tLoss: 4.053567\n",
      "Train Epoche: 3 [1637/96 (1705%)]\tLoss: 0.004219\n",
      "Train Epoche: 3 [1638/96 (1706%)]\tLoss: 10.163366\n",
      "Train Epoche: 3 [1639/96 (1707%)]\tLoss: 58.675781\n",
      "Train Epoche: 3 [1640/96 (1708%)]\tLoss: 18.161287\n",
      "Train Epoche: 3 [1641/96 (1709%)]\tLoss: 0.193086\n",
      "Train Epoche: 3 [1642/96 (1710%)]\tLoss: 0.002273\n",
      "Train Epoche: 3 [1643/96 (1711%)]\tLoss: 5.135114\n",
      "Train Epoche: 3 [1644/96 (1712%)]\tLoss: 80.666260\n",
      "Train Epoche: 3 [1645/96 (1714%)]\tLoss: 0.002046\n",
      "Train Epoche: 3 [1646/96 (1715%)]\tLoss: 0.523087\n",
      "Train Epoche: 3 [1647/96 (1716%)]\tLoss: 0.470634\n",
      "Train Epoche: 3 [1648/96 (1717%)]\tLoss: 0.753530\n",
      "Train Epoche: 3 [1649/96 (1718%)]\tLoss: 2.746439\n",
      "Train Epoche: 3 [1650/96 (1719%)]\tLoss: 0.712724\n",
      "Train Epoche: 3 [1651/96 (1720%)]\tLoss: 17.649445\n",
      "Train Epoche: 3 [1652/96 (1721%)]\tLoss: 16.573864\n",
      "Train Epoche: 3 [1653/96 (1722%)]\tLoss: 31.116798\n",
      "Train Epoche: 3 [1654/96 (1723%)]\tLoss: 16.258667\n",
      "Train Epoche: 3 [1655/96 (1724%)]\tLoss: 59.971626\n",
      "Train Epoche: 3 [1656/96 (1725%)]\tLoss: 45.023754\n",
      "Train Epoche: 3 [1657/96 (1726%)]\tLoss: 0.002845\n",
      "Train Epoche: 3 [1658/96 (1727%)]\tLoss: 0.171089\n",
      "Train Epoche: 3 [1659/96 (1728%)]\tLoss: 20.877419\n",
      "Train Epoche: 3 [1660/96 (1729%)]\tLoss: 4.034996\n",
      "Train Epoche: 3 [1661/96 (1730%)]\tLoss: 18.479185\n",
      "Train Epoche: 3 [1662/96 (1731%)]\tLoss: 12.976639\n",
      "Train Epoche: 3 [1663/96 (1732%)]\tLoss: 9.417551\n",
      "Train Epoche: 3 [1664/96 (1733%)]\tLoss: 1.299324\n",
      "Train Epoche: 3 [1665/96 (1734%)]\tLoss: 1.210922\n",
      "Train Epoche: 3 [1666/96 (1735%)]\tLoss: 0.251085\n",
      "Train Epoche: 3 [1667/96 (1736%)]\tLoss: 0.514746\n",
      "Train Epoche: 3 [1668/96 (1738%)]\tLoss: 0.773177\n",
      "Train Epoche: 3 [1669/96 (1739%)]\tLoss: 0.626642\n",
      "Train Epoche: 3 [1670/96 (1740%)]\tLoss: 2.116503\n",
      "Train Epoche: 3 [1671/96 (1741%)]\tLoss: 2.796437\n",
      "Train Epoche: 3 [1672/96 (1742%)]\tLoss: 21.431753\n",
      "Train Epoche: 3 [1673/96 (1743%)]\tLoss: 65.869827\n",
      "Train Epoche: 3 [1674/96 (1744%)]\tLoss: 24.814417\n",
      "Train Epoche: 3 [1675/96 (1745%)]\tLoss: 8.998790\n",
      "Train Epoche: 3 [1676/96 (1746%)]\tLoss: 11.762318\n",
      "Train Epoche: 3 [1677/96 (1747%)]\tLoss: 4.406788\n",
      "Train Epoche: 3 [1678/96 (1748%)]\tLoss: 97.325729\n",
      "Train Epoche: 3 [1679/96 (1749%)]\tLoss: 34.732784\n",
      "Train Epoche: 3 [1680/96 (1750%)]\tLoss: 63.434116\n",
      "Train Epoche: 3 [1681/96 (1751%)]\tLoss: 4.681631\n",
      "Train Epoche: 3 [1682/96 (1752%)]\tLoss: 27.216145\n",
      "Train Epoche: 3 [1683/96 (1753%)]\tLoss: 0.538763\n",
      "Train Epoche: 3 [1684/96 (1754%)]\tLoss: 0.033062\n",
      "Train Epoche: 3 [1685/96 (1755%)]\tLoss: 204.988647\n",
      "Train Epoche: 3 [1686/96 (1756%)]\tLoss: 0.133897\n",
      "Train Epoche: 3 [1687/96 (1757%)]\tLoss: 0.071767\n",
      "Train Epoche: 3 [1688/96 (1758%)]\tLoss: 2.151353\n",
      "Train Epoche: 3 [1689/96 (1759%)]\tLoss: 0.993161\n",
      "Train Epoche: 3 [1690/96 (1760%)]\tLoss: 6.291676\n",
      "Train Epoche: 3 [1691/96 (1761%)]\tLoss: 27.068520\n",
      "Train Epoche: 3 [1692/96 (1762%)]\tLoss: 4.001640\n",
      "Train Epoche: 3 [1693/96 (1764%)]\tLoss: 6.624256\n",
      "Train Epoche: 3 [1694/96 (1765%)]\tLoss: 33.070541\n",
      "Train Epoche: 3 [1695/96 (1766%)]\tLoss: 2.305647\n",
      "Train Epoche: 3 [1696/96 (1767%)]\tLoss: 1.908833\n",
      "Train Epoche: 3 [1697/96 (1768%)]\tLoss: 11.780347\n",
      "Train Epoche: 3 [1698/96 (1769%)]\tLoss: 100.649399\n",
      "Train Epoche: 3 [1699/96 (1770%)]\tLoss: 0.532883\n",
      "Train Epoche: 3 [1700/96 (1771%)]\tLoss: 14.258147\n",
      "Train Epoche: 3 [1701/96 (1772%)]\tLoss: 0.242969\n",
      "Train Epoche: 3 [1702/96 (1773%)]\tLoss: 18.364853\n",
      "Train Epoche: 3 [1703/96 (1774%)]\tLoss: 14.235620\n",
      "Train Epoche: 3 [1704/96 (1775%)]\tLoss: 9.141768\n",
      "Train Epoche: 3 [1705/96 (1776%)]\tLoss: 197.247803\n",
      "Train Epoche: 3 [1706/96 (1777%)]\tLoss: 0.977099\n",
      "Train Epoche: 3 [1707/96 (1778%)]\tLoss: 28.177361\n",
      "Train Epoche: 3 [1708/96 (1779%)]\tLoss: 31.389870\n",
      "Train Epoche: 3 [1709/96 (1780%)]\tLoss: 11.211554\n",
      "Train Epoche: 3 [1710/96 (1781%)]\tLoss: 0.980732\n",
      "Train Epoche: 3 [1711/96 (1782%)]\tLoss: 1.800276\n",
      "Train Epoche: 3 [1712/96 (1783%)]\tLoss: 1.666815\n",
      "Train Epoche: 3 [1713/96 (1784%)]\tLoss: 0.259466\n",
      "Train Epoche: 3 [1714/96 (1785%)]\tLoss: 0.674483\n",
      "Train Epoche: 3 [1715/96 (1786%)]\tLoss: 38.249641\n",
      "Train Epoche: 3 [1716/96 (1788%)]\tLoss: 3.552536\n",
      "Train Epoche: 3 [1717/96 (1789%)]\tLoss: 5.012529\n",
      "Train Epoche: 3 [1718/96 (1790%)]\tLoss: 3.130856\n",
      "Train Epoche: 3 [1719/96 (1791%)]\tLoss: 2.038049\n",
      "Train Epoche: 3 [1720/96 (1792%)]\tLoss: 14.278443\n",
      "Train Epoche: 3 [1721/96 (1793%)]\tLoss: 13.981835\n",
      "Train Epoche: 3 [1722/96 (1794%)]\tLoss: 0.000561\n",
      "Train Epoche: 3 [1723/96 (1795%)]\tLoss: 1.451216\n",
      "Train Epoche: 3 [1724/96 (1796%)]\tLoss: 2.074393\n",
      "Train Epoche: 3 [1725/96 (1797%)]\tLoss: 0.510392\n",
      "Train Epoche: 3 [1726/96 (1798%)]\tLoss: 2.340727\n",
      "Train Epoche: 3 [1727/96 (1799%)]\tLoss: 57.263535\n",
      "Train Epoche: 3 [1728/96 (1800%)]\tLoss: 9.951394\n",
      "Train Epoche: 3 [1729/96 (1801%)]\tLoss: 1.789367\n",
      "Train Epoche: 3 [1730/96 (1802%)]\tLoss: 0.037163\n",
      "Train Epoche: 3 [1731/96 (1803%)]\tLoss: 14.083880\n",
      "Train Epoche: 3 [1732/96 (1804%)]\tLoss: 2.423954\n",
      "Train Epoche: 3 [1733/96 (1805%)]\tLoss: 0.018910\n",
      "Train Epoche: 3 [1734/96 (1806%)]\tLoss: 13.426263\n",
      "Train Epoche: 3 [1735/96 (1807%)]\tLoss: 6.397268\n",
      "Train Epoche: 3 [1736/96 (1808%)]\tLoss: 0.379997\n",
      "Train Epoche: 3 [1737/96 (1809%)]\tLoss: 1.083561\n",
      "Train Epoche: 3 [1738/96 (1810%)]\tLoss: 10.574163\n",
      "Train Epoche: 3 [1739/96 (1811%)]\tLoss: 3.926509\n",
      "Train Epoche: 3 [1740/96 (1812%)]\tLoss: 0.004950\n",
      "Train Epoche: 3 [1741/96 (1814%)]\tLoss: 68.639816\n",
      "Train Epoche: 3 [1742/96 (1815%)]\tLoss: 6.576151\n",
      "Train Epoche: 3 [1743/96 (1816%)]\tLoss: 20.634995\n",
      "Train Epoche: 3 [1744/96 (1817%)]\tLoss: 1.033238\n",
      "Train Epoche: 3 [1745/96 (1818%)]\tLoss: 17.033600\n",
      "Train Epoche: 3 [1746/96 (1819%)]\tLoss: 3.225638\n",
      "Train Epoche: 3 [1747/96 (1820%)]\tLoss: 1.861095\n",
      "Train Epoche: 3 [1748/96 (1821%)]\tLoss: 7.019799\n",
      "Train Epoche: 3 [1749/96 (1822%)]\tLoss: 1.216562\n",
      "Train Epoche: 3 [1750/96 (1823%)]\tLoss: 0.161042\n",
      "Train Epoche: 3 [1751/96 (1824%)]\tLoss: 1.479022\n",
      "Train Epoche: 3 [1752/96 (1825%)]\tLoss: 2.638251\n",
      "Train Epoche: 3 [1753/96 (1826%)]\tLoss: 0.017241\n",
      "Train Epoche: 3 [1754/96 (1827%)]\tLoss: 50.832455\n",
      "Train Epoche: 3 [1755/96 (1828%)]\tLoss: 2.467647\n",
      "Train Epoche: 3 [1756/96 (1829%)]\tLoss: 10.980378\n",
      "Train Epoche: 3 [1757/96 (1830%)]\tLoss: 2.556243\n",
      "Train Epoche: 3 [1758/96 (1831%)]\tLoss: 3.717587\n",
      "Train Epoche: 3 [1759/96 (1832%)]\tLoss: 85.037949\n",
      "Train Epoche: 3 [1760/96 (1833%)]\tLoss: 5.948233\n",
      "Train Epoche: 3 [1761/96 (1834%)]\tLoss: 3.122548\n",
      "Train Epoche: 3 [1762/96 (1835%)]\tLoss: 6.399085\n",
      "Train Epoche: 3 [1763/96 (1836%)]\tLoss: 186.960220\n",
      "Train Epoche: 3 [1764/96 (1838%)]\tLoss: 141.465622\n",
      "Train Epoche: 3 [1765/96 (1839%)]\tLoss: 16.299215\n",
      "Train Epoche: 3 [1766/96 (1840%)]\tLoss: 21.447826\n",
      "Train Epoche: 3 [1767/96 (1841%)]\tLoss: 19.809196\n",
      "Train Epoche: 3 [1768/96 (1842%)]\tLoss: 0.497138\n",
      "Train Epoche: 3 [1769/96 (1843%)]\tLoss: 9.486405\n",
      "Train Epoche: 3 [1770/96 (1844%)]\tLoss: 98.258644\n",
      "Train Epoche: 3 [1771/96 (1845%)]\tLoss: 74.042572\n",
      "Train Epoche: 3 [1772/96 (1846%)]\tLoss: 4.983746\n",
      "Train Epoche: 3 [1773/96 (1847%)]\tLoss: 145.206619\n",
      "Train Epoche: 3 [1774/96 (1848%)]\tLoss: 2.548202\n",
      "Train Epoche: 3 [1775/96 (1849%)]\tLoss: 0.350514\n",
      "Train Epoche: 3 [1776/96 (1850%)]\tLoss: 0.609935\n",
      "Train Epoche: 3 [1777/96 (1851%)]\tLoss: 1.747828\n",
      "Train Epoche: 3 [1778/96 (1852%)]\tLoss: 2.986336\n",
      "Train Epoche: 3 [1779/96 (1853%)]\tLoss: 0.797220\n",
      "Train Epoche: 3 [1780/96 (1854%)]\tLoss: 21.487272\n",
      "Train Epoche: 3 [1781/96 (1855%)]\tLoss: 8.971775\n",
      "Train Epoche: 3 [1782/96 (1856%)]\tLoss: 0.538324\n",
      "Train Epoche: 3 [1783/96 (1857%)]\tLoss: 15.508082\n",
      "Train Epoche: 3 [1784/96 (1858%)]\tLoss: 12.538386\n",
      "Train Epoche: 3 [1785/96 (1859%)]\tLoss: 16.531752\n",
      "Train Epoche: 3 [1786/96 (1860%)]\tLoss: 0.783065\n",
      "Train Epoche: 3 [1787/96 (1861%)]\tLoss: 0.025495\n",
      "Train Epoche: 3 [1788/96 (1862%)]\tLoss: 15.520523\n",
      "Train Epoche: 3 [1789/96 (1864%)]\tLoss: 28.157793\n",
      "Train Epoche: 3 [1790/96 (1865%)]\tLoss: 0.376904\n",
      "Train Epoche: 3 [1791/96 (1866%)]\tLoss: 1.602926\n",
      "Train Epoche: 3 [1792/96 (1867%)]\tLoss: 4.020113\n",
      "Train Epoche: 3 [1793/96 (1868%)]\tLoss: 5.580069\n",
      "Train Epoche: 3 [1794/96 (1869%)]\tLoss: 13.970618\n",
      "Train Epoche: 3 [1795/96 (1870%)]\tLoss: 15.857390\n",
      "Train Epoche: 3 [1796/96 (1871%)]\tLoss: 9.186713\n",
      "Train Epoche: 3 [1797/96 (1872%)]\tLoss: 4.452509\n",
      "Train Epoche: 3 [1798/96 (1873%)]\tLoss: 0.493038\n",
      "Train Epoche: 3 [1799/96 (1874%)]\tLoss: 14.607994\n",
      "Train Epoche: 3 [1800/96 (1875%)]\tLoss: 2.779566\n",
      "Train Epoche: 3 [1801/96 (1876%)]\tLoss: 18.307550\n",
      "Train Epoche: 3 [1802/96 (1877%)]\tLoss: 2.749519\n",
      "Train Epoche: 3 [1803/96 (1878%)]\tLoss: 1.743250\n",
      "Train Epoche: 3 [1804/96 (1879%)]\tLoss: 29.126316\n",
      "Train Epoche: 3 [1805/96 (1880%)]\tLoss: 72.166595\n",
      "Train Epoche: 3 [1806/96 (1881%)]\tLoss: 0.028726\n",
      "Train Epoche: 3 [1807/96 (1882%)]\tLoss: 0.017573\n",
      "Train Epoche: 3 [1808/96 (1883%)]\tLoss: 33.850643\n",
      "Train Epoche: 3 [1809/96 (1884%)]\tLoss: 4.237643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1810/96 (1885%)]\tLoss: 43.806797\n",
      "Train Epoche: 3 [1811/96 (1886%)]\tLoss: 10.948509\n",
      "Train Epoche: 3 [1812/96 (1888%)]\tLoss: 4.038836\n",
      "Train Epoche: 3 [1813/96 (1889%)]\tLoss: 0.635743\n",
      "Train Epoche: 3 [1814/96 (1890%)]\tLoss: 0.525514\n",
      "Train Epoche: 3 [1815/96 (1891%)]\tLoss: 0.003690\n",
      "Train Epoche: 3 [1816/96 (1892%)]\tLoss: 0.002154\n",
      "Train Epoche: 3 [1817/96 (1893%)]\tLoss: 22.772387\n",
      "Train Epoche: 3 [1818/96 (1894%)]\tLoss: 16.637997\n",
      "Train Epoche: 3 [1819/96 (1895%)]\tLoss: 0.704121\n",
      "Train Epoche: 3 [1820/96 (1896%)]\tLoss: 121.194656\n",
      "Train Epoche: 3 [1821/96 (1897%)]\tLoss: 3.425443\n",
      "Train Epoche: 3 [1822/96 (1898%)]\tLoss: 17.100151\n",
      "Train Epoche: 3 [1823/96 (1899%)]\tLoss: 0.996158\n",
      "Train Epoche: 3 [1824/96 (1900%)]\tLoss: 19.876547\n",
      "Train Epoche: 3 [1825/96 (1901%)]\tLoss: 7.652881\n",
      "Train Epoche: 3 [1826/96 (1902%)]\tLoss: 29.625677\n",
      "Train Epoche: 3 [1827/96 (1903%)]\tLoss: 0.982468\n",
      "Train Epoche: 3 [1828/96 (1904%)]\tLoss: 0.162217\n",
      "Train Epoche: 3 [1829/96 (1905%)]\tLoss: 39.463806\n",
      "Train Epoche: 3 [1830/96 (1906%)]\tLoss: 1.150681\n",
      "Train Epoche: 3 [1831/96 (1907%)]\tLoss: 1.030479\n",
      "Train Epoche: 3 [1832/96 (1908%)]\tLoss: 20.370470\n",
      "Train Epoche: 3 [1833/96 (1909%)]\tLoss: 38.520798\n",
      "Train Epoche: 3 [1834/96 (1910%)]\tLoss: 12.836188\n",
      "Train Epoche: 3 [1835/96 (1911%)]\tLoss: 0.424913\n",
      "Train Epoche: 3 [1836/96 (1912%)]\tLoss: 7.515665\n",
      "Train Epoche: 3 [1837/96 (1914%)]\tLoss: 4.834889\n",
      "Train Epoche: 3 [1838/96 (1915%)]\tLoss: 9.638938\n",
      "Train Epoche: 3 [1839/96 (1916%)]\tLoss: 10.526651\n",
      "Train Epoche: 3 [1840/96 (1917%)]\tLoss: 3.133371\n",
      "Train Epoche: 3 [1841/96 (1918%)]\tLoss: 15.315989\n",
      "Train Epoche: 3 [1842/96 (1919%)]\tLoss: 18.033852\n",
      "Train Epoche: 3 [1843/96 (1920%)]\tLoss: 0.120881\n",
      "Train Epoche: 3 [1844/96 (1921%)]\tLoss: 25.304188\n",
      "Train Epoche: 3 [1845/96 (1922%)]\tLoss: 1.825226\n",
      "Train Epoche: 3 [1846/96 (1923%)]\tLoss: 3.144583\n",
      "Train Epoche: 3 [1847/96 (1924%)]\tLoss: 1.579512\n",
      "Train Epoche: 3 [1848/96 (1925%)]\tLoss: 36.238819\n",
      "Train Epoche: 3 [1849/96 (1926%)]\tLoss: 2.384436\n",
      "Train Epoche: 3 [1850/96 (1927%)]\tLoss: 3.163649\n",
      "Train Epoche: 3 [1851/96 (1928%)]\tLoss: 0.362567\n",
      "Train Epoche: 3 [1852/96 (1929%)]\tLoss: 17.706175\n",
      "Train Epoche: 3 [1853/96 (1930%)]\tLoss: 1.968130\n",
      "Train Epoche: 3 [1854/96 (1931%)]\tLoss: 154.501465\n",
      "Train Epoche: 3 [1855/96 (1932%)]\tLoss: 27.320944\n",
      "Train Epoche: 3 [1856/96 (1933%)]\tLoss: 0.000004\n",
      "Train Epoche: 3 [1857/96 (1934%)]\tLoss: 1.775621\n",
      "Train Epoche: 3 [1858/96 (1935%)]\tLoss: 0.570307\n",
      "Train Epoche: 3 [1859/96 (1936%)]\tLoss: 7.189682\n",
      "Train Epoche: 3 [1860/96 (1938%)]\tLoss: 7.969846\n",
      "Train Epoche: 3 [1861/96 (1939%)]\tLoss: 4.126833\n",
      "Train Epoche: 3 [1862/96 (1940%)]\tLoss: 68.999802\n",
      "Train Epoche: 3 [1863/96 (1941%)]\tLoss: 36.460670\n",
      "Train Epoche: 3 [1864/96 (1942%)]\tLoss: 0.229229\n",
      "Train Epoche: 3 [1865/96 (1943%)]\tLoss: 1.462787\n",
      "Train Epoche: 3 [1866/96 (1944%)]\tLoss: 26.531572\n",
      "Train Epoche: 3 [1867/96 (1945%)]\tLoss: 6.696176\n",
      "Train Epoche: 3 [1868/96 (1946%)]\tLoss: 0.017541\n",
      "Train Epoche: 3 [1869/96 (1947%)]\tLoss: 28.641247\n",
      "Train Epoche: 3 [1870/96 (1948%)]\tLoss: 3.005872\n",
      "Train Epoche: 3 [1871/96 (1949%)]\tLoss: 1.909086\n",
      "Train Epoche: 3 [1872/96 (1950%)]\tLoss: 0.145257\n",
      "Train Epoche: 3 [1873/96 (1951%)]\tLoss: 1.730157\n",
      "Train Epoche: 3 [1874/96 (1952%)]\tLoss: 1.217338\n",
      "Train Epoche: 3 [1875/96 (1953%)]\tLoss: 6.839700\n",
      "Train Epoche: 3 [1876/96 (1954%)]\tLoss: 5.754337\n",
      "Train Epoche: 3 [1877/96 (1955%)]\tLoss: 1.461182\n",
      "Train Epoche: 3 [1878/96 (1956%)]\tLoss: 7.227322\n",
      "Train Epoche: 3 [1879/96 (1957%)]\tLoss: 73.645370\n",
      "Train Epoche: 3 [1880/96 (1958%)]\tLoss: 4.467812\n",
      "Train Epoche: 3 [1881/96 (1959%)]\tLoss: 6.414451\n",
      "Train Epoche: 3 [1882/96 (1960%)]\tLoss: 0.245814\n",
      "Train Epoche: 3 [1883/96 (1961%)]\tLoss: 4.676218\n",
      "Train Epoche: 3 [1884/96 (1962%)]\tLoss: 216.097565\n",
      "Train Epoche: 3 [1885/96 (1964%)]\tLoss: 2.137074\n",
      "Train Epoche: 3 [1886/96 (1965%)]\tLoss: 4.068794\n",
      "Train Epoche: 3 [1887/96 (1966%)]\tLoss: 0.366307\n",
      "Train Epoche: 3 [1888/96 (1967%)]\tLoss: 1.445964\n",
      "Train Epoche: 3 [1889/96 (1968%)]\tLoss: 0.176137\n",
      "Train Epoche: 3 [1890/96 (1969%)]\tLoss: 0.542376\n",
      "Train Epoche: 3 [1891/96 (1970%)]\tLoss: 1.476066\n",
      "Train Epoche: 3 [1892/96 (1971%)]\tLoss: 3.437946\n",
      "Train Epoche: 3 [1893/96 (1972%)]\tLoss: 61.099754\n",
      "Train Epoche: 3 [1894/96 (1973%)]\tLoss: 0.271205\n",
      "Train Epoche: 3 [1895/96 (1974%)]\tLoss: 0.030104\n",
      "Train Epoche: 3 [1896/96 (1975%)]\tLoss: 46.878746\n",
      "Train Epoche: 3 [1897/96 (1976%)]\tLoss: 0.999060\n",
      "Train Epoche: 3 [1898/96 (1977%)]\tLoss: 3.279372\n",
      "Train Epoche: 3 [1899/96 (1978%)]\tLoss: 0.812006\n",
      "Train Epoche: 3 [1900/96 (1979%)]\tLoss: 14.415782\n",
      "Train Epoche: 3 [1901/96 (1980%)]\tLoss: 1.027303\n",
      "Train Epoche: 3 [1902/96 (1981%)]\tLoss: 4.561240\n",
      "Train Epoche: 3 [1903/96 (1982%)]\tLoss: 8.075767\n",
      "Train Epoche: 3 [1904/96 (1983%)]\tLoss: 6.235151\n",
      "Train Epoche: 3 [1905/96 (1984%)]\tLoss: 6.835630\n",
      "Train Epoche: 3 [1906/96 (1985%)]\tLoss: 15.482307\n",
      "Train Epoche: 3 [1907/96 (1986%)]\tLoss: 15.282775\n",
      "Train Epoche: 3 [1908/96 (1988%)]\tLoss: 13.931201\n",
      "Train Epoche: 3 [1909/96 (1989%)]\tLoss: 3.637160\n",
      "Train Epoche: 3 [1910/96 (1990%)]\tLoss: 48.128311\n",
      "Train Epoche: 3 [1911/96 (1991%)]\tLoss: 9.622968\n",
      "Train Epoche: 3 [1912/96 (1992%)]\tLoss: 3.883024\n",
      "Train Epoche: 3 [1913/96 (1993%)]\tLoss: 60.770618\n",
      "Train Epoche: 3 [1914/96 (1994%)]\tLoss: 19.464050\n",
      "Train Epoche: 3 [1915/96 (1995%)]\tLoss: 2.850522\n",
      "Train Epoche: 3 [1916/96 (1996%)]\tLoss: 6.744617\n",
      "Train Epoche: 3 [1917/96 (1997%)]\tLoss: 5.776471\n",
      "Train Epoche: 3 [1918/96 (1998%)]\tLoss: 0.029023\n",
      "Train Epoche: 3 [1919/96 (1999%)]\tLoss: 1.901789\n",
      "Train Epoche: 3 [1920/96 (2000%)]\tLoss: 7.521271\n",
      "Train Epoche: 3 [1921/96 (2001%)]\tLoss: 1.858316\n",
      "Train Epoche: 3 [1922/96 (2002%)]\tLoss: 0.151046\n",
      "Train Epoche: 3 [1923/96 (2003%)]\tLoss: 15.676661\n",
      "Train Epoche: 3 [1924/96 (2004%)]\tLoss: 199.316742\n",
      "Train Epoche: 3 [1925/96 (2005%)]\tLoss: 0.020537\n",
      "Train Epoche: 3 [1926/96 (2006%)]\tLoss: 0.403282\n",
      "Train Epoche: 3 [1927/96 (2007%)]\tLoss: 0.194704\n",
      "Train Epoche: 3 [1928/96 (2008%)]\tLoss: 15.301802\n",
      "Train Epoche: 3 [1929/96 (2009%)]\tLoss: 10.714381\n",
      "Train Epoche: 3 [1930/96 (2010%)]\tLoss: 1.377773\n",
      "Train Epoche: 3 [1931/96 (2011%)]\tLoss: 0.239804\n",
      "Train Epoche: 3 [1932/96 (2012%)]\tLoss: 0.508888\n",
      "Train Epoche: 3 [1933/96 (2014%)]\tLoss: 12.714417\n",
      "Train Epoche: 3 [1934/96 (2015%)]\tLoss: 0.051646\n",
      "Train Epoche: 3 [1935/96 (2016%)]\tLoss: 7.426454\n",
      "Train Epoche: 3 [1936/96 (2017%)]\tLoss: 4.677785\n",
      "Train Epoche: 3 [1937/96 (2018%)]\tLoss: 138.629776\n",
      "Train Epoche: 3 [1938/96 (2019%)]\tLoss: 0.035091\n",
      "Train Epoche: 3 [1939/96 (2020%)]\tLoss: 7.115584\n",
      "Train Epoche: 3 [1940/96 (2021%)]\tLoss: 3.566303\n",
      "Train Epoche: 3 [1941/96 (2022%)]\tLoss: 6.259116\n",
      "Train Epoche: 3 [1942/96 (2023%)]\tLoss: 3.234108\n",
      "Train Epoche: 3 [1943/96 (2024%)]\tLoss: 1.641267\n",
      "Train Epoche: 3 [1944/96 (2025%)]\tLoss: 0.278554\n",
      "Train Epoche: 3 [1945/96 (2026%)]\tLoss: 13.529578\n",
      "Train Epoche: 3 [1946/96 (2027%)]\tLoss: 0.974188\n",
      "Train Epoche: 3 [1947/96 (2028%)]\tLoss: 0.620698\n",
      "Train Epoche: 3 [1948/96 (2029%)]\tLoss: 9.330160\n",
      "Train Epoche: 3 [1949/96 (2030%)]\tLoss: 0.273971\n",
      "Train Epoche: 3 [1950/96 (2031%)]\tLoss: 0.002597\n",
      "Train Epoche: 3 [1951/96 (2032%)]\tLoss: 3.054774\n",
      "Train Epoche: 3 [1952/96 (2033%)]\tLoss: 1.736733\n",
      "Train Epoche: 3 [1953/96 (2034%)]\tLoss: 0.000693\n",
      "Train Epoche: 3 [1954/96 (2035%)]\tLoss: 0.034349\n",
      "Train Epoche: 3 [1955/96 (2036%)]\tLoss: 40.888382\n",
      "Train Epoche: 3 [1956/96 (2038%)]\tLoss: 10.385365\n",
      "Train Epoche: 3 [1957/96 (2039%)]\tLoss: 5.910530\n",
      "Train Epoche: 3 [1958/96 (2040%)]\tLoss: 4.345102\n",
      "Train Epoche: 3 [1959/96 (2041%)]\tLoss: 0.001879\n",
      "Train Epoche: 3 [1960/96 (2042%)]\tLoss: 0.089465\n",
      "Train Epoche: 3 [1961/96 (2043%)]\tLoss: 19.814119\n",
      "Train Epoche: 3 [1962/96 (2044%)]\tLoss: 39.141541\n",
      "Train Epoche: 3 [1963/96 (2045%)]\tLoss: 2.386259\n",
      "Train Epoche: 3 [1964/96 (2046%)]\tLoss: 9.273675\n",
      "Train Epoche: 3 [1965/96 (2047%)]\tLoss: 0.114051\n",
      "Train Epoche: 3 [1966/96 (2048%)]\tLoss: 3.997520\n",
      "Train Epoche: 3 [1967/96 (2049%)]\tLoss: 7.500274\n",
      "Train Epoche: 3 [1968/96 (2050%)]\tLoss: 0.720976\n",
      "Train Epoche: 3 [1969/96 (2051%)]\tLoss: 3.846914\n",
      "Train Epoche: 3 [1970/96 (2052%)]\tLoss: 3.272443\n",
      "Train Epoche: 3 [1971/96 (2053%)]\tLoss: 4.181739\n",
      "Train Epoche: 3 [1972/96 (2054%)]\tLoss: 1.849197\n",
      "Train Epoche: 3 [1973/96 (2055%)]\tLoss: 15.360181\n",
      "Train Epoche: 3 [1974/96 (2056%)]\tLoss: 15.099926\n",
      "Train Epoche: 3 [1975/96 (2057%)]\tLoss: 0.884266\n",
      "Train Epoche: 3 [1976/96 (2058%)]\tLoss: 0.840427\n",
      "Train Epoche: 3 [1977/96 (2059%)]\tLoss: 5.552767\n",
      "Train Epoche: 3 [1978/96 (2060%)]\tLoss: 2.020838\n",
      "Train Epoche: 3 [1979/96 (2061%)]\tLoss: 12.630637\n",
      "Train Epoche: 3 [1980/96 (2062%)]\tLoss: 2.979764\n",
      "Train Epoche: 3 [1981/96 (2064%)]\tLoss: 11.134321\n",
      "Train Epoche: 3 [1982/96 (2065%)]\tLoss: 0.039196\n",
      "Train Epoche: 3 [1983/96 (2066%)]\tLoss: 10.175367\n",
      "Train Epoche: 3 [1984/96 (2067%)]\tLoss: 0.105437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1985/96 (2068%)]\tLoss: 18.730536\n",
      "Train Epoche: 3 [1986/96 (2069%)]\tLoss: 13.777725\n",
      "Train Epoche: 3 [1987/96 (2070%)]\tLoss: 1.551816\n",
      "Train Epoche: 3 [1988/96 (2071%)]\tLoss: 3.077402\n",
      "Train Epoche: 3 [1989/96 (2072%)]\tLoss: 34.620255\n",
      "Train Epoche: 3 [1990/96 (2073%)]\tLoss: 15.762742\n",
      "Train Epoche: 3 [1991/96 (2074%)]\tLoss: 2.013078\n",
      "Train Epoche: 3 [1992/96 (2075%)]\tLoss: 11.014915\n",
      "Train Epoche: 3 [1993/96 (2076%)]\tLoss: 2.053020\n",
      "Train Epoche: 3 [1994/96 (2077%)]\tLoss: 27.387701\n",
      "Train Epoche: 3 [1995/96 (2078%)]\tLoss: 1.606249\n",
      "Train Epoche: 3 [1996/96 (2079%)]\tLoss: 0.631029\n",
      "Train Epoche: 3 [1997/96 (2080%)]\tLoss: 0.865297\n",
      "Train Epoche: 3 [1998/96 (2081%)]\tLoss: 3.060745\n",
      "Train Epoche: 3 [1999/96 (2082%)]\tLoss: 1.118588\n",
      "Train Epoche: 3 [2000/96 (2083%)]\tLoss: 12.143049\n",
      "Train Epoche: 3 [2001/96 (2084%)]\tLoss: 1.573388\n",
      "Train Epoche: 3 [2002/96 (2085%)]\tLoss: 38.666172\n",
      "Train Epoche: 3 [2003/96 (2086%)]\tLoss: 1.487063\n",
      "Train Epoche: 3 [2004/96 (2088%)]\tLoss: 13.901772\n",
      "Train Epoche: 3 [2005/96 (2089%)]\tLoss: 21.334822\n",
      "Train Epoche: 3 [2006/96 (2090%)]\tLoss: 29.495552\n",
      "Train Epoche: 3 [2007/96 (2091%)]\tLoss: 13.819428\n",
      "Train Epoche: 3 [2008/96 (2092%)]\tLoss: 22.444803\n",
      "Train Epoche: 3 [2009/96 (2093%)]\tLoss: 1.061755\n",
      "Train Epoche: 3 [2010/96 (2094%)]\tLoss: 2.370493\n",
      "Train Epoche: 3 [2011/96 (2095%)]\tLoss: 8.301197\n",
      "Train Epoche: 3 [2012/96 (2096%)]\tLoss: 0.051210\n",
      "Train Epoche: 3 [2013/96 (2097%)]\tLoss: 5.115003\n",
      "Train Epoche: 3 [2014/96 (2098%)]\tLoss: 29.578510\n",
      "Train Epoche: 3 [2015/96 (2099%)]\tLoss: 1.830129\n",
      "Train Epoche: 3 [2016/96 (2100%)]\tLoss: 0.033952\n",
      "Train Epoche: 3 [2017/96 (2101%)]\tLoss: 1.015833\n",
      "Train Epoche: 3 [2018/96 (2102%)]\tLoss: 17.441713\n",
      "Train Epoche: 3 [2019/96 (2103%)]\tLoss: 2.293244\n",
      "Train Epoche: 3 [2020/96 (2104%)]\tLoss: 22.220301\n",
      "Train Epoche: 3 [2021/96 (2105%)]\tLoss: 0.569670\n",
      "Train Epoche: 3 [2022/96 (2106%)]\tLoss: 17.579945\n",
      "Train Epoche: 3 [2023/96 (2107%)]\tLoss: 0.447393\n",
      "Train Epoche: 3 [2024/96 (2108%)]\tLoss: 15.183378\n",
      "Train Epoche: 3 [2025/96 (2109%)]\tLoss: 2.851420\n",
      "Train Epoche: 3 [2026/96 (2110%)]\tLoss: 20.919010\n"
     ]
    }
   ],
   "source": [
    "max_epochs = h.opt_combination['epochen']\n",
    "#max_epochs_dyn = h_dynamic.opt_combination['epochen']\n",
    "lr = h.opt_combination['lr']\n",
    "#lr_dynamic = h_dynamic.opt_combination['lr']\n",
    "model = Netz()\n",
    "\n",
    "#bestes dynamisches netz nach neuer lernrate finden\n",
    "dyn_idx = auswertung.where(auswertung.mae_l == min(auswertung.mae_l)).dropna(how = 'all').index\n",
    "lr_dynamic = auswertung.loc[dyn_idx, 'lr'].values.tolist()[0]\n",
    "max_epochs_dyn = auswertung.loc[dyn_idx, 'epochs'].values.tolist()[0]\n",
    "i_ = int(dyn_idx[0])#int64 type\n",
    "netz_dyn = dynamische_netze[i_]\n",
    "opt={}\n",
    "for key in netz_dyn.keys():\n",
    "    if key not in ['mae', 'lr', 'epochen']:#mae wird noch in dictionary von HP_Optimizer hinzugefügt und muss entfernt werden\n",
    "        opt[key] = netz_dyn[key]\n",
    "model_dynamic = NetzDynamic(opt)\n",
    "if cuda.lower() == 'y':\n",
    "    model.cuda() \n",
    "    model_dynamic.cuda()\n",
    "\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)     \n",
    "optimizer_d = optim.Adam(model_dynamic.parameters(), lr = lr_dynamic)   \n",
    "\n",
    "def train_cuda(epoch):\n",
    "    model.train()\n",
    "    batch_id = 0\n",
    "    for key in train_T.keys():\n",
    "        \n",
    "        for data, target in train_T[key]:\n",
    "            data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id *len(data), len(train),\n",
    "            100. * batch_id / len(train), loss.item()))\n",
    "            batch_id +=1\n",
    "def train_(epoch):\n",
    "    model.train()\n",
    "    batch_id = 0\n",
    "    for key in train_T.keys():\n",
    "        for data, target in train_T[key]:\n",
    "            #data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)#.cuda()\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1)#.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            #print(\"Out: \", out, out.size())\n",
    "            #print(\"Target: \", target, target.size())\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id *len(data), len(train),\n",
    "            100. * batch_id / len(train), loss.item()))\n",
    "            batch_id +=1\n",
    "            \n",
    "def train_cuda_dynamic(epoch):\n",
    "    model_dynamic.train()\n",
    "    batch_id = 0\n",
    "    for key in train_T.keys():\n",
    "        \n",
    "        for data, target in train_T[key]:\n",
    "            data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1).cuda()\n",
    "            optimizer_d.zero_grad()\n",
    "            out = model_dynamic(data)\n",
    "            #print(\"Out: \", out, out.size())\n",
    "            #print(\"Target: \", target, target.size())\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer_d.step()\n",
    "            print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id *len(data), len(train),\n",
    "            100. * batch_id / len(train), loss.item()))\n",
    "            batch_id +=1\n",
    "def train_dynamic(epoch):\n",
    "    model_dynamic.train()\n",
    "    batch_id = 0\n",
    "    for key in train_T.keys():\n",
    "        for data, target in train_T[key]:\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1)\n",
    "            optimizer_d.zero_grad()\n",
    "            out = model_dynamic(data)\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer_d.step()\n",
    "            print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id *len(data), len(train),\n",
    "            100. * batch_id / len(train), loss.item()))\n",
    "            batch_id +=1\n",
    "if cuda.lower() == 'y':\n",
    "    for epoch in range(1,max_epochs):\n",
    "        train_cuda(epoch) \n",
    "    for epoch in range(1,max_epochs_dyn):\n",
    "        train_cuda_dynamic(epoch) \n",
    "else:\n",
    "    for epoch in range(1,max_epochs):\n",
    "        train_(epoch)  \n",
    "    for epoch in range(1,max_epochs_dyn):\n",
    "        train_dynamic(epoch)  \n",
    "\n",
    "        \n",
    "def test_times_cuda(test_final):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    help_dict = {}\n",
    "    for key in test_final.keys():\n",
    "        help_dict = {}\n",
    "        for data, target in test_final[key]:\n",
    "            model.eval()\n",
    "            data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1).cuda()\n",
    "            out = model(data).cpu()\n",
    "            out = out.detach().numpy()\n",
    "            target = target.cpu()\n",
    "            target = target.detach().numpy()\n",
    "            total += abs(out - target[0][0])\n",
    "            help_dict[target[0][0]] = out\n",
    "            count+=1\n",
    "        result_dict[key] = help_dict\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "def test_times(test_final):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    help_dict = {}\n",
    "    for key in test_final.keys():\n",
    "        help_dict = {}\n",
    "        for data, target in test_final[key]:\n",
    "            model.eval()\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1)\n",
    "            out = model(data)\n",
    "            out = out.detach().numpy()\n",
    "            target = target.detach().numpy()\n",
    "            total += abs(out - target[0][0])\n",
    "            help_dict[target[0][0]] = out\n",
    "            count+=1\n",
    "        result_dict[key] = help_dict\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "def test_times_cuda_dynamic(test_final):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    help_dict = {}\n",
    "    for key in test_final.keys():\n",
    "        help_dict = {}\n",
    "        for data, target in test_final[key]:\n",
    "            model_dynamic.eval()\n",
    "            data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1).cuda()\n",
    "            out = model_dynamic(data).cpu()\n",
    "            out = out.detach().numpy()\n",
    "            target = target.cpu()\n",
    "            target = target.detach().numpy()\n",
    "            total += abs(out - target[0][0])\n",
    "            help_dict[target[0][0]] = out\n",
    "            count+=1\n",
    "        result_dict[key] = help_dict\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "def test_times_dynamic(test_final):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    help_dict = {}\n",
    "    for key in test_final.keys():\n",
    "        #print(key)\n",
    "        help_dict = {}\n",
    "        for data, target in test_final[key]:\n",
    "            model_dynamic.eval()\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1)\n",
    "            out = model_dynamic(data)\n",
    "            out = out.detach().numpy()\n",
    "            target = target.detach().numpy()\n",
    "            total += abs(out - target[0][0])\n",
    "            help_dict[target[0][0]] = out\n",
    "            count+=1\n",
    "        result_dict[key] = help_dict\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "if cuda.lower() =='y':\n",
    "    total_results = test_times_cuda(test_T)\n",
    "    total_results_dynamic = test_times_cuda_dynamic(test_T)\n",
    "else:\n",
    "    total_results = test_times(test_T)\n",
    "    total_results_dynamic = test_times_dynamic(test_T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaceId: 855\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    1.969489\n",
      "1     2.0                   3    2.752062\n",
      "2     3.0                   2    2.104323\n",
      "3     4.0                   4    3.632024\n",
      "4     5.0                   6    4.742852\n",
      "5     6.0                   7    6.634777\n",
      "6     7.0                   5    4.732176\n",
      "7     8.0                  11   10.419172\n",
      "8     9.0                   9    9.890911\n",
      "9    10.0                  12   11.543364 \n",
      "\n",
      "RaceId: 855_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.334302\n",
      "1     2.0                   3    3.738838\n",
      "2     3.0                   2    2.506932\n",
      "3     4.0                   4    4.492972\n",
      "4     5.0                   5    5.360361\n",
      "5     6.0                   7    7.574710\n",
      "6     7.0                   6    5.492785\n",
      "7     8.0                  11   11.528127\n",
      "8     9.0                   9   10.326706\n",
      "9    10.0                  13   13.102722 \n",
      "\n",
      "RaceId: 877\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    3.298685\n",
      "1     2.0                   2    4.600136\n",
      "2     3.0                   6    8.667691\n",
      "3     4.0                   3    5.402825\n",
      "4     5.0                   4    6.352438\n",
      "5     6.0                  12   13.618393\n",
      "6     7.0                  11   12.615148\n",
      "7     8.0                   9   11.927445\n",
      "8     9.0                   8   11.234004\n",
      "9    10.0                  14   15.006287 \n",
      "\n",
      "RaceId: 877_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    3.345010\n",
      "1     2.0                   2    4.684868\n",
      "2     3.0                   6    9.217855\n",
      "3     4.0                   3    5.261699\n",
      "4     5.0                   4    6.158089\n",
      "5     6.0                  12   13.967982\n",
      "6     7.0                  11   13.064846\n",
      "7     8.0                   9   11.794653\n",
      "8     9.0                   8   11.547455\n",
      "9    10.0                  14   15.268867 \n",
      "\n",
      "RaceId: 977\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.216396\n",
      "1     2.0                   2    2.939312\n",
      "2     3.0                   3    3.613929\n",
      "3     4.0                   5    4.796861\n",
      "4     5.0                   4    4.079588\n",
      "5     6.0                   8    7.659750\n",
      "6     7.0                   7    7.637377\n",
      "7     8.0                   6    7.364086\n",
      "8     9.0                  10   10.927745\n",
      "9    10.0                   9   10.789797 \n",
      "\n",
      "RaceId: 977_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.678433\n",
      "1     2.0                   2    3.643045\n",
      "2     3.0                   3    4.190677\n",
      "3     4.0                   5    5.583187\n",
      "4     5.0                   4    4.532601\n",
      "5     6.0                   7    7.727184\n",
      "6     7.0                   8    8.047565\n",
      "7     8.0                   6    7.550454\n",
      "8     9.0                   9   11.551391\n",
      "9    10.0                  10   11.655461 \n",
      "\n",
      "RaceId: 857\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    1.736305\n",
      "1     2.0                   2    2.237946\n",
      "2     3.0                   4    3.070213\n",
      "3     4.0                   3    2.654300\n",
      "4     5.0                   7    7.017690\n",
      "5     6.0                   6    6.976031\n",
      "6     7.0                   5    6.525575\n",
      "7     8.0                   8    8.797129\n",
      "8     9.0                   9    9.129277\n",
      "9    10.0                  10   11.257835 \n",
      "\n",
      "RaceId: 857_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.099780\n",
      "1     2.0                   2    2.546852\n",
      "2     3.0                   4    3.398429\n",
      "3     4.0                   3    2.922233\n",
      "4     5.0                   7    7.598070\n",
      "5     6.0                   6    7.230050\n",
      "6     7.0                   5    6.790337\n",
      "7     8.0                   8    9.017125\n",
      "8     9.0                   9    9.222274\n",
      "9    10.0                  11   12.136248 \n",
      "\n",
      "RaceId: 893\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.969208\n",
      "1     2.0                   4    5.426068\n",
      "2     3.0                   8    9.978534\n",
      "3     4.0                   2    3.876593\n",
      "4     5.0                   3    5.013982\n",
      "5     6.0                   5    8.066024\n",
      "6     7.0                   7    9.199100\n",
      "7     8.0                   9   11.278325\n",
      "8     9.0                  10   11.699040\n",
      "9    10.0                  13   14.121448 \n",
      "\n",
      "RaceId: 893_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    3.308283\n",
      "1     2.0                   4    6.094203\n",
      "2     3.0                   8   10.597676\n",
      "3     4.0                   2    4.392516\n",
      "4     5.0                   3    5.584785\n",
      "5     6.0                   5    8.508061\n",
      "6     7.0                   6    9.683828\n",
      "7     8.0                   9   12.168986\n",
      "8     9.0                  10   12.432469\n",
      "9    10.0                  13   14.891267 \n",
      "\n",
      "RaceId: 979\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.814246\n",
      "1     2.0                   3    3.394538\n",
      "2     3.0                   2    3.300517\n",
      "3     4.0                   5    4.123469\n",
      "4     5.0                   4    4.002053\n",
      "5     6.0                   9    9.549350\n",
      "6     7.0                   8    9.462714\n",
      "7     8.0                   6    7.958740\n",
      "8     9.0                   7    9.351709\n",
      "9    10.0                  10    9.988204 \n",
      "\n",
      "RaceId: 979_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    3.083651\n",
      "1     2.0                   2    3.765261\n",
      "2     3.0                   3    3.983046\n",
      "3     4.0                   5    4.791452\n",
      "4     5.0                   4    4.296921\n",
      "5     6.0                   9    9.549900\n",
      "6     7.0                   8    9.424944\n",
      "7     8.0                   6    8.223456\n",
      "8     9.0                   7    9.307780\n",
      "9    10.0                  10   10.123676 \n",
      "\n",
      "RaceId: 980\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    1.993070\n",
      "1     2.0                   2    2.491533\n",
      "2     3.0                   4    5.089157\n",
      "3     4.0                   5    5.220174\n",
      "4     5.0                   3    2.788266\n",
      "5     6.0                   7    7.305157\n",
      "6     7.0                   8    9.909569\n",
      "7     8.0                  10   11.366668\n",
      "8     9.0                   6    7.086251\n",
      "9    10.0                  14   14.141122 \n",
      "\n",
      "RaceId: 980_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.680274\n",
      "1     2.0                   2    3.289066\n",
      "2     3.0                   4    5.766286\n",
      "3     4.0                   5    6.161867\n",
      "4     5.0                   3    3.791149\n",
      "5     6.0                   7    8.018864\n",
      "6     7.0                   8   10.831732\n",
      "7     8.0                  10   12.502624\n",
      "8     9.0                   6    7.891134\n",
      "9    10.0                  14   15.137416 \n",
      "\n",
      "RaceId: 985\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.192866\n",
      "1     2.0                   3    3.759550\n",
      "2     3.0                   4    6.631875\n",
      "3     4.0                   5    7.356046\n",
      "4     5.0                   2    3.299906\n",
      "5     6.0                   6    7.463345\n",
      "6     7.0                   9   10.366511\n",
      "7     8.0                   7    7.920755\n",
      "8     9.0                   8    7.921302\n",
      "9    10.0                  13   12.519170 \n",
      "\n",
      "RaceId: 985_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.433292\n",
      "1     2.0                   3    4.245652\n",
      "2     3.0                   4    6.906504\n",
      "3     4.0                   7    7.927156\n",
      "4     5.0                   2    3.850515\n",
      "5     6.0                   5    7.452744\n",
      "6     7.0                   9   10.644383\n",
      "7     8.0                   8    8.038692\n",
      "8     9.0                   6    7.890889\n",
      "9    10.0                  13   12.882725 \n",
      "\n",
      "RaceId: 938\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.399254\n",
      "1     2.0                   2    3.588464\n",
      "2     3.0                   5    6.306099\n",
      "3     4.0                   6    7.117742\n",
      "4     5.0                   4    4.968153\n",
      "5     6.0                   7    8.659059\n",
      "6     7.0                  10   10.817144\n",
      "7     8.0                   8    9.323973\n",
      "8     9.0                  11   11.060480\n",
      "9    10.0                   9    9.442021 \n",
      "\n",
      "RaceId: 938_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    3.061630\n",
      "1     2.0                   2    4.163329\n",
      "2     3.0                   5    6.413811\n",
      "3     4.0                   6    7.275927\n",
      "4     5.0                   4    5.255074\n",
      "5     6.0                   7    8.843591\n",
      "6     7.0                  10   10.990281\n",
      "7     8.0                   9    9.952283\n",
      "8     9.0                  11   11.665713\n",
      "9    10.0                   8    9.777903 \n",
      "\n",
      "RaceId: 972\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.353761\n",
      "1     2.0                   2    2.630976\n",
      "2     3.0                   3    3.512562\n",
      "3     4.0                   4    3.626631\n",
      "4     5.0                   5    5.776200\n",
      "5     6.0                   6    6.922090\n",
      "6     7.0                   7    8.188903\n",
      "7     8.0                   9    9.073327\n",
      "8     9.0                   8    8.693829\n",
      "9    10.0                  10   11.433002 \n",
      "\n",
      "RaceId: 972_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.853765\n",
      "1     2.0                   2    3.003194\n",
      "2     3.0                   3    3.820013\n",
      "3     4.0                   4    4.315788\n",
      "4     5.0                   5    5.783925\n",
      "5     6.0                   6    6.925481\n",
      "6     7.0                   7    8.244061\n",
      "7     8.0                   9    9.224752\n",
      "8     9.0                   8    8.990365\n",
      "9    10.0                  10   12.121891 \n",
      "\n",
      "RaceId: 973\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   2    2.898085\n",
      "1     2.0                   1    2.582713\n",
      "2     3.0                   3    5.374536\n",
      "3     4.0                   4    6.783604\n",
      "4     5.0                   5    8.056207\n",
      "5     6.0                   6    9.868425\n",
      "6     7.0                   8   11.700225\n",
      "7     8.0                   7   10.724551\n",
      "8     9.0                  11   13.405486\n",
      "9    10.0                  10   12.397973 \n",
      "\n",
      "RaceId: 973_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   2    3.148439\n",
      "1     2.0                   1    2.656131\n",
      "2     3.0                   3    5.246004\n",
      "3     4.0                   4    6.453642\n",
      "4     5.0                   5    7.801452\n",
      "5     6.0                   6    9.860707\n",
      "6     7.0                   8   11.701539\n",
      "7     8.0                   7   10.772779\n",
      "8     9.0                  11   13.509139\n",
      "9    10.0                  10   12.347611 \n",
      "\n",
      "RaceId: 933\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   2    2.902585\n",
      "1     2.0                   1    2.141480\n",
      "2     3.0                   4    5.507939\n",
      "3     4.0                   3    3.081235\n",
      "4     5.0                   5    6.918883\n",
      "5     6.0                   8    8.730861\n",
      "6     7.0                   7    8.235898\n",
      "7     8.0                  11   11.271609\n",
      "8     9.0                   6    7.305307\n",
      "9    10.0                   9    9.714780 \n",
      "\n",
      "RaceId: 933_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   3    3.621149\n",
      "1     2.0                   1    2.534528\n",
      "2     3.0                   4    5.774815\n",
      "3     4.0                   2    3.438987\n",
      "4     5.0                   5    7.192769\n",
      "5     6.0                   8    8.929185\n",
      "6     7.0                   7    8.325829\n",
      "7     8.0                  11   11.721952\n",
      "8     9.0                   6    7.533661\n",
      "9    10.0                   9   10.138014 \n",
      "\n",
      "RaceId: 901\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.287636\n",
      "1     2.0                   2    2.445440\n",
      "2     3.0                   6    6.260720\n",
      "3     4.0                   3    4.501012\n",
      "4     5.0                   5    6.177286\n",
      "5     6.0                   7    7.361946\n",
      "6     7.0                   4    6.018660\n",
      "7     8.0                   8    8.251798\n",
      "8     9.0                  10    9.415618\n",
      "9    10.0                  11    9.927896 \n",
      "\n",
      "RaceId: 901_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.784472\n",
      "1     2.0                   2    3.022220\n",
      "2     3.0                   6    6.719912\n",
      "3     4.0                   3    5.190549\n",
      "4     5.0                   4    6.354625\n",
      "5     6.0                   7    8.185903\n",
      "6     7.0                   5    6.681755\n",
      "7     8.0                   8    8.992886\n",
      "8     9.0                  10   10.239510\n",
      "9    10.0                  11   10.689214 \n",
      "\n",
      "RaceId: 950\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.009214\n",
      "1     2.0                   3    5.130564\n",
      "2     3.0                   2    3.401047\n",
      "3     4.0                   6    5.627607\n",
      "4     5.0                   7    7.308486\n",
      "5     6.0                   4    5.356592\n",
      "6     7.0                   9    8.566843\n",
      "7     8.0                  11   11.378903\n",
      "8     9.0                  10    8.993260\n",
      "9    10.0                   5    5.567030 \n",
      "\n",
      "RaceId: 950_dynamic\n",
      "   target  predicted_position  prediction\n",
      "0     1.0                   1    2.652525\n",
      "1     2.0                   4    6.285006\n",
      "2     3.0                   2    4.539932\n",
      "3     4.0                   6    6.381786\n",
      "4     5.0                   7    8.301388\n",
      "5     6.0                   3    6.007775\n",
      "6     7.0                  10   10.190583\n",
      "7     8.0                  11   12.246655\n",
      "8     9.0                   9    9.827516\n",
      "9    10.0                   5    6.339672 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dfs = {}\n",
    "for raceId, dict in total_results.items():    \n",
    "    #Auswerten der Vorhersagen aus den Outputdaten des Modells\n",
    "    A = [x[0][0] for x in list((dict.values()))]\n",
    "    A_dyn = [x[0][0] for x in list((total_results_dynamic[raceId].values()))]\n",
    "    y = list(dict.keys())\n",
    "    y_dyn = list(total_results_dynamic[raceId].keys())\n",
    "    t = pd.DataFrame(columns = ['target', 'prediction'])\n",
    "    t_dyn = pd.DataFrame(columns = ['target', 'prediction'])\n",
    "    t['target'] = y\n",
    "    t['prediction'] = A\n",
    "    t_dyn['target'] = y_dyn\n",
    "    t_dyn['prediction'] = A_dyn\n",
    "    #print(\"raceId\", raceId)\n",
    "    temp = sliced_races[raceId]\n",
    "    #name = split_by_race[raceId].where()\n",
    "    #sortieren des DataFrames nach den vorhergesagten Positionen, aufsteigend\n",
    "    end = sqldf.sqldf('''select * from t order by prediction ASC''')\n",
    "    end.reset_index(inplace = True)\n",
    "    end_dyn = sqldf.sqldf('''select * from t_dyn order by prediction ASC''')\n",
    "    end_dyn.reset_index(inplace = True)\n",
    "    #Da DF nun nach prediction aufsteigend sortiert ist, kann veränderter Index als predictete Position gesetzt werden\n",
    "    end.rename(columns = {'index': 'predicted_position'},inplace = True)\n",
    "    end_dyn.rename(columns = {'index': 'predicted_position'},inplace = True)\n",
    "    #zur Übersichtlichkeit wird nun der endgültige DF nach den richtigen Positionen (target) sortiert (aufsteigend)\n",
    "    end = sqldf.sqldf('''select * from end order by target ASC''')\n",
    "    end_dyn = sqldf.sqldf('''select * from end_dyn order by target ASC''')\n",
    "    end['predicted_position'] = end['predicted_position']+1\n",
    "    end_dyn['predicted_position'] = end_dyn['predicted_position']+1\n",
    "    #pred_name = sqldf.sqldf(\"\"\"select \n",
    "    #            distinct\n",
    "    #            d.driver_fullname as name,\n",
    "    #            t.predicted_position as pred,\n",
    "    #            d.podium_position as target_position\n",
    "    #            from \n",
    "    #            end t inner join temp d\n",
    "    #            on t.predicted_position = d.podium_position\"\"\")\n",
    "    end[\"driver_pred\"] = 0\n",
    "    end[\"driver_target\"] = 0\n",
    "    end_dyn[\"driver_pred\"] = 0\n",
    "    end_dyn[\"driver_target\"] = 0\n",
    "    for pos in temp.podium_position.unique():\n",
    "        \n",
    "        name = temp.where(temp.podium_position == pos).dropna(how = \"all\")\n",
    "        #print(name)\n",
    "        name_ = name[\"driver_fullname\"][list(name.index)[0]]\n",
    "        idx_target = end.where(end.target == pos).dropna(how = \"all\").index\n",
    "        idx_pred = end.where(end.predicted_position == pos).dropna(how = \"all\").index\n",
    "        end.loc[idx_target,\"driver_target\"] = name_\n",
    "        end.loc[idx_pred,\"driver_pred\"] = name_\n",
    "        \n",
    "        idx_target = end_dyn.where(end_dyn.target == pos).dropna(how = \"all\").index\n",
    "        idx_pred = end_dyn.where(end_dyn.predicted_position == pos).dropna(how = \"all\").index\n",
    "        end_dyn.loc[idx_target,\"driver_target\"] = name_\n",
    "        end_dyn.loc[idx_pred,\"driver_pred\"] = name_\n",
    "        \n",
    "    #umstellen der Spaltenreihenfolge\n",
    "    '''end = end[['target', 'predicted_position', 'prediction',\"driver_target\", \"driver_pred\"]]\n",
    "    end_dyn = end_dyn[['target', 'predicted_position', 'prediction',\"driver_target\", \"driver_pred\"]]'''\n",
    "    end = end[['target', 'predicted_position', 'prediction']]\n",
    "    end_dyn = end_dyn[['target', 'predicted_position', 'prediction']]\n",
    "    result_dfs[raceId] = end\n",
    "    r = str(raceId)+'_dynamic'\n",
    "    result_dfs[r] = end_dyn\n",
    "    \n",
    "for key, value in result_dfs.items():\n",
    "    print('RaceId:',key)\n",
    "    print(value.head(10),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
