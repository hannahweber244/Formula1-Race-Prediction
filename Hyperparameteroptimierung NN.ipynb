{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import pandasql as sqldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel dieses Notebooks ist es die Hyperparameter Lernrate und Epochenanzahl für das gegebene Neuronale Netz zu optimieren. Hierfür wird eine Klasse verwendet, die in gegebenen Intervallen verschiedene Kombinationen dieser Parameter geordnet ausprobiert (kein random search) und das beste Resultat, basierend auf übergebenen Trainingsdaten zurück gibt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory = 'sliced_data'):\n",
    "    '''\n",
    "        Funktion, die die aufbereiteten/vorbereiteten Daten aus existierenden CSV Dateien \n",
    "        einliest und je nach Vollständigkeit in zwei Dictionaries abspeichert,\n",
    "        geschlüsselt nach der jeweiligen RaceId\n",
    "    '''\n",
    "    #Ids von Rennen, die als Regenrennen identifiziert wurden\n",
    "    rain_id = [847,861,879,910,914,934,942,953,957,967,950,982]\n",
    "    if os.path.exists(directory):\n",
    "        csv_filenames = []\n",
    "        #auslesen aller csv file dateinamen aus formula 1 datensatz und abspeichern in liste\n",
    "        for filename in os.listdir(os.getcwd()+'/'+directory):\n",
    "            typ = filename.split('.')[-1]\n",
    "            name = filename.split('.')[0]\n",
    "            if typ == 'csv':\n",
    "                csv_filenames.append(filename)\n",
    "        sliced_races = {}\n",
    "        #einlesen und abspeichern als dataframe aller dateien\n",
    "        for file in csv_filenames:\n",
    "            try:\n",
    "                df = pd.read_csv(directory+'/'+file, engine = 'python', sep = ';', decimal = '.')\n",
    "                del df['Unnamed: 0']\n",
    "            except Exception as e:\n",
    "                df = pd.read_csv(directory+'/'+file, engine = 'c', sep = ';', decimal = '.')\n",
    "                del df['Unnamed: 0']\n",
    "                print(e)\n",
    "            #print(df.head())\n",
    "            f = int(file.split('_')[-1].split('.')[0])\n",
    "            df[\"rain\"] = 0\n",
    "            #setzen der regenkomponente auf 1 für regenrennen\n",
    "            if list(df[\"raceId\"])[0] in rain_id:\n",
    "                df[\"rain\"] = 1\n",
    "            sliced_races[f] = df\n",
    "        print('Einlesen der sliced Dateien erfolgreich')\n",
    "    else:\n",
    "        raise ('sliced Dateien können nicht eingelesen werden, da kein entsprechendes Verzeichnis existiert!')\n",
    "        \n",
    "    if os.path.exists('split_data'):\n",
    "        csv_filenames = []\n",
    "        #auslesen aller csv file dateinamen aus formula 1 datensatz und abspeichern in liste\n",
    "        for filename in os.listdir(os.getcwd()+'/split_data'):\n",
    "            typ = filename.split('.')[-1]\n",
    "            name = filename.split('.')[0]\n",
    "            if typ == 'csv':\n",
    "                csv_filenames.append(filename)\n",
    "        split_by_race = {}\n",
    "        #einlesen und abspeichern als dataframe aller dateien\n",
    "        for file in csv_filenames:\n",
    "            try:\n",
    "                df = pd.read_csv('split_data/'+file, engine = 'python', sep = ';', decimal = '.')\n",
    "                del df['Unnamed: 0']\n",
    "            except Exception as e:\n",
    "                df = pd.read_csv('split_data/'+file, engine = 'c', sep = ';', decimal = '.')\n",
    "                del df['Unnamed: 0']\n",
    "                print(e)\n",
    "            f = int(file.split('_')[-1].split('.')[0])\n",
    "            split_by_race[f] = df\n",
    "        print('Einlesen der split Dateien erfolgreich')\n",
    "    else:\n",
    "        raise('split Dateien können nicht eingelesen werden, da kein entsprechendes Verzeichnis existiert!')\n",
    "        \n",
    "    return sliced_races, split_by_race\n",
    "\n",
    "def train_test (data_dict, test_num = 5, nogo_columns = []):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    test_final = {}\n",
    "    temp_y_podium = []\n",
    "    test_races = list(data_dict.keys())\n",
    "    random.shuffle(test_races)\n",
    "    test_races = test_races[0:test_num]\n",
    "    for key, value in data_dict.items():\n",
    "        helper = key\n",
    "        for did in value.driverId.unique():\n",
    "            temp = value.where(value.driverId == did).dropna(how = \"all\")\n",
    "            if list(temp[\"podium_position\"])[0] < 0: #Top x finish positions\n",
    "                pp = 1\n",
    "            else:\n",
    "                if key in test_races:\n",
    "                    temp['sum_milliseconds_pro_lap'] = temp['sum_milliseconds_pro_lap']/60000\n",
    "                    temp_y = list(temp[\"podium_position\"])#list((temp[\"total_milliseconds\"]/60000))\n",
    "                    #temp_y = temp_y[0]\n",
    "                    cols = [col for col in temp.columns if col not in nogo_columns]\n",
    "                    temp_x = temp[cols]\n",
    "                    stops=temp_x.sum(axis = 0)[2] #Addierte anzahl an stops bis zu dem Zeitpunkt\n",
    "                    temp_x = temp_x.tail(1)\n",
    "                    temp_x['stop_binary'] = stops\n",
    "                    x_tensor = torch.tensor(temp_x[temp_x.columns].values)\n",
    "                    #temp_x = x_tensor.float()\n",
    "                    test_data.append((x_tensor, [temp_y[0]]))\n",
    "                else:\n",
    "                    temp['sum_milliseconds_pro_lap'] = temp['sum_milliseconds_pro_lap']/60000\n",
    "                    temp_y = list(temp[\"podium_position\"])#list((temp[\"total_milliseconds\"]/60000))\n",
    "                    #temp_y = temp_y[0]\n",
    "                    cols = [col for col in temp.columns if col not in nogo_columns]\n",
    "                    temp_x = temp[cols]\n",
    "                    stops=temp_x.sum(axis = 0)[2] #Addierte anzahl an stops bis zu dem Zeitpunkt\n",
    "                    temp_x = temp_x.tail(1)\n",
    "                    temp_x['stop_binary'] = stops\n",
    "                    x_tensor = torch.tensor(temp_x[temp_x.columns].values)\n",
    "                    #temp_x = x_tensor.float()\n",
    "                    train_data.append((x_tensor, [temp_y[0]]))\n",
    "        if key in test_races:\n",
    "            test_final[key]=test_data\n",
    "        test_data = []\n",
    "    random.shuffle(train_data)\n",
    "    #random.shuffle(test_data)\n",
    "    #test_data = train_data[len(train_data)-100:]\n",
    "    train_data = train_data#[0:len(train_data)-100]\n",
    "            \n",
    "            #break\n",
    "            #for i, row in temp.iterrows():\n",
    "            \n",
    "    return train_data, test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen der Dateien als Dataframes, welche in zwei Dictionaries gespeichert werden:<br>\n",
    "- Attribut directory verweißt auf den Ordnernamen der sliced Dateien, Default ist 'sliced_data'\n",
    "- für die split_by_race Daten sollte kein separates Directory angegeben werden müssen, da es keinen Unterschied zwischen den Rennen gibt! (Nur bei Slicing: Wie viele Prozent des Rennens möchte ich? ergibt ein anderes Directory Sinn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einlesen der sliced Dateien erfolgreich\n",
      "Einlesen der split Dateien erfolgreich\n"
     ]
    }
   ],
   "source": [
    "sliced_races, split_by_race = load_data(directory = 'sliced_data50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufteilen des Datensatzes in einen Trainingsteil und einen Testteil, test enthält ein Dictionary mit Tensoren, die einer RaceId zugeordnet sind, train ist eine Liste von Tensoren. Test_num gibt die Anzahl Rennen an, die als Testdaten zufällig gewählt werden sollen, nogo_columns bezeichnet die Spalten, die nicht beachtet werden sollen, bisher aber im DS enthalten sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nogo_columns = ['year', 'podium_position', 'raceId','lap_number','total_laps','driverId',\n",
    "                'grandprix_name', 'driver_fullname',\n",
    "               'constructor_name', #'total_laps',\n",
    "               #'status_clean', 'constructorId',\n",
    "                'total_milliseconds',\n",
    "               'lap_in_milliseconds']\n",
    "train, test = train_test(sliced_races, test_num = 5, nogo_columns = nogo_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definieren von notwendigen Klassen: \n",
    "    \n",
    "Zuerst wird die Klasse für das Neuronale Netz definiert, danach die Klasse für die Optimierung der Hyperparameter Lernrate und Epochen Anzahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Netz(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Netz,self).__init__()\n",
    "        self.fc1 = nn.Linear(110, 150)\n",
    "        self.fc2 = nn.Linear(150, 180)\n",
    "        self.fc3 = nn.Linear(180, 190)\n",
    "        self.fc4 = nn.Linear(190, 120)\n",
    "        self.fc5 = nn.Linear(120, 100)\n",
    "        self.fc6 = nn.Linear(100, 70)\n",
    "        self.fc7 = nn.Linear(70, 30)\n",
    "        self.fc8 = nn.Linear(30, 1)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc2(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc4(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc5(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc6(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc7(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc8(x.float())\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HP_Optimizer(object):\n",
    "    \n",
    "    def __init__(self, lr_range = (0.0001,0.0001), step_size = 0.0001, max_epochs = (2,2), opt = 'Adam', cuda = True):\n",
    "        \n",
    "        self.__model = Netz()\n",
    "        self.__lr = lr_range\n",
    "        self.__epochs = max_epochs\n",
    "        self.__optimizer = opt\n",
    "        self.__steps = step_size\n",
    "        self.__combination_results = {}\n",
    "        self.__combination_overview = {}\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.cuda = cuda\n",
    "        self.opt_combination = {}\n",
    "        \n",
    "        \n",
    "    def validate_combinations(self):\n",
    "        \n",
    "        specifics = {}\n",
    "        if self.__optimizer == 'Adam':\n",
    "            \n",
    "            #definieren der range für die lernratenoptimierung\n",
    "            lr_s = self.__lr[0]\n",
    "            lr_e = self.__lr[1]\n",
    "            \n",
    "            #wurde eine range für die anzahl der epochen übergeben?\n",
    "            if self.__epochs[0] == self.__epochs[1]:\n",
    "                #hyperparamter epochenanzahl wird nicht optimiert\n",
    "                max_epoch = self.__epochs[0]\n",
    "                \n",
    "                if lr_s == lr_e:\n",
    "                    #es ist keine range für die lernrate gegeben, in der diese optimiert werden soll\n",
    "                    print('Parameter Epochen und Lernrate können nicht optimiert werden, da kein Intervall übergeben wurde')\n",
    "    \n",
    "                    #setzen des optimizers als Adam\n",
    "                    self.__model = Netz()\n",
    "                    optimizer = optim.Adam(self.__model.parameters(), lr = lr_s)\n",
    "                \n",
    "                    for epoch in range(1,max_epoch):\n",
    "                        self.__train(epoch, optimizer)  \n",
    "                        \n",
    "                    result = self.__test()\n",
    "                    A = result.prediction.tolist()\n",
    "                    y = result.target.tolist()\n",
    "                    mae = MAE(A,y)\n",
    "                    specifics = {}\n",
    "                    specifics['lr'] = lr_s\n",
    "                    specifics['epochen'] = max_epoch\n",
    "                    \n",
    "                    key = random.randint(0,10000)\n",
    "                    while key in list(self.__combination_results.keys()):\n",
    "                         key = random.randint(0,10000)\n",
    "                    \n",
    "                    self.__combination_results[key] = mae\n",
    "                    self.__combination_overview[key] = specifics\n",
    "                    \n",
    "                            \n",
    "                else:\n",
    "                \n",
    "                    for l in np.arange(lr_s,lr_e, self.__steps):\n",
    "                \n",
    "                        #setzen des optimizers als Adam\n",
    "                        self.__model = Netz()\n",
    "                        optimizer = optim.Adam(self.__model.parameters(), lr = l)\n",
    "                        #trainieren des modells\n",
    "                        for epoch in range(1,max_epoch):\n",
    "                            self.__train(epoch, optimizer)  \n",
    "                        \n",
    "                        result = self.__test()\n",
    "                        A = result.prediction.tolist()\n",
    "                        y = result.target.tolist()\n",
    "                        mae = MAE(A,y)\n",
    "                        specifics = {}\n",
    "                        specifics['lr'] = l\n",
    "                        specifics['epochen'] = max_epoch\n",
    "                        \n",
    "                        #abspeichern der gewonnenen informationen (MAE nach lr und anzahl durchgeführter epochen)\n",
    "                        key = random.randint(0,10000)\n",
    "                        while key in list(self.__combination_results.keys()):\n",
    "                             key = random.randint(0,10000)\n",
    "                        \n",
    "                        self.__combination_results[key] = mae\n",
    "                        self.__combination_overview[key] = specifics\n",
    "                    \n",
    "                    \n",
    "            else:\n",
    "                for max_epoch in range(self.__epochs[0], self.__epochs[1]):\n",
    "                    #definieren der range für die lernratenoptimierung\n",
    "                    lr_s = self.__lr[0]\n",
    "                    lr_e = self.__lr[1]\n",
    "            \n",
    "                    #hyperparamter epochenanzahl wird nicht optimiert\n",
    "                        \n",
    "                    if lr_s == lr_e:\n",
    "                        #es ist keine range für die lernrate gegeben, in der diese optimiert werden soll\n",
    "                        #print('Parameter Epochen und Lernrate können nicht optimiert werden, da kein Intervall übergeben wurde')\n",
    "            \n",
    "                        #setzen des optimizers als Adam\n",
    "                        self.__model = Netz()\n",
    "                        optimizer = optim.Adam(self.__model.parameters(), lr = lr_s)\n",
    "                        \n",
    "                        for epoch in range(1,max_epoch):\n",
    "                            self.__train(epoch, optimizer)  \n",
    "                                \n",
    "                        result = self.__test()\n",
    "                        A = result.prediction.tolist()\n",
    "                        y = result.target.tolist()\n",
    "                        mae = MAE(A,y)\n",
    "                        specifics = {}\n",
    "                        specifics['lr'] = lr_s\n",
    "                        specifics['epochen'] = max_epoch\n",
    "                          \n",
    "                        key = random.randint(0,10000)\n",
    "                        while key in list(self.__combination_results.keys()):\n",
    "                             key = random.randint(0,10000)\n",
    "                            \n",
    "                        self.__combination_results[key] = mae\n",
    "                        self.__combination_overview[key] = specifics\n",
    "                            \n",
    "                                    \n",
    "                    else:\n",
    "                        \n",
    "                        for l in np.arange(lr_s,lr_e, self.__steps):\n",
    "                        \n",
    "                            #setzen des optimizers als Adam\n",
    "                            self.__model = Netz()\n",
    "                            optimizer = optim.Adam(self.__model.parameters(), lr = l)\n",
    "                            #trainieren des modells\n",
    "                            for epoch in range(1,max_epoch):\n",
    "                                self.__train(epoch, optimizer)  \n",
    "                            \n",
    "                            result = self.__test()\n",
    "                            A = result.prediction.tolist()\n",
    "                            y = result.target.tolist()\n",
    "                            mae = MAE(A,y)\n",
    "                            specifics = {}\n",
    "                            specifics['lr'] = l\n",
    "                            specifics['epochen'] = max_epoch\n",
    "                            \n",
    "                            #abspeichern der gewonnenen informationen (MAE nach lr und anzahl durchgeführter epochen)\n",
    "                            key = random.randint(0,10000)\n",
    "                            while key in list(self.__combination_results.keys()):\n",
    "                                 key = random.randint(0,10000)\n",
    "                            \n",
    "                            self.__combination_results[key] = mae\n",
    "                            self.__combination_overview[key] = specifics\n",
    "            \n",
    "            #finden der besten kombination nach minimalstem Error (MAE)\n",
    "            key_min = min(self.__combination_results.keys(), key=(lambda k: self.__combination_results[k]))\n",
    "            best_combination = self.__combination_overview[key_min]\n",
    "            best_combination['mae'] = self.__combination_results[key_min]\n",
    "            self.opt_combination = best_combination\n",
    "        else:\n",
    "            raise ('No valid optimizer given! Try Adam for example!')\n",
    "            \n",
    "    def __train(self, epoch, optimizer):\n",
    "        if self.cuda:\n",
    "            self.__model.cuda()\n",
    "            self.__model.train()\n",
    "            batch_id = 0\n",
    "            for data, target in self.train_data:\n",
    "                data = data.cuda()\n",
    "                target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "                shape = target.size()[1]\n",
    "                target = target.resize(shape,1).cuda()\n",
    "                optimizer.zero_grad()\n",
    "                out = self.__model(data)\n",
    "                #print(\"Out: \", out, out.size())\n",
    "                #print(\"Target: \", target, target.size())\n",
    "                criterion = nn.MSELoss()\n",
    "                loss = criterion(out, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch, batch_id *len(data), len(train_data),\n",
    "                100. * batch_id / len(train_data), loss.item()))\n",
    "                batch_id +=1\n",
    "        else:\n",
    "            self.__model.train()\n",
    "            batch_id = 0\n",
    "            for data, target in self.train_data:\n",
    "                #data = data.cuda()\n",
    "                target = torch.Tensor(target).unsqueeze(0)#.cuda()\n",
    "                shape = target.size()[1]\n",
    "                target = target.resize(shape,1)#.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                out = self.__model(data)\n",
    "                #print(\"Out: \", out, out.size())\n",
    "                #print(\"Target: \", target, target.size())\n",
    "                criterion = nn.MSELoss()\n",
    "                loss = criterion(out, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                #    epoch, batch_id *len(data), len(train_data),\n",
    "                #100. * batch_id / len(train_data), loss.item()))\n",
    "                batch_id +=1\n",
    "            \n",
    "    def __test(self):\n",
    "        total = 0\n",
    "        count = 0\n",
    "        result_dict = {}\n",
    "        result = pd.DataFrame(columns = ['target','prediction'])\n",
    "        help_dict = {}\n",
    "        if self.cuda:\n",
    "            self.__model.cuda()\n",
    "            for key in self.test_data.keys():\n",
    "                #print(key)\n",
    "                help_dict = {}\n",
    "                for data, target in self.test_data[key]:\n",
    "                    self.__model.eval()\n",
    "                    #files.listdir(path)\n",
    "                    data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1).cuda()\n",
    "                    out = self.__model(data).cpu()\n",
    "                    #print(out)\n",
    "                    out = out.detach().numpy()\n",
    "                    #out = np.round(out)\n",
    "                    target = target.cpu()\n",
    "                    target = target.detach().numpy()\n",
    "                    #print(data)\n",
    "                    #print(data[\"driverId\"])\n",
    "                    total += abs(out - target[0][0])\n",
    "                    #print(\"current_position: \", data[0][0].item())\n",
    "                    #print(\"Output: \", out)\n",
    "                    #print(\"Target: \", target)\n",
    "                    help_dict[target[0][0]] = out\n",
    "                    #print(\"Difference: \", out - target)\n",
    "                    count+=1\n",
    "                #Auslesen der predicteten Werte A und der zugehörigen targets y\n",
    "                A = [x[0][0] for x in list((help_dict.values()))]\n",
    "                y = list(help_dict.keys())\n",
    "                \n",
    "                #Anfügen der Werte an Result\n",
    "                t = pd.DataFrame()\n",
    "                t['target'] = y\n",
    "                t['prediction_value'] = A\n",
    "                t = sqldf.sqldf('''select * from t order by prediction_value ASC''')\n",
    "                t.reset_index(inplace = True)\n",
    "                t.rename(columns = {'index':'prediction'}, inplace = True)\n",
    "                t['prediction'] = t['prediction']+1\n",
    "                \n",
    "                \n",
    "                result = result.append(t)\n",
    "                \n",
    "        else:\n",
    "            for raceId in self.test_data.keys():\n",
    "                #print(key)\n",
    "                help_dict = {}\n",
    "                for data, target in self.test_data[raceId]:\n",
    "                    self.__model.eval()\n",
    "                    #files.listdir(path)\n",
    "                    #data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1)#.cuda()\n",
    "                    out = self.__model(data)#.cpu()\n",
    "                    #print(out)\n",
    "                    out = out.detach().numpy()\n",
    "                    #out = np.round(out)\n",
    "                    #target = target.cpu()\n",
    "                    target = target.detach().numpy()\n",
    "                    #print(data)\n",
    "                    #print(data[\"driverId\"])\n",
    "                    total += abs(out - target[0][0])\n",
    "                    #print(\"current_position: \", data[0][0].item())\n",
    "                    #print(\"Output: \", out)\n",
    "                    #print(\"Target: \", target)\n",
    "                    help_dict[target[0][0]] = out\n",
    "                    #print(\"Difference: \", out - target)\n",
    "                    count+=1\n",
    "                #Auslesen der predicteten Werte A und der zugehörigen targets y\n",
    "                A = [x[0][0] for x in list((help_dict.values()))]\n",
    "                y = list(help_dict.keys())\n",
    "                \n",
    "                #Anfügen der Werte an Result\n",
    "                t = pd.DataFrame()\n",
    "                t['target'] = y\n",
    "                t['prediction_value'] = A\n",
    "                t = sqldf.sqldf('''select * from t order by prediction_value ASC''')\n",
    "                t.reset_index(inplace = True)\n",
    "                t.rename(columns = {'index':'prediction'}, inplace = True)\n",
    "                t['prediction'] = t['prediction']+1\n",
    "                \n",
    "                result = result.append(t)\n",
    "        return result\n",
    "         \n",
    "            \n",
    "    def get_all_information(self):\n",
    "        \n",
    "        print('Chosen Model:',self.__model)\n",
    "        print('Learningrate Range:',self.__lr)\n",
    "        print('Maximum Epochs:', self.__epochs)\n",
    "        print('Chosen Optimizer:', self.__optimizer)\n",
    "        print('Result Encoding:', self.__combination_overview)\n",
    "        print('Results:', self.__combination_results)\n",
    "        print('Optimale Kombination:', self.opt_combination)\n",
    "        \n",
    "    def help(self):\n",
    "        print('Parameters with defaults:\\nlr_range --> (0.0001,0.0001),\\nstep_size--> 0.0001,\\nmax_epochs-->(2,2),\\nopt-->\"Adam\",\\ncuda=True')\n",
    "        print('lr_range: Tupel with learnrate range')\n",
    "        print('step_size: float/int for step_size of learnrate')\n",
    "        print('max_epochs: Tupel with number of epochs range')\n",
    "        print('opt: Optimizer (by default Adam)')\n",
    "        print('cuda: True/False if cuda should be used, default = True\\n')\n",
    "        print('Attributes:')\n",
    "        print('set self.train_data as dictionary with races (form: {raceId: race(dataframe)})')\n",
    "        print('set self.test_data as dictionary with races (form: {raceId: race(dataframe)})')\n",
    "        print('self.opt_combination: Dictionary which contains the best combination of the given parameters\\n')\n",
    "        print('Methods:')\n",
    "        print('call self.validate_combination() to compare all combinations')\n",
    "        print('get all information/results with self.get_all_information()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufrufen des Optimierers, übergeben der zuvor erzeugten Test- und Trainingsdatensätze, angeben der Intervalle für die die Hyperparameter getestet werden sollen (Lernrate und Epochenanzahl). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters with defaults:\n",
      "lr_range --> (0.0001,0.0001),\n",
      "step_size--> 0.0001,\n",
      "max_epochs-->(2,2),\n",
      "opt-->\"Adam\",\n",
      "cuda=True\n",
      "lr_range: Tupel with learnrate range\n",
      "step_size: float/int for step_size of learnrate\n",
      "max_epochs: Tupel with number of epochs range\n",
      "opt: Optimizer (by default Adam)\n",
      "cuda: True/False if cuda should be used, default = True\n",
      "\n",
      "Attributes:\n",
      "set self.train_data as dictionary with races (form: {raceId: race(dataframe)})\n",
      "set self.test_data as dictionary with races (form: {raceId: race(dataframe)})\n",
      "self.opt_combination: Dictionary which contains the best combination of the given parameters\n",
      "\n",
      "Methods:\n",
      "call self.validate_combination() to compare all combinations\n",
      "get all information/results with self.get_all_information()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Model: Netz(\n",
      "  (fc1): Linear(in_features=110, out_features=150, bias=True)\n",
      "  (fc2): Linear(in_features=150, out_features=180, bias=True)\n",
      "  (fc3): Linear(in_features=180, out_features=190, bias=True)\n",
      "  (fc4): Linear(in_features=190, out_features=120, bias=True)\n",
      "  (fc5): Linear(in_features=120, out_features=100, bias=True)\n",
      "  (fc6): Linear(in_features=100, out_features=70, bias=True)\n",
      "  (fc7): Linear(in_features=70, out_features=30, bias=True)\n",
      "  (fc8): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Learningrate Range: (1e-05, 0.0001)\n",
      "Maximum Epochs: (1, 5)\n",
      "Chosen Optimizer: Adam\n",
      "Result Encoding: {6907: {'lr': 1e-05, 'epochen': 1}, 4621: {'lr': 2e-05, 'epochen': 1}, 7786: {'lr': 3.0000000000000004e-05, 'epochen': 1}, 8612: {'lr': 4e-05, 'epochen': 1}, 1995: {'lr': 5e-05, 'epochen': 1}, 5925: {'lr': 6e-05, 'epochen': 1}, 7085: {'lr': 7.000000000000001e-05, 'epochen': 1}, 9093: {'lr': 8e-05, 'epochen': 1}, 2904: {'lr': 9e-05, 'epochen': 1}, 1821: {'lr': 1e-05, 'epochen': 2}, 501: {'lr': 2e-05, 'epochen': 2}, 1680: {'lr': 3.0000000000000004e-05, 'epochen': 2}, 2408: {'lr': 4e-05, 'epochen': 2}, 983: {'lr': 5e-05, 'epochen': 2}, 6428: {'lr': 6e-05, 'epochen': 2}, 642: {'lr': 7.000000000000001e-05, 'epochen': 2}, 9297: {'lr': 8e-05, 'epochen': 2}, 8540: {'lr': 9e-05, 'epochen': 2}, 6758: {'lr': 1e-05, 'epochen': 3}, 4469: {'lr': 2e-05, 'epochen': 3}, 9769: {'lr': 3.0000000000000004e-05, 'epochen': 3}, 7437: {'lr': 4e-05, 'epochen': 3}, 5479: {'lr': 5e-05, 'epochen': 3}, 3482: {'lr': 6e-05, 'epochen': 3}, 4626: {'lr': 7.000000000000001e-05, 'epochen': 3}, 5358: {'lr': 8e-05, 'epochen': 3}, 1691: {'lr': 9e-05, 'epochen': 3, 'mae': 1.0681818181818181}, 5468: {'lr': 1e-05, 'epochen': 4}, 3729: {'lr': 2e-05, 'epochen': 4}, 6187: {'lr': 3.0000000000000004e-05, 'epochen': 4}, 2142: {'lr': 4e-05, 'epochen': 4}, 2187: {'lr': 5e-05, 'epochen': 4}, 3357: {'lr': 6e-05, 'epochen': 4}, 7880: {'lr': 7.000000000000001e-05, 'epochen': 4}, 2895: {'lr': 8e-05, 'epochen': 4}, 3294: {'lr': 9e-05, 'epochen': 4}}\n",
      "Results: {6907: 5.113636363636363, 4621: 3.8181818181818183, 7786: 2.7045454545454546, 8612: 8.5, 1995: 8.590909090909092, 5925: 8.590909090909092, 7085: 8.659090909090908, 9093: 7.818181818181818, 2904: 4.5, 1821: 2.1136363636363638, 501: 1.7954545454545454, 1680: 1.6590909090909092, 2408: 1.6363636363636365, 983: 1.5227272727272727, 6428: 1.3636363636363635, 642: 1.3636363636363635, 9297: 1.3409090909090908, 8540: 1.3636363636363635, 6758: 1.8636363636363635, 4469: 1.6136363636363635, 9769: 1.5227272727272727, 7437: 1.3181818181818181, 5479: 1.3181818181818181, 3482: 1.2272727272727273, 4626: 1.25, 5358: 1.1363636363636365, 1691: 1.0681818181818181, 5468: 2.0, 3729: 1.3409090909090908, 6187: 1.2727272727272727, 2142: 1.25, 2187: 1.0681818181818181, 3357: 1.0681818181818181, 7880: 1.1136363636363635, 2895: 1.0681818181818181, 3294: 1.0909090909090908}\n",
      "Optimale Kombination: {'lr': 9e-05, 'epochen': 3, 'mae': 1.0681818181818181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "h = HP_Optimizer(lr_range = (0.00001,0.0001),step_size = 0.00001, max_epochs=(1,5),cuda = False)\n",
    "h.help()\n",
    "h.train_data = train\n",
    "h.test_data = test\n",
    "h.validate_combinations()\n",
    "h.get_all_information()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden die als optimal in den gegebenen Intervallen genommenen Werte aus dem Optimierer genommen und ein neues Modell mit diesen Parametern trainiert. Zum Schluss werden die Ergebnisse des Testlaufes auf den Testdaten ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda? [y/n]: n\n",
      "Train Epoche: 1 [0/2802 (0%)]\tLoss: 577.555786\n",
      "Train Epoche: 1 [1/2802 (0%)]\tLoss: 196.803192\n",
      "Train Epoche: 1 [2/2802 (0%)]\tLoss: 121.375008\n",
      "Train Epoche: 1 [3/2802 (0%)]\tLoss: 196.755829\n",
      "Train Epoche: 1 [4/2802 (0%)]\tLoss: 196.190277\n",
      "Train Epoche: 1 [5/2802 (0%)]\tLoss: 577.447449\n",
      "Train Epoche: 1 [6/2802 (0%)]\tLoss: 400.494629\n",
      "Train Epoche: 1 [7/2802 (0%)]\tLoss: 81.297401\n",
      "Train Epoche: 1 [8/2802 (0%)]\tLoss: 169.080841\n",
      "Train Epoche: 1 [9/2802 (0%)]\tLoss: 196.392975\n",
      "Train Epoche: 1 [10/2802 (0%)]\tLoss: 9.017655\n",
      "Train Epoche: 1 [11/2802 (0%)]\tLoss: 80.904480\n",
      "Train Epoche: 1 [12/2802 (0%)]\tLoss: 360.761444\n",
      "Train Epoche: 1 [13/2802 (0%)]\tLoss: 528.656677\n",
      "Train Epoche: 1 [14/2802 (0%)]\tLoss: 63.752804\n",
      "Train Epoche: 1 [15/2802 (1%)]\tLoss: 48.829136\n",
      "Train Epoche: 1 [16/2802 (1%)]\tLoss: 195.564667\n",
      "Train Epoche: 1 [17/2802 (1%)]\tLoss: 576.341003\n",
      "Train Epoche: 1 [18/2802 (1%)]\tLoss: 9.039769\n",
      "Train Epoche: 1 [19/2802 (1%)]\tLoss: 255.236832\n",
      "Train Epoche: 1 [20/2802 (1%)]\tLoss: 3.928746\n",
      "Train Epoche: 1 [21/2802 (1%)]\tLoss: 3.911119\n",
      "Train Epoche: 1 [22/2802 (1%)]\tLoss: 3.919278\n",
      "Train Epoche: 1 [23/2802 (1%)]\tLoss: 0.939499\n",
      "Train Epoche: 1 [24/2802 (1%)]\tLoss: 63.501949\n",
      "Train Epoche: 1 [25/2802 (1%)]\tLoss: 120.222527\n",
      "Train Epoche: 1 [26/2802 (1%)]\tLoss: 3.837241\n",
      "Train Epoche: 1 [27/2802 (1%)]\tLoss: 119.792809\n",
      "Train Epoche: 1 [28/2802 (1%)]\tLoss: 3.795088\n",
      "Train Epoche: 1 [29/2802 (1%)]\tLoss: 48.019215\n",
      "Train Epoche: 1 [30/2802 (1%)]\tLoss: 24.434469\n",
      "Train Epoche: 1 [31/2802 (1%)]\tLoss: 8.710354\n",
      "Train Epoche: 1 [32/2802 (1%)]\tLoss: 483.325745\n",
      "Train Epoche: 1 [33/2802 (1%)]\tLoss: 35.445198\n",
      "Train Epoche: 1 [34/2802 (1%)]\tLoss: 483.725708\n",
      "Train Epoche: 1 [35/2802 (1%)]\tLoss: 483.232574\n",
      "Train Epoche: 1 [36/2802 (1%)]\tLoss: 358.089386\n",
      "Train Epoche: 1 [37/2802 (1%)]\tLoss: 3.701528\n",
      "Train Epoche: 1 [38/2802 (1%)]\tLoss: 8.284313\n",
      "Train Epoche: 1 [39/2802 (1%)]\tLoss: 47.630962\n",
      "Train Epoche: 1 [40/2802 (1%)]\tLoss: 474.606232\n",
      "Train Epoche: 1 [41/2802 (1%)]\tLoss: 482.034088\n",
      "Train Epoche: 1 [42/2802 (1%)]\tLoss: 355.726807\n",
      "Train Epoche: 1 [43/2802 (2%)]\tLoss: 192.939316\n",
      "Train Epoche: 1 [44/2802 (2%)]\tLoss: 166.345093\n",
      "Train Epoche: 1 [45/2802 (2%)]\tLoss: 320.626953\n",
      "Train Epoche: 1 [46/2802 (2%)]\tLoss: 141.085312\n",
      "Train Epoche: 1 [47/2802 (2%)]\tLoss: 61.859894\n",
      "Train Epoche: 1 [48/2802 (2%)]\tLoss: 163.896454\n",
      "Train Epoche: 1 [49/2802 (2%)]\tLoss: 14.752725\n",
      "Train Epoche: 1 [50/2802 (2%)]\tLoss: 23.818762\n",
      "Train Epoche: 1 [51/2802 (2%)]\tLoss: 282.191071\n",
      "Train Epoche: 1 [52/2802 (2%)]\tLoss: 470.147705\n",
      "Train Epoche: 1 [53/2802 (2%)]\tLoss: 165.037888\n",
      "Train Epoche: 1 [54/2802 (2%)]\tLoss: 471.620758\n",
      "Train Epoche: 1 [55/2802 (2%)]\tLoss: 283.258392\n",
      "Train Epoche: 1 [56/2802 (2%)]\tLoss: 395.626801\n",
      "Train Epoche: 1 [57/2802 (2%)]\tLoss: 58.248852\n",
      "Train Epoche: 1 [58/2802 (2%)]\tLoss: 7.314692\n",
      "Train Epoche: 1 [59/2802 (2%)]\tLoss: 13.778021\n",
      "Train Epoche: 1 [60/2802 (2%)]\tLoss: 469.335663\n",
      "Train Epoche: 1 [61/2802 (2%)]\tLoss: 214.289658\n",
      "Train Epoche: 1 [62/2802 (2%)]\tLoss: 277.252472\n",
      "Train Epoche: 1 [63/2802 (2%)]\tLoss: 74.855148\n",
      "Train Epoche: 1 [64/2802 (2%)]\tLoss: 382.356262\n",
      "Train Epoche: 1 [65/2802 (2%)]\tLoss: 43.075054\n",
      "Train Epoche: 1 [66/2802 (2%)]\tLoss: 547.125427\n",
      "Train Epoche: 1 [67/2802 (2%)]\tLoss: 114.439323\n",
      "Train Epoche: 1 [68/2802 (2%)]\tLoss: 464.248260\n",
      "Train Epoche: 1 [69/2802 (2%)]\tLoss: 0.247515\n",
      "Train Epoche: 1 [70/2802 (2%)]\tLoss: 20.813387\n",
      "Train Epoche: 1 [71/2802 (3%)]\tLoss: 56.295486\n",
      "Train Epoche: 1 [72/2802 (3%)]\tLoss: 2.485362\n",
      "Train Epoche: 1 [73/2802 (3%)]\tLoss: 20.175241\n",
      "Train Epoche: 1 [74/2802 (3%)]\tLoss: 548.178406\n",
      "Train Epoche: 1 [75/2802 (3%)]\tLoss: 87.111977\n",
      "Train Epoche: 1 [76/2802 (3%)]\tLoss: 294.364349\n",
      "Train Epoche: 1 [77/2802 (3%)]\tLoss: 448.514618\n",
      "Train Epoche: 1 [78/2802 (3%)]\tLoss: 454.293793\n",
      "Train Epoche: 1 [79/2802 (3%)]\tLoss: 289.317261\n",
      "Train Epoche: 1 [80/2802 (3%)]\tLoss: 398.503815\n",
      "Train Epoche: 1 [81/2802 (3%)]\tLoss: 106.185326\n",
      "Train Epoche: 1 [82/2802 (3%)]\tLoss: 284.344788\n",
      "Train Epoche: 1 [83/2802 (3%)]\tLoss: 14.659236\n",
      "Train Epoche: 1 [84/2802 (3%)]\tLoss: 23.408535\n",
      "Train Epoche: 1 [85/2802 (3%)]\tLoss: 169.145782\n",
      "Train Epoche: 1 [86/2802 (3%)]\tLoss: 15.112872\n",
      "Train Epoche: 1 [87/2802 (3%)]\tLoss: 520.153381\n",
      "Train Epoche: 1 [88/2802 (3%)]\tLoss: 6.634595\n",
      "Train Epoche: 1 [89/2802 (3%)]\tLoss: 16.311354\n",
      "Train Epoche: 1 [90/2802 (3%)]\tLoss: 158.384888\n",
      "Train Epoche: 1 [91/2802 (3%)]\tLoss: 103.691772\n",
      "Train Epoche: 1 [92/2802 (3%)]\tLoss: 12.165920\n",
      "Train Epoche: 1 [93/2802 (3%)]\tLoss: 530.331299\n",
      "Train Epoche: 1 [94/2802 (3%)]\tLoss: 89.087952\n",
      "Train Epoche: 1 [95/2802 (3%)]\tLoss: 176.305511\n",
      "Train Epoche: 1 [96/2802 (3%)]\tLoss: 90.038666\n",
      "Train Epoche: 1 [97/2802 (3%)]\tLoss: 411.966156\n",
      "Train Epoche: 1 [98/2802 (3%)]\tLoss: 59.101578\n",
      "Train Epoche: 1 [99/2802 (4%)]\tLoss: 186.240845\n",
      "Train Epoche: 1 [100/2802 (4%)]\tLoss: 123.298737\n",
      "Train Epoche: 1 [101/2802 (4%)]\tLoss: 1.379436\n",
      "Train Epoche: 1 [102/2802 (4%)]\tLoss: 534.153442\n",
      "Train Epoche: 1 [103/2802 (4%)]\tLoss: 332.448395\n",
      "Train Epoche: 1 [104/2802 (4%)]\tLoss: 0.000802\n",
      "Train Epoche: 1 [105/2802 (4%)]\tLoss: 67.861595\n",
      "Train Epoche: 1 [106/2802 (4%)]\tLoss: 78.102188\n",
      "Train Epoche: 1 [107/2802 (4%)]\tLoss: 13.466615\n",
      "Train Epoche: 1 [108/2802 (4%)]\tLoss: 11.154278\n",
      "Train Epoche: 1 [109/2802 (4%)]\tLoss: 317.512970\n",
      "Train Epoche: 1 [110/2802 (4%)]\tLoss: 130.180573\n",
      "Train Epoche: 1 [111/2802 (4%)]\tLoss: 336.760529\n",
      "Train Epoche: 1 [112/2802 (4%)]\tLoss: 192.665146\n",
      "Train Epoche: 1 [113/2802 (4%)]\tLoss: 0.895909\n",
      "Train Epoche: 1 [114/2802 (4%)]\tLoss: 72.533966\n",
      "Train Epoche: 1 [115/2802 (4%)]\tLoss: 7.637706\n",
      "Train Epoche: 1 [116/2802 (4%)]\tLoss: 11.514071\n",
      "Train Epoche: 1 [117/2802 (4%)]\tLoss: 0.210942\n",
      "Train Epoche: 1 [118/2802 (4%)]\tLoss: 28.340847\n",
      "Train Epoche: 1 [119/2802 (4%)]\tLoss: 192.316818\n",
      "Train Epoche: 1 [120/2802 (4%)]\tLoss: 0.739623\n",
      "Train Epoche: 1 [121/2802 (4%)]\tLoss: 11.631820\n",
      "Train Epoche: 1 [122/2802 (4%)]\tLoss: 7.063519\n",
      "Train Epoche: 1 [123/2802 (4%)]\tLoss: 38.389217\n",
      "Train Epoche: 1 [124/2802 (4%)]\tLoss: 82.400719\n",
      "Train Epoche: 1 [125/2802 (4%)]\tLoss: 64.888481\n",
      "Train Epoche: 1 [126/2802 (4%)]\tLoss: 1.064519\n",
      "Train Epoche: 1 [127/2802 (5%)]\tLoss: 229.120636\n",
      "Train Epoche: 1 [128/2802 (5%)]\tLoss: 27.796753\n",
      "Train Epoche: 1 [129/2802 (5%)]\tLoss: 3.276663\n",
      "Train Epoche: 1 [130/2802 (5%)]\tLoss: 111.862228\n",
      "Train Epoche: 1 [131/2802 (5%)]\tLoss: 50.190941\n",
      "Train Epoche: 1 [132/2802 (5%)]\tLoss: 23.662868\n",
      "Train Epoche: 1 [133/2802 (5%)]\tLoss: 3.994570\n",
      "Train Epoche: 1 [134/2802 (5%)]\tLoss: 31.282187\n",
      "Train Epoche: 1 [135/2802 (5%)]\tLoss: 4.391236\n",
      "Train Epoche: 1 [136/2802 (5%)]\tLoss: 6.655573\n",
      "Train Epoche: 1 [137/2802 (5%)]\tLoss: 59.528320\n",
      "Train Epoche: 1 [138/2802 (5%)]\tLoss: 9.992803\n",
      "Train Epoche: 1 [139/2802 (5%)]\tLoss: 13.877283\n",
      "Train Epoche: 1 [140/2802 (5%)]\tLoss: 56.856346\n",
      "Train Epoche: 1 [141/2802 (5%)]\tLoss: 125.343475\n",
      "Train Epoche: 1 [142/2802 (5%)]\tLoss: 11.509542\n",
      "Train Epoche: 1 [143/2802 (5%)]\tLoss: 2.948426\n",
      "Train Epoche: 1 [144/2802 (5%)]\tLoss: 115.131744\n",
      "Train Epoche: 1 [145/2802 (5%)]\tLoss: 28.900858\n",
      "Train Epoche: 1 [146/2802 (5%)]\tLoss: 112.821266\n",
      "Train Epoche: 1 [147/2802 (5%)]\tLoss: 100.329803\n",
      "Train Epoche: 1 [148/2802 (5%)]\tLoss: 13.302573\n",
      "Train Epoche: 1 [149/2802 (5%)]\tLoss: 11.226566\n",
      "Train Epoche: 1 [150/2802 (5%)]\tLoss: 685.970154\n",
      "Train Epoche: 1 [151/2802 (5%)]\tLoss: 16.058945\n",
      "Train Epoche: 1 [152/2802 (5%)]\tLoss: 44.786789\n",
      "Train Epoche: 1 [153/2802 (5%)]\tLoss: 22.214611\n",
      "Train Epoche: 1 [154/2802 (5%)]\tLoss: 0.652284\n",
      "Train Epoche: 1 [155/2802 (6%)]\tLoss: 7.931397\n",
      "Train Epoche: 1 [156/2802 (6%)]\tLoss: 6.263669\n",
      "Train Epoche: 1 [157/2802 (6%)]\tLoss: 24.477655\n",
      "Train Epoche: 1 [158/2802 (6%)]\tLoss: 114.382179\n",
      "Train Epoche: 1 [159/2802 (6%)]\tLoss: 27.787653\n",
      "Train Epoche: 1 [160/2802 (6%)]\tLoss: 273.038483\n",
      "Train Epoche: 1 [161/2802 (6%)]\tLoss: 65.796227\n",
      "Train Epoche: 1 [162/2802 (6%)]\tLoss: 1.161198\n",
      "Train Epoche: 1 [163/2802 (6%)]\tLoss: 32.595768\n",
      "Train Epoche: 1 [164/2802 (6%)]\tLoss: 6.863510\n",
      "Train Epoche: 1 [165/2802 (6%)]\tLoss: 4.786153\n",
      "Train Epoche: 1 [166/2802 (6%)]\tLoss: 36.360634\n",
      "Train Epoche: 1 [167/2802 (6%)]\tLoss: 63.292435\n",
      "Train Epoche: 1 [168/2802 (6%)]\tLoss: 8.383314\n",
      "Train Epoche: 1 [169/2802 (6%)]\tLoss: 11.043996\n",
      "Train Epoche: 1 [170/2802 (6%)]\tLoss: 51.681175\n",
      "Train Epoche: 1 [171/2802 (6%)]\tLoss: 1.805997\n",
      "Train Epoche: 1 [172/2802 (6%)]\tLoss: 33.288910\n",
      "Train Epoche: 1 [173/2802 (6%)]\tLoss: 21.771044\n",
      "Train Epoche: 1 [174/2802 (6%)]\tLoss: 91.124176\n",
      "Train Epoche: 1 [175/2802 (6%)]\tLoss: 27.989326\n",
      "Train Epoche: 1 [176/2802 (6%)]\tLoss: 24.918814\n",
      "Train Epoche: 1 [177/2802 (6%)]\tLoss: 254.213699\n",
      "Train Epoche: 1 [178/2802 (6%)]\tLoss: 0.224483\n",
      "Train Epoche: 1 [179/2802 (6%)]\tLoss: 16.238691\n",
      "Train Epoche: 1 [180/2802 (6%)]\tLoss: 20.098612\n",
      "Train Epoche: 1 [181/2802 (6%)]\tLoss: 36.878490\n",
      "Train Epoche: 1 [182/2802 (6%)]\tLoss: 3.950168\n",
      "Train Epoche: 1 [183/2802 (7%)]\tLoss: 33.995739\n",
      "Train Epoche: 1 [184/2802 (7%)]\tLoss: 10.774383\n",
      "Train Epoche: 1 [185/2802 (7%)]\tLoss: 82.225319\n",
      "Train Epoche: 1 [186/2802 (7%)]\tLoss: 112.911621\n",
      "Train Epoche: 1 [187/2802 (7%)]\tLoss: 0.000044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [188/2802 (7%)]\tLoss: 40.286591\n",
      "Train Epoche: 1 [189/2802 (7%)]\tLoss: 30.393295\n",
      "Train Epoche: 1 [190/2802 (7%)]\tLoss: 6.484061\n",
      "Train Epoche: 1 [191/2802 (7%)]\tLoss: 10.575177\n",
      "Train Epoche: 1 [192/2802 (7%)]\tLoss: 12.674343\n",
      "Train Epoche: 1 [193/2802 (7%)]\tLoss: 252.448502\n",
      "Train Epoche: 1 [194/2802 (7%)]\tLoss: 192.831894\n",
      "Train Epoche: 1 [195/2802 (7%)]\tLoss: 35.838978\n",
      "Train Epoche: 1 [196/2802 (7%)]\tLoss: 11.741378\n",
      "Train Epoche: 1 [197/2802 (7%)]\tLoss: 1.618123\n",
      "Train Epoche: 1 [198/2802 (7%)]\tLoss: 0.023427\n",
      "Train Epoche: 1 [199/2802 (7%)]\tLoss: 2.890275\n",
      "Train Epoche: 1 [200/2802 (7%)]\tLoss: 13.785327\n",
      "Train Epoche: 1 [201/2802 (7%)]\tLoss: 38.039478\n",
      "Train Epoche: 1 [202/2802 (7%)]\tLoss: 2.809848\n",
      "Train Epoche: 1 [203/2802 (7%)]\tLoss: 173.343414\n",
      "Train Epoche: 1 [204/2802 (7%)]\tLoss: 6.083939\n",
      "Train Epoche: 1 [205/2802 (7%)]\tLoss: 12.494363\n",
      "Train Epoche: 1 [206/2802 (7%)]\tLoss: 174.459763\n",
      "Train Epoche: 1 [207/2802 (7%)]\tLoss: 0.566170\n",
      "Train Epoche: 1 [208/2802 (7%)]\tLoss: 0.105190\n",
      "Train Epoche: 1 [209/2802 (7%)]\tLoss: 61.946445\n",
      "Train Epoche: 1 [210/2802 (7%)]\tLoss: 0.234298\n",
      "Train Epoche: 1 [211/2802 (8%)]\tLoss: 7.841638\n",
      "Train Epoche: 1 [212/2802 (8%)]\tLoss: 15.831394\n",
      "Train Epoche: 1 [213/2802 (8%)]\tLoss: 23.984955\n",
      "Train Epoche: 1 [214/2802 (8%)]\tLoss: 10.340307\n",
      "Train Epoche: 1 [215/2802 (8%)]\tLoss: 48.274426\n",
      "Train Epoche: 1 [216/2802 (8%)]\tLoss: 2.357516\n",
      "Train Epoche: 1 [217/2802 (8%)]\tLoss: 5.269858\n",
      "Train Epoche: 1 [218/2802 (8%)]\tLoss: 7.429125\n",
      "Train Epoche: 1 [219/2802 (8%)]\tLoss: 341.123627\n",
      "Train Epoche: 1 [220/2802 (8%)]\tLoss: 11.511282\n",
      "Train Epoche: 1 [221/2802 (8%)]\tLoss: 203.918686\n",
      "Train Epoche: 1 [222/2802 (8%)]\tLoss: 0.742856\n",
      "Train Epoche: 1 [223/2802 (8%)]\tLoss: 12.554655\n",
      "Train Epoche: 1 [224/2802 (8%)]\tLoss: 215.273270\n",
      "Train Epoche: 1 [225/2802 (8%)]\tLoss: 34.884838\n",
      "Train Epoche: 1 [226/2802 (8%)]\tLoss: 74.564201\n",
      "Train Epoche: 1 [227/2802 (8%)]\tLoss: 13.676311\n",
      "Train Epoche: 1 [228/2802 (8%)]\tLoss: 17.354645\n",
      "Train Epoche: 1 [229/2802 (8%)]\tLoss: 25.377497\n",
      "Train Epoche: 1 [230/2802 (8%)]\tLoss: 53.956985\n",
      "Train Epoche: 1 [231/2802 (8%)]\tLoss: 0.019992\n",
      "Train Epoche: 1 [232/2802 (8%)]\tLoss: 11.015193\n",
      "Train Epoche: 1 [233/2802 (8%)]\tLoss: 9.348988\n",
      "Train Epoche: 1 [234/2802 (8%)]\tLoss: 26.086693\n",
      "Train Epoche: 1 [235/2802 (8%)]\tLoss: 20.859478\n",
      "Train Epoche: 1 [236/2802 (8%)]\tLoss: 7.092067\n",
      "Train Epoche: 1 [237/2802 (8%)]\tLoss: 46.047482\n",
      "Train Epoche: 1 [238/2802 (8%)]\tLoss: 75.762329\n",
      "Train Epoche: 1 [239/2802 (9%)]\tLoss: 52.871307\n",
      "Train Epoche: 1 [240/2802 (9%)]\tLoss: 50.992176\n",
      "Train Epoche: 1 [241/2802 (9%)]\tLoss: 4.057825\n",
      "Train Epoche: 1 [242/2802 (9%)]\tLoss: 0.062698\n",
      "Train Epoche: 1 [243/2802 (9%)]\tLoss: 38.190506\n",
      "Train Epoche: 1 [244/2802 (9%)]\tLoss: 0.318089\n",
      "Train Epoche: 1 [245/2802 (9%)]\tLoss: 14.380579\n",
      "Train Epoche: 1 [246/2802 (9%)]\tLoss: 5.545578\n",
      "Train Epoche: 1 [247/2802 (9%)]\tLoss: 0.433697\n",
      "Train Epoche: 1 [248/2802 (9%)]\tLoss: 0.004942\n",
      "Train Epoche: 1 [249/2802 (9%)]\tLoss: 51.819893\n",
      "Train Epoche: 1 [250/2802 (9%)]\tLoss: 14.058981\n",
      "Train Epoche: 1 [251/2802 (9%)]\tLoss: 1.179235\n",
      "Train Epoche: 1 [252/2802 (9%)]\tLoss: 116.469810\n",
      "Train Epoche: 1 [253/2802 (9%)]\tLoss: 24.991417\n",
      "Train Epoche: 1 [254/2802 (9%)]\tLoss: 14.567708\n",
      "Train Epoche: 1 [255/2802 (9%)]\tLoss: 53.880699\n",
      "Train Epoche: 1 [256/2802 (9%)]\tLoss: 22.811562\n",
      "Train Epoche: 1 [257/2802 (9%)]\tLoss: 11.356083\n",
      "Train Epoche: 1 [258/2802 (9%)]\tLoss: 30.091425\n",
      "Train Epoche: 1 [259/2802 (9%)]\tLoss: 88.606476\n",
      "Train Epoche: 1 [260/2802 (9%)]\tLoss: 27.404222\n",
      "Train Epoche: 1 [261/2802 (9%)]\tLoss: 36.304943\n",
      "Train Epoche: 1 [262/2802 (9%)]\tLoss: 58.822216\n",
      "Train Epoche: 1 [263/2802 (9%)]\tLoss: 3.834758\n",
      "Train Epoche: 1 [264/2802 (9%)]\tLoss: 30.578535\n",
      "Train Epoche: 1 [265/2802 (9%)]\tLoss: 42.748325\n",
      "Train Epoche: 1 [266/2802 (9%)]\tLoss: 15.236572\n",
      "Train Epoche: 1 [267/2802 (10%)]\tLoss: 44.284763\n",
      "Train Epoche: 1 [268/2802 (10%)]\tLoss: 0.836358\n",
      "Train Epoche: 1 [269/2802 (10%)]\tLoss: 36.778507\n",
      "Train Epoche: 1 [270/2802 (10%)]\tLoss: 0.109976\n",
      "Train Epoche: 1 [271/2802 (10%)]\tLoss: 0.370502\n",
      "Train Epoche: 1 [272/2802 (10%)]\tLoss: 0.241822\n",
      "Train Epoche: 1 [273/2802 (10%)]\tLoss: 91.841660\n",
      "Train Epoche: 1 [274/2802 (10%)]\tLoss: 186.236374\n",
      "Train Epoche: 1 [275/2802 (10%)]\tLoss: 33.857246\n",
      "Train Epoche: 1 [276/2802 (10%)]\tLoss: 66.830017\n",
      "Train Epoche: 1 [277/2802 (10%)]\tLoss: 8.007502\n",
      "Train Epoche: 1 [278/2802 (10%)]\tLoss: 0.828907\n",
      "Train Epoche: 1 [279/2802 (10%)]\tLoss: 10.175014\n",
      "Train Epoche: 1 [280/2802 (10%)]\tLoss: 5.765236\n",
      "Train Epoche: 1 [281/2802 (10%)]\tLoss: 36.568249\n",
      "Train Epoche: 1 [282/2802 (10%)]\tLoss: 15.354029\n",
      "Train Epoche: 1 [283/2802 (10%)]\tLoss: 187.294662\n",
      "Train Epoche: 1 [284/2802 (10%)]\tLoss: 157.980606\n",
      "Train Epoche: 1 [285/2802 (10%)]\tLoss: 1.159507\n",
      "Train Epoche: 1 [286/2802 (10%)]\tLoss: 328.244507\n",
      "Train Epoche: 1 [287/2802 (10%)]\tLoss: 0.060472\n",
      "Train Epoche: 1 [288/2802 (10%)]\tLoss: 154.459076\n",
      "Train Epoche: 1 [289/2802 (10%)]\tLoss: 140.825562\n",
      "Train Epoche: 1 [290/2802 (10%)]\tLoss: 9.272333\n",
      "Train Epoche: 1 [291/2802 (10%)]\tLoss: 39.178028\n",
      "Train Epoche: 1 [292/2802 (10%)]\tLoss: 104.294762\n",
      "Train Epoche: 1 [293/2802 (10%)]\tLoss: 20.498327\n",
      "Train Epoche: 1 [294/2802 (10%)]\tLoss: 291.855743\n",
      "Train Epoche: 1 [295/2802 (11%)]\tLoss: 66.969627\n",
      "Train Epoche: 1 [296/2802 (11%)]\tLoss: 180.995636\n",
      "Train Epoche: 1 [297/2802 (11%)]\tLoss: 3.144671\n",
      "Train Epoche: 1 [298/2802 (11%)]\tLoss: 55.051292\n",
      "Train Epoche: 1 [299/2802 (11%)]\tLoss: 7.885767\n",
      "Train Epoche: 1 [300/2802 (11%)]\tLoss: 0.114154\n",
      "Train Epoche: 1 [301/2802 (11%)]\tLoss: 20.075293\n",
      "Train Epoche: 1 [302/2802 (11%)]\tLoss: 2.633229\n",
      "Train Epoche: 1 [303/2802 (11%)]\tLoss: 55.452133\n",
      "Train Epoche: 1 [304/2802 (11%)]\tLoss: 15.776383\n",
      "Train Epoche: 1 [305/2802 (11%)]\tLoss: 7.761006\n",
      "Train Epoche: 1 [306/2802 (11%)]\tLoss: 85.178894\n",
      "Train Epoche: 1 [307/2802 (11%)]\tLoss: 43.227501\n",
      "Train Epoche: 1 [308/2802 (11%)]\tLoss: 8.419941\n",
      "Train Epoche: 1 [309/2802 (11%)]\tLoss: 33.486362\n",
      "Train Epoche: 1 [310/2802 (11%)]\tLoss: 154.649460\n",
      "Train Epoche: 1 [311/2802 (11%)]\tLoss: 13.194513\n",
      "Train Epoche: 1 [312/2802 (11%)]\tLoss: 57.820251\n",
      "Train Epoche: 1 [313/2802 (11%)]\tLoss: 57.337936\n",
      "Train Epoche: 1 [314/2802 (11%)]\tLoss: 49.434185\n",
      "Train Epoche: 1 [315/2802 (11%)]\tLoss: 1.597715\n",
      "Train Epoche: 1 [316/2802 (11%)]\tLoss: 3.707561\n",
      "Train Epoche: 1 [317/2802 (11%)]\tLoss: 7.816701\n",
      "Train Epoche: 1 [318/2802 (11%)]\tLoss: 0.617694\n",
      "Train Epoche: 1 [319/2802 (11%)]\tLoss: 4.676342\n",
      "Train Epoche: 1 [320/2802 (11%)]\tLoss: 33.617294\n",
      "Train Epoche: 1 [321/2802 (11%)]\tLoss: 18.028469\n",
      "Train Epoche: 1 [322/2802 (11%)]\tLoss: 2.152621\n",
      "Train Epoche: 1 [323/2802 (12%)]\tLoss: 59.297325\n",
      "Train Epoche: 1 [324/2802 (12%)]\tLoss: 46.299839\n",
      "Train Epoche: 1 [325/2802 (12%)]\tLoss: 52.601212\n",
      "Train Epoche: 1 [326/2802 (12%)]\tLoss: 0.244707\n",
      "Train Epoche: 1 [327/2802 (12%)]\tLoss: 18.090055\n",
      "Train Epoche: 1 [328/2802 (12%)]\tLoss: 3.648245\n",
      "Train Epoche: 1 [329/2802 (12%)]\tLoss: 191.518570\n",
      "Train Epoche: 1 [330/2802 (12%)]\tLoss: 0.034894\n",
      "Train Epoche: 1 [331/2802 (12%)]\tLoss: 102.284668\n",
      "Train Epoche: 1 [332/2802 (12%)]\tLoss: 20.123922\n",
      "Train Epoche: 1 [333/2802 (12%)]\tLoss: 19.472483\n",
      "Train Epoche: 1 [334/2802 (12%)]\tLoss: 67.534744\n",
      "Train Epoche: 1 [335/2802 (12%)]\tLoss: 32.156895\n",
      "Train Epoche: 1 [336/2802 (12%)]\tLoss: 29.848467\n",
      "Train Epoche: 1 [337/2802 (12%)]\tLoss: 5.033364\n",
      "Train Epoche: 1 [338/2802 (12%)]\tLoss: 7.139528\n",
      "Train Epoche: 1 [339/2802 (12%)]\tLoss: 49.831959\n",
      "Train Epoche: 1 [340/2802 (12%)]\tLoss: 122.240318\n",
      "Train Epoche: 1 [341/2802 (12%)]\tLoss: 5.018069\n",
      "Train Epoche: 1 [342/2802 (12%)]\tLoss: 11.487739\n",
      "Train Epoche: 1 [343/2802 (12%)]\tLoss: 12.029858\n",
      "Train Epoche: 1 [344/2802 (12%)]\tLoss: 0.354283\n",
      "Train Epoche: 1 [345/2802 (12%)]\tLoss: 20.814501\n",
      "Train Epoche: 1 [346/2802 (12%)]\tLoss: 6.894100\n",
      "Train Epoche: 1 [347/2802 (12%)]\tLoss: 2.178184\n",
      "Train Epoche: 1 [348/2802 (12%)]\tLoss: 45.410721\n",
      "Train Epoche: 1 [349/2802 (12%)]\tLoss: 88.546196\n",
      "Train Epoche: 1 [350/2802 (12%)]\tLoss: 0.061964\n",
      "Train Epoche: 1 [351/2802 (13%)]\tLoss: 211.342224\n",
      "Train Epoche: 1 [352/2802 (13%)]\tLoss: 1.446195\n",
      "Train Epoche: 1 [353/2802 (13%)]\tLoss: 5.243689\n",
      "Train Epoche: 1 [354/2802 (13%)]\tLoss: 3.924145\n",
      "Train Epoche: 1 [355/2802 (13%)]\tLoss: 32.998955\n",
      "Train Epoche: 1 [356/2802 (13%)]\tLoss: 0.790733\n",
      "Train Epoche: 1 [357/2802 (13%)]\tLoss: 0.536391\n",
      "Train Epoche: 1 [358/2802 (13%)]\tLoss: 30.821220\n",
      "Train Epoche: 1 [359/2802 (13%)]\tLoss: 0.107031\n",
      "Train Epoche: 1 [360/2802 (13%)]\tLoss: 0.136731\n",
      "Train Epoche: 1 [361/2802 (13%)]\tLoss: 53.046158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [362/2802 (13%)]\tLoss: 6.039726\n",
      "Train Epoche: 1 [363/2802 (13%)]\tLoss: 19.174118\n",
      "Train Epoche: 1 [364/2802 (13%)]\tLoss: 10.165446\n",
      "Train Epoche: 1 [365/2802 (13%)]\tLoss: 0.000461\n",
      "Train Epoche: 1 [366/2802 (13%)]\tLoss: 72.096046\n",
      "Train Epoche: 1 [367/2802 (13%)]\tLoss: 58.251369\n",
      "Train Epoche: 1 [368/2802 (13%)]\tLoss: 198.292969\n",
      "Train Epoche: 1 [369/2802 (13%)]\tLoss: 2.113568\n",
      "Train Epoche: 1 [370/2802 (13%)]\tLoss: 8.043991\n",
      "Train Epoche: 1 [371/2802 (13%)]\tLoss: 8.421689\n",
      "Train Epoche: 1 [372/2802 (13%)]\tLoss: 11.173714\n",
      "Train Epoche: 1 [373/2802 (13%)]\tLoss: 60.794041\n",
      "Train Epoche: 1 [374/2802 (13%)]\tLoss: 0.174085\n",
      "Train Epoche: 1 [375/2802 (13%)]\tLoss: 0.007558\n",
      "Train Epoche: 1 [376/2802 (13%)]\tLoss: 0.281251\n",
      "Train Epoche: 1 [377/2802 (13%)]\tLoss: 58.387718\n",
      "Train Epoche: 1 [378/2802 (13%)]\tLoss: 9.277590\n",
      "Train Epoche: 1 [379/2802 (14%)]\tLoss: 180.108414\n",
      "Train Epoche: 1 [380/2802 (14%)]\tLoss: 38.541481\n",
      "Train Epoche: 1 [381/2802 (14%)]\tLoss: 35.960255\n",
      "Train Epoche: 1 [382/2802 (14%)]\tLoss: 8.375816\n",
      "Train Epoche: 1 [383/2802 (14%)]\tLoss: 0.842182\n",
      "Train Epoche: 1 [384/2802 (14%)]\tLoss: 7.835174\n",
      "Train Epoche: 1 [385/2802 (14%)]\tLoss: 1.171390\n",
      "Train Epoche: 1 [386/2802 (14%)]\tLoss: 4.925789\n",
      "Train Epoche: 1 [387/2802 (14%)]\tLoss: 61.281048\n",
      "Train Epoche: 1 [388/2802 (14%)]\tLoss: 3.942909\n",
      "Train Epoche: 1 [389/2802 (14%)]\tLoss: 35.189011\n",
      "Train Epoche: 1 [390/2802 (14%)]\tLoss: 2.949527\n",
      "Train Epoche: 1 [391/2802 (14%)]\tLoss: 4.487540\n",
      "Train Epoche: 1 [392/2802 (14%)]\tLoss: 0.000128\n",
      "Train Epoche: 1 [393/2802 (14%)]\tLoss: 20.196032\n",
      "Train Epoche: 1 [394/2802 (14%)]\tLoss: 254.011932\n",
      "Train Epoche: 1 [395/2802 (14%)]\tLoss: 8.509061\n",
      "Train Epoche: 1 [396/2802 (14%)]\tLoss: 21.765802\n",
      "Train Epoche: 1 [397/2802 (14%)]\tLoss: 5.184119\n",
      "Train Epoche: 1 [398/2802 (14%)]\tLoss: 231.190109\n",
      "Train Epoche: 1 [399/2802 (14%)]\tLoss: 50.331192\n",
      "Train Epoche: 1 [400/2802 (14%)]\tLoss: 29.431652\n",
      "Train Epoche: 1 [401/2802 (14%)]\tLoss: 22.291210\n",
      "Train Epoche: 1 [402/2802 (14%)]\tLoss: 1.477125\n",
      "Train Epoche: 1 [403/2802 (14%)]\tLoss: 20.478210\n",
      "Train Epoche: 1 [404/2802 (14%)]\tLoss: 6.614465\n",
      "Train Epoche: 1 [405/2802 (14%)]\tLoss: 0.020618\n",
      "Train Epoche: 1 [406/2802 (14%)]\tLoss: 58.551857\n",
      "Train Epoche: 1 [407/2802 (15%)]\tLoss: 216.283844\n",
      "Train Epoche: 1 [408/2802 (15%)]\tLoss: 136.274841\n",
      "Train Epoche: 1 [409/2802 (15%)]\tLoss: 20.104240\n",
      "Train Epoche: 1 [410/2802 (15%)]\tLoss: 8.192045\n",
      "Train Epoche: 1 [411/2802 (15%)]\tLoss: 6.504276\n",
      "Train Epoche: 1 [412/2802 (15%)]\tLoss: 120.792526\n",
      "Train Epoche: 1 [413/2802 (15%)]\tLoss: 1.944709\n",
      "Train Epoche: 1 [414/2802 (15%)]\tLoss: 4.481902\n",
      "Train Epoche: 1 [415/2802 (15%)]\tLoss: 87.618469\n",
      "Train Epoche: 1 [416/2802 (15%)]\tLoss: 0.300833\n",
      "Train Epoche: 1 [417/2802 (15%)]\tLoss: 5.713579\n",
      "Train Epoche: 1 [418/2802 (15%)]\tLoss: 21.661077\n",
      "Train Epoche: 1 [419/2802 (15%)]\tLoss: 182.296448\n",
      "Train Epoche: 1 [420/2802 (15%)]\tLoss: 144.019394\n",
      "Train Epoche: 1 [421/2802 (15%)]\tLoss: 7.059627\n",
      "Train Epoche: 1 [422/2802 (15%)]\tLoss: 151.387970\n",
      "Train Epoche: 1 [423/2802 (15%)]\tLoss: 0.003542\n",
      "Train Epoche: 1 [424/2802 (15%)]\tLoss: 21.267998\n",
      "Train Epoche: 1 [425/2802 (15%)]\tLoss: 4.385287\n",
      "Train Epoche: 1 [426/2802 (15%)]\tLoss: 8.919392\n",
      "Train Epoche: 1 [427/2802 (15%)]\tLoss: 0.719634\n",
      "Train Epoche: 1 [428/2802 (15%)]\tLoss: 163.303268\n",
      "Train Epoche: 1 [429/2802 (15%)]\tLoss: 13.509293\n",
      "Train Epoche: 1 [430/2802 (15%)]\tLoss: 29.989143\n",
      "Train Epoche: 1 [431/2802 (15%)]\tLoss: 9.004464\n",
      "Train Epoche: 1 [432/2802 (15%)]\tLoss: 36.316906\n",
      "Train Epoche: 1 [433/2802 (15%)]\tLoss: 3.990595\n",
      "Train Epoche: 1 [434/2802 (15%)]\tLoss: 5.820489\n",
      "Train Epoche: 1 [435/2802 (16%)]\tLoss: 52.048355\n",
      "Train Epoche: 1 [436/2802 (16%)]\tLoss: 6.230932\n",
      "Train Epoche: 1 [437/2802 (16%)]\tLoss: 0.000507\n",
      "Train Epoche: 1 [438/2802 (16%)]\tLoss: 26.374437\n",
      "Train Epoche: 1 [439/2802 (16%)]\tLoss: 1.669759\n",
      "Train Epoche: 1 [440/2802 (16%)]\tLoss: 0.083191\n",
      "Train Epoche: 1 [441/2802 (16%)]\tLoss: 27.615738\n",
      "Train Epoche: 1 [442/2802 (16%)]\tLoss: 125.534027\n",
      "Train Epoche: 1 [443/2802 (16%)]\tLoss: 62.380795\n",
      "Train Epoche: 1 [444/2802 (16%)]\tLoss: 21.769442\n",
      "Train Epoche: 1 [445/2802 (16%)]\tLoss: 1.338560\n",
      "Train Epoche: 1 [446/2802 (16%)]\tLoss: 33.723644\n",
      "Train Epoche: 1 [447/2802 (16%)]\tLoss: 46.432846\n",
      "Train Epoche: 1 [448/2802 (16%)]\tLoss: 2.152302\n",
      "Train Epoche: 1 [449/2802 (16%)]\tLoss: 78.864075\n",
      "Train Epoche: 1 [450/2802 (16%)]\tLoss: 56.589039\n",
      "Train Epoche: 1 [451/2802 (16%)]\tLoss: 12.487575\n",
      "Train Epoche: 1 [452/2802 (16%)]\tLoss: 11.636627\n",
      "Train Epoche: 1 [453/2802 (16%)]\tLoss: 52.314560\n",
      "Train Epoche: 1 [454/2802 (16%)]\tLoss: 3.965430\n",
      "Train Epoche: 1 [455/2802 (16%)]\tLoss: 20.389473\n",
      "Train Epoche: 1 [456/2802 (16%)]\tLoss: 45.774044\n",
      "Train Epoche: 1 [457/2802 (16%)]\tLoss: 157.904068\n",
      "Train Epoche: 1 [458/2802 (16%)]\tLoss: 4.860309\n",
      "Train Epoche: 1 [459/2802 (16%)]\tLoss: 39.920555\n",
      "Train Epoche: 1 [460/2802 (16%)]\tLoss: 15.451110\n",
      "Train Epoche: 1 [461/2802 (16%)]\tLoss: 55.254757\n",
      "Train Epoche: 1 [462/2802 (16%)]\tLoss: 25.296005\n",
      "Train Epoche: 1 [463/2802 (17%)]\tLoss: 0.474344\n",
      "Train Epoche: 1 [464/2802 (17%)]\tLoss: 28.076225\n",
      "Train Epoche: 1 [465/2802 (17%)]\tLoss: 1.124068\n",
      "Train Epoche: 1 [466/2802 (17%)]\tLoss: 57.296059\n",
      "Train Epoche: 1 [467/2802 (17%)]\tLoss: 0.263469\n",
      "Train Epoche: 1 [468/2802 (17%)]\tLoss: 22.410723\n",
      "Train Epoche: 1 [469/2802 (17%)]\tLoss: 26.743271\n",
      "Train Epoche: 1 [470/2802 (17%)]\tLoss: 932.932373\n",
      "Train Epoche: 1 [471/2802 (17%)]\tLoss: 1.619815\n",
      "Train Epoche: 1 [472/2802 (17%)]\tLoss: 134.604996\n",
      "Train Epoche: 1 [473/2802 (17%)]\tLoss: 39.018394\n",
      "Train Epoche: 1 [474/2802 (17%)]\tLoss: 34.904045\n",
      "Train Epoche: 1 [475/2802 (17%)]\tLoss: 128.173706\n",
      "Train Epoche: 1 [476/2802 (17%)]\tLoss: 15.089981\n",
      "Train Epoche: 1 [477/2802 (17%)]\tLoss: 0.120154\n",
      "Train Epoche: 1 [478/2802 (17%)]\tLoss: 45.434231\n",
      "Train Epoche: 1 [479/2802 (17%)]\tLoss: 2.589449\n",
      "Train Epoche: 1 [480/2802 (17%)]\tLoss: 17.524179\n",
      "Train Epoche: 1 [481/2802 (17%)]\tLoss: 21.341179\n",
      "Train Epoche: 1 [482/2802 (17%)]\tLoss: 8.351149\n",
      "Train Epoche: 1 [483/2802 (17%)]\tLoss: 7.042609\n",
      "Train Epoche: 1 [484/2802 (17%)]\tLoss: 0.378355\n",
      "Train Epoche: 1 [485/2802 (17%)]\tLoss: 11.432551\n",
      "Train Epoche: 1 [486/2802 (17%)]\tLoss: 0.177503\n",
      "Train Epoche: 1 [487/2802 (17%)]\tLoss: 230.707001\n",
      "Train Epoche: 1 [488/2802 (17%)]\tLoss: 313.163513\n",
      "Train Epoche: 1 [489/2802 (17%)]\tLoss: 128.304642\n",
      "Train Epoche: 1 [490/2802 (17%)]\tLoss: 0.743594\n",
      "Train Epoche: 1 [491/2802 (18%)]\tLoss: 2.313075\n",
      "Train Epoche: 1 [492/2802 (18%)]\tLoss: 0.359595\n",
      "Train Epoche: 1 [493/2802 (18%)]\tLoss: 193.555328\n",
      "Train Epoche: 1 [494/2802 (18%)]\tLoss: 1.653956\n",
      "Train Epoche: 1 [495/2802 (18%)]\tLoss: 0.344898\n",
      "Train Epoche: 1 [496/2802 (18%)]\tLoss: 6.667964\n",
      "Train Epoche: 1 [497/2802 (18%)]\tLoss: 11.809231\n",
      "Train Epoche: 1 [498/2802 (18%)]\tLoss: 1.453910\n",
      "Train Epoche: 1 [499/2802 (18%)]\tLoss: 6.041029\n",
      "Train Epoche: 1 [500/2802 (18%)]\tLoss: 27.864254\n",
      "Train Epoche: 1 [501/2802 (18%)]\tLoss: 20.704063\n",
      "Train Epoche: 1 [502/2802 (18%)]\tLoss: 63.579613\n",
      "Train Epoche: 1 [503/2802 (18%)]\tLoss: 62.697460\n",
      "Train Epoche: 1 [504/2802 (18%)]\tLoss: 1.336835\n",
      "Train Epoche: 1 [505/2802 (18%)]\tLoss: 55.718880\n",
      "Train Epoche: 1 [506/2802 (18%)]\tLoss: 0.741006\n",
      "Train Epoche: 1 [507/2802 (18%)]\tLoss: 94.991310\n",
      "Train Epoche: 1 [508/2802 (18%)]\tLoss: 2.752415\n",
      "Train Epoche: 1 [509/2802 (18%)]\tLoss: 7.989371\n",
      "Train Epoche: 1 [510/2802 (18%)]\tLoss: 0.246558\n",
      "Train Epoche: 1 [511/2802 (18%)]\tLoss: 49.735142\n",
      "Train Epoche: 1 [512/2802 (18%)]\tLoss: 1.241005\n",
      "Train Epoche: 1 [513/2802 (18%)]\tLoss: 213.744263\n",
      "Train Epoche: 1 [514/2802 (18%)]\tLoss: 26.086460\n",
      "Train Epoche: 1 [515/2802 (18%)]\tLoss: 24.321718\n",
      "Train Epoche: 1 [516/2802 (18%)]\tLoss: 0.028651\n",
      "Train Epoche: 1 [517/2802 (18%)]\tLoss: 5.341144\n",
      "Train Epoche: 1 [518/2802 (18%)]\tLoss: 94.017799\n",
      "Train Epoche: 1 [519/2802 (19%)]\tLoss: 80.542740\n",
      "Train Epoche: 1 [520/2802 (19%)]\tLoss: 239.148026\n",
      "Train Epoche: 1 [521/2802 (19%)]\tLoss: 0.918420\n",
      "Train Epoche: 1 [522/2802 (19%)]\tLoss: 3.797324\n",
      "Train Epoche: 1 [523/2802 (19%)]\tLoss: 11.849796\n",
      "Train Epoche: 1 [524/2802 (19%)]\tLoss: 14.089320\n",
      "Train Epoche: 1 [525/2802 (19%)]\tLoss: 115.324814\n",
      "Train Epoche: 1 [526/2802 (19%)]\tLoss: 10.532933\n",
      "Train Epoche: 1 [527/2802 (19%)]\tLoss: 14.711640\n",
      "Train Epoche: 1 [528/2802 (19%)]\tLoss: 13.817248\n",
      "Train Epoche: 1 [529/2802 (19%)]\tLoss: 11.584556\n",
      "Train Epoche: 1 [530/2802 (19%)]\tLoss: 24.479618\n",
      "Train Epoche: 1 [531/2802 (19%)]\tLoss: 230.384979\n",
      "Train Epoche: 1 [532/2802 (19%)]\tLoss: 251.586655\n",
      "Train Epoche: 1 [533/2802 (19%)]\tLoss: 15.569668\n",
      "Train Epoche: 1 [534/2802 (19%)]\tLoss: 8.204467\n",
      "Train Epoche: 1 [535/2802 (19%)]\tLoss: 27.854233\n",
      "Train Epoche: 1 [536/2802 (19%)]\tLoss: 1.134133\n",
      "Train Epoche: 1 [537/2802 (19%)]\tLoss: 0.528338\n",
      "Train Epoche: 1 [538/2802 (19%)]\tLoss: 3.117061\n",
      "Train Epoche: 1 [539/2802 (19%)]\tLoss: 2.399669\n",
      "Train Epoche: 1 [540/2802 (19%)]\tLoss: 29.237310\n",
      "Train Epoche: 1 [541/2802 (19%)]\tLoss: 1.499252\n",
      "Train Epoche: 1 [542/2802 (19%)]\tLoss: 0.000290\n",
      "Train Epoche: 1 [543/2802 (19%)]\tLoss: 0.187652\n",
      "Train Epoche: 1 [544/2802 (19%)]\tLoss: 3.937646\n",
      "Train Epoche: 1 [545/2802 (19%)]\tLoss: 28.196804\n",
      "Train Epoche: 1 [546/2802 (19%)]\tLoss: 3.916685\n",
      "Train Epoche: 1 [547/2802 (20%)]\tLoss: 56.721508\n",
      "Train Epoche: 1 [548/2802 (20%)]\tLoss: 29.819220\n",
      "Train Epoche: 1 [549/2802 (20%)]\tLoss: 39.945843\n",
      "Train Epoche: 1 [550/2802 (20%)]\tLoss: 0.029207\n",
      "Train Epoche: 1 [551/2802 (20%)]\tLoss: 30.569628\n",
      "Train Epoche: 1 [552/2802 (20%)]\tLoss: 154.256683\n",
      "Train Epoche: 1 [553/2802 (20%)]\tLoss: 2.867521\n",
      "Train Epoche: 1 [554/2802 (20%)]\tLoss: 20.935017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [555/2802 (20%)]\tLoss: 1.518784\n",
      "Train Epoche: 1 [556/2802 (20%)]\tLoss: 15.219476\n",
      "Train Epoche: 1 [557/2802 (20%)]\tLoss: 5.250278\n",
      "Train Epoche: 1 [558/2802 (20%)]\tLoss: 89.566910\n",
      "Train Epoche: 1 [559/2802 (20%)]\tLoss: 8.778607\n",
      "Train Epoche: 1 [560/2802 (20%)]\tLoss: 12.288650\n",
      "Train Epoche: 1 [561/2802 (20%)]\tLoss: 3.969556\n",
      "Train Epoche: 1 [562/2802 (20%)]\tLoss: 11.360184\n",
      "Train Epoche: 1 [563/2802 (20%)]\tLoss: 5.381106\n",
      "Train Epoche: 1 [564/2802 (20%)]\tLoss: 16.178934\n",
      "Train Epoche: 1 [565/2802 (20%)]\tLoss: 0.157291\n",
      "Train Epoche: 1 [566/2802 (20%)]\tLoss: 13.198557\n",
      "Train Epoche: 1 [567/2802 (20%)]\tLoss: 36.422997\n",
      "Train Epoche: 1 [568/2802 (20%)]\tLoss: 3.546684\n",
      "Train Epoche: 1 [569/2802 (20%)]\tLoss: 0.302103\n",
      "Train Epoche: 1 [570/2802 (20%)]\tLoss: 8.666778\n",
      "Train Epoche: 1 [571/2802 (20%)]\tLoss: 46.946850\n",
      "Train Epoche: 1 [572/2802 (20%)]\tLoss: 0.716276\n",
      "Train Epoche: 1 [573/2802 (20%)]\tLoss: 0.589724\n",
      "Train Epoche: 1 [574/2802 (20%)]\tLoss: 7.769999\n",
      "Train Epoche: 1 [575/2802 (21%)]\tLoss: 25.158293\n",
      "Train Epoche: 1 [576/2802 (21%)]\tLoss: 0.677602\n",
      "Train Epoche: 1 [577/2802 (21%)]\tLoss: 9.682600\n",
      "Train Epoche: 1 [578/2802 (21%)]\tLoss: 29.439486\n",
      "Train Epoche: 1 [579/2802 (21%)]\tLoss: 23.524326\n",
      "Train Epoche: 1 [580/2802 (21%)]\tLoss: 0.981894\n",
      "Train Epoche: 1 [581/2802 (21%)]\tLoss: 25.260149\n",
      "Train Epoche: 1 [582/2802 (21%)]\tLoss: 30.806726\n",
      "Train Epoche: 1 [583/2802 (21%)]\tLoss: 0.002723\n",
      "Train Epoche: 1 [584/2802 (21%)]\tLoss: 32.038437\n",
      "Train Epoche: 1 [585/2802 (21%)]\tLoss: 31.306744\n",
      "Train Epoche: 1 [586/2802 (21%)]\tLoss: 5.047246\n",
      "Train Epoche: 1 [587/2802 (21%)]\tLoss: 0.707794\n",
      "Train Epoche: 1 [588/2802 (21%)]\tLoss: 15.876468\n",
      "Train Epoche: 1 [589/2802 (21%)]\tLoss: 1.840741\n",
      "Train Epoche: 1 [590/2802 (21%)]\tLoss: 1.882574\n",
      "Train Epoche: 1 [591/2802 (21%)]\tLoss: 0.429703\n",
      "Train Epoche: 1 [592/2802 (21%)]\tLoss: 12.863986\n",
      "Train Epoche: 1 [593/2802 (21%)]\tLoss: 0.074178\n",
      "Train Epoche: 1 [594/2802 (21%)]\tLoss: 0.937605\n",
      "Train Epoche: 1 [595/2802 (21%)]\tLoss: 20.445473\n",
      "Train Epoche: 1 [596/2802 (21%)]\tLoss: 2.592618\n",
      "Train Epoche: 1 [597/2802 (21%)]\tLoss: 70.566399\n",
      "Train Epoche: 1 [598/2802 (21%)]\tLoss: 29.015938\n",
      "Train Epoche: 1 [599/2802 (21%)]\tLoss: 23.380836\n",
      "Train Epoche: 1 [600/2802 (21%)]\tLoss: 46.748051\n",
      "Train Epoche: 1 [601/2802 (21%)]\tLoss: 22.742361\n",
      "Train Epoche: 1 [602/2802 (21%)]\tLoss: 0.071486\n",
      "Train Epoche: 1 [603/2802 (22%)]\tLoss: 1.504643\n",
      "Train Epoche: 1 [604/2802 (22%)]\tLoss: 7.683084\n",
      "Train Epoche: 1 [605/2802 (22%)]\tLoss: 189.589035\n",
      "Train Epoche: 1 [606/2802 (22%)]\tLoss: 0.408499\n",
      "Train Epoche: 1 [607/2802 (22%)]\tLoss: 0.039145\n",
      "Train Epoche: 1 [608/2802 (22%)]\tLoss: 8.039340\n",
      "Train Epoche: 1 [609/2802 (22%)]\tLoss: 230.665451\n",
      "Train Epoche: 1 [610/2802 (22%)]\tLoss: 9.074841\n",
      "Train Epoche: 1 [611/2802 (22%)]\tLoss: 31.830509\n",
      "Train Epoche: 1 [612/2802 (22%)]\tLoss: 0.531679\n",
      "Train Epoche: 1 [613/2802 (22%)]\tLoss: 7.056870\n",
      "Train Epoche: 1 [614/2802 (22%)]\tLoss: 19.025928\n",
      "Train Epoche: 1 [615/2802 (22%)]\tLoss: 25.172325\n",
      "Train Epoche: 1 [616/2802 (22%)]\tLoss: 14.991270\n",
      "Train Epoche: 1 [617/2802 (22%)]\tLoss: 0.891918\n",
      "Train Epoche: 1 [618/2802 (22%)]\tLoss: 58.689056\n",
      "Train Epoche: 1 [619/2802 (22%)]\tLoss: 0.368796\n",
      "Train Epoche: 1 [620/2802 (22%)]\tLoss: 11.510738\n",
      "Train Epoche: 1 [621/2802 (22%)]\tLoss: 23.640654\n",
      "Train Epoche: 1 [622/2802 (22%)]\tLoss: 22.535917\n",
      "Train Epoche: 1 [623/2802 (22%)]\tLoss: 280.595215\n",
      "Train Epoche: 1 [624/2802 (22%)]\tLoss: 124.827347\n",
      "Train Epoche: 1 [625/2802 (22%)]\tLoss: 165.513046\n",
      "Train Epoche: 1 [626/2802 (22%)]\tLoss: 124.770859\n",
      "Train Epoche: 1 [627/2802 (22%)]\tLoss: 14.155967\n",
      "Train Epoche: 1 [628/2802 (22%)]\tLoss: 6.929919\n",
      "Train Epoche: 1 [629/2802 (22%)]\tLoss: 236.265015\n",
      "Train Epoche: 1 [630/2802 (22%)]\tLoss: 0.270139\n",
      "Train Epoche: 1 [631/2802 (23%)]\tLoss: 29.508356\n",
      "Train Epoche: 1 [632/2802 (23%)]\tLoss: 1.748081\n",
      "Train Epoche: 1 [633/2802 (23%)]\tLoss: 42.515865\n",
      "Train Epoche: 1 [634/2802 (23%)]\tLoss: 3.279600\n",
      "Train Epoche: 1 [635/2802 (23%)]\tLoss: 20.830193\n",
      "Train Epoche: 1 [636/2802 (23%)]\tLoss: 5.315260\n",
      "Train Epoche: 1 [637/2802 (23%)]\tLoss: 158.321747\n",
      "Train Epoche: 1 [638/2802 (23%)]\tLoss: 1.390295\n",
      "Train Epoche: 1 [639/2802 (23%)]\tLoss: 4.442873\n",
      "Train Epoche: 1 [640/2802 (23%)]\tLoss: 1.429515\n",
      "Train Epoche: 1 [641/2802 (23%)]\tLoss: 10.713439\n",
      "Train Epoche: 1 [642/2802 (23%)]\tLoss: 98.303459\n",
      "Train Epoche: 1 [643/2802 (23%)]\tLoss: 0.520987\n",
      "Train Epoche: 1 [644/2802 (23%)]\tLoss: 53.785187\n",
      "Train Epoche: 1 [645/2802 (23%)]\tLoss: 190.023117\n",
      "Train Epoche: 1 [646/2802 (23%)]\tLoss: 8.720238\n",
      "Train Epoche: 1 [647/2802 (23%)]\tLoss: 33.473869\n",
      "Train Epoche: 1 [648/2802 (23%)]\tLoss: 111.800064\n",
      "Train Epoche: 1 [649/2802 (23%)]\tLoss: 31.485003\n",
      "Train Epoche: 1 [650/2802 (23%)]\tLoss: 186.842361\n",
      "Train Epoche: 1 [651/2802 (23%)]\tLoss: 17.025492\n",
      "Train Epoche: 1 [652/2802 (23%)]\tLoss: 15.731286\n",
      "Train Epoche: 1 [653/2802 (23%)]\tLoss: 4.293471\n",
      "Train Epoche: 1 [654/2802 (23%)]\tLoss: 29.641573\n",
      "Train Epoche: 1 [655/2802 (23%)]\tLoss: 158.425079\n",
      "Train Epoche: 1 [656/2802 (23%)]\tLoss: 76.900932\n",
      "Train Epoche: 1 [657/2802 (23%)]\tLoss: 148.422760\n",
      "Train Epoche: 1 [658/2802 (23%)]\tLoss: 13.542901\n",
      "Train Epoche: 1 [659/2802 (24%)]\tLoss: 42.526249\n",
      "Train Epoche: 1 [660/2802 (24%)]\tLoss: 1.775613\n",
      "Train Epoche: 1 [661/2802 (24%)]\tLoss: 0.031795\n",
      "Train Epoche: 1 [662/2802 (24%)]\tLoss: 127.557442\n",
      "Train Epoche: 1 [663/2802 (24%)]\tLoss: 10.603992\n",
      "Train Epoche: 1 [664/2802 (24%)]\tLoss: 53.518581\n",
      "Train Epoche: 1 [665/2802 (24%)]\tLoss: 0.352943\n",
      "Train Epoche: 1 [666/2802 (24%)]\tLoss: 13.734035\n",
      "Train Epoche: 1 [667/2802 (24%)]\tLoss: 1.408558\n",
      "Train Epoche: 1 [668/2802 (24%)]\tLoss: 155.969681\n",
      "Train Epoche: 1 [669/2802 (24%)]\tLoss: 12.654414\n",
      "Train Epoche: 1 [670/2802 (24%)]\tLoss: 26.335030\n",
      "Train Epoche: 1 [671/2802 (24%)]\tLoss: 18.899469\n",
      "Train Epoche: 1 [672/2802 (24%)]\tLoss: 143.311996\n",
      "Train Epoche: 1 [673/2802 (24%)]\tLoss: 11.789710\n",
      "Train Epoche: 1 [674/2802 (24%)]\tLoss: 17.914024\n",
      "Train Epoche: 1 [675/2802 (24%)]\tLoss: 7.263064\n",
      "Train Epoche: 1 [676/2802 (24%)]\tLoss: 0.683987\n",
      "Train Epoche: 1 [677/2802 (24%)]\tLoss: 31.706905\n",
      "Train Epoche: 1 [678/2802 (24%)]\tLoss: 1.634022\n",
      "Train Epoche: 1 [679/2802 (24%)]\tLoss: 43.353069\n",
      "Train Epoche: 1 [680/2802 (24%)]\tLoss: 101.428459\n",
      "Train Epoche: 1 [681/2802 (24%)]\tLoss: 10.628036\n",
      "Train Epoche: 1 [682/2802 (24%)]\tLoss: 32.369595\n",
      "Train Epoche: 1 [683/2802 (24%)]\tLoss: 18.594358\n",
      "Train Epoche: 1 [684/2802 (24%)]\tLoss: 18.272596\n",
      "Train Epoche: 1 [685/2802 (24%)]\tLoss: 15.758158\n",
      "Train Epoche: 1 [686/2802 (24%)]\tLoss: 17.447920\n",
      "Train Epoche: 1 [687/2802 (25%)]\tLoss: 0.274548\n",
      "Train Epoche: 1 [688/2802 (25%)]\tLoss: 0.577893\n",
      "Train Epoche: 1 [689/2802 (25%)]\tLoss: 55.296562\n",
      "Train Epoche: 1 [690/2802 (25%)]\tLoss: 24.906475\n",
      "Train Epoche: 1 [691/2802 (25%)]\tLoss: 15.290605\n",
      "Train Epoche: 1 [692/2802 (25%)]\tLoss: 1.317678\n",
      "Train Epoche: 1 [693/2802 (25%)]\tLoss: 28.337132\n",
      "Train Epoche: 1 [694/2802 (25%)]\tLoss: 28.543129\n",
      "Train Epoche: 1 [695/2802 (25%)]\tLoss: 1.292795\n",
      "Train Epoche: 1 [696/2802 (25%)]\tLoss: 36.856926\n",
      "Train Epoche: 1 [697/2802 (25%)]\tLoss: 15.602890\n",
      "Train Epoche: 1 [698/2802 (25%)]\tLoss: 48.751087\n",
      "Train Epoche: 1 [699/2802 (25%)]\tLoss: 7.724422\n",
      "Train Epoche: 1 [700/2802 (25%)]\tLoss: 4.312232\n",
      "Train Epoche: 1 [701/2802 (25%)]\tLoss: 0.014333\n",
      "Train Epoche: 1 [702/2802 (25%)]\tLoss: 71.099129\n",
      "Train Epoche: 1 [703/2802 (25%)]\tLoss: 15.069427\n",
      "Train Epoche: 1 [704/2802 (25%)]\tLoss: 32.473248\n",
      "Train Epoche: 1 [705/2802 (25%)]\tLoss: 6.534459\n",
      "Train Epoche: 1 [706/2802 (25%)]\tLoss: 119.235657\n",
      "Train Epoche: 1 [707/2802 (25%)]\tLoss: 2.713364\n",
      "Train Epoche: 1 [708/2802 (25%)]\tLoss: 0.474452\n",
      "Train Epoche: 1 [709/2802 (25%)]\tLoss: 25.874260\n",
      "Train Epoche: 1 [710/2802 (25%)]\tLoss: 3.010449\n",
      "Train Epoche: 1 [711/2802 (25%)]\tLoss: 14.227785\n",
      "Train Epoche: 1 [712/2802 (25%)]\tLoss: 3.505847\n",
      "Train Epoche: 1 [713/2802 (25%)]\tLoss: 9.753457\n",
      "Train Epoche: 1 [714/2802 (25%)]\tLoss: 28.756685\n",
      "Train Epoche: 1 [715/2802 (26%)]\tLoss: 3.868076\n",
      "Train Epoche: 1 [716/2802 (26%)]\tLoss: 0.805238\n",
      "Train Epoche: 1 [717/2802 (26%)]\tLoss: 150.821487\n",
      "Train Epoche: 1 [718/2802 (26%)]\tLoss: 19.619217\n",
      "Train Epoche: 1 [719/2802 (26%)]\tLoss: 12.930961\n",
      "Train Epoche: 1 [720/2802 (26%)]\tLoss: 43.086498\n",
      "Train Epoche: 1 [721/2802 (26%)]\tLoss: 0.028773\n",
      "Train Epoche: 1 [722/2802 (26%)]\tLoss: 8.940070\n",
      "Train Epoche: 1 [723/2802 (26%)]\tLoss: 65.880447\n",
      "Train Epoche: 1 [724/2802 (26%)]\tLoss: 1.095001\n",
      "Train Epoche: 1 [725/2802 (26%)]\tLoss: 186.509521\n",
      "Train Epoche: 1 [726/2802 (26%)]\tLoss: 0.732088\n",
      "Train Epoche: 1 [727/2802 (26%)]\tLoss: 88.152443\n",
      "Train Epoche: 1 [728/2802 (26%)]\tLoss: 0.000253\n",
      "Train Epoche: 1 [729/2802 (26%)]\tLoss: 0.454970\n",
      "Train Epoche: 1 [730/2802 (26%)]\tLoss: 0.000211\n",
      "Train Epoche: 1 [731/2802 (26%)]\tLoss: 0.742815\n",
      "Train Epoche: 1 [732/2802 (26%)]\tLoss: 6.843721\n",
      "Train Epoche: 1 [733/2802 (26%)]\tLoss: 40.003185\n",
      "Train Epoche: 1 [734/2802 (26%)]\tLoss: 3.271598\n",
      "Train Epoche: 1 [735/2802 (26%)]\tLoss: 18.062078\n",
      "Train Epoche: 1 [736/2802 (26%)]\tLoss: 0.258177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [737/2802 (26%)]\tLoss: 61.112637\n",
      "Train Epoche: 1 [738/2802 (26%)]\tLoss: 36.536480\n",
      "Train Epoche: 1 [739/2802 (26%)]\tLoss: 24.589525\n",
      "Train Epoche: 1 [740/2802 (26%)]\tLoss: 18.351057\n",
      "Train Epoche: 1 [741/2802 (26%)]\tLoss: 23.965231\n",
      "Train Epoche: 1 [742/2802 (26%)]\tLoss: 79.494827\n",
      "Train Epoche: 1 [743/2802 (27%)]\tLoss: 19.727835\n",
      "Train Epoche: 1 [744/2802 (27%)]\tLoss: 1.152572\n",
      "Train Epoche: 1 [745/2802 (27%)]\tLoss: 26.754339\n",
      "Train Epoche: 1 [746/2802 (27%)]\tLoss: 396.057037\n",
      "Train Epoche: 1 [747/2802 (27%)]\tLoss: 1.422052\n",
      "Train Epoche: 1 [748/2802 (27%)]\tLoss: 107.568741\n",
      "Train Epoche: 1 [749/2802 (27%)]\tLoss: 8.256437\n",
      "Train Epoche: 1 [750/2802 (27%)]\tLoss: 6.751346\n",
      "Train Epoche: 1 [751/2802 (27%)]\tLoss: 0.850735\n",
      "Train Epoche: 1 [752/2802 (27%)]\tLoss: 4.391913\n",
      "Train Epoche: 1 [753/2802 (27%)]\tLoss: 14.353930\n",
      "Train Epoche: 1 [754/2802 (27%)]\tLoss: 1.517306\n",
      "Train Epoche: 1 [755/2802 (27%)]\tLoss: 0.420751\n",
      "Train Epoche: 1 [756/2802 (27%)]\tLoss: 7.278072\n",
      "Train Epoche: 1 [757/2802 (27%)]\tLoss: 0.042226\n",
      "Train Epoche: 1 [758/2802 (27%)]\tLoss: 29.241337\n",
      "Train Epoche: 1 [759/2802 (27%)]\tLoss: 118.919060\n",
      "Train Epoche: 1 [760/2802 (27%)]\tLoss: 4.399514\n",
      "Train Epoche: 1 [761/2802 (27%)]\tLoss: 4.637634\n",
      "Train Epoche: 1 [762/2802 (27%)]\tLoss: 9.708782\n",
      "Train Epoche: 1 [763/2802 (27%)]\tLoss: 6.738843\n",
      "Train Epoche: 1 [764/2802 (27%)]\tLoss: 0.000823\n",
      "Train Epoche: 1 [765/2802 (27%)]\tLoss: 74.811127\n",
      "Train Epoche: 1 [766/2802 (27%)]\tLoss: 165.283203\n",
      "Train Epoche: 1 [767/2802 (27%)]\tLoss: 0.037422\n",
      "Train Epoche: 1 [768/2802 (27%)]\tLoss: 77.957901\n",
      "Train Epoche: 1 [769/2802 (27%)]\tLoss: 0.695807\n",
      "Train Epoche: 1 [770/2802 (27%)]\tLoss: 0.292345\n",
      "Train Epoche: 1 [771/2802 (28%)]\tLoss: 23.556540\n",
      "Train Epoche: 1 [772/2802 (28%)]\tLoss: 20.316263\n",
      "Train Epoche: 1 [773/2802 (28%)]\tLoss: 0.283501\n",
      "Train Epoche: 1 [774/2802 (28%)]\tLoss: 6.841551\n",
      "Train Epoche: 1 [775/2802 (28%)]\tLoss: 2.349636\n",
      "Train Epoche: 1 [776/2802 (28%)]\tLoss: 4.080186\n",
      "Train Epoche: 1 [777/2802 (28%)]\tLoss: 0.002193\n",
      "Train Epoche: 1 [778/2802 (28%)]\tLoss: 0.388603\n",
      "Train Epoche: 1 [779/2802 (28%)]\tLoss: 3.251949\n",
      "Train Epoche: 1 [780/2802 (28%)]\tLoss: 19.308002\n",
      "Train Epoche: 1 [781/2802 (28%)]\tLoss: 1.736058\n",
      "Train Epoche: 1 [782/2802 (28%)]\tLoss: 142.526932\n",
      "Train Epoche: 1 [783/2802 (28%)]\tLoss: 27.360268\n",
      "Train Epoche: 1 [784/2802 (28%)]\tLoss: 11.565657\n",
      "Train Epoche: 1 [785/2802 (28%)]\tLoss: 6.053505\n",
      "Train Epoche: 1 [786/2802 (28%)]\tLoss: 6.392568\n",
      "Train Epoche: 1 [787/2802 (28%)]\tLoss: 153.023361\n",
      "Train Epoche: 1 [788/2802 (28%)]\tLoss: 0.879323\n",
      "Train Epoche: 1 [789/2802 (28%)]\tLoss: 5.347698\n",
      "Train Epoche: 1 [790/2802 (28%)]\tLoss: 1.237784\n",
      "Train Epoche: 1 [791/2802 (28%)]\tLoss: 25.414320\n",
      "Train Epoche: 1 [792/2802 (28%)]\tLoss: 16.570946\n",
      "Train Epoche: 1 [793/2802 (28%)]\tLoss: 0.057789\n",
      "Train Epoche: 1 [794/2802 (28%)]\tLoss: 60.840748\n",
      "Train Epoche: 1 [795/2802 (28%)]\tLoss: 6.338512\n",
      "Train Epoche: 1 [796/2802 (28%)]\tLoss: 4.084502\n",
      "Train Epoche: 1 [797/2802 (28%)]\tLoss: 0.611045\n",
      "Train Epoche: 1 [798/2802 (28%)]\tLoss: 13.204145\n",
      "Train Epoche: 1 [799/2802 (29%)]\tLoss: 0.180830\n",
      "Train Epoche: 1 [800/2802 (29%)]\tLoss: 0.023523\n",
      "Train Epoche: 1 [801/2802 (29%)]\tLoss: 17.724205\n",
      "Train Epoche: 1 [802/2802 (29%)]\tLoss: 53.924221\n",
      "Train Epoche: 1 [803/2802 (29%)]\tLoss: 16.324188\n",
      "Train Epoche: 1 [804/2802 (29%)]\tLoss: 0.913499\n",
      "Train Epoche: 1 [805/2802 (29%)]\tLoss: 6.664684\n",
      "Train Epoche: 1 [806/2802 (29%)]\tLoss: 3.903595\n",
      "Train Epoche: 1 [807/2802 (29%)]\tLoss: 64.721031\n",
      "Train Epoche: 1 [808/2802 (29%)]\tLoss: 5.540926\n",
      "Train Epoche: 1 [809/2802 (29%)]\tLoss: 3.757388\n",
      "Train Epoche: 1 [810/2802 (29%)]\tLoss: 104.904457\n",
      "Train Epoche: 1 [811/2802 (29%)]\tLoss: 13.552968\n",
      "Train Epoche: 1 [812/2802 (29%)]\tLoss: 2.835162\n",
      "Train Epoche: 1 [813/2802 (29%)]\tLoss: 80.931061\n",
      "Train Epoche: 1 [814/2802 (29%)]\tLoss: 0.603635\n",
      "Train Epoche: 1 [815/2802 (29%)]\tLoss: 22.218369\n",
      "Train Epoche: 1 [816/2802 (29%)]\tLoss: 9.763113\n",
      "Train Epoche: 1 [817/2802 (29%)]\tLoss: 1.656616\n",
      "Train Epoche: 1 [818/2802 (29%)]\tLoss: 0.015456\n",
      "Train Epoche: 1 [819/2802 (29%)]\tLoss: 1.115816\n",
      "Train Epoche: 1 [820/2802 (29%)]\tLoss: 0.781661\n",
      "Train Epoche: 1 [821/2802 (29%)]\tLoss: 60.509357\n",
      "Train Epoche: 1 [822/2802 (29%)]\tLoss: 11.549994\n",
      "Train Epoche: 1 [823/2802 (29%)]\tLoss: 0.127072\n",
      "Train Epoche: 1 [824/2802 (29%)]\tLoss: 17.336287\n",
      "Train Epoche: 1 [825/2802 (29%)]\tLoss: 84.608185\n",
      "Train Epoche: 1 [826/2802 (29%)]\tLoss: 83.170135\n",
      "Train Epoche: 1 [827/2802 (30%)]\tLoss: 21.181330\n",
      "Train Epoche: 1 [828/2802 (30%)]\tLoss: 98.850029\n",
      "Train Epoche: 1 [829/2802 (30%)]\tLoss: 18.081118\n",
      "Train Epoche: 1 [830/2802 (30%)]\tLoss: 6.553706\n",
      "Train Epoche: 1 [831/2802 (30%)]\tLoss: 84.823448\n",
      "Train Epoche: 1 [832/2802 (30%)]\tLoss: 67.690155\n",
      "Train Epoche: 1 [833/2802 (30%)]\tLoss: 15.491048\n",
      "Train Epoche: 1 [834/2802 (30%)]\tLoss: 4.396062\n",
      "Train Epoche: 1 [835/2802 (30%)]\tLoss: 41.077412\n",
      "Train Epoche: 1 [836/2802 (30%)]\tLoss: 4.742930\n",
      "Train Epoche: 1 [837/2802 (30%)]\tLoss: 21.372906\n",
      "Train Epoche: 1 [838/2802 (30%)]\tLoss: 161.574142\n",
      "Train Epoche: 1 [839/2802 (30%)]\tLoss: 8.928354\n",
      "Train Epoche: 1 [840/2802 (30%)]\tLoss: 13.056401\n",
      "Train Epoche: 1 [841/2802 (30%)]\tLoss: 29.313002\n",
      "Train Epoche: 1 [842/2802 (30%)]\tLoss: 3.367432\n",
      "Train Epoche: 1 [843/2802 (30%)]\tLoss: 10.427673\n",
      "Train Epoche: 1 [844/2802 (30%)]\tLoss: 9.929985\n",
      "Train Epoche: 1 [845/2802 (30%)]\tLoss: 5.950513\n",
      "Train Epoche: 1 [846/2802 (30%)]\tLoss: 128.144058\n",
      "Train Epoche: 1 [847/2802 (30%)]\tLoss: 54.039829\n",
      "Train Epoche: 1 [848/2802 (30%)]\tLoss: 5.620679\n",
      "Train Epoche: 1 [849/2802 (30%)]\tLoss: 27.917475\n",
      "Train Epoche: 1 [850/2802 (30%)]\tLoss: 3.299962\n",
      "Train Epoche: 1 [851/2802 (30%)]\tLoss: 0.610748\n",
      "Train Epoche: 1 [852/2802 (30%)]\tLoss: 29.242163\n",
      "Train Epoche: 1 [853/2802 (30%)]\tLoss: 16.060680\n",
      "Train Epoche: 1 [854/2802 (30%)]\tLoss: 43.222809\n",
      "Train Epoche: 1 [855/2802 (31%)]\tLoss: 31.971581\n",
      "Train Epoche: 1 [856/2802 (31%)]\tLoss: 40.942844\n",
      "Train Epoche: 1 [857/2802 (31%)]\tLoss: 0.123803\n",
      "Train Epoche: 1 [858/2802 (31%)]\tLoss: 27.429920\n",
      "Train Epoche: 1 [859/2802 (31%)]\tLoss: 159.975174\n",
      "Train Epoche: 1 [860/2802 (31%)]\tLoss: 0.749510\n",
      "Train Epoche: 1 [861/2802 (31%)]\tLoss: 49.897739\n",
      "Train Epoche: 1 [862/2802 (31%)]\tLoss: 9.489543\n",
      "Train Epoche: 1 [863/2802 (31%)]\tLoss: 0.000022\n",
      "Train Epoche: 1 [864/2802 (31%)]\tLoss: 0.089739\n",
      "Train Epoche: 1 [865/2802 (31%)]\tLoss: 3.644133\n",
      "Train Epoche: 1 [866/2802 (31%)]\tLoss: 6.397403\n",
      "Train Epoche: 1 [867/2802 (31%)]\tLoss: 21.086967\n",
      "Train Epoche: 1 [868/2802 (31%)]\tLoss: 0.835927\n",
      "Train Epoche: 1 [869/2802 (31%)]\tLoss: 5.605776\n",
      "Train Epoche: 1 [870/2802 (31%)]\tLoss: 2.452434\n",
      "Train Epoche: 1 [871/2802 (31%)]\tLoss: 2.354910\n",
      "Train Epoche: 1 [872/2802 (31%)]\tLoss: 0.122715\n",
      "Train Epoche: 1 [873/2802 (31%)]\tLoss: 13.104682\n",
      "Train Epoche: 1 [874/2802 (31%)]\tLoss: 0.715148\n",
      "Train Epoche: 1 [875/2802 (31%)]\tLoss: 165.457428\n",
      "Train Epoche: 1 [876/2802 (31%)]\tLoss: 189.601639\n",
      "Train Epoche: 1 [877/2802 (31%)]\tLoss: 177.159622\n",
      "Train Epoche: 1 [878/2802 (31%)]\tLoss: 0.024813\n",
      "Train Epoche: 1 [879/2802 (31%)]\tLoss: 26.296055\n",
      "Train Epoche: 1 [880/2802 (31%)]\tLoss: 21.808331\n",
      "Train Epoche: 1 [881/2802 (31%)]\tLoss: 5.040301\n",
      "Train Epoche: 1 [882/2802 (31%)]\tLoss: 18.146074\n",
      "Train Epoche: 1 [883/2802 (32%)]\tLoss: 8.555955\n",
      "Train Epoche: 1 [884/2802 (32%)]\tLoss: 4.286664\n",
      "Train Epoche: 1 [885/2802 (32%)]\tLoss: 0.019854\n",
      "Train Epoche: 1 [886/2802 (32%)]\tLoss: 0.219961\n",
      "Train Epoche: 1 [887/2802 (32%)]\tLoss: 2.448525\n",
      "Train Epoche: 1 [888/2802 (32%)]\tLoss: 3.134283\n",
      "Train Epoche: 1 [889/2802 (32%)]\tLoss: 11.420804\n",
      "Train Epoche: 1 [890/2802 (32%)]\tLoss: 26.746481\n",
      "Train Epoche: 1 [891/2802 (32%)]\tLoss: 11.548068\n",
      "Train Epoche: 1 [892/2802 (32%)]\tLoss: 192.001678\n",
      "Train Epoche: 1 [893/2802 (32%)]\tLoss: 2.612216\n",
      "Train Epoche: 1 [894/2802 (32%)]\tLoss: 0.118314\n",
      "Train Epoche: 1 [895/2802 (32%)]\tLoss: 6.863300\n",
      "Train Epoche: 1 [896/2802 (32%)]\tLoss: 11.011041\n",
      "Train Epoche: 1 [897/2802 (32%)]\tLoss: 204.254852\n",
      "Train Epoche: 1 [898/2802 (32%)]\tLoss: 11.640219\n",
      "Train Epoche: 1 [899/2802 (32%)]\tLoss: 2.580029\n",
      "Train Epoche: 1 [900/2802 (32%)]\tLoss: 1.579078\n",
      "Train Epoche: 1 [901/2802 (32%)]\tLoss: 14.798786\n",
      "Train Epoche: 1 [902/2802 (32%)]\tLoss: 11.676096\n",
      "Train Epoche: 1 [903/2802 (32%)]\tLoss: 15.596072\n",
      "Train Epoche: 1 [904/2802 (32%)]\tLoss: 0.594467\n",
      "Train Epoche: 1 [905/2802 (32%)]\tLoss: 3.109898\n",
      "Train Epoche: 1 [906/2802 (32%)]\tLoss: 26.270805\n",
      "Train Epoche: 1 [907/2802 (32%)]\tLoss: 13.259338\n",
      "Train Epoche: 1 [908/2802 (32%)]\tLoss: 135.104858\n",
      "Train Epoche: 1 [909/2802 (32%)]\tLoss: 1.241602\n",
      "Train Epoche: 1 [910/2802 (32%)]\tLoss: 42.183437\n",
      "Train Epoche: 1 [911/2802 (33%)]\tLoss: 12.741514\n",
      "Train Epoche: 1 [912/2802 (33%)]\tLoss: 9.893368\n",
      "Train Epoche: 1 [913/2802 (33%)]\tLoss: 24.369961\n",
      "Train Epoche: 1 [914/2802 (33%)]\tLoss: 9.018303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [915/2802 (33%)]\tLoss: 14.517324\n",
      "Train Epoche: 1 [916/2802 (33%)]\tLoss: 49.822292\n",
      "Train Epoche: 1 [917/2802 (33%)]\tLoss: 21.169594\n",
      "Train Epoche: 1 [918/2802 (33%)]\tLoss: 11.119961\n",
      "Train Epoche: 1 [919/2802 (33%)]\tLoss: 0.940557\n",
      "Train Epoche: 1 [920/2802 (33%)]\tLoss: 44.547188\n",
      "Train Epoche: 1 [921/2802 (33%)]\tLoss: 4.915897\n",
      "Train Epoche: 1 [922/2802 (33%)]\tLoss: 86.833885\n",
      "Train Epoche: 1 [923/2802 (33%)]\tLoss: 14.753966\n",
      "Train Epoche: 1 [924/2802 (33%)]\tLoss: 20.930458\n",
      "Train Epoche: 1 [925/2802 (33%)]\tLoss: 25.648153\n",
      "Train Epoche: 1 [926/2802 (33%)]\tLoss: 12.858654\n",
      "Train Epoche: 1 [927/2802 (33%)]\tLoss: 3.187913\n",
      "Train Epoche: 1 [928/2802 (33%)]\tLoss: 16.429264\n",
      "Train Epoche: 1 [929/2802 (33%)]\tLoss: 41.147930\n",
      "Train Epoche: 1 [930/2802 (33%)]\tLoss: 7.203004\n",
      "Train Epoche: 1 [931/2802 (33%)]\tLoss: 1.929981\n",
      "Train Epoche: 1 [932/2802 (33%)]\tLoss: 176.730728\n",
      "Train Epoche: 1 [933/2802 (33%)]\tLoss: 0.005883\n",
      "Train Epoche: 1 [934/2802 (33%)]\tLoss: 13.967072\n",
      "Train Epoche: 1 [935/2802 (33%)]\tLoss: 248.474915\n",
      "Train Epoche: 1 [936/2802 (33%)]\tLoss: 10.789125\n",
      "Train Epoche: 1 [937/2802 (33%)]\tLoss: 57.386055\n",
      "Train Epoche: 1 [938/2802 (33%)]\tLoss: 22.811405\n",
      "Train Epoche: 1 [939/2802 (34%)]\tLoss: 18.316618\n",
      "Train Epoche: 1 [940/2802 (34%)]\tLoss: 6.511565\n",
      "Train Epoche: 1 [941/2802 (34%)]\tLoss: 12.722295\n",
      "Train Epoche: 1 [942/2802 (34%)]\tLoss: 2.035133\n",
      "Train Epoche: 1 [943/2802 (34%)]\tLoss: 0.888398\n",
      "Train Epoche: 1 [944/2802 (34%)]\tLoss: 13.519418\n",
      "Train Epoche: 1 [945/2802 (34%)]\tLoss: 16.002258\n",
      "Train Epoche: 1 [946/2802 (34%)]\tLoss: 418.464600\n",
      "Train Epoche: 1 [947/2802 (34%)]\tLoss: 11.936748\n",
      "Train Epoche: 1 [948/2802 (34%)]\tLoss: 17.607786\n",
      "Train Epoche: 1 [949/2802 (34%)]\tLoss: 2.086927\n",
      "Train Epoche: 1 [950/2802 (34%)]\tLoss: 24.237980\n",
      "Train Epoche: 1 [951/2802 (34%)]\tLoss: 3.301924\n",
      "Train Epoche: 1 [952/2802 (34%)]\tLoss: 11.424620\n",
      "Train Epoche: 1 [953/2802 (34%)]\tLoss: 3.288099\n",
      "Train Epoche: 1 [954/2802 (34%)]\tLoss: 0.769569\n",
      "Train Epoche: 1 [955/2802 (34%)]\tLoss: 25.614367\n",
      "Train Epoche: 1 [956/2802 (34%)]\tLoss: 19.470327\n",
      "Train Epoche: 1 [957/2802 (34%)]\tLoss: 12.495152\n",
      "Train Epoche: 1 [958/2802 (34%)]\tLoss: 1.988697\n",
      "Train Epoche: 1 [959/2802 (34%)]\tLoss: 19.368584\n",
      "Train Epoche: 1 [960/2802 (34%)]\tLoss: 1.469353\n",
      "Train Epoche: 1 [961/2802 (34%)]\tLoss: 8.106843\n",
      "Train Epoche: 1 [962/2802 (34%)]\tLoss: 9.518137\n",
      "Train Epoche: 1 [963/2802 (34%)]\tLoss: 6.147153\n",
      "Train Epoche: 1 [964/2802 (34%)]\tLoss: 14.449404\n",
      "Train Epoche: 1 [965/2802 (34%)]\tLoss: 27.233252\n",
      "Train Epoche: 1 [966/2802 (34%)]\tLoss: 14.672852\n",
      "Train Epoche: 1 [967/2802 (35%)]\tLoss: 13.742236\n",
      "Train Epoche: 1 [968/2802 (35%)]\tLoss: 3.064794\n",
      "Train Epoche: 1 [969/2802 (35%)]\tLoss: 0.000020\n",
      "Train Epoche: 1 [970/2802 (35%)]\tLoss: 5.210265\n",
      "Train Epoche: 1 [971/2802 (35%)]\tLoss: 69.116669\n",
      "Train Epoche: 1 [972/2802 (35%)]\tLoss: 0.421786\n",
      "Train Epoche: 1 [973/2802 (35%)]\tLoss: 99.785500\n",
      "Train Epoche: 1 [974/2802 (35%)]\tLoss: 23.160763\n",
      "Train Epoche: 1 [975/2802 (35%)]\tLoss: 1.501280\n",
      "Train Epoche: 1 [976/2802 (35%)]\tLoss: 9.032990\n",
      "Train Epoche: 1 [977/2802 (35%)]\tLoss: 34.853470\n",
      "Train Epoche: 1 [978/2802 (35%)]\tLoss: 2.851024\n",
      "Train Epoche: 1 [979/2802 (35%)]\tLoss: 9.035316\n",
      "Train Epoche: 1 [980/2802 (35%)]\tLoss: 2.525461\n",
      "Train Epoche: 1 [981/2802 (35%)]\tLoss: 0.126651\n",
      "Train Epoche: 1 [982/2802 (35%)]\tLoss: 5.819128\n",
      "Train Epoche: 1 [983/2802 (35%)]\tLoss: 3.432078\n",
      "Train Epoche: 1 [984/2802 (35%)]\tLoss: 0.588108\n",
      "Train Epoche: 1 [985/2802 (35%)]\tLoss: 0.385735\n",
      "Train Epoche: 1 [986/2802 (35%)]\tLoss: 10.032286\n",
      "Train Epoche: 1 [987/2802 (35%)]\tLoss: 3.240093\n",
      "Train Epoche: 1 [988/2802 (35%)]\tLoss: 12.432219\n",
      "Train Epoche: 1 [989/2802 (35%)]\tLoss: 7.194174\n",
      "Train Epoche: 1 [990/2802 (35%)]\tLoss: 7.062500\n",
      "Train Epoche: 1 [991/2802 (35%)]\tLoss: 18.036165\n",
      "Train Epoche: 1 [992/2802 (35%)]\tLoss: 3.031560\n",
      "Train Epoche: 1 [993/2802 (35%)]\tLoss: 5.069742\n",
      "Train Epoche: 1 [994/2802 (35%)]\tLoss: 0.720430\n",
      "Train Epoche: 1 [995/2802 (36%)]\tLoss: 1.466258\n",
      "Train Epoche: 1 [996/2802 (36%)]\tLoss: 0.072645\n",
      "Train Epoche: 1 [997/2802 (36%)]\tLoss: 17.667799\n",
      "Train Epoche: 1 [998/2802 (36%)]\tLoss: 0.000968\n",
      "Train Epoche: 1 [999/2802 (36%)]\tLoss: 4.095917\n",
      "Train Epoche: 1 [1000/2802 (36%)]\tLoss: 0.601123\n",
      "Train Epoche: 1 [1001/2802 (36%)]\tLoss: 2.117827\n",
      "Train Epoche: 1 [1002/2802 (36%)]\tLoss: 4.276063\n",
      "Train Epoche: 1 [1003/2802 (36%)]\tLoss: 113.118385\n",
      "Train Epoche: 1 [1004/2802 (36%)]\tLoss: 2.096536\n",
      "Train Epoche: 1 [1005/2802 (36%)]\tLoss: 49.130596\n",
      "Train Epoche: 1 [1006/2802 (36%)]\tLoss: 0.000732\n",
      "Train Epoche: 1 [1007/2802 (36%)]\tLoss: 49.618496\n",
      "Train Epoche: 1 [1008/2802 (36%)]\tLoss: 2.089788\n",
      "Train Epoche: 1 [1009/2802 (36%)]\tLoss: 50.226864\n",
      "Train Epoche: 1 [1010/2802 (36%)]\tLoss: 1.202426\n",
      "Train Epoche: 1 [1011/2802 (36%)]\tLoss: 21.612892\n",
      "Train Epoche: 1 [1012/2802 (36%)]\tLoss: 0.168123\n",
      "Train Epoche: 1 [1013/2802 (36%)]\tLoss: 5.832921\n",
      "Train Epoche: 1 [1014/2802 (36%)]\tLoss: 6.301911\n",
      "Train Epoche: 1 [1015/2802 (36%)]\tLoss: 9.986479\n",
      "Train Epoche: 1 [1016/2802 (36%)]\tLoss: 0.210919\n",
      "Train Epoche: 1 [1017/2802 (36%)]\tLoss: 8.857415\n",
      "Train Epoche: 1 [1018/2802 (36%)]\tLoss: 0.002148\n",
      "Train Epoche: 1 [1019/2802 (36%)]\tLoss: 13.716292\n",
      "Train Epoche: 1 [1020/2802 (36%)]\tLoss: 2.056552\n",
      "Train Epoche: 1 [1021/2802 (36%)]\tLoss: 3.257470\n",
      "Train Epoche: 1 [1022/2802 (36%)]\tLoss: 0.535616\n",
      "Train Epoche: 1 [1023/2802 (37%)]\tLoss: 89.027153\n",
      "Train Epoche: 1 [1024/2802 (37%)]\tLoss: 3.123810\n",
      "Train Epoche: 1 [1025/2802 (37%)]\tLoss: 5.744271\n",
      "Train Epoche: 1 [1026/2802 (37%)]\tLoss: 141.623901\n",
      "Train Epoche: 1 [1027/2802 (37%)]\tLoss: 15.768188\n",
      "Train Epoche: 1 [1028/2802 (37%)]\tLoss: 8.097179\n",
      "Train Epoche: 1 [1029/2802 (37%)]\tLoss: 2.268678\n",
      "Train Epoche: 1 [1030/2802 (37%)]\tLoss: 0.253578\n",
      "Train Epoche: 1 [1031/2802 (37%)]\tLoss: 1.736840\n",
      "Train Epoche: 1 [1032/2802 (37%)]\tLoss: 0.812249\n",
      "Train Epoche: 1 [1033/2802 (37%)]\tLoss: 4.266370\n",
      "Train Epoche: 1 [1034/2802 (37%)]\tLoss: 4.372076\n",
      "Train Epoche: 1 [1035/2802 (37%)]\tLoss: 0.300539\n",
      "Train Epoche: 1 [1036/2802 (37%)]\tLoss: 0.569747\n",
      "Train Epoche: 1 [1037/2802 (37%)]\tLoss: 0.000123\n",
      "Train Epoche: 1 [1038/2802 (37%)]\tLoss: 4.605250\n",
      "Train Epoche: 1 [1039/2802 (37%)]\tLoss: 1.152072\n",
      "Train Epoche: 1 [1040/2802 (37%)]\tLoss: 6.340255\n",
      "Train Epoche: 1 [1041/2802 (37%)]\tLoss: 407.652771\n",
      "Train Epoche: 1 [1042/2802 (37%)]\tLoss: 0.096372\n",
      "Train Epoche: 1 [1043/2802 (37%)]\tLoss: 0.194446\n",
      "Train Epoche: 1 [1044/2802 (37%)]\tLoss: 0.884849\n",
      "Train Epoche: 1 [1045/2802 (37%)]\tLoss: 377.748718\n",
      "Train Epoche: 1 [1046/2802 (37%)]\tLoss: 0.180218\n",
      "Train Epoche: 1 [1047/2802 (37%)]\tLoss: 1.675566\n",
      "Train Epoche: 1 [1048/2802 (37%)]\tLoss: 0.113684\n",
      "Train Epoche: 1 [1049/2802 (37%)]\tLoss: 54.178440\n",
      "Train Epoche: 1 [1050/2802 (37%)]\tLoss: 52.899284\n",
      "Train Epoche: 1 [1051/2802 (38%)]\tLoss: 0.001707\n",
      "Train Epoche: 1 [1052/2802 (38%)]\tLoss: 2.109167\n",
      "Train Epoche: 1 [1053/2802 (38%)]\tLoss: 14.487928\n",
      "Train Epoche: 1 [1054/2802 (38%)]\tLoss: 1.068141\n",
      "Train Epoche: 1 [1055/2802 (38%)]\tLoss: 15.885490\n",
      "Train Epoche: 1 [1056/2802 (38%)]\tLoss: 0.756797\n",
      "Train Epoche: 1 [1057/2802 (38%)]\tLoss: 98.329521\n",
      "Train Epoche: 1 [1058/2802 (38%)]\tLoss: 4.087587\n",
      "Train Epoche: 1 [1059/2802 (38%)]\tLoss: 31.349270\n",
      "Train Epoche: 1 [1060/2802 (38%)]\tLoss: 4.406181\n",
      "Train Epoche: 1 [1061/2802 (38%)]\tLoss: 16.883240\n",
      "Train Epoche: 1 [1062/2802 (38%)]\tLoss: 27.089323\n",
      "Train Epoche: 1 [1063/2802 (38%)]\tLoss: 28.655024\n",
      "Train Epoche: 1 [1064/2802 (38%)]\tLoss: 20.572399\n",
      "Train Epoche: 1 [1065/2802 (38%)]\tLoss: 5.102511\n",
      "Train Epoche: 1 [1066/2802 (38%)]\tLoss: 1.070715\n",
      "Train Epoche: 1 [1067/2802 (38%)]\tLoss: 18.336077\n",
      "Train Epoche: 1 [1068/2802 (38%)]\tLoss: 3.357728\n",
      "Train Epoche: 1 [1069/2802 (38%)]\tLoss: 18.261787\n",
      "Train Epoche: 1 [1070/2802 (38%)]\tLoss: 3.377931\n",
      "Train Epoche: 1 [1071/2802 (38%)]\tLoss: 2.105726\n",
      "Train Epoche: 1 [1072/2802 (38%)]\tLoss: 40.984390\n",
      "Train Epoche: 1 [1073/2802 (38%)]\tLoss: 6.043490\n",
      "Train Epoche: 1 [1074/2802 (38%)]\tLoss: 0.658855\n",
      "Train Epoche: 1 [1075/2802 (38%)]\tLoss: 1.919663\n",
      "Train Epoche: 1 [1076/2802 (38%)]\tLoss: 2.147480\n",
      "Train Epoche: 1 [1077/2802 (38%)]\tLoss: 38.252754\n",
      "Train Epoche: 1 [1078/2802 (38%)]\tLoss: 2.207078\n",
      "Train Epoche: 1 [1079/2802 (39%)]\tLoss: 2.120222\n",
      "Train Epoche: 1 [1080/2802 (39%)]\tLoss: 25.389576\n",
      "Train Epoche: 1 [1081/2802 (39%)]\tLoss: 0.424120\n",
      "Train Epoche: 1 [1082/2802 (39%)]\tLoss: 1.184708\n",
      "Train Epoche: 1 [1083/2802 (39%)]\tLoss: 6.805653\n",
      "Train Epoche: 1 [1084/2802 (39%)]\tLoss: 11.775817\n",
      "Train Epoche: 1 [1085/2802 (39%)]\tLoss: 0.077696\n",
      "Train Epoche: 1 [1086/2802 (39%)]\tLoss: 11.675171\n",
      "Train Epoche: 1 [1087/2802 (39%)]\tLoss: 4.428349\n",
      "Train Epoche: 1 [1088/2802 (39%)]\tLoss: 7.973688\n",
      "Train Epoche: 1 [1089/2802 (39%)]\tLoss: 8.573013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1090/2802 (39%)]\tLoss: 3.072602\n",
      "Train Epoche: 1 [1091/2802 (39%)]\tLoss: 5.622469\n",
      "Train Epoche: 1 [1092/2802 (39%)]\tLoss: 119.068092\n",
      "Train Epoche: 1 [1093/2802 (39%)]\tLoss: 0.241479\n",
      "Train Epoche: 1 [1094/2802 (39%)]\tLoss: 0.003701\n",
      "Train Epoche: 1 [1095/2802 (39%)]\tLoss: 18.353239\n",
      "Train Epoche: 1 [1096/2802 (39%)]\tLoss: 13.748431\n",
      "Train Epoche: 1 [1097/2802 (39%)]\tLoss: 19.132761\n",
      "Train Epoche: 1 [1098/2802 (39%)]\tLoss: 20.513243\n",
      "Train Epoche: 1 [1099/2802 (39%)]\tLoss: 267.393250\n",
      "Train Epoche: 1 [1100/2802 (39%)]\tLoss: 0.376498\n",
      "Train Epoche: 1 [1101/2802 (39%)]\tLoss: 17.574619\n",
      "Train Epoche: 1 [1102/2802 (39%)]\tLoss: 25.228199\n",
      "Train Epoche: 1 [1103/2802 (39%)]\tLoss: 10.335067\n",
      "Train Epoche: 1 [1104/2802 (39%)]\tLoss: 3.271415\n",
      "Train Epoche: 1 [1105/2802 (39%)]\tLoss: 2.829144\n",
      "Train Epoche: 1 [1106/2802 (39%)]\tLoss: 0.259351\n",
      "Train Epoche: 1 [1107/2802 (40%)]\tLoss: 9.951947\n",
      "Train Epoche: 1 [1108/2802 (40%)]\tLoss: 2.519858\n",
      "Train Epoche: 1 [1109/2802 (40%)]\tLoss: 2.255222\n",
      "Train Epoche: 1 [1110/2802 (40%)]\tLoss: 0.052622\n",
      "Train Epoche: 1 [1111/2802 (40%)]\tLoss: 3.152676\n",
      "Train Epoche: 1 [1112/2802 (40%)]\tLoss: 1.148960\n",
      "Train Epoche: 1 [1113/2802 (40%)]\tLoss: 6.614108\n",
      "Train Epoche: 1 [1114/2802 (40%)]\tLoss: 6.621229\n",
      "Train Epoche: 1 [1115/2802 (40%)]\tLoss: 22.160776\n",
      "Train Epoche: 1 [1116/2802 (40%)]\tLoss: 70.644867\n",
      "Train Epoche: 1 [1117/2802 (40%)]\tLoss: 8.726294\n",
      "Train Epoche: 1 [1118/2802 (40%)]\tLoss: 42.985748\n",
      "Train Epoche: 1 [1119/2802 (40%)]\tLoss: 23.055157\n",
      "Train Epoche: 1 [1120/2802 (40%)]\tLoss: 15.130329\n",
      "Train Epoche: 1 [1121/2802 (40%)]\tLoss: 5.118759\n",
      "Train Epoche: 1 [1122/2802 (40%)]\tLoss: 16.233543\n",
      "Train Epoche: 1 [1123/2802 (40%)]\tLoss: 6.685852\n",
      "Train Epoche: 1 [1124/2802 (40%)]\tLoss: 66.239441\n",
      "Train Epoche: 1 [1125/2802 (40%)]\tLoss: 25.861891\n",
      "Train Epoche: 1 [1126/2802 (40%)]\tLoss: 19.165220\n",
      "Train Epoche: 1 [1127/2802 (40%)]\tLoss: 0.229595\n",
      "Train Epoche: 1 [1128/2802 (40%)]\tLoss: 55.701073\n",
      "Train Epoche: 1 [1129/2802 (40%)]\tLoss: 2.007202\n",
      "Train Epoche: 1 [1130/2802 (40%)]\tLoss: 21.358932\n",
      "Train Epoche: 1 [1131/2802 (40%)]\tLoss: 19.917725\n",
      "Train Epoche: 1 [1132/2802 (40%)]\tLoss: 5.506015\n",
      "Train Epoche: 1 [1133/2802 (40%)]\tLoss: 0.313148\n",
      "Train Epoche: 1 [1134/2802 (40%)]\tLoss: 0.031466\n",
      "Train Epoche: 1 [1135/2802 (41%)]\tLoss: 30.414946\n",
      "Train Epoche: 1 [1136/2802 (41%)]\tLoss: 13.074378\n",
      "Train Epoche: 1 [1137/2802 (41%)]\tLoss: 1.693459\n",
      "Train Epoche: 1 [1138/2802 (41%)]\tLoss: 9.449015\n",
      "Train Epoche: 1 [1139/2802 (41%)]\tLoss: 2.953351\n",
      "Train Epoche: 1 [1140/2802 (41%)]\tLoss: 6.558940\n",
      "Train Epoche: 1 [1141/2802 (41%)]\tLoss: 4.997663\n",
      "Train Epoche: 1 [1142/2802 (41%)]\tLoss: 83.329544\n",
      "Train Epoche: 1 [1143/2802 (41%)]\tLoss: 3.283911\n",
      "Train Epoche: 1 [1144/2802 (41%)]\tLoss: 8.114671\n",
      "Train Epoche: 1 [1145/2802 (41%)]\tLoss: 0.208377\n",
      "Train Epoche: 1 [1146/2802 (41%)]\tLoss: 11.004421\n",
      "Train Epoche: 1 [1147/2802 (41%)]\tLoss: 9.046019\n",
      "Train Epoche: 1 [1148/2802 (41%)]\tLoss: 21.686749\n",
      "Train Epoche: 1 [1149/2802 (41%)]\tLoss: 0.024660\n",
      "Train Epoche: 1 [1150/2802 (41%)]\tLoss: 10.031165\n",
      "Train Epoche: 1 [1151/2802 (41%)]\tLoss: 18.722464\n",
      "Train Epoche: 1 [1152/2802 (41%)]\tLoss: 0.149139\n",
      "Train Epoche: 1 [1153/2802 (41%)]\tLoss: 37.699043\n",
      "Train Epoche: 1 [1154/2802 (41%)]\tLoss: 11.802255\n",
      "Train Epoche: 1 [1155/2802 (41%)]\tLoss: 0.715409\n",
      "Train Epoche: 1 [1156/2802 (41%)]\tLoss: 0.166060\n",
      "Train Epoche: 1 [1157/2802 (41%)]\tLoss: 13.563243\n",
      "Train Epoche: 1 [1158/2802 (41%)]\tLoss: 6.211317\n",
      "Train Epoche: 1 [1159/2802 (41%)]\tLoss: 8.236257\n",
      "Train Epoche: 1 [1160/2802 (41%)]\tLoss: 160.980103\n",
      "Train Epoche: 1 [1161/2802 (41%)]\tLoss: 3.046166\n",
      "Train Epoche: 1 [1162/2802 (41%)]\tLoss: 6.269632\n",
      "Train Epoche: 1 [1163/2802 (42%)]\tLoss: 5.124236\n",
      "Train Epoche: 1 [1164/2802 (42%)]\tLoss: 18.133711\n",
      "Train Epoche: 1 [1165/2802 (42%)]\tLoss: 10.969017\n",
      "Train Epoche: 1 [1166/2802 (42%)]\tLoss: 1.783746\n",
      "Train Epoche: 1 [1167/2802 (42%)]\tLoss: 0.462883\n",
      "Train Epoche: 1 [1168/2802 (42%)]\tLoss: 41.323124\n",
      "Train Epoche: 1 [1169/2802 (42%)]\tLoss: 2.131705\n",
      "Train Epoche: 1 [1170/2802 (42%)]\tLoss: 21.395248\n",
      "Train Epoche: 1 [1171/2802 (42%)]\tLoss: 27.816999\n",
      "Train Epoche: 1 [1172/2802 (42%)]\tLoss: 7.425461\n",
      "Train Epoche: 1 [1173/2802 (42%)]\tLoss: 42.786007\n",
      "Train Epoche: 1 [1174/2802 (42%)]\tLoss: 46.628708\n",
      "Train Epoche: 1 [1175/2802 (42%)]\tLoss: 3.841331\n",
      "Train Epoche: 1 [1176/2802 (42%)]\tLoss: 2.770247\n",
      "Train Epoche: 1 [1177/2802 (42%)]\tLoss: 13.813559\n",
      "Train Epoche: 1 [1178/2802 (42%)]\tLoss: 0.152256\n",
      "Train Epoche: 1 [1179/2802 (42%)]\tLoss: 125.955719\n",
      "Train Epoche: 1 [1180/2802 (42%)]\tLoss: 0.002873\n",
      "Train Epoche: 1 [1181/2802 (42%)]\tLoss: 19.161459\n",
      "Train Epoche: 1 [1182/2802 (42%)]\tLoss: 1.112454\n",
      "Train Epoche: 1 [1183/2802 (42%)]\tLoss: 1.394427\n",
      "Train Epoche: 1 [1184/2802 (42%)]\tLoss: 0.497487\n",
      "Train Epoche: 1 [1185/2802 (42%)]\tLoss: 4.476298\n",
      "Train Epoche: 1 [1186/2802 (42%)]\tLoss: 0.329192\n",
      "Train Epoche: 1 [1187/2802 (42%)]\tLoss: 0.199673\n",
      "Train Epoche: 1 [1188/2802 (42%)]\tLoss: 0.729306\n",
      "Train Epoche: 1 [1189/2802 (42%)]\tLoss: 37.299564\n",
      "Train Epoche: 1 [1190/2802 (42%)]\tLoss: 4.493575\n",
      "Train Epoche: 1 [1191/2802 (43%)]\tLoss: 10.229992\n",
      "Train Epoche: 1 [1192/2802 (43%)]\tLoss: 17.999432\n",
      "Train Epoche: 1 [1193/2802 (43%)]\tLoss: 0.025672\n",
      "Train Epoche: 1 [1194/2802 (43%)]\tLoss: 0.560485\n",
      "Train Epoche: 1 [1195/2802 (43%)]\tLoss: 8.300776\n",
      "Train Epoche: 1 [1196/2802 (43%)]\tLoss: 6.574043\n",
      "Train Epoche: 1 [1197/2802 (43%)]\tLoss: 1.233122\n",
      "Train Epoche: 1 [1198/2802 (43%)]\tLoss: 3.450267\n",
      "Train Epoche: 1 [1199/2802 (43%)]\tLoss: 0.032532\n",
      "Train Epoche: 1 [1200/2802 (43%)]\tLoss: 0.035806\n",
      "Train Epoche: 1 [1201/2802 (43%)]\tLoss: 66.532906\n",
      "Train Epoche: 1 [1202/2802 (43%)]\tLoss: 21.632120\n",
      "Train Epoche: 1 [1203/2802 (43%)]\tLoss: 4.250732\n",
      "Train Epoche: 1 [1204/2802 (43%)]\tLoss: 2.572620\n",
      "Train Epoche: 1 [1205/2802 (43%)]\tLoss: 4.519776\n",
      "Train Epoche: 1 [1206/2802 (43%)]\tLoss: 3.397058\n",
      "Train Epoche: 1 [1207/2802 (43%)]\tLoss: 301.310883\n",
      "Train Epoche: 1 [1208/2802 (43%)]\tLoss: 5.174270\n",
      "Train Epoche: 1 [1209/2802 (43%)]\tLoss: 18.906509\n",
      "Train Epoche: 1 [1210/2802 (43%)]\tLoss: 17.161879\n",
      "Train Epoche: 1 [1211/2802 (43%)]\tLoss: 25.028017\n",
      "Train Epoche: 1 [1212/2802 (43%)]\tLoss: 223.190216\n",
      "Train Epoche: 1 [1213/2802 (43%)]\tLoss: 9.221583\n",
      "Train Epoche: 1 [1214/2802 (43%)]\tLoss: 3.338365\n",
      "Train Epoche: 1 [1215/2802 (43%)]\tLoss: 22.254868\n",
      "Train Epoche: 1 [1216/2802 (43%)]\tLoss: 99.827324\n",
      "Train Epoche: 1 [1217/2802 (43%)]\tLoss: 84.009636\n",
      "Train Epoche: 1 [1218/2802 (43%)]\tLoss: 0.881452\n",
      "Train Epoche: 1 [1219/2802 (44%)]\tLoss: 6.044796\n",
      "Train Epoche: 1 [1220/2802 (44%)]\tLoss: 5.089177\n",
      "Train Epoche: 1 [1221/2802 (44%)]\tLoss: 193.014725\n",
      "Train Epoche: 1 [1222/2802 (44%)]\tLoss: 6.328120\n",
      "Train Epoche: 1 [1223/2802 (44%)]\tLoss: 20.422737\n",
      "Train Epoche: 1 [1224/2802 (44%)]\tLoss: 6.551839\n",
      "Train Epoche: 1 [1225/2802 (44%)]\tLoss: 2.503300\n",
      "Train Epoche: 1 [1226/2802 (44%)]\tLoss: 0.167719\n",
      "Train Epoche: 1 [1227/2802 (44%)]\tLoss: 4.558629\n",
      "Train Epoche: 1 [1228/2802 (44%)]\tLoss: 21.745401\n",
      "Train Epoche: 1 [1229/2802 (44%)]\tLoss: 8.622156\n",
      "Train Epoche: 1 [1230/2802 (44%)]\tLoss: 15.668347\n",
      "Train Epoche: 1 [1231/2802 (44%)]\tLoss: 16.687435\n",
      "Train Epoche: 1 [1232/2802 (44%)]\tLoss: 41.681564\n",
      "Train Epoche: 1 [1233/2802 (44%)]\tLoss: 21.430111\n",
      "Train Epoche: 1 [1234/2802 (44%)]\tLoss: 28.628708\n",
      "Train Epoche: 1 [1235/2802 (44%)]\tLoss: 10.591524\n",
      "Train Epoche: 1 [1236/2802 (44%)]\tLoss: 2.551223\n",
      "Train Epoche: 1 [1237/2802 (44%)]\tLoss: 0.061121\n",
      "Train Epoche: 1 [1238/2802 (44%)]\tLoss: 2.085966\n",
      "Train Epoche: 1 [1239/2802 (44%)]\tLoss: 27.239851\n",
      "Train Epoche: 1 [1240/2802 (44%)]\tLoss: 50.076298\n",
      "Train Epoche: 1 [1241/2802 (44%)]\tLoss: 31.465302\n",
      "Train Epoche: 1 [1242/2802 (44%)]\tLoss: 9.705577\n",
      "Train Epoche: 1 [1243/2802 (44%)]\tLoss: 0.391823\n",
      "Train Epoche: 1 [1244/2802 (44%)]\tLoss: 19.681166\n",
      "Train Epoche: 1 [1245/2802 (44%)]\tLoss: 4.952952\n",
      "Train Epoche: 1 [1246/2802 (44%)]\tLoss: 0.516514\n",
      "Train Epoche: 1 [1247/2802 (45%)]\tLoss: 4.258836\n",
      "Train Epoche: 1 [1248/2802 (45%)]\tLoss: 16.160097\n",
      "Train Epoche: 1 [1249/2802 (45%)]\tLoss: 20.948908\n",
      "Train Epoche: 1 [1250/2802 (45%)]\tLoss: 0.472100\n",
      "Train Epoche: 1 [1251/2802 (45%)]\tLoss: 0.009736\n",
      "Train Epoche: 1 [1252/2802 (45%)]\tLoss: 0.233321\n",
      "Train Epoche: 1 [1253/2802 (45%)]\tLoss: 7.812732\n",
      "Train Epoche: 1 [1254/2802 (45%)]\tLoss: 19.643641\n",
      "Train Epoche: 1 [1255/2802 (45%)]\tLoss: 9.131684\n",
      "Train Epoche: 1 [1256/2802 (45%)]\tLoss: 4.735964\n",
      "Train Epoche: 1 [1257/2802 (45%)]\tLoss: 0.119726\n",
      "Train Epoche: 1 [1258/2802 (45%)]\tLoss: 0.241232\n",
      "Train Epoche: 1 [1259/2802 (45%)]\tLoss: 1.038639\n",
      "Train Epoche: 1 [1260/2802 (45%)]\tLoss: 0.040994\n",
      "Train Epoche: 1 [1261/2802 (45%)]\tLoss: 14.175874\n",
      "Train Epoche: 1 [1262/2802 (45%)]\tLoss: 32.139881\n",
      "Train Epoche: 1 [1263/2802 (45%)]\tLoss: 23.633688\n",
      "Train Epoche: 1 [1264/2802 (45%)]\tLoss: 1.936037\n",
      "Train Epoche: 1 [1265/2802 (45%)]\tLoss: 0.034037\n",
      "Train Epoche: 1 [1266/2802 (45%)]\tLoss: 8.289978\n",
      "Train Epoche: 1 [1267/2802 (45%)]\tLoss: 0.397339\n",
      "Train Epoche: 1 [1268/2802 (45%)]\tLoss: 9.653570\n",
      "Train Epoche: 1 [1269/2802 (45%)]\tLoss: 6.101377\n",
      "Train Epoche: 1 [1270/2802 (45%)]\tLoss: 0.074779\n",
      "Train Epoche: 1 [1271/2802 (45%)]\tLoss: 0.101153\n",
      "Train Epoche: 1 [1272/2802 (45%)]\tLoss: 0.114739\n",
      "Train Epoche: 1 [1273/2802 (45%)]\tLoss: 10.942830\n",
      "Train Epoche: 1 [1274/2802 (45%)]\tLoss: 11.708898\n",
      "Train Epoche: 1 [1275/2802 (46%)]\tLoss: 0.512737\n",
      "Train Epoche: 1 [1276/2802 (46%)]\tLoss: 1.775476\n",
      "Train Epoche: 1 [1277/2802 (46%)]\tLoss: 2.702839\n",
      "Train Epoche: 1 [1278/2802 (46%)]\tLoss: 3.259736\n",
      "Train Epoche: 1 [1279/2802 (46%)]\tLoss: 20.326633\n",
      "Train Epoche: 1 [1280/2802 (46%)]\tLoss: 81.846138\n",
      "Train Epoche: 1 [1281/2802 (46%)]\tLoss: 0.889216\n",
      "Train Epoche: 1 [1282/2802 (46%)]\tLoss: 63.932537\n",
      "Train Epoche: 1 [1283/2802 (46%)]\tLoss: 0.117087\n",
      "Train Epoche: 1 [1284/2802 (46%)]\tLoss: 187.758850\n",
      "Train Epoche: 1 [1285/2802 (46%)]\tLoss: 17.122030\n",
      "Train Epoche: 1 [1286/2802 (46%)]\tLoss: 22.819040\n",
      "Train Epoche: 1 [1287/2802 (46%)]\tLoss: 6.009740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1288/2802 (46%)]\tLoss: 25.381002\n",
      "Train Epoche: 1 [1289/2802 (46%)]\tLoss: 0.232284\n",
      "Train Epoche: 1 [1290/2802 (46%)]\tLoss: 2.381846\n",
      "Train Epoche: 1 [1291/2802 (46%)]\tLoss: 0.535745\n",
      "Train Epoche: 1 [1292/2802 (46%)]\tLoss: 139.474243\n",
      "Train Epoche: 1 [1293/2802 (46%)]\tLoss: 21.129438\n",
      "Train Epoche: 1 [1294/2802 (46%)]\tLoss: 0.639375\n",
      "Train Epoche: 1 [1295/2802 (46%)]\tLoss: 2.397628\n",
      "Train Epoche: 1 [1296/2802 (46%)]\tLoss: 0.458306\n",
      "Train Epoche: 1 [1297/2802 (46%)]\tLoss: 0.103087\n",
      "Train Epoche: 1 [1298/2802 (46%)]\tLoss: 158.376297\n",
      "Train Epoche: 1 [1299/2802 (46%)]\tLoss: 0.290731\n",
      "Train Epoche: 1 [1300/2802 (46%)]\tLoss: 42.525852\n",
      "Train Epoche: 1 [1301/2802 (46%)]\tLoss: 9.852854\n",
      "Train Epoche: 1 [1302/2802 (46%)]\tLoss: 112.227135\n",
      "Train Epoche: 1 [1303/2802 (47%)]\tLoss: 25.356424\n",
      "Train Epoche: 1 [1304/2802 (47%)]\tLoss: 0.464741\n",
      "Train Epoche: 1 [1305/2802 (47%)]\tLoss: 2.809304\n",
      "Train Epoche: 1 [1306/2802 (47%)]\tLoss: 5.193325\n",
      "Train Epoche: 1 [1307/2802 (47%)]\tLoss: 0.058201\n",
      "Train Epoche: 1 [1308/2802 (47%)]\tLoss: 6.497488\n",
      "Train Epoche: 1 [1309/2802 (47%)]\tLoss: 0.778942\n",
      "Train Epoche: 1 [1310/2802 (47%)]\tLoss: 3.983943\n",
      "Train Epoche: 1 [1311/2802 (47%)]\tLoss: 0.000599\n",
      "Train Epoche: 1 [1312/2802 (47%)]\tLoss: 9.953487\n",
      "Train Epoche: 1 [1313/2802 (47%)]\tLoss: 0.294487\n",
      "Train Epoche: 1 [1314/2802 (47%)]\tLoss: 0.351850\n",
      "Train Epoche: 1 [1315/2802 (47%)]\tLoss: 1.060680\n",
      "Train Epoche: 1 [1316/2802 (47%)]\tLoss: 0.004395\n",
      "Train Epoche: 1 [1317/2802 (47%)]\tLoss: 54.529705\n",
      "Train Epoche: 1 [1318/2802 (47%)]\tLoss: 1.012022\n",
      "Train Epoche: 1 [1319/2802 (47%)]\tLoss: 1.352261\n",
      "Train Epoche: 1 [1320/2802 (47%)]\tLoss: 0.067323\n",
      "Train Epoche: 1 [1321/2802 (47%)]\tLoss: 0.976315\n",
      "Train Epoche: 1 [1322/2802 (47%)]\tLoss: 125.887207\n",
      "Train Epoche: 1 [1323/2802 (47%)]\tLoss: 8.075642\n",
      "Train Epoche: 1 [1324/2802 (47%)]\tLoss: 32.498848\n",
      "Train Epoche: 1 [1325/2802 (47%)]\tLoss: 3.121346\n",
      "Train Epoche: 1 [1326/2802 (47%)]\tLoss: 156.787552\n",
      "Train Epoche: 1 [1327/2802 (47%)]\tLoss: 0.285316\n",
      "Train Epoche: 1 [1328/2802 (47%)]\tLoss: 7.634499\n",
      "Train Epoche: 1 [1329/2802 (47%)]\tLoss: 42.296230\n",
      "Train Epoche: 1 [1330/2802 (47%)]\tLoss: 0.441599\n",
      "Train Epoche: 1 [1331/2802 (48%)]\tLoss: 4.868742\n",
      "Train Epoche: 1 [1332/2802 (48%)]\tLoss: 19.399376\n",
      "Train Epoche: 1 [1333/2802 (48%)]\tLoss: 0.871982\n",
      "Train Epoche: 1 [1334/2802 (48%)]\tLoss: 5.079504\n",
      "Train Epoche: 1 [1335/2802 (48%)]\tLoss: 359.473328\n",
      "Train Epoche: 1 [1336/2802 (48%)]\tLoss: 4.717861\n",
      "Train Epoche: 1 [1337/2802 (48%)]\tLoss: 10.177579\n",
      "Train Epoche: 1 [1338/2802 (48%)]\tLoss: 145.330276\n",
      "Train Epoche: 1 [1339/2802 (48%)]\tLoss: 13.454240\n",
      "Train Epoche: 1 [1340/2802 (48%)]\tLoss: 25.472603\n",
      "Train Epoche: 1 [1341/2802 (48%)]\tLoss: 0.037692\n",
      "Train Epoche: 1 [1342/2802 (48%)]\tLoss: 34.700844\n",
      "Train Epoche: 1 [1343/2802 (48%)]\tLoss: 35.080048\n",
      "Train Epoche: 1 [1344/2802 (48%)]\tLoss: 4.428353\n",
      "Train Epoche: 1 [1345/2802 (48%)]\tLoss: 77.151054\n",
      "Train Epoche: 1 [1346/2802 (48%)]\tLoss: 156.776535\n",
      "Train Epoche: 1 [1347/2802 (48%)]\tLoss: 14.628446\n",
      "Train Epoche: 1 [1348/2802 (48%)]\tLoss: 7.561795\n",
      "Train Epoche: 1 [1349/2802 (48%)]\tLoss: 97.356346\n",
      "Train Epoche: 1 [1350/2802 (48%)]\tLoss: 33.909729\n",
      "Train Epoche: 1 [1351/2802 (48%)]\tLoss: 6.675314\n",
      "Train Epoche: 1 [1352/2802 (48%)]\tLoss: 1.881564\n",
      "Train Epoche: 1 [1353/2802 (48%)]\tLoss: 12.040061\n",
      "Train Epoche: 1 [1354/2802 (48%)]\tLoss: 28.393499\n",
      "Train Epoche: 1 [1355/2802 (48%)]\tLoss: 0.426565\n",
      "Train Epoche: 1 [1356/2802 (48%)]\tLoss: 0.175470\n",
      "Train Epoche: 1 [1357/2802 (48%)]\tLoss: 42.126312\n",
      "Train Epoche: 1 [1358/2802 (48%)]\tLoss: 4.034501\n",
      "Train Epoche: 1 [1359/2802 (49%)]\tLoss: 6.447361\n",
      "Train Epoche: 1 [1360/2802 (49%)]\tLoss: 51.782005\n",
      "Train Epoche: 1 [1361/2802 (49%)]\tLoss: 5.712215\n",
      "Train Epoche: 1 [1362/2802 (49%)]\tLoss: 0.983881\n",
      "Train Epoche: 1 [1363/2802 (49%)]\tLoss: 3.688078\n",
      "Train Epoche: 1 [1364/2802 (49%)]\tLoss: 188.437637\n",
      "Train Epoche: 1 [1365/2802 (49%)]\tLoss: 2.340672\n",
      "Train Epoche: 1 [1366/2802 (49%)]\tLoss: 0.700069\n",
      "Train Epoche: 1 [1367/2802 (49%)]\tLoss: 6.155252\n",
      "Train Epoche: 1 [1368/2802 (49%)]\tLoss: 11.560179\n",
      "Train Epoche: 1 [1369/2802 (49%)]\tLoss: 2.499134\n",
      "Train Epoche: 1 [1370/2802 (49%)]\tLoss: 2.348148\n",
      "Train Epoche: 1 [1371/2802 (49%)]\tLoss: 0.586585\n",
      "Train Epoche: 1 [1372/2802 (49%)]\tLoss: 0.278949\n",
      "Train Epoche: 1 [1373/2802 (49%)]\tLoss: 6.636627\n",
      "Train Epoche: 1 [1374/2802 (49%)]\tLoss: 60.670788\n",
      "Train Epoche: 1 [1375/2802 (49%)]\tLoss: 11.566558\n",
      "Train Epoche: 1 [1376/2802 (49%)]\tLoss: 0.196412\n",
      "Train Epoche: 1 [1377/2802 (49%)]\tLoss: 0.391417\n",
      "Train Epoche: 1 [1378/2802 (49%)]\tLoss: 38.183681\n",
      "Train Epoche: 1 [1379/2802 (49%)]\tLoss: 0.219939\n",
      "Train Epoche: 1 [1380/2802 (49%)]\tLoss: 0.725822\n",
      "Train Epoche: 1 [1381/2802 (49%)]\tLoss: 2.490515\n",
      "Train Epoche: 1 [1382/2802 (49%)]\tLoss: 97.163086\n",
      "Train Epoche: 1 [1383/2802 (49%)]\tLoss: 9.898840\n",
      "Train Epoche: 1 [1384/2802 (49%)]\tLoss: 9.526059\n",
      "Train Epoche: 1 [1385/2802 (49%)]\tLoss: 0.203420\n",
      "Train Epoche: 1 [1386/2802 (49%)]\tLoss: 6.652203\n",
      "Train Epoche: 1 [1387/2802 (50%)]\tLoss: 3.239626\n",
      "Train Epoche: 1 [1388/2802 (50%)]\tLoss: 4.940023\n",
      "Train Epoche: 1 [1389/2802 (50%)]\tLoss: 2.530143\n",
      "Train Epoche: 1 [1390/2802 (50%)]\tLoss: 1.236879\n",
      "Train Epoche: 1 [1391/2802 (50%)]\tLoss: 8.931452\n",
      "Train Epoche: 1 [1392/2802 (50%)]\tLoss: 138.094955\n",
      "Train Epoche: 1 [1393/2802 (50%)]\tLoss: 115.321396\n",
      "Train Epoche: 1 [1394/2802 (50%)]\tLoss: 23.296804\n",
      "Train Epoche: 1 [1395/2802 (50%)]\tLoss: 0.449317\n",
      "Train Epoche: 1 [1396/2802 (50%)]\tLoss: 7.061319\n",
      "Train Epoche: 1 [1397/2802 (50%)]\tLoss: 33.554134\n",
      "Train Epoche: 1 [1398/2802 (50%)]\tLoss: 153.859482\n",
      "Train Epoche: 1 [1399/2802 (50%)]\tLoss: 1.161030\n",
      "Train Epoche: 1 [1400/2802 (50%)]\tLoss: 4.489019\n",
      "Train Epoche: 1 [1401/2802 (50%)]\tLoss: 6.789282\n",
      "Train Epoche: 1 [1402/2802 (50%)]\tLoss: 14.885590\n",
      "Train Epoche: 1 [1403/2802 (50%)]\tLoss: 62.918518\n",
      "Train Epoche: 1 [1404/2802 (50%)]\tLoss: 5.799332\n",
      "Train Epoche: 1 [1405/2802 (50%)]\tLoss: 11.318012\n",
      "Train Epoche: 1 [1406/2802 (50%)]\tLoss: 0.158100\n",
      "Train Epoche: 1 [1407/2802 (50%)]\tLoss: 0.000325\n",
      "Train Epoche: 1 [1408/2802 (50%)]\tLoss: 0.142110\n",
      "Train Epoche: 1 [1409/2802 (50%)]\tLoss: 1.268457\n",
      "Train Epoche: 1 [1410/2802 (50%)]\tLoss: 0.605496\n",
      "Train Epoche: 1 [1411/2802 (50%)]\tLoss: 51.376278\n",
      "Train Epoche: 1 [1412/2802 (50%)]\tLoss: 4.549231\n",
      "Train Epoche: 1 [1413/2802 (50%)]\tLoss: 7.992143\n",
      "Train Epoche: 1 [1414/2802 (50%)]\tLoss: 22.841967\n",
      "Train Epoche: 1 [1415/2802 (50%)]\tLoss: 5.577019\n",
      "Train Epoche: 1 [1416/2802 (51%)]\tLoss: 20.847990\n",
      "Train Epoche: 1 [1417/2802 (51%)]\tLoss: 0.965200\n",
      "Train Epoche: 1 [1418/2802 (51%)]\tLoss: 2.553231\n",
      "Train Epoche: 1 [1419/2802 (51%)]\tLoss: 56.018124\n",
      "Train Epoche: 1 [1420/2802 (51%)]\tLoss: 3.051092\n",
      "Train Epoche: 1 [1421/2802 (51%)]\tLoss: 4.630338\n",
      "Train Epoche: 1 [1422/2802 (51%)]\tLoss: 114.335945\n",
      "Train Epoche: 1 [1423/2802 (51%)]\tLoss: 4.001741\n",
      "Train Epoche: 1 [1424/2802 (51%)]\tLoss: 0.016379\n",
      "Train Epoche: 1 [1425/2802 (51%)]\tLoss: 9.872148\n",
      "Train Epoche: 1 [1426/2802 (51%)]\tLoss: 10.318156\n",
      "Train Epoche: 1 [1427/2802 (51%)]\tLoss: 12.700752\n",
      "Train Epoche: 1 [1428/2802 (51%)]\tLoss: 8.749793\n",
      "Train Epoche: 1 [1429/2802 (51%)]\tLoss: 4.276775\n",
      "Train Epoche: 1 [1430/2802 (51%)]\tLoss: 1.856217\n",
      "Train Epoche: 1 [1431/2802 (51%)]\tLoss: 70.526207\n",
      "Train Epoche: 1 [1432/2802 (51%)]\tLoss: 72.389725\n",
      "Train Epoche: 1 [1433/2802 (51%)]\tLoss: 82.769852\n",
      "Train Epoche: 1 [1434/2802 (51%)]\tLoss: 1.784735\n",
      "Train Epoche: 1 [1435/2802 (51%)]\tLoss: 8.257347\n",
      "Train Epoche: 1 [1436/2802 (51%)]\tLoss: 0.032238\n",
      "Train Epoche: 1 [1437/2802 (51%)]\tLoss: 79.883301\n",
      "Train Epoche: 1 [1438/2802 (51%)]\tLoss: 16.578812\n",
      "Train Epoche: 1 [1439/2802 (51%)]\tLoss: 8.338427\n",
      "Train Epoche: 1 [1440/2802 (51%)]\tLoss: 2.336777\n",
      "Train Epoche: 1 [1441/2802 (51%)]\tLoss: 0.582822\n",
      "Train Epoche: 1 [1442/2802 (51%)]\tLoss: 0.639102\n",
      "Train Epoche: 1 [1443/2802 (51%)]\tLoss: 47.696327\n",
      "Train Epoche: 1 [1444/2802 (52%)]\tLoss: 13.185390\n",
      "Train Epoche: 1 [1445/2802 (52%)]\tLoss: 2.070741\n",
      "Train Epoche: 1 [1446/2802 (52%)]\tLoss: 11.358461\n",
      "Train Epoche: 1 [1447/2802 (52%)]\tLoss: 0.142813\n",
      "Train Epoche: 1 [1448/2802 (52%)]\tLoss: 7.074250\n",
      "Train Epoche: 1 [1449/2802 (52%)]\tLoss: 5.857490\n",
      "Train Epoche: 1 [1450/2802 (52%)]\tLoss: 11.120769\n",
      "Train Epoche: 1 [1451/2802 (52%)]\tLoss: 14.499474\n",
      "Train Epoche: 1 [1452/2802 (52%)]\tLoss: 0.014949\n",
      "Train Epoche: 1 [1453/2802 (52%)]\tLoss: 0.626091\n",
      "Train Epoche: 1 [1454/2802 (52%)]\tLoss: 26.929058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1455/2802 (52%)]\tLoss: 0.457512\n",
      "Train Epoche: 1 [1456/2802 (52%)]\tLoss: 0.252770\n",
      "Train Epoche: 1 [1457/2802 (52%)]\tLoss: 13.993776\n",
      "Train Epoche: 1 [1458/2802 (52%)]\tLoss: 5.121125\n",
      "Train Epoche: 1 [1459/2802 (52%)]\tLoss: 0.717380\n",
      "Train Epoche: 1 [1460/2802 (52%)]\tLoss: 15.765059\n",
      "Train Epoche: 1 [1461/2802 (52%)]\tLoss: 2.836065\n",
      "Train Epoche: 1 [1462/2802 (52%)]\tLoss: 2.137018\n",
      "Train Epoche: 1 [1463/2802 (52%)]\tLoss: 0.100799\n",
      "Train Epoche: 1 [1464/2802 (52%)]\tLoss: 1.254783\n",
      "Train Epoche: 1 [1465/2802 (52%)]\tLoss: 1.950456\n",
      "Train Epoche: 1 [1466/2802 (52%)]\tLoss: 101.620163\n",
      "Train Epoche: 1 [1467/2802 (52%)]\tLoss: 0.166603\n",
      "Train Epoche: 1 [1468/2802 (52%)]\tLoss: 9.801361\n",
      "Train Epoche: 1 [1469/2802 (52%)]\tLoss: 235.102646\n",
      "Train Epoche: 1 [1470/2802 (52%)]\tLoss: 5.909686\n",
      "Train Epoche: 1 [1471/2802 (52%)]\tLoss: 9.823742\n",
      "Train Epoche: 1 [1472/2802 (53%)]\tLoss: 1.335640\n",
      "Train Epoche: 1 [1473/2802 (53%)]\tLoss: 2.374175\n",
      "Train Epoche: 1 [1474/2802 (53%)]\tLoss: 0.999828\n",
      "Train Epoche: 1 [1475/2802 (53%)]\tLoss: 4.333822\n",
      "Train Epoche: 1 [1476/2802 (53%)]\tLoss: 7.477600\n",
      "Train Epoche: 1 [1477/2802 (53%)]\tLoss: 35.624519\n",
      "Train Epoche: 1 [1478/2802 (53%)]\tLoss: 30.069017\n",
      "Train Epoche: 1 [1479/2802 (53%)]\tLoss: 3.543755\n",
      "Train Epoche: 1 [1480/2802 (53%)]\tLoss: 0.867851\n",
      "Train Epoche: 1 [1481/2802 (53%)]\tLoss: 4.671938\n",
      "Train Epoche: 1 [1482/2802 (53%)]\tLoss: 1.743029\n",
      "Train Epoche: 1 [1483/2802 (53%)]\tLoss: 8.166702\n",
      "Train Epoche: 1 [1484/2802 (53%)]\tLoss: 0.648498\n",
      "Train Epoche: 1 [1485/2802 (53%)]\tLoss: 2.932138\n",
      "Train Epoche: 1 [1486/2802 (53%)]\tLoss: 230.904678\n",
      "Train Epoche: 1 [1487/2802 (53%)]\tLoss: 29.691782\n",
      "Train Epoche: 1 [1488/2802 (53%)]\tLoss: 0.185248\n",
      "Train Epoche: 1 [1489/2802 (53%)]\tLoss: 119.312897\n",
      "Train Epoche: 1 [1490/2802 (53%)]\tLoss: 0.026478\n",
      "Train Epoche: 1 [1491/2802 (53%)]\tLoss: 9.852841\n",
      "Train Epoche: 1 [1492/2802 (53%)]\tLoss: 21.384424\n",
      "Train Epoche: 1 [1493/2802 (53%)]\tLoss: 14.199259\n",
      "Train Epoche: 1 [1494/2802 (53%)]\tLoss: 56.149078\n",
      "Train Epoche: 1 [1495/2802 (53%)]\tLoss: 4.865388\n",
      "Train Epoche: 1 [1496/2802 (53%)]\tLoss: 0.134533\n",
      "Train Epoche: 1 [1497/2802 (53%)]\tLoss: 4.250778\n",
      "Train Epoche: 1 [1498/2802 (53%)]\tLoss: 0.601033\n",
      "Train Epoche: 1 [1499/2802 (53%)]\tLoss: 19.435400\n",
      "Train Epoche: 1 [1500/2802 (54%)]\tLoss: 47.958969\n",
      "Train Epoche: 1 [1501/2802 (54%)]\tLoss: 6.718545\n",
      "Train Epoche: 1 [1502/2802 (54%)]\tLoss: 2.850317\n",
      "Train Epoche: 1 [1503/2802 (54%)]\tLoss: 0.065499\n",
      "Train Epoche: 1 [1504/2802 (54%)]\tLoss: 0.649002\n",
      "Train Epoche: 1 [1505/2802 (54%)]\tLoss: 0.171134\n",
      "Train Epoche: 1 [1506/2802 (54%)]\tLoss: 5.208595\n",
      "Train Epoche: 1 [1507/2802 (54%)]\tLoss: 0.391246\n",
      "Train Epoche: 1 [1508/2802 (54%)]\tLoss: 10.590339\n",
      "Train Epoche: 1 [1509/2802 (54%)]\tLoss: 0.030192\n",
      "Train Epoche: 1 [1510/2802 (54%)]\tLoss: 0.806595\n",
      "Train Epoche: 1 [1511/2802 (54%)]\tLoss: 5.498440\n",
      "Train Epoche: 1 [1512/2802 (54%)]\tLoss: 6.525883\n",
      "Train Epoche: 1 [1513/2802 (54%)]\tLoss: 4.000259\n",
      "Train Epoche: 1 [1514/2802 (54%)]\tLoss: 1.489690\n",
      "Train Epoche: 1 [1515/2802 (54%)]\tLoss: 43.908066\n",
      "Train Epoche: 1 [1516/2802 (54%)]\tLoss: 3.658022\n",
      "Train Epoche: 1 [1517/2802 (54%)]\tLoss: 2.498561\n",
      "Train Epoche: 1 [1518/2802 (54%)]\tLoss: 33.039669\n",
      "Train Epoche: 1 [1519/2802 (54%)]\tLoss: 2.920631\n",
      "Train Epoche: 1 [1520/2802 (54%)]\tLoss: 6.232570\n",
      "Train Epoche: 1 [1521/2802 (54%)]\tLoss: 1.447245\n",
      "Train Epoche: 1 [1522/2802 (54%)]\tLoss: 13.581372\n",
      "Train Epoche: 1 [1523/2802 (54%)]\tLoss: 106.787979\n",
      "Train Epoche: 1 [1524/2802 (54%)]\tLoss: 67.716492\n",
      "Train Epoche: 1 [1525/2802 (54%)]\tLoss: 1.347561\n",
      "Train Epoche: 1 [1526/2802 (54%)]\tLoss: 5.852316\n",
      "Train Epoche: 1 [1527/2802 (54%)]\tLoss: 0.261614\n",
      "Train Epoche: 1 [1528/2802 (55%)]\tLoss: 15.270824\n",
      "Train Epoche: 1 [1529/2802 (55%)]\tLoss: 18.375839\n",
      "Train Epoche: 1 [1530/2802 (55%)]\tLoss: 2.019131\n",
      "Train Epoche: 1 [1531/2802 (55%)]\tLoss: 2.013758\n",
      "Train Epoche: 1 [1532/2802 (55%)]\tLoss: 9.498058\n",
      "Train Epoche: 1 [1533/2802 (55%)]\tLoss: 3.062275\n",
      "Train Epoche: 1 [1534/2802 (55%)]\tLoss: 14.239204\n",
      "Train Epoche: 1 [1535/2802 (55%)]\tLoss: 87.422707\n",
      "Train Epoche: 1 [1536/2802 (55%)]\tLoss: 1.029105\n",
      "Train Epoche: 1 [1537/2802 (55%)]\tLoss: 0.866478\n",
      "Train Epoche: 1 [1538/2802 (55%)]\tLoss: 13.308027\n",
      "Train Epoche: 1 [1539/2802 (55%)]\tLoss: 0.524545\n",
      "Train Epoche: 1 [1540/2802 (55%)]\tLoss: 37.569759\n",
      "Train Epoche: 1 [1541/2802 (55%)]\tLoss: 23.508823\n",
      "Train Epoche: 1 [1542/2802 (55%)]\tLoss: 9.585527\n",
      "Train Epoche: 1 [1543/2802 (55%)]\tLoss: 1.171937\n",
      "Train Epoche: 1 [1544/2802 (55%)]\tLoss: 0.032111\n",
      "Train Epoche: 1 [1545/2802 (55%)]\tLoss: 1.604000\n",
      "Train Epoche: 1 [1546/2802 (55%)]\tLoss: 0.157513\n",
      "Train Epoche: 1 [1547/2802 (55%)]\tLoss: 1.468849\n",
      "Train Epoche: 1 [1548/2802 (55%)]\tLoss: 184.356644\n",
      "Train Epoche: 1 [1549/2802 (55%)]\tLoss: 4.562562\n",
      "Train Epoche: 1 [1550/2802 (55%)]\tLoss: 4.846980\n",
      "Train Epoche: 1 [1551/2802 (55%)]\tLoss: 2.883029\n",
      "Train Epoche: 1 [1552/2802 (55%)]\tLoss: 57.153908\n",
      "Train Epoche: 1 [1553/2802 (55%)]\tLoss: 7.096421\n",
      "Train Epoche: 1 [1554/2802 (55%)]\tLoss: 0.280394\n",
      "Train Epoche: 1 [1555/2802 (55%)]\tLoss: 141.357666\n",
      "Train Epoche: 1 [1556/2802 (56%)]\tLoss: 133.446167\n",
      "Train Epoche: 1 [1557/2802 (56%)]\tLoss: 1.748164\n",
      "Train Epoche: 1 [1558/2802 (56%)]\tLoss: 17.864243\n",
      "Train Epoche: 1 [1559/2802 (56%)]\tLoss: 1.030278\n",
      "Train Epoche: 1 [1560/2802 (56%)]\tLoss: 18.148447\n",
      "Train Epoche: 1 [1561/2802 (56%)]\tLoss: 4.257463\n",
      "Train Epoche: 1 [1562/2802 (56%)]\tLoss: 0.190009\n",
      "Train Epoche: 1 [1563/2802 (56%)]\tLoss: 1.886106\n",
      "Train Epoche: 1 [1564/2802 (56%)]\tLoss: 4.259951\n",
      "Train Epoche: 1 [1565/2802 (56%)]\tLoss: 0.353144\n",
      "Train Epoche: 1 [1566/2802 (56%)]\tLoss: 0.181262\n",
      "Train Epoche: 1 [1567/2802 (56%)]\tLoss: 0.244712\n",
      "Train Epoche: 1 [1568/2802 (56%)]\tLoss: 2.664671\n",
      "Train Epoche: 1 [1569/2802 (56%)]\tLoss: 1.888391\n",
      "Train Epoche: 1 [1570/2802 (56%)]\tLoss: 17.086222\n",
      "Train Epoche: 1 [1571/2802 (56%)]\tLoss: 0.003782\n",
      "Train Epoche: 1 [1572/2802 (56%)]\tLoss: 9.017079\n",
      "Train Epoche: 1 [1573/2802 (56%)]\tLoss: 1.275866\n",
      "Train Epoche: 1 [1574/2802 (56%)]\tLoss: 7.585100\n",
      "Train Epoche: 1 [1575/2802 (56%)]\tLoss: 2.314062\n",
      "Train Epoche: 1 [1576/2802 (56%)]\tLoss: 2.237754\n",
      "Train Epoche: 1 [1577/2802 (56%)]\tLoss: 99.380959\n",
      "Train Epoche: 1 [1578/2802 (56%)]\tLoss: 268.494110\n",
      "Train Epoche: 1 [1579/2802 (56%)]\tLoss: 1.514417\n",
      "Train Epoche: 1 [1580/2802 (56%)]\tLoss: 14.624685\n",
      "Train Epoche: 1 [1581/2802 (56%)]\tLoss: 15.739609\n",
      "Train Epoche: 1 [1582/2802 (56%)]\tLoss: 20.633938\n",
      "Train Epoche: 1 [1583/2802 (56%)]\tLoss: 78.718147\n",
      "Train Epoche: 1 [1584/2802 (57%)]\tLoss: 3.108638\n",
      "Train Epoche: 1 [1585/2802 (57%)]\tLoss: 0.714037\n",
      "Train Epoche: 1 [1586/2802 (57%)]\tLoss: 2.079334\n",
      "Train Epoche: 1 [1587/2802 (57%)]\tLoss: 19.027281\n",
      "Train Epoche: 1 [1588/2802 (57%)]\tLoss: 14.733072\n",
      "Train Epoche: 1 [1589/2802 (57%)]\tLoss: 25.545547\n",
      "Train Epoche: 1 [1590/2802 (57%)]\tLoss: 44.699993\n",
      "Train Epoche: 1 [1591/2802 (57%)]\tLoss: 19.595306\n",
      "Train Epoche: 1 [1592/2802 (57%)]\tLoss: 26.819788\n",
      "Train Epoche: 1 [1593/2802 (57%)]\tLoss: 0.000364\n",
      "Train Epoche: 1 [1594/2802 (57%)]\tLoss: 16.667021\n",
      "Train Epoche: 1 [1595/2802 (57%)]\tLoss: 105.425392\n",
      "Train Epoche: 1 [1596/2802 (57%)]\tLoss: 28.106876\n",
      "Train Epoche: 1 [1597/2802 (57%)]\tLoss: 38.170483\n",
      "Train Epoche: 1 [1598/2802 (57%)]\tLoss: 10.441499\n",
      "Train Epoche: 1 [1599/2802 (57%)]\tLoss: 40.940002\n",
      "Train Epoche: 1 [1600/2802 (57%)]\tLoss: 0.991217\n",
      "Train Epoche: 1 [1601/2802 (57%)]\tLoss: 1.913189\n",
      "Train Epoche: 1 [1602/2802 (57%)]\tLoss: 11.573282\n",
      "Train Epoche: 1 [1603/2802 (57%)]\tLoss: 11.281999\n",
      "Train Epoche: 1 [1604/2802 (57%)]\tLoss: 0.244884\n",
      "Train Epoche: 1 [1605/2802 (57%)]\tLoss: 58.074440\n",
      "Train Epoche: 1 [1606/2802 (57%)]\tLoss: 3.320221\n",
      "Train Epoche: 1 [1607/2802 (57%)]\tLoss: 2.899746\n",
      "Train Epoche: 1 [1608/2802 (57%)]\tLoss: 11.903584\n",
      "Train Epoche: 1 [1609/2802 (57%)]\tLoss: 3.637837\n",
      "Train Epoche: 1 [1610/2802 (57%)]\tLoss: 30.740131\n",
      "Train Epoche: 1 [1611/2802 (57%)]\tLoss: 0.021454\n",
      "Train Epoche: 1 [1612/2802 (58%)]\tLoss: 0.117962\n",
      "Train Epoche: 1 [1613/2802 (58%)]\tLoss: 0.359483\n",
      "Train Epoche: 1 [1614/2802 (58%)]\tLoss: 0.798421\n",
      "Train Epoche: 1 [1615/2802 (58%)]\tLoss: 1.802231\n",
      "Train Epoche: 1 [1616/2802 (58%)]\tLoss: 13.054078\n",
      "Train Epoche: 1 [1617/2802 (58%)]\tLoss: 0.017642\n",
      "Train Epoche: 1 [1618/2802 (58%)]\tLoss: 0.121294\n",
      "Train Epoche: 1 [1619/2802 (58%)]\tLoss: 43.253464\n",
      "Train Epoche: 1 [1620/2802 (58%)]\tLoss: 4.128852\n",
      "Train Epoche: 1 [1621/2802 (58%)]\tLoss: 2.383328\n",
      "Train Epoche: 1 [1622/2802 (58%)]\tLoss: 4.578381\n",
      "Train Epoche: 1 [1623/2802 (58%)]\tLoss: 5.945287\n",
      "Train Epoche: 1 [1624/2802 (58%)]\tLoss: 0.939966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1625/2802 (58%)]\tLoss: 32.385700\n",
      "Train Epoche: 1 [1626/2802 (58%)]\tLoss: 0.232584\n",
      "Train Epoche: 1 [1627/2802 (58%)]\tLoss: 9.383718\n",
      "Train Epoche: 1 [1628/2802 (58%)]\tLoss: 0.002616\n",
      "Train Epoche: 1 [1629/2802 (58%)]\tLoss: 0.673025\n",
      "Train Epoche: 1 [1630/2802 (58%)]\tLoss: 0.001209\n",
      "Train Epoche: 1 [1631/2802 (58%)]\tLoss: 3.820168\n",
      "Train Epoche: 1 [1632/2802 (58%)]\tLoss: 20.612526\n",
      "Train Epoche: 1 [1633/2802 (58%)]\tLoss: 0.007167\n",
      "Train Epoche: 1 [1634/2802 (58%)]\tLoss: 2.599705\n",
      "Train Epoche: 1 [1635/2802 (58%)]\tLoss: 109.436325\n",
      "Train Epoche: 1 [1636/2802 (58%)]\tLoss: 26.290821\n",
      "Train Epoche: 1 [1637/2802 (58%)]\tLoss: 6.668895\n",
      "Train Epoche: 1 [1638/2802 (58%)]\tLoss: 18.294569\n",
      "Train Epoche: 1 [1639/2802 (58%)]\tLoss: 1.833580\n",
      "Train Epoche: 1 [1640/2802 (59%)]\tLoss: 170.418640\n",
      "Train Epoche: 1 [1641/2802 (59%)]\tLoss: 2.289720\n",
      "Train Epoche: 1 [1642/2802 (59%)]\tLoss: 238.103104\n",
      "Train Epoche: 1 [1643/2802 (59%)]\tLoss: 10.445105\n",
      "Train Epoche: 1 [1644/2802 (59%)]\tLoss: 3.308869\n",
      "Train Epoche: 1 [1645/2802 (59%)]\tLoss: 0.128596\n",
      "Train Epoche: 1 [1646/2802 (59%)]\tLoss: 0.979979\n",
      "Train Epoche: 1 [1647/2802 (59%)]\tLoss: 0.027289\n",
      "Train Epoche: 1 [1648/2802 (59%)]\tLoss: 0.084322\n",
      "Train Epoche: 1 [1649/2802 (59%)]\tLoss: 6.164824\n",
      "Train Epoche: 1 [1650/2802 (59%)]\tLoss: 9.649446\n",
      "Train Epoche: 1 [1651/2802 (59%)]\tLoss: 2.973814\n",
      "Train Epoche: 1 [1652/2802 (59%)]\tLoss: 8.465324\n",
      "Train Epoche: 1 [1653/2802 (59%)]\tLoss: 0.170172\n",
      "Train Epoche: 1 [1654/2802 (59%)]\tLoss: 0.686387\n",
      "Train Epoche: 1 [1655/2802 (59%)]\tLoss: 2.222438\n",
      "Train Epoche: 1 [1656/2802 (59%)]\tLoss: 1.837991\n",
      "Train Epoche: 1 [1657/2802 (59%)]\tLoss: 2.867091\n",
      "Train Epoche: 1 [1658/2802 (59%)]\tLoss: 31.938835\n",
      "Train Epoche: 1 [1659/2802 (59%)]\tLoss: 0.052949\n",
      "Train Epoche: 1 [1660/2802 (59%)]\tLoss: 2.136498\n",
      "Train Epoche: 1 [1661/2802 (59%)]\tLoss: 309.109039\n",
      "Train Epoche: 1 [1662/2802 (59%)]\tLoss: 34.023045\n",
      "Train Epoche: 1 [1663/2802 (59%)]\tLoss: 0.919516\n",
      "Train Epoche: 1 [1664/2802 (59%)]\tLoss: 352.458862\n",
      "Train Epoche: 1 [1665/2802 (59%)]\tLoss: 9.494368\n",
      "Train Epoche: 1 [1666/2802 (59%)]\tLoss: 54.415356\n",
      "Train Epoche: 1 [1667/2802 (59%)]\tLoss: 12.815922\n",
      "Train Epoche: 1 [1668/2802 (60%)]\tLoss: 3.372740\n",
      "Train Epoche: 1 [1669/2802 (60%)]\tLoss: 0.194501\n",
      "Train Epoche: 1 [1670/2802 (60%)]\tLoss: 15.022611\n",
      "Train Epoche: 1 [1671/2802 (60%)]\tLoss: 0.112066\n",
      "Train Epoche: 1 [1672/2802 (60%)]\tLoss: 31.542265\n",
      "Train Epoche: 1 [1673/2802 (60%)]\tLoss: 19.572121\n",
      "Train Epoche: 1 [1674/2802 (60%)]\tLoss: 7.509407\n",
      "Train Epoche: 1 [1675/2802 (60%)]\tLoss: 2.668469\n",
      "Train Epoche: 1 [1676/2802 (60%)]\tLoss: 9.576940\n",
      "Train Epoche: 1 [1677/2802 (60%)]\tLoss: 1.551345\n",
      "Train Epoche: 1 [1678/2802 (60%)]\tLoss: 19.117802\n",
      "Train Epoche: 1 [1679/2802 (60%)]\tLoss: 3.080701\n",
      "Train Epoche: 1 [1680/2802 (60%)]\tLoss: 6.859608\n",
      "Train Epoche: 1 [1681/2802 (60%)]\tLoss: 21.587486\n",
      "Train Epoche: 1 [1682/2802 (60%)]\tLoss: 6.950048\n",
      "Train Epoche: 1 [1683/2802 (60%)]\tLoss: 110.784447\n",
      "Train Epoche: 1 [1684/2802 (60%)]\tLoss: 1.469748\n",
      "Train Epoche: 1 [1685/2802 (60%)]\tLoss: 0.000186\n",
      "Train Epoche: 1 [1686/2802 (60%)]\tLoss: 156.541534\n",
      "Train Epoche: 1 [1687/2802 (60%)]\tLoss: 4.767843\n",
      "Train Epoche: 1 [1688/2802 (60%)]\tLoss: 50.472996\n",
      "Train Epoche: 1 [1689/2802 (60%)]\tLoss: 3.999077\n",
      "Train Epoche: 1 [1690/2802 (60%)]\tLoss: 2.357718\n",
      "Train Epoche: 1 [1691/2802 (60%)]\tLoss: 1.438096\n",
      "Train Epoche: 1 [1692/2802 (60%)]\tLoss: 43.976189\n",
      "Train Epoche: 1 [1693/2802 (60%)]\tLoss: 5.334414\n",
      "Train Epoche: 1 [1694/2802 (60%)]\tLoss: 13.371944\n",
      "Train Epoche: 1 [1695/2802 (60%)]\tLoss: 1.841279\n",
      "Train Epoche: 1 [1696/2802 (61%)]\tLoss: 1.749472\n",
      "Train Epoche: 1 [1697/2802 (61%)]\tLoss: 0.006188\n",
      "Train Epoche: 1 [1698/2802 (61%)]\tLoss: 0.813544\n",
      "Train Epoche: 1 [1699/2802 (61%)]\tLoss: 0.014613\n",
      "Train Epoche: 1 [1700/2802 (61%)]\tLoss: 0.080046\n",
      "Train Epoche: 1 [1701/2802 (61%)]\tLoss: 3.349524\n",
      "Train Epoche: 1 [1702/2802 (61%)]\tLoss: 0.176215\n",
      "Train Epoche: 1 [1703/2802 (61%)]\tLoss: 50.096855\n",
      "Train Epoche: 1 [1704/2802 (61%)]\tLoss: 44.725834\n",
      "Train Epoche: 1 [1705/2802 (61%)]\tLoss: 8.143335\n",
      "Train Epoche: 1 [1706/2802 (61%)]\tLoss: 0.109860\n",
      "Train Epoche: 1 [1707/2802 (61%)]\tLoss: 13.418757\n",
      "Train Epoche: 1 [1708/2802 (61%)]\tLoss: 0.043657\n",
      "Train Epoche: 1 [1709/2802 (61%)]\tLoss: 20.149326\n",
      "Train Epoche: 1 [1710/2802 (61%)]\tLoss: 7.517657\n",
      "Train Epoche: 1 [1711/2802 (61%)]\tLoss: 5.629346\n",
      "Train Epoche: 1 [1712/2802 (61%)]\tLoss: 9.878957\n",
      "Train Epoche: 1 [1713/2802 (61%)]\tLoss: 7.789849\n",
      "Train Epoche: 1 [1714/2802 (61%)]\tLoss: 2.225886\n",
      "Train Epoche: 1 [1715/2802 (61%)]\tLoss: 11.247314\n",
      "Train Epoche: 1 [1716/2802 (61%)]\tLoss: 8.361900\n",
      "Train Epoche: 1 [1717/2802 (61%)]\tLoss: 60.480610\n",
      "Train Epoche: 1 [1718/2802 (61%)]\tLoss: 42.599892\n",
      "Train Epoche: 1 [1719/2802 (61%)]\tLoss: 45.477257\n",
      "Train Epoche: 1 [1720/2802 (61%)]\tLoss: 28.965492\n",
      "Train Epoche: 1 [1721/2802 (61%)]\tLoss: 38.923019\n",
      "Train Epoche: 1 [1722/2802 (61%)]\tLoss: 6.418389\n",
      "Train Epoche: 1 [1723/2802 (61%)]\tLoss: 0.775026\n",
      "Train Epoche: 1 [1724/2802 (62%)]\tLoss: 4.340559\n",
      "Train Epoche: 1 [1725/2802 (62%)]\tLoss: 19.022423\n",
      "Train Epoche: 1 [1726/2802 (62%)]\tLoss: 63.463226\n",
      "Train Epoche: 1 [1727/2802 (62%)]\tLoss: 3.639009\n",
      "Train Epoche: 1 [1728/2802 (62%)]\tLoss: 7.008706\n",
      "Train Epoche: 1 [1729/2802 (62%)]\tLoss: 2.114156\n",
      "Train Epoche: 1 [1730/2802 (62%)]\tLoss: 24.470531\n",
      "Train Epoche: 1 [1731/2802 (62%)]\tLoss: 0.500881\n",
      "Train Epoche: 1 [1732/2802 (62%)]\tLoss: 0.277663\n",
      "Train Epoche: 1 [1733/2802 (62%)]\tLoss: 0.001785\n",
      "Train Epoche: 1 [1734/2802 (62%)]\tLoss: 2.359024\n",
      "Train Epoche: 1 [1735/2802 (62%)]\tLoss: 105.304260\n",
      "Train Epoche: 1 [1736/2802 (62%)]\tLoss: 7.571351\n",
      "Train Epoche: 1 [1737/2802 (62%)]\tLoss: 125.789238\n",
      "Train Epoche: 1 [1738/2802 (62%)]\tLoss: 9.251396\n",
      "Train Epoche: 1 [1739/2802 (62%)]\tLoss: 34.011642\n",
      "Train Epoche: 1 [1740/2802 (62%)]\tLoss: 0.139651\n",
      "Train Epoche: 1 [1741/2802 (62%)]\tLoss: 0.584848\n",
      "Train Epoche: 1 [1742/2802 (62%)]\tLoss: 2.205179\n",
      "Train Epoche: 1 [1743/2802 (62%)]\tLoss: 15.935871\n",
      "Train Epoche: 1 [1744/2802 (62%)]\tLoss: 0.825028\n",
      "Train Epoche: 1 [1745/2802 (62%)]\tLoss: 0.099292\n",
      "Train Epoche: 1 [1746/2802 (62%)]\tLoss: 3.535303\n",
      "Train Epoche: 1 [1747/2802 (62%)]\tLoss: 29.856325\n",
      "Train Epoche: 1 [1748/2802 (62%)]\tLoss: 2.707975\n",
      "Train Epoche: 1 [1749/2802 (62%)]\tLoss: 0.330843\n",
      "Train Epoche: 1 [1750/2802 (62%)]\tLoss: 7.769398\n",
      "Train Epoche: 1 [1751/2802 (62%)]\tLoss: 30.551275\n",
      "Train Epoche: 1 [1752/2802 (63%)]\tLoss: 1.984994\n",
      "Train Epoche: 1 [1753/2802 (63%)]\tLoss: 6.818576\n",
      "Train Epoche: 1 [1754/2802 (63%)]\tLoss: 2.299265\n",
      "Train Epoche: 1 [1755/2802 (63%)]\tLoss: 0.000098\n",
      "Train Epoche: 1 [1756/2802 (63%)]\tLoss: 0.600506\n",
      "Train Epoche: 1 [1757/2802 (63%)]\tLoss: 3.184246\n",
      "Train Epoche: 1 [1758/2802 (63%)]\tLoss: 2.000439\n",
      "Train Epoche: 1 [1759/2802 (63%)]\tLoss: 5.246710\n",
      "Train Epoche: 1 [1760/2802 (63%)]\tLoss: 2.792361\n",
      "Train Epoche: 1 [1761/2802 (63%)]\tLoss: 119.555237\n",
      "Train Epoche: 1 [1762/2802 (63%)]\tLoss: 5.727790\n",
      "Train Epoche: 1 [1763/2802 (63%)]\tLoss: 0.801623\n",
      "Train Epoche: 1 [1764/2802 (63%)]\tLoss: 97.867859\n",
      "Train Epoche: 1 [1765/2802 (63%)]\tLoss: 192.123520\n",
      "Train Epoche: 1 [1766/2802 (63%)]\tLoss: 1.754333\n",
      "Train Epoche: 1 [1767/2802 (63%)]\tLoss: 2.464207\n",
      "Train Epoche: 1 [1768/2802 (63%)]\tLoss: 14.303131\n",
      "Train Epoche: 1 [1769/2802 (63%)]\tLoss: 10.268027\n",
      "Train Epoche: 1 [1770/2802 (63%)]\tLoss: 22.464741\n",
      "Train Epoche: 1 [1771/2802 (63%)]\tLoss: 26.558342\n",
      "Train Epoche: 1 [1772/2802 (63%)]\tLoss: 14.282479\n",
      "Train Epoche: 1 [1773/2802 (63%)]\tLoss: 17.745394\n",
      "Train Epoche: 1 [1774/2802 (63%)]\tLoss: 2.696390\n",
      "Train Epoche: 1 [1775/2802 (63%)]\tLoss: 0.841560\n",
      "Train Epoche: 1 [1776/2802 (63%)]\tLoss: 8.345653\n",
      "Train Epoche: 1 [1777/2802 (63%)]\tLoss: 0.272701\n",
      "Train Epoche: 1 [1778/2802 (63%)]\tLoss: 6.035821\n",
      "Train Epoche: 1 [1779/2802 (63%)]\tLoss: 1.303398\n",
      "Train Epoche: 1 [1780/2802 (64%)]\tLoss: 1.530567\n",
      "Train Epoche: 1 [1781/2802 (64%)]\tLoss: 1.219996\n",
      "Train Epoche: 1 [1782/2802 (64%)]\tLoss: 12.253879\n",
      "Train Epoche: 1 [1783/2802 (64%)]\tLoss: 2.699726\n",
      "Train Epoche: 1 [1784/2802 (64%)]\tLoss: 10.691431\n",
      "Train Epoche: 1 [1785/2802 (64%)]\tLoss: 4.561314\n",
      "Train Epoche: 1 [1786/2802 (64%)]\tLoss: 52.464489\n",
      "Train Epoche: 1 [1787/2802 (64%)]\tLoss: 1.810029\n",
      "Train Epoche: 1 [1788/2802 (64%)]\tLoss: 113.806007\n",
      "Train Epoche: 1 [1789/2802 (64%)]\tLoss: 21.709839\n",
      "Train Epoche: 1 [1790/2802 (64%)]\tLoss: 10.087246\n",
      "Train Epoche: 1 [1791/2802 (64%)]\tLoss: 38.542202\n",
      "Train Epoche: 1 [1792/2802 (64%)]\tLoss: 38.663174\n",
      "Train Epoche: 1 [1793/2802 (64%)]\tLoss: 3.934399\n",
      "Train Epoche: 1 [1794/2802 (64%)]\tLoss: 7.431772\n",
      "Train Epoche: 1 [1795/2802 (64%)]\tLoss: 0.740528\n",
      "Train Epoche: 1 [1796/2802 (64%)]\tLoss: 362.753082\n",
      "Train Epoche: 1 [1797/2802 (64%)]\tLoss: 0.538292\n",
      "Train Epoche: 1 [1798/2802 (64%)]\tLoss: 4.836060\n",
      "Train Epoche: 1 [1799/2802 (64%)]\tLoss: 166.674622\n",
      "Train Epoche: 1 [1800/2802 (64%)]\tLoss: 20.851473\n",
      "Train Epoche: 1 [1801/2802 (64%)]\tLoss: 9.195078\n",
      "Train Epoche: 1 [1802/2802 (64%)]\tLoss: 3.228119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1803/2802 (64%)]\tLoss: 159.116928\n",
      "Train Epoche: 1 [1804/2802 (64%)]\tLoss: 0.099125\n",
      "Train Epoche: 1 [1805/2802 (64%)]\tLoss: 0.425068\n",
      "Train Epoche: 1 [1806/2802 (64%)]\tLoss: 9.181632\n",
      "Train Epoche: 1 [1807/2802 (64%)]\tLoss: 8.339677\n",
      "Train Epoche: 1 [1808/2802 (65%)]\tLoss: 0.772178\n",
      "Train Epoche: 1 [1809/2802 (65%)]\tLoss: 2.172982\n",
      "Train Epoche: 1 [1810/2802 (65%)]\tLoss: 3.938625\n",
      "Train Epoche: 1 [1811/2802 (65%)]\tLoss: 18.889679\n",
      "Train Epoche: 1 [1812/2802 (65%)]\tLoss: 12.319064\n",
      "Train Epoche: 1 [1813/2802 (65%)]\tLoss: 1.628928\n",
      "Train Epoche: 1 [1814/2802 (65%)]\tLoss: 7.324198\n",
      "Train Epoche: 1 [1815/2802 (65%)]\tLoss: 11.532475\n",
      "Train Epoche: 1 [1816/2802 (65%)]\tLoss: 0.079157\n",
      "Train Epoche: 1 [1817/2802 (65%)]\tLoss: 0.297296\n",
      "Train Epoche: 1 [1818/2802 (65%)]\tLoss: 1.244303\n",
      "Train Epoche: 1 [1819/2802 (65%)]\tLoss: 3.120208\n",
      "Train Epoche: 1 [1820/2802 (65%)]\tLoss: 5.491908\n",
      "Train Epoche: 1 [1821/2802 (65%)]\tLoss: 169.115067\n",
      "Train Epoche: 1 [1822/2802 (65%)]\tLoss: 1.933689\n",
      "Train Epoche: 1 [1823/2802 (65%)]\tLoss: 34.693195\n",
      "Train Epoche: 1 [1824/2802 (65%)]\tLoss: 0.334138\n",
      "Train Epoche: 1 [1825/2802 (65%)]\tLoss: 3.957575\n",
      "Train Epoche: 1 [1826/2802 (65%)]\tLoss: 2.302524\n",
      "Train Epoche: 1 [1827/2802 (65%)]\tLoss: 0.234207\n",
      "Train Epoche: 1 [1828/2802 (65%)]\tLoss: 13.373967\n",
      "Train Epoche: 1 [1829/2802 (65%)]\tLoss: 223.802658\n",
      "Train Epoche: 1 [1830/2802 (65%)]\tLoss: 0.007160\n",
      "Train Epoche: 1 [1831/2802 (65%)]\tLoss: 0.228002\n",
      "Train Epoche: 1 [1832/2802 (65%)]\tLoss: 24.891113\n",
      "Train Epoche: 1 [1833/2802 (65%)]\tLoss: 7.092209\n",
      "Train Epoche: 1 [1834/2802 (65%)]\tLoss: 0.120352\n",
      "Train Epoche: 1 [1835/2802 (65%)]\tLoss: 0.077328\n",
      "Train Epoche: 1 [1836/2802 (66%)]\tLoss: 16.371447\n",
      "Train Epoche: 1 [1837/2802 (66%)]\tLoss: 93.001846\n",
      "Train Epoche: 1 [1838/2802 (66%)]\tLoss: 0.410586\n",
      "Train Epoche: 1 [1839/2802 (66%)]\tLoss: 36.898693\n",
      "Train Epoche: 1 [1840/2802 (66%)]\tLoss: 0.762206\n",
      "Train Epoche: 1 [1841/2802 (66%)]\tLoss: 6.539133\n",
      "Train Epoche: 1 [1842/2802 (66%)]\tLoss: 1.821421\n",
      "Train Epoche: 1 [1843/2802 (66%)]\tLoss: 1.771138\n",
      "Train Epoche: 1 [1844/2802 (66%)]\tLoss: 0.250200\n",
      "Train Epoche: 1 [1845/2802 (66%)]\tLoss: 2.230973\n",
      "Train Epoche: 1 [1846/2802 (66%)]\tLoss: 12.864105\n",
      "Train Epoche: 1 [1847/2802 (66%)]\tLoss: 5.775506\n",
      "Train Epoche: 1 [1848/2802 (66%)]\tLoss: 2.285542\n",
      "Train Epoche: 1 [1849/2802 (66%)]\tLoss: 0.463672\n",
      "Train Epoche: 1 [1850/2802 (66%)]\tLoss: 2.052143\n",
      "Train Epoche: 1 [1851/2802 (66%)]\tLoss: 394.709381\n",
      "Train Epoche: 1 [1852/2802 (66%)]\tLoss: 0.443691\n",
      "Train Epoche: 1 [1853/2802 (66%)]\tLoss: 9.615726\n",
      "Train Epoche: 1 [1854/2802 (66%)]\tLoss: 15.422214\n",
      "Train Epoche: 1 [1855/2802 (66%)]\tLoss: 3.161921\n",
      "Train Epoche: 1 [1856/2802 (66%)]\tLoss: 57.235035\n",
      "Train Epoche: 1 [1857/2802 (66%)]\tLoss: 3.759019\n",
      "Train Epoche: 1 [1858/2802 (66%)]\tLoss: 1.228936\n",
      "Train Epoche: 1 [1859/2802 (66%)]\tLoss: 1.675867\n",
      "Train Epoche: 1 [1860/2802 (66%)]\tLoss: 0.003685\n",
      "Train Epoche: 1 [1861/2802 (66%)]\tLoss: 0.062997\n",
      "Train Epoche: 1 [1862/2802 (66%)]\tLoss: 125.837181\n",
      "Train Epoche: 1 [1863/2802 (66%)]\tLoss: 0.897941\n",
      "Train Epoche: 1 [1864/2802 (67%)]\tLoss: 0.412249\n",
      "Train Epoche: 1 [1865/2802 (67%)]\tLoss: 13.214121\n",
      "Train Epoche: 1 [1866/2802 (67%)]\tLoss: 1.382897\n",
      "Train Epoche: 1 [1867/2802 (67%)]\tLoss: 1.958125\n",
      "Train Epoche: 1 [1868/2802 (67%)]\tLoss: 19.870144\n",
      "Train Epoche: 1 [1869/2802 (67%)]\tLoss: 26.820093\n",
      "Train Epoche: 1 [1870/2802 (67%)]\tLoss: 25.479996\n",
      "Train Epoche: 1 [1871/2802 (67%)]\tLoss: 5.228767\n",
      "Train Epoche: 1 [1872/2802 (67%)]\tLoss: 0.334309\n",
      "Train Epoche: 1 [1873/2802 (67%)]\tLoss: 11.120559\n",
      "Train Epoche: 1 [1874/2802 (67%)]\tLoss: 0.244926\n",
      "Train Epoche: 1 [1875/2802 (67%)]\tLoss: 0.630316\n",
      "Train Epoche: 1 [1876/2802 (67%)]\tLoss: 0.109424\n",
      "Train Epoche: 1 [1877/2802 (67%)]\tLoss: 68.629021\n",
      "Train Epoche: 1 [1878/2802 (67%)]\tLoss: 0.196199\n",
      "Train Epoche: 1 [1879/2802 (67%)]\tLoss: 1.373490\n",
      "Train Epoche: 1 [1880/2802 (67%)]\tLoss: 28.375584\n",
      "Train Epoche: 1 [1881/2802 (67%)]\tLoss: 2.430619\n",
      "Train Epoche: 1 [1882/2802 (67%)]\tLoss: 1.594057\n",
      "Train Epoche: 1 [1883/2802 (67%)]\tLoss: 3.107300\n",
      "Train Epoche: 1 [1884/2802 (67%)]\tLoss: 0.004660\n",
      "Train Epoche: 1 [1885/2802 (67%)]\tLoss: 0.407707\n",
      "Train Epoche: 1 [1886/2802 (67%)]\tLoss: 9.308485\n",
      "Train Epoche: 1 [1887/2802 (67%)]\tLoss: 29.290030\n",
      "Train Epoche: 1 [1888/2802 (67%)]\tLoss: 0.900079\n",
      "Train Epoche: 1 [1889/2802 (67%)]\tLoss: 0.000332\n",
      "Train Epoche: 1 [1890/2802 (67%)]\tLoss: 0.263731\n",
      "Train Epoche: 1 [1891/2802 (67%)]\tLoss: 2.617071\n",
      "Train Epoche: 1 [1892/2802 (68%)]\tLoss: 2.076393\n",
      "Train Epoche: 1 [1893/2802 (68%)]\tLoss: 1.028103\n",
      "Train Epoche: 1 [1894/2802 (68%)]\tLoss: 1.331325\n",
      "Train Epoche: 1 [1895/2802 (68%)]\tLoss: 2.673633\n",
      "Train Epoche: 1 [1896/2802 (68%)]\tLoss: 9.411183\n",
      "Train Epoche: 1 [1897/2802 (68%)]\tLoss: 1.380280\n",
      "Train Epoche: 1 [1898/2802 (68%)]\tLoss: 11.600961\n",
      "Train Epoche: 1 [1899/2802 (68%)]\tLoss: 1.736038\n",
      "Train Epoche: 1 [1900/2802 (68%)]\tLoss: 12.053804\n",
      "Train Epoche: 1 [1901/2802 (68%)]\tLoss: 0.286944\n",
      "Train Epoche: 1 [1902/2802 (68%)]\tLoss: 0.060422\n",
      "Train Epoche: 1 [1903/2802 (68%)]\tLoss: 0.853549\n",
      "Train Epoche: 1 [1904/2802 (68%)]\tLoss: 20.501936\n",
      "Train Epoche: 1 [1905/2802 (68%)]\tLoss: 155.336960\n",
      "Train Epoche: 1 [1906/2802 (68%)]\tLoss: 3.610542\n",
      "Train Epoche: 1 [1907/2802 (68%)]\tLoss: 0.503228\n",
      "Train Epoche: 1 [1908/2802 (68%)]\tLoss: 133.752762\n",
      "Train Epoche: 1 [1909/2802 (68%)]\tLoss: 1.049041\n",
      "Train Epoche: 1 [1910/2802 (68%)]\tLoss: 1.329323\n",
      "Train Epoche: 1 [1911/2802 (68%)]\tLoss: 6.103874\n",
      "Train Epoche: 1 [1912/2802 (68%)]\tLoss: 68.667160\n",
      "Train Epoche: 1 [1913/2802 (68%)]\tLoss: 0.641138\n",
      "Train Epoche: 1 [1914/2802 (68%)]\tLoss: 8.680810\n",
      "Train Epoche: 1 [1915/2802 (68%)]\tLoss: 0.000139\n",
      "Train Epoche: 1 [1916/2802 (68%)]\tLoss: 47.377388\n",
      "Train Epoche: 1 [1917/2802 (68%)]\tLoss: 29.819317\n",
      "Train Epoche: 1 [1918/2802 (68%)]\tLoss: 0.006358\n",
      "Train Epoche: 1 [1919/2802 (68%)]\tLoss: 0.509563\n",
      "Train Epoche: 1 [1920/2802 (69%)]\tLoss: 0.161617\n",
      "Train Epoche: 1 [1921/2802 (69%)]\tLoss: 5.269792\n",
      "Train Epoche: 1 [1922/2802 (69%)]\tLoss: 17.128115\n",
      "Train Epoche: 1 [1923/2802 (69%)]\tLoss: 0.774884\n",
      "Train Epoche: 1 [1924/2802 (69%)]\tLoss: 99.299118\n",
      "Train Epoche: 1 [1925/2802 (69%)]\tLoss: 0.464318\n",
      "Train Epoche: 1 [1926/2802 (69%)]\tLoss: 92.076569\n",
      "Train Epoche: 1 [1927/2802 (69%)]\tLoss: 8.379009\n",
      "Train Epoche: 1 [1928/2802 (69%)]\tLoss: 19.663012\n",
      "Train Epoche: 1 [1929/2802 (69%)]\tLoss: 21.560669\n",
      "Train Epoche: 1 [1930/2802 (69%)]\tLoss: 5.752182\n",
      "Train Epoche: 1 [1931/2802 (69%)]\tLoss: 20.531776\n",
      "Train Epoche: 1 [1932/2802 (69%)]\tLoss: 44.717606\n",
      "Train Epoche: 1 [1933/2802 (69%)]\tLoss: 14.332379\n",
      "Train Epoche: 1 [1934/2802 (69%)]\tLoss: 1.980797\n",
      "Train Epoche: 1 [1935/2802 (69%)]\tLoss: 20.127110\n",
      "Train Epoche: 1 [1936/2802 (69%)]\tLoss: 0.068547\n",
      "Train Epoche: 1 [1937/2802 (69%)]\tLoss: 98.819420\n",
      "Train Epoche: 1 [1938/2802 (69%)]\tLoss: 13.266173\n",
      "Train Epoche: 1 [1939/2802 (69%)]\tLoss: 0.002713\n",
      "Train Epoche: 1 [1940/2802 (69%)]\tLoss: 1.135734\n",
      "Train Epoche: 1 [1941/2802 (69%)]\tLoss: 0.092422\n",
      "Train Epoche: 1 [1942/2802 (69%)]\tLoss: 0.893357\n",
      "Train Epoche: 1 [1943/2802 (69%)]\tLoss: 1.607741\n",
      "Train Epoche: 1 [1944/2802 (69%)]\tLoss: 5.546861\n",
      "Train Epoche: 1 [1945/2802 (69%)]\tLoss: 13.761418\n",
      "Train Epoche: 1 [1946/2802 (69%)]\tLoss: 1.514405\n",
      "Train Epoche: 1 [1947/2802 (69%)]\tLoss: 7.329908\n",
      "Train Epoche: 1 [1948/2802 (70%)]\tLoss: 33.807011\n",
      "Train Epoche: 1 [1949/2802 (70%)]\tLoss: 32.947552\n",
      "Train Epoche: 1 [1950/2802 (70%)]\tLoss: 1.468594\n",
      "Train Epoche: 1 [1951/2802 (70%)]\tLoss: 147.536804\n",
      "Train Epoche: 1 [1952/2802 (70%)]\tLoss: 10.604362\n",
      "Train Epoche: 1 [1953/2802 (70%)]\tLoss: 37.016930\n",
      "Train Epoche: 1 [1954/2802 (70%)]\tLoss: 0.334814\n",
      "Train Epoche: 1 [1955/2802 (70%)]\tLoss: 2.312220\n",
      "Train Epoche: 1 [1956/2802 (70%)]\tLoss: 2.479240\n",
      "Train Epoche: 1 [1957/2802 (70%)]\tLoss: 94.563721\n",
      "Train Epoche: 1 [1958/2802 (70%)]\tLoss: 104.799576\n",
      "Train Epoche: 1 [1959/2802 (70%)]\tLoss: 0.612978\n",
      "Train Epoche: 1 [1960/2802 (70%)]\tLoss: 4.668579\n",
      "Train Epoche: 1 [1961/2802 (70%)]\tLoss: 0.003556\n",
      "Train Epoche: 1 [1962/2802 (70%)]\tLoss: 0.003713\n",
      "Train Epoche: 1 [1963/2802 (70%)]\tLoss: 17.443785\n",
      "Train Epoche: 1 [1964/2802 (70%)]\tLoss: 2.507687\n",
      "Train Epoche: 1 [1965/2802 (70%)]\tLoss: 0.006115\n",
      "Train Epoche: 1 [1966/2802 (70%)]\tLoss: 3.370203\n",
      "Train Epoche: 1 [1967/2802 (70%)]\tLoss: 0.013469\n",
      "Train Epoche: 1 [1968/2802 (70%)]\tLoss: 58.198391\n",
      "Train Epoche: 1 [1969/2802 (70%)]\tLoss: 37.141880\n",
      "Train Epoche: 1 [1970/2802 (70%)]\tLoss: 1.563540\n",
      "Train Epoche: 1 [1971/2802 (70%)]\tLoss: 52.641891\n",
      "Train Epoche: 1 [1972/2802 (70%)]\tLoss: 0.654621\n",
      "Train Epoche: 1 [1973/2802 (70%)]\tLoss: 22.999754\n",
      "Train Epoche: 1 [1974/2802 (70%)]\tLoss: 32.758564\n",
      "Train Epoche: 1 [1975/2802 (70%)]\tLoss: 38.837852\n",
      "Train Epoche: 1 [1976/2802 (71%)]\tLoss: 5.067316\n",
      "Train Epoche: 1 [1977/2802 (71%)]\tLoss: 0.520708\n",
      "Train Epoche: 1 [1978/2802 (71%)]\tLoss: 3.799365\n",
      "Train Epoche: 1 [1979/2802 (71%)]\tLoss: 23.692690\n",
      "Train Epoche: 1 [1980/2802 (71%)]\tLoss: 5.571939\n",
      "Train Epoche: 1 [1981/2802 (71%)]\tLoss: 0.294634\n",
      "Train Epoche: 1 [1982/2802 (71%)]\tLoss: 6.622783\n",
      "Train Epoche: 1 [1983/2802 (71%)]\tLoss: 7.152560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1984/2802 (71%)]\tLoss: 28.801041\n",
      "Train Epoche: 1 [1985/2802 (71%)]\tLoss: 9.742363\n",
      "Train Epoche: 1 [1986/2802 (71%)]\tLoss: 346.095062\n",
      "Train Epoche: 1 [1987/2802 (71%)]\tLoss: 99.811012\n",
      "Train Epoche: 1 [1988/2802 (71%)]\tLoss: 2.222069\n",
      "Train Epoche: 1 [1989/2802 (71%)]\tLoss: 8.436370\n",
      "Train Epoche: 1 [1990/2802 (71%)]\tLoss: 26.229919\n",
      "Train Epoche: 1 [1991/2802 (71%)]\tLoss: 7.802294\n",
      "Train Epoche: 1 [1992/2802 (71%)]\tLoss: 0.001358\n",
      "Train Epoche: 1 [1993/2802 (71%)]\tLoss: 1.599565\n",
      "Train Epoche: 1 [1994/2802 (71%)]\tLoss: 2.606235\n",
      "Train Epoche: 1 [1995/2802 (71%)]\tLoss: 0.004309\n",
      "Train Epoche: 1 [1996/2802 (71%)]\tLoss: 0.916621\n",
      "Train Epoche: 1 [1997/2802 (71%)]\tLoss: 17.065338\n",
      "Train Epoche: 1 [1998/2802 (71%)]\tLoss: 41.607246\n",
      "Train Epoche: 1 [1999/2802 (71%)]\tLoss: 16.588419\n",
      "Train Epoche: 1 [2000/2802 (71%)]\tLoss: 133.430969\n",
      "Train Epoche: 1 [2001/2802 (71%)]\tLoss: 4.211209\n",
      "Train Epoche: 1 [2002/2802 (71%)]\tLoss: 1.291369\n",
      "Train Epoche: 1 [2003/2802 (71%)]\tLoss: 2.443034\n",
      "Train Epoche: 1 [2004/2802 (72%)]\tLoss: 14.881543\n",
      "Train Epoche: 1 [2005/2802 (72%)]\tLoss: 31.460787\n",
      "Train Epoche: 1 [2006/2802 (72%)]\tLoss: 3.938490\n",
      "Train Epoche: 1 [2007/2802 (72%)]\tLoss: 2.438936\n",
      "Train Epoche: 1 [2008/2802 (72%)]\tLoss: 31.132420\n",
      "Train Epoche: 1 [2009/2802 (72%)]\tLoss: 5.357267\n",
      "Train Epoche: 1 [2010/2802 (72%)]\tLoss: 1.133670\n",
      "Train Epoche: 1 [2011/2802 (72%)]\tLoss: 3.479893\n",
      "Train Epoche: 1 [2012/2802 (72%)]\tLoss: 0.011228\n",
      "Train Epoche: 1 [2013/2802 (72%)]\tLoss: 3.644264\n",
      "Train Epoche: 1 [2014/2802 (72%)]\tLoss: 0.011538\n",
      "Train Epoche: 1 [2015/2802 (72%)]\tLoss: 18.818338\n",
      "Train Epoche: 1 [2016/2802 (72%)]\tLoss: 64.715767\n",
      "Train Epoche: 1 [2017/2802 (72%)]\tLoss: 85.612419\n",
      "Train Epoche: 1 [2018/2802 (72%)]\tLoss: 2.023318\n",
      "Train Epoche: 1 [2019/2802 (72%)]\tLoss: 76.042328\n",
      "Train Epoche: 1 [2020/2802 (72%)]\tLoss: 0.507376\n",
      "Train Epoche: 1 [2021/2802 (72%)]\tLoss: 3.661434\n",
      "Train Epoche: 1 [2022/2802 (72%)]\tLoss: 0.048595\n",
      "Train Epoche: 1 [2023/2802 (72%)]\tLoss: 27.050255\n",
      "Train Epoche: 1 [2024/2802 (72%)]\tLoss: 1.057438\n",
      "Train Epoche: 1 [2025/2802 (72%)]\tLoss: 3.175115\n",
      "Train Epoche: 1 [2026/2802 (72%)]\tLoss: 4.058090\n",
      "Train Epoche: 1 [2027/2802 (72%)]\tLoss: 91.702797\n",
      "Train Epoche: 1 [2028/2802 (72%)]\tLoss: 2.176368\n",
      "Train Epoche: 1 [2029/2802 (72%)]\tLoss: 51.641117\n",
      "Train Epoche: 1 [2030/2802 (72%)]\tLoss: 14.048370\n",
      "Train Epoche: 1 [2031/2802 (72%)]\tLoss: 0.003916\n",
      "Train Epoche: 1 [2032/2802 (73%)]\tLoss: 0.578798\n",
      "Train Epoche: 1 [2033/2802 (73%)]\tLoss: 4.002244\n",
      "Train Epoche: 1 [2034/2802 (73%)]\tLoss: 10.728771\n",
      "Train Epoche: 1 [2035/2802 (73%)]\tLoss: 5.299726\n",
      "Train Epoche: 1 [2036/2802 (73%)]\tLoss: 11.094959\n",
      "Train Epoche: 1 [2037/2802 (73%)]\tLoss: 2.777407\n",
      "Train Epoche: 1 [2038/2802 (73%)]\tLoss: 41.974388\n",
      "Train Epoche: 1 [2039/2802 (73%)]\tLoss: 8.455125\n",
      "Train Epoche: 1 [2040/2802 (73%)]\tLoss: 6.860212\n",
      "Train Epoche: 1 [2041/2802 (73%)]\tLoss: 1.549049\n",
      "Train Epoche: 1 [2042/2802 (73%)]\tLoss: 288.884460\n",
      "Train Epoche: 1 [2043/2802 (73%)]\tLoss: 6.166046\n",
      "Train Epoche: 1 [2044/2802 (73%)]\tLoss: 7.309460\n",
      "Train Epoche: 1 [2045/2802 (73%)]\tLoss: 2.196524\n",
      "Train Epoche: 1 [2046/2802 (73%)]\tLoss: 5.310987\n",
      "Train Epoche: 1 [2047/2802 (73%)]\tLoss: 45.202892\n",
      "Train Epoche: 1 [2048/2802 (73%)]\tLoss: 2.567578\n",
      "Train Epoche: 1 [2049/2802 (73%)]\tLoss: 0.076707\n",
      "Train Epoche: 1 [2050/2802 (73%)]\tLoss: 41.141594\n",
      "Train Epoche: 1 [2051/2802 (73%)]\tLoss: 1.289018\n",
      "Train Epoche: 1 [2052/2802 (73%)]\tLoss: 0.068924\n",
      "Train Epoche: 1 [2053/2802 (73%)]\tLoss: 1.015894\n",
      "Train Epoche: 1 [2054/2802 (73%)]\tLoss: 8.228334\n",
      "Train Epoche: 1 [2055/2802 (73%)]\tLoss: 47.532681\n",
      "Train Epoche: 1 [2056/2802 (73%)]\tLoss: 11.127289\n",
      "Train Epoche: 1 [2057/2802 (73%)]\tLoss: 3.800748\n",
      "Train Epoche: 1 [2058/2802 (73%)]\tLoss: 0.000866\n",
      "Train Epoche: 1 [2059/2802 (73%)]\tLoss: 8.900478\n",
      "Train Epoche: 1 [2060/2802 (74%)]\tLoss: 8.504070\n",
      "Train Epoche: 1 [2061/2802 (74%)]\tLoss: 2.426445\n",
      "Train Epoche: 1 [2062/2802 (74%)]\tLoss: 155.453705\n",
      "Train Epoche: 1 [2063/2802 (74%)]\tLoss: 5.979954\n",
      "Train Epoche: 1 [2064/2802 (74%)]\tLoss: 1.593778\n",
      "Train Epoche: 1 [2065/2802 (74%)]\tLoss: 10.439428\n",
      "Train Epoche: 1 [2066/2802 (74%)]\tLoss: 4.097249\n",
      "Train Epoche: 1 [2067/2802 (74%)]\tLoss: 21.036036\n",
      "Train Epoche: 1 [2068/2802 (74%)]\tLoss: 1.545903\n",
      "Train Epoche: 1 [2069/2802 (74%)]\tLoss: 48.705482\n",
      "Train Epoche: 1 [2070/2802 (74%)]\tLoss: 0.571010\n",
      "Train Epoche: 1 [2071/2802 (74%)]\tLoss: 31.345266\n",
      "Train Epoche: 1 [2072/2802 (74%)]\tLoss: 5.486376\n",
      "Train Epoche: 1 [2073/2802 (74%)]\tLoss: 3.380295\n",
      "Train Epoche: 1 [2074/2802 (74%)]\tLoss: 108.928307\n",
      "Train Epoche: 1 [2075/2802 (74%)]\tLoss: 0.363555\n",
      "Train Epoche: 1 [2076/2802 (74%)]\tLoss: 22.217901\n",
      "Train Epoche: 1 [2077/2802 (74%)]\tLoss: 84.180855\n",
      "Train Epoche: 1 [2078/2802 (74%)]\tLoss: 15.267523\n",
      "Train Epoche: 1 [2079/2802 (74%)]\tLoss: 5.764906\n",
      "Train Epoche: 1 [2080/2802 (74%)]\tLoss: 0.005988\n",
      "Train Epoche: 1 [2081/2802 (74%)]\tLoss: 3.395056\n",
      "Train Epoche: 1 [2082/2802 (74%)]\tLoss: 2.321329\n",
      "Train Epoche: 1 [2083/2802 (74%)]\tLoss: 22.109985\n",
      "Train Epoche: 1 [2084/2802 (74%)]\tLoss: 0.004428\n",
      "Train Epoche: 1 [2085/2802 (74%)]\tLoss: 18.091005\n",
      "Train Epoche: 1 [2086/2802 (74%)]\tLoss: 2.052791\n",
      "Train Epoche: 1 [2087/2802 (74%)]\tLoss: 36.621395\n",
      "Train Epoche: 1 [2088/2802 (75%)]\tLoss: 18.693722\n",
      "Train Epoche: 1 [2089/2802 (75%)]\tLoss: 0.018548\n",
      "Train Epoche: 1 [2090/2802 (75%)]\tLoss: 1.569286\n",
      "Train Epoche: 1 [2091/2802 (75%)]\tLoss: 18.229916\n",
      "Train Epoche: 1 [2092/2802 (75%)]\tLoss: 7.051348\n",
      "Train Epoche: 1 [2093/2802 (75%)]\tLoss: 5.237546\n",
      "Train Epoche: 1 [2094/2802 (75%)]\tLoss: 1.902621\n",
      "Train Epoche: 1 [2095/2802 (75%)]\tLoss: 4.154393\n",
      "Train Epoche: 1 [2096/2802 (75%)]\tLoss: 0.000659\n",
      "Train Epoche: 1 [2097/2802 (75%)]\tLoss: 1.177571\n",
      "Train Epoche: 1 [2098/2802 (75%)]\tLoss: 8.995492\n",
      "Train Epoche: 1 [2099/2802 (75%)]\tLoss: 0.312859\n",
      "Train Epoche: 1 [2100/2802 (75%)]\tLoss: 0.938405\n",
      "Train Epoche: 1 [2101/2802 (75%)]\tLoss: 3.212302\n",
      "Train Epoche: 1 [2102/2802 (75%)]\tLoss: 0.000758\n",
      "Train Epoche: 1 [2103/2802 (75%)]\tLoss: 0.553406\n",
      "Train Epoche: 1 [2104/2802 (75%)]\tLoss: 83.889923\n",
      "Train Epoche: 1 [2105/2802 (75%)]\tLoss: 6.950506\n",
      "Train Epoche: 1 [2106/2802 (75%)]\tLoss: 0.652430\n",
      "Train Epoche: 1 [2107/2802 (75%)]\tLoss: 1.593672\n",
      "Train Epoche: 1 [2108/2802 (75%)]\tLoss: 326.589417\n",
      "Train Epoche: 1 [2109/2802 (75%)]\tLoss: 35.821648\n",
      "Train Epoche: 1 [2110/2802 (75%)]\tLoss: 10.102869\n",
      "Train Epoche: 1 [2111/2802 (75%)]\tLoss: 1.345832\n",
      "Train Epoche: 1 [2112/2802 (75%)]\tLoss: 0.381368\n",
      "Train Epoche: 1 [2113/2802 (75%)]\tLoss: 2.825010\n",
      "Train Epoche: 1 [2114/2802 (75%)]\tLoss: 2.143304\n",
      "Train Epoche: 1 [2115/2802 (75%)]\tLoss: 134.439651\n",
      "Train Epoche: 1 [2116/2802 (76%)]\tLoss: 3.229085\n",
      "Train Epoche: 1 [2117/2802 (76%)]\tLoss: 12.938163\n",
      "Train Epoche: 1 [2118/2802 (76%)]\tLoss: 9.450193\n",
      "Train Epoche: 1 [2119/2802 (76%)]\tLoss: 0.596521\n",
      "Train Epoche: 1 [2120/2802 (76%)]\tLoss: 0.982983\n",
      "Train Epoche: 1 [2121/2802 (76%)]\tLoss: 12.459231\n",
      "Train Epoche: 1 [2122/2802 (76%)]\tLoss: 3.081622\n",
      "Train Epoche: 1 [2123/2802 (76%)]\tLoss: 46.581650\n",
      "Train Epoche: 1 [2124/2802 (76%)]\tLoss: 47.475246\n",
      "Train Epoche: 1 [2125/2802 (76%)]\tLoss: 11.269067\n",
      "Train Epoche: 1 [2126/2802 (76%)]\tLoss: 2.232284\n",
      "Train Epoche: 1 [2127/2802 (76%)]\tLoss: 10.414127\n",
      "Train Epoche: 1 [2128/2802 (76%)]\tLoss: 6.000663\n",
      "Train Epoche: 1 [2129/2802 (76%)]\tLoss: 0.319693\n",
      "Train Epoche: 1 [2130/2802 (76%)]\tLoss: 6.370483\n",
      "Train Epoche: 1 [2131/2802 (76%)]\tLoss: 2.082685\n",
      "Train Epoche: 1 [2132/2802 (76%)]\tLoss: 4.857622\n",
      "Train Epoche: 1 [2133/2802 (76%)]\tLoss: 13.391355\n",
      "Train Epoche: 1 [2134/2802 (76%)]\tLoss: 386.931488\n",
      "Train Epoche: 1 [2135/2802 (76%)]\tLoss: 4.711736\n",
      "Train Epoche: 1 [2136/2802 (76%)]\tLoss: 6.323848\n",
      "Train Epoche: 1 [2137/2802 (76%)]\tLoss: 50.179565\n",
      "Train Epoche: 1 [2138/2802 (76%)]\tLoss: 4.769056\n",
      "Train Epoche: 1 [2139/2802 (76%)]\tLoss: 2.101533\n",
      "Train Epoche: 1 [2140/2802 (76%)]\tLoss: 12.668403\n",
      "Train Epoche: 1 [2141/2802 (76%)]\tLoss: 15.104474\n",
      "Train Epoche: 1 [2142/2802 (76%)]\tLoss: 0.195415\n",
      "Train Epoche: 1 [2143/2802 (76%)]\tLoss: 7.711684\n",
      "Train Epoche: 1 [2144/2802 (77%)]\tLoss: 0.992227\n",
      "Train Epoche: 1 [2145/2802 (77%)]\tLoss: 53.449589\n",
      "Train Epoche: 1 [2146/2802 (77%)]\tLoss: 1.147801\n",
      "Train Epoche: 1 [2147/2802 (77%)]\tLoss: 0.002893\n",
      "Train Epoche: 1 [2148/2802 (77%)]\tLoss: 0.846368\n",
      "Train Epoche: 1 [2149/2802 (77%)]\tLoss: 195.162430\n",
      "Train Epoche: 1 [2150/2802 (77%)]\tLoss: 35.693199\n",
      "Train Epoche: 1 [2151/2802 (77%)]\tLoss: 1.981946\n",
      "Train Epoche: 1 [2152/2802 (77%)]\tLoss: 5.574771\n",
      "Train Epoche: 1 [2153/2802 (77%)]\tLoss: 5.462460\n",
      "Train Epoche: 1 [2154/2802 (77%)]\tLoss: 3.236486\n",
      "Train Epoche: 1 [2155/2802 (77%)]\tLoss: 142.334259\n",
      "Train Epoche: 1 [2156/2802 (77%)]\tLoss: 10.344549\n",
      "Train Epoche: 1 [2157/2802 (77%)]\tLoss: 8.472979\n",
      "Train Epoche: 1 [2158/2802 (77%)]\tLoss: 4.935632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [2159/2802 (77%)]\tLoss: 10.744508\n",
      "Train Epoche: 1 [2160/2802 (77%)]\tLoss: 3.473114\n",
      "Train Epoche: 1 [2161/2802 (77%)]\tLoss: 0.028790\n",
      "Train Epoche: 1 [2162/2802 (77%)]\tLoss: 8.023280\n",
      "Train Epoche: 1 [2163/2802 (77%)]\tLoss: 0.705555\n",
      "Train Epoche: 1 [2164/2802 (77%)]\tLoss: 11.505827\n",
      "Train Epoche: 1 [2165/2802 (77%)]\tLoss: 2.802680\n",
      "Train Epoche: 1 [2166/2802 (77%)]\tLoss: 7.095735\n",
      "Train Epoche: 1 [2167/2802 (77%)]\tLoss: 4.594872\n",
      "Train Epoche: 1 [2168/2802 (77%)]\tLoss: 43.364525\n",
      "Train Epoche: 1 [2169/2802 (77%)]\tLoss: 16.125477\n",
      "Train Epoche: 1 [2170/2802 (77%)]\tLoss: 3.946674\n",
      "Train Epoche: 1 [2171/2802 (77%)]\tLoss: 0.003908\n",
      "Train Epoche: 1 [2172/2802 (78%)]\tLoss: 4.801601\n",
      "Train Epoche: 1 [2173/2802 (78%)]\tLoss: 3.541328\n",
      "Train Epoche: 1 [2174/2802 (78%)]\tLoss: 17.922926\n",
      "Train Epoche: 1 [2175/2802 (78%)]\tLoss: 2.955208\n",
      "Train Epoche: 1 [2176/2802 (78%)]\tLoss: 12.537536\n",
      "Train Epoche: 1 [2177/2802 (78%)]\tLoss: 0.275479\n",
      "Train Epoche: 1 [2178/2802 (78%)]\tLoss: 0.310629\n",
      "Train Epoche: 1 [2179/2802 (78%)]\tLoss: 3.315772\n",
      "Train Epoche: 1 [2180/2802 (78%)]\tLoss: 0.817206\n",
      "Train Epoche: 1 [2181/2802 (78%)]\tLoss: 1.314890\n",
      "Train Epoche: 1 [2182/2802 (78%)]\tLoss: 2.910473\n",
      "Train Epoche: 1 [2183/2802 (78%)]\tLoss: 4.031378\n",
      "Train Epoche: 1 [2184/2802 (78%)]\tLoss: 29.976381\n",
      "Train Epoche: 1 [2185/2802 (78%)]\tLoss: 27.539534\n",
      "Train Epoche: 1 [2186/2802 (78%)]\tLoss: 169.699539\n",
      "Train Epoche: 1 [2187/2802 (78%)]\tLoss: 1.938955\n",
      "Train Epoche: 1 [2188/2802 (78%)]\tLoss: 3.785457\n",
      "Train Epoche: 1 [2189/2802 (78%)]\tLoss: 0.180237\n",
      "Train Epoche: 1 [2190/2802 (78%)]\tLoss: 5.023071\n",
      "Train Epoche: 1 [2191/2802 (78%)]\tLoss: 26.819738\n",
      "Train Epoche: 1 [2192/2802 (78%)]\tLoss: 5.284050\n",
      "Train Epoche: 1 [2193/2802 (78%)]\tLoss: 5.817269\n",
      "Train Epoche: 1 [2194/2802 (78%)]\tLoss: 1.687818\n",
      "Train Epoche: 1 [2195/2802 (78%)]\tLoss: 1.019804\n",
      "Train Epoche: 1 [2196/2802 (78%)]\tLoss: 23.236378\n",
      "Train Epoche: 1 [2197/2802 (78%)]\tLoss: 0.015916\n",
      "Train Epoche: 1 [2198/2802 (78%)]\tLoss: 0.325992\n",
      "Train Epoche: 1 [2199/2802 (78%)]\tLoss: 29.597342\n",
      "Train Epoche: 1 [2200/2802 (79%)]\tLoss: 54.976990\n",
      "Train Epoche: 1 [2201/2802 (79%)]\tLoss: 0.025086\n",
      "Train Epoche: 1 [2202/2802 (79%)]\tLoss: 0.129401\n",
      "Train Epoche: 1 [2203/2802 (79%)]\tLoss: 0.014063\n",
      "Train Epoche: 1 [2204/2802 (79%)]\tLoss: 6.304813\n",
      "Train Epoche: 1 [2205/2802 (79%)]\tLoss: 53.013241\n",
      "Train Epoche: 1 [2206/2802 (79%)]\tLoss: 18.380640\n",
      "Train Epoche: 1 [2207/2802 (79%)]\tLoss: 2.709127\n",
      "Train Epoche: 1 [2208/2802 (79%)]\tLoss: 0.503295\n",
      "Train Epoche: 1 [2209/2802 (79%)]\tLoss: 5.075986\n",
      "Train Epoche: 1 [2210/2802 (79%)]\tLoss: 0.495137\n",
      "Train Epoche: 1 [2211/2802 (79%)]\tLoss: 0.054607\n",
      "Train Epoche: 1 [2212/2802 (79%)]\tLoss: 0.877548\n",
      "Train Epoche: 1 [2213/2802 (79%)]\tLoss: 95.111977\n",
      "Train Epoche: 1 [2214/2802 (79%)]\tLoss: 2.886972\n",
      "Train Epoche: 1 [2215/2802 (79%)]\tLoss: 6.382391\n",
      "Train Epoche: 1 [2216/2802 (79%)]\tLoss: 0.007993\n",
      "Train Epoche: 1 [2217/2802 (79%)]\tLoss: 6.941847\n",
      "Train Epoche: 1 [2218/2802 (79%)]\tLoss: 0.002738\n",
      "Train Epoche: 1 [2219/2802 (79%)]\tLoss: 47.337116\n",
      "Train Epoche: 1 [2220/2802 (79%)]\tLoss: 11.286784\n",
      "Train Epoche: 1 [2221/2802 (79%)]\tLoss: 0.000470\n",
      "Train Epoche: 1 [2222/2802 (79%)]\tLoss: 0.727708\n",
      "Train Epoche: 1 [2223/2802 (79%)]\tLoss: 8.435614\n",
      "Train Epoche: 1 [2224/2802 (79%)]\tLoss: 20.755892\n",
      "Train Epoche: 1 [2225/2802 (79%)]\tLoss: 0.710275\n",
      "Train Epoche: 1 [2226/2802 (79%)]\tLoss: 1.736578\n",
      "Train Epoche: 1 [2227/2802 (79%)]\tLoss: 3.293703\n",
      "Train Epoche: 1 [2228/2802 (80%)]\tLoss: 1.145687\n",
      "Train Epoche: 1 [2229/2802 (80%)]\tLoss: 23.123592\n",
      "Train Epoche: 1 [2230/2802 (80%)]\tLoss: 10.592822\n",
      "Train Epoche: 1 [2231/2802 (80%)]\tLoss: 0.162966\n",
      "Train Epoche: 1 [2232/2802 (80%)]\tLoss: 2.284853\n",
      "Train Epoche: 1 [2233/2802 (80%)]\tLoss: 1.791405\n",
      "Train Epoche: 1 [2234/2802 (80%)]\tLoss: 329.770081\n",
      "Train Epoche: 1 [2235/2802 (80%)]\tLoss: 1.426017\n",
      "Train Epoche: 1 [2236/2802 (80%)]\tLoss: 6.705385\n",
      "Train Epoche: 1 [2237/2802 (80%)]\tLoss: 7.183930\n",
      "Train Epoche: 1 [2238/2802 (80%)]\tLoss: 34.233997\n",
      "Train Epoche: 1 [2239/2802 (80%)]\tLoss: 0.134624\n",
      "Train Epoche: 1 [2240/2802 (80%)]\tLoss: 0.514747\n",
      "Train Epoche: 1 [2241/2802 (80%)]\tLoss: 1.435241\n",
      "Train Epoche: 1 [2242/2802 (80%)]\tLoss: 0.453311\n",
      "Train Epoche: 1 [2243/2802 (80%)]\tLoss: 442.026306\n",
      "Train Epoche: 1 [2244/2802 (80%)]\tLoss: 6.959877\n",
      "Train Epoche: 1 [2245/2802 (80%)]\tLoss: 1.220961\n",
      "Train Epoche: 1 [2246/2802 (80%)]\tLoss: 2.110440\n",
      "Train Epoche: 1 [2247/2802 (80%)]\tLoss: 0.998509\n",
      "Train Epoche: 1 [2248/2802 (80%)]\tLoss: 1.092097\n",
      "Train Epoche: 1 [2249/2802 (80%)]\tLoss: 11.664641\n",
      "Train Epoche: 1 [2250/2802 (80%)]\tLoss: 19.535744\n",
      "Train Epoche: 1 [2251/2802 (80%)]\tLoss: 0.151221\n",
      "Train Epoche: 1 [2252/2802 (80%)]\tLoss: 1.461426\n",
      "Train Epoche: 1 [2253/2802 (80%)]\tLoss: 0.077401\n",
      "Train Epoche: 1 [2254/2802 (80%)]\tLoss: 8.463772\n",
      "Train Epoche: 1 [2255/2802 (80%)]\tLoss: 2.926194\n",
      "Train Epoche: 1 [2256/2802 (81%)]\tLoss: 5.419072\n",
      "Train Epoche: 1 [2257/2802 (81%)]\tLoss: 10.670424\n",
      "Train Epoche: 1 [2258/2802 (81%)]\tLoss: 29.956522\n",
      "Train Epoche: 1 [2259/2802 (81%)]\tLoss: 1.944497\n",
      "Train Epoche: 1 [2260/2802 (81%)]\tLoss: 0.672032\n",
      "Train Epoche: 1 [2261/2802 (81%)]\tLoss: 26.681807\n",
      "Train Epoche: 1 [2262/2802 (81%)]\tLoss: 2.349495\n",
      "Train Epoche: 1 [2263/2802 (81%)]\tLoss: 0.197673\n",
      "Train Epoche: 1 [2264/2802 (81%)]\tLoss: 0.719224\n",
      "Train Epoche: 1 [2265/2802 (81%)]\tLoss: 1.658942\n",
      "Train Epoche: 1 [2266/2802 (81%)]\tLoss: 7.033744\n",
      "Train Epoche: 1 [2267/2802 (81%)]\tLoss: 2.284610\n",
      "Train Epoche: 1 [2268/2802 (81%)]\tLoss: 3.465647\n",
      "Train Epoche: 1 [2269/2802 (81%)]\tLoss: 2.200354\n",
      "Train Epoche: 1 [2270/2802 (81%)]\tLoss: 5.888182\n",
      "Train Epoche: 1 [2271/2802 (81%)]\tLoss: 0.273237\n",
      "Train Epoche: 1 [2272/2802 (81%)]\tLoss: 1.066039\n",
      "Train Epoche: 1 [2273/2802 (81%)]\tLoss: 14.945264\n",
      "Train Epoche: 1 [2274/2802 (81%)]\tLoss: 11.777035\n",
      "Train Epoche: 1 [2275/2802 (81%)]\tLoss: 0.483999\n",
      "Train Epoche: 1 [2276/2802 (81%)]\tLoss: 0.656606\n",
      "Train Epoche: 1 [2277/2802 (81%)]\tLoss: 3.814986\n",
      "Train Epoche: 1 [2278/2802 (81%)]\tLoss: 0.231809\n",
      "Train Epoche: 1 [2279/2802 (81%)]\tLoss: 16.417286\n",
      "Train Epoche: 1 [2280/2802 (81%)]\tLoss: 5.637941\n",
      "Train Epoche: 1 [2281/2802 (81%)]\tLoss: 4.207042\n",
      "Train Epoche: 1 [2282/2802 (81%)]\tLoss: 0.167937\n",
      "Train Epoche: 1 [2283/2802 (81%)]\tLoss: 88.353951\n",
      "Train Epoche: 1 [2284/2802 (82%)]\tLoss: 30.999777\n",
      "Train Epoche: 1 [2285/2802 (82%)]\tLoss: 0.964050\n",
      "Train Epoche: 1 [2286/2802 (82%)]\tLoss: 24.764788\n",
      "Train Epoche: 1 [2287/2802 (82%)]\tLoss: 154.754013\n",
      "Train Epoche: 1 [2288/2802 (82%)]\tLoss: 6.437224\n",
      "Train Epoche: 1 [2289/2802 (82%)]\tLoss: 0.000105\n",
      "Train Epoche: 1 [2290/2802 (82%)]\tLoss: 312.274750\n",
      "Train Epoche: 1 [2291/2802 (82%)]\tLoss: 3.054688\n",
      "Train Epoche: 1 [2292/2802 (82%)]\tLoss: 0.820161\n",
      "Train Epoche: 1 [2293/2802 (82%)]\tLoss: 7.230255\n",
      "Train Epoche: 1 [2294/2802 (82%)]\tLoss: 0.435597\n",
      "Train Epoche: 1 [2295/2802 (82%)]\tLoss: 1.916046\n",
      "Train Epoche: 1 [2296/2802 (82%)]\tLoss: 1.873378\n",
      "Train Epoche: 1 [2297/2802 (82%)]\tLoss: 0.017416\n",
      "Train Epoche: 1 [2298/2802 (82%)]\tLoss: 54.233051\n",
      "Train Epoche: 1 [2299/2802 (82%)]\tLoss: 8.377969\n",
      "Train Epoche: 1 [2300/2802 (82%)]\tLoss: 4.098513\n",
      "Train Epoche: 1 [2301/2802 (82%)]\tLoss: 0.287762\n",
      "Train Epoche: 1 [2302/2802 (82%)]\tLoss: 1.060975\n",
      "Train Epoche: 1 [2303/2802 (82%)]\tLoss: 89.401154\n",
      "Train Epoche: 1 [2304/2802 (82%)]\tLoss: 65.033653\n",
      "Train Epoche: 1 [2305/2802 (82%)]\tLoss: 2.184074\n",
      "Train Epoche: 1 [2306/2802 (82%)]\tLoss: 85.072853\n",
      "Train Epoche: 1 [2307/2802 (82%)]\tLoss: 0.515911\n",
      "Train Epoche: 1 [2308/2802 (82%)]\tLoss: 45.723976\n",
      "Train Epoche: 1 [2309/2802 (82%)]\tLoss: 0.008040\n",
      "Train Epoche: 1 [2310/2802 (82%)]\tLoss: 10.360591\n",
      "Train Epoche: 1 [2311/2802 (82%)]\tLoss: 4.364039\n",
      "Train Epoche: 1 [2312/2802 (83%)]\tLoss: 11.157513\n",
      "Train Epoche: 1 [2313/2802 (83%)]\tLoss: 10.951488\n",
      "Train Epoche: 1 [2314/2802 (83%)]\tLoss: 0.152135\n",
      "Train Epoche: 1 [2315/2802 (83%)]\tLoss: 11.153814\n",
      "Train Epoche: 1 [2316/2802 (83%)]\tLoss: 120.999123\n",
      "Train Epoche: 1 [2317/2802 (83%)]\tLoss: 1.309046\n",
      "Train Epoche: 1 [2318/2802 (83%)]\tLoss: 0.059048\n",
      "Train Epoche: 1 [2319/2802 (83%)]\tLoss: 0.236535\n",
      "Train Epoche: 1 [2320/2802 (83%)]\tLoss: 8.232087\n",
      "Train Epoche: 1 [2321/2802 (83%)]\tLoss: 6.392710\n",
      "Train Epoche: 1 [2322/2802 (83%)]\tLoss: 2.686999\n",
      "Train Epoche: 1 [2323/2802 (83%)]\tLoss: 0.285580\n",
      "Train Epoche: 1 [2324/2802 (83%)]\tLoss: 18.631033\n",
      "Train Epoche: 1 [2325/2802 (83%)]\tLoss: 11.459860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [2326/2802 (83%)]\tLoss: 17.126127\n",
      "Train Epoche: 1 [2327/2802 (83%)]\tLoss: 8.331086\n",
      "Train Epoche: 1 [2328/2802 (83%)]\tLoss: 101.405800\n",
      "Train Epoche: 1 [2329/2802 (83%)]\tLoss: 3.716801\n",
      "Train Epoche: 1 [2330/2802 (83%)]\tLoss: 26.956017\n",
      "Train Epoche: 1 [2331/2802 (83%)]\tLoss: 17.799204\n",
      "Train Epoche: 1 [2332/2802 (83%)]\tLoss: 142.864563\n",
      "Train Epoche: 1 [2333/2802 (83%)]\tLoss: 2.209960\n",
      "Train Epoche: 1 [2334/2802 (83%)]\tLoss: 2.427697\n",
      "Train Epoche: 1 [2335/2802 (83%)]\tLoss: 47.615009\n",
      "Train Epoche: 1 [2336/2802 (83%)]\tLoss: 9.032158\n",
      "Train Epoche: 1 [2337/2802 (83%)]\tLoss: 0.066555\n",
      "Train Epoche: 1 [2338/2802 (83%)]\tLoss: 19.043041\n",
      "Train Epoche: 1 [2339/2802 (83%)]\tLoss: 8.797815\n",
      "Train Epoche: 1 [2340/2802 (84%)]\tLoss: 3.662730\n",
      "Train Epoche: 1 [2341/2802 (84%)]\tLoss: 3.621100\n",
      "Train Epoche: 1 [2342/2802 (84%)]\tLoss: 0.645072\n",
      "Train Epoche: 1 [2343/2802 (84%)]\tLoss: 2.077878\n",
      "Train Epoche: 1 [2344/2802 (84%)]\tLoss: 45.718933\n",
      "Train Epoche: 1 [2345/2802 (84%)]\tLoss: 1.368632\n",
      "Train Epoche: 1 [2346/2802 (84%)]\tLoss: 111.086250\n",
      "Train Epoche: 1 [2347/2802 (84%)]\tLoss: 5.016281\n",
      "Train Epoche: 1 [2348/2802 (84%)]\tLoss: 0.549528\n",
      "Train Epoche: 1 [2349/2802 (84%)]\tLoss: 8.923368\n",
      "Train Epoche: 1 [2350/2802 (84%)]\tLoss: 22.514599\n",
      "Train Epoche: 1 [2351/2802 (84%)]\tLoss: 2.379364\n",
      "Train Epoche: 1 [2352/2802 (84%)]\tLoss: 3.257990\n",
      "Train Epoche: 1 [2353/2802 (84%)]\tLoss: 9.781487\n",
      "Train Epoche: 1 [2354/2802 (84%)]\tLoss: 25.374315\n",
      "Train Epoche: 1 [2355/2802 (84%)]\tLoss: 9.478159\n",
      "Train Epoche: 1 [2356/2802 (84%)]\tLoss: 3.256947\n",
      "Train Epoche: 1 [2357/2802 (84%)]\tLoss: 2.628035\n",
      "Train Epoche: 1 [2358/2802 (84%)]\tLoss: 25.277426\n",
      "Train Epoche: 1 [2359/2802 (84%)]\tLoss: 61.951790\n",
      "Train Epoche: 1 [2360/2802 (84%)]\tLoss: 18.690134\n",
      "Train Epoche: 1 [2361/2802 (84%)]\tLoss: 7.627973\n",
      "Train Epoche: 1 [2362/2802 (84%)]\tLoss: 17.432737\n",
      "Train Epoche: 1 [2363/2802 (84%)]\tLoss: 127.721733\n",
      "Train Epoche: 1 [2364/2802 (84%)]\tLoss: 12.204554\n",
      "Train Epoche: 1 [2365/2802 (84%)]\tLoss: 0.550525\n",
      "Train Epoche: 1 [2366/2802 (84%)]\tLoss: 8.187941\n",
      "Train Epoche: 1 [2367/2802 (84%)]\tLoss: 2.014559\n",
      "Train Epoche: 1 [2368/2802 (85%)]\tLoss: 3.306070\n",
      "Train Epoche: 1 [2369/2802 (85%)]\tLoss: 6.121532\n",
      "Train Epoche: 1 [2370/2802 (85%)]\tLoss: 5.577803\n",
      "Train Epoche: 1 [2371/2802 (85%)]\tLoss: 3.337628\n",
      "Train Epoche: 1 [2372/2802 (85%)]\tLoss: 1.177375\n",
      "Train Epoche: 1 [2373/2802 (85%)]\tLoss: 36.584839\n",
      "Train Epoche: 1 [2374/2802 (85%)]\tLoss: 0.057865\n",
      "Train Epoche: 1 [2375/2802 (85%)]\tLoss: 0.058498\n",
      "Train Epoche: 1 [2376/2802 (85%)]\tLoss: 23.423155\n",
      "Train Epoche: 1 [2377/2802 (85%)]\tLoss: 8.830110\n",
      "Train Epoche: 1 [2378/2802 (85%)]\tLoss: 292.165894\n",
      "Train Epoche: 1 [2379/2802 (85%)]\tLoss: 3.559470\n",
      "Train Epoche: 1 [2380/2802 (85%)]\tLoss: 8.466953\n",
      "Train Epoche: 1 [2381/2802 (85%)]\tLoss: 0.286062\n",
      "Train Epoche: 1 [2382/2802 (85%)]\tLoss: 6.732442\n",
      "Train Epoche: 1 [2383/2802 (85%)]\tLoss: 6.187164\n",
      "Train Epoche: 1 [2384/2802 (85%)]\tLoss: 87.496193\n",
      "Train Epoche: 1 [2385/2802 (85%)]\tLoss: 1.573303\n",
      "Train Epoche: 1 [2386/2802 (85%)]\tLoss: 1.618322\n",
      "Train Epoche: 1 [2387/2802 (85%)]\tLoss: 130.110901\n",
      "Train Epoche: 1 [2388/2802 (85%)]\tLoss: 0.078062\n",
      "Train Epoche: 1 [2389/2802 (85%)]\tLoss: 0.140817\n",
      "Train Epoche: 1 [2390/2802 (85%)]\tLoss: 1.374999\n",
      "Train Epoche: 1 [2391/2802 (85%)]\tLoss: 0.401751\n",
      "Train Epoche: 1 [2392/2802 (85%)]\tLoss: 0.043714\n",
      "Train Epoche: 1 [2393/2802 (85%)]\tLoss: 22.311302\n",
      "Train Epoche: 1 [2394/2802 (85%)]\tLoss: 85.249428\n",
      "Train Epoche: 1 [2395/2802 (85%)]\tLoss: 79.211113\n",
      "Train Epoche: 1 [2396/2802 (86%)]\tLoss: 26.558027\n",
      "Train Epoche: 1 [2397/2802 (86%)]\tLoss: 0.400009\n",
      "Train Epoche: 1 [2398/2802 (86%)]\tLoss: 0.125203\n",
      "Train Epoche: 1 [2399/2802 (86%)]\tLoss: 17.720913\n",
      "Train Epoche: 1 [2400/2802 (86%)]\tLoss: 4.685421\n",
      "Train Epoche: 1 [2401/2802 (86%)]\tLoss: 174.003922\n",
      "Train Epoche: 1 [2402/2802 (86%)]\tLoss: 4.571288\n",
      "Train Epoche: 1 [2403/2802 (86%)]\tLoss: 6.911539\n",
      "Train Epoche: 1 [2404/2802 (86%)]\tLoss: 17.346462\n",
      "Train Epoche: 1 [2405/2802 (86%)]\tLoss: 0.657149\n",
      "Train Epoche: 1 [2406/2802 (86%)]\tLoss: 2.615986\n",
      "Train Epoche: 1 [2407/2802 (86%)]\tLoss: 1.426687\n",
      "Train Epoche: 1 [2408/2802 (86%)]\tLoss: 93.066200\n",
      "Train Epoche: 1 [2409/2802 (86%)]\tLoss: 3.095037\n",
      "Train Epoche: 1 [2410/2802 (86%)]\tLoss: 0.986430\n",
      "Train Epoche: 1 [2411/2802 (86%)]\tLoss: 0.038513\n",
      "Train Epoche: 1 [2412/2802 (86%)]\tLoss: 2.452998\n",
      "Train Epoche: 1 [2413/2802 (86%)]\tLoss: 18.171043\n",
      "Train Epoche: 1 [2414/2802 (86%)]\tLoss: 0.210599\n",
      "Train Epoche: 1 [2415/2802 (86%)]\tLoss: 15.177217\n",
      "Train Epoche: 1 [2416/2802 (86%)]\tLoss: 13.493019\n",
      "Train Epoche: 1 [2417/2802 (86%)]\tLoss: 6.763995\n",
      "Train Epoche: 1 [2418/2802 (86%)]\tLoss: 3.242205\n",
      "Train Epoche: 1 [2419/2802 (86%)]\tLoss: 9.121883\n",
      "Train Epoche: 1 [2420/2802 (86%)]\tLoss: 9.807154\n",
      "Train Epoche: 1 [2421/2802 (86%)]\tLoss: 13.330734\n",
      "Train Epoche: 1 [2422/2802 (86%)]\tLoss: 23.111422\n",
      "Train Epoche: 1 [2423/2802 (86%)]\tLoss: 0.952537\n",
      "Train Epoche: 1 [2424/2802 (87%)]\tLoss: 3.697512\n",
      "Train Epoche: 1 [2425/2802 (87%)]\tLoss: 2.150195\n",
      "Train Epoche: 1 [2426/2802 (87%)]\tLoss: 6.569099\n",
      "Train Epoche: 1 [2427/2802 (87%)]\tLoss: 5.821267\n",
      "Train Epoche: 1 [2428/2802 (87%)]\tLoss: 27.174009\n",
      "Train Epoche: 1 [2429/2802 (87%)]\tLoss: 38.740788\n",
      "Train Epoche: 1 [2430/2802 (87%)]\tLoss: 5.155859\n",
      "Train Epoche: 1 [2431/2802 (87%)]\tLoss: 4.069683\n",
      "Train Epoche: 1 [2432/2802 (87%)]\tLoss: 16.177185\n",
      "Train Epoche: 1 [2433/2802 (87%)]\tLoss: 5.456843\n",
      "Train Epoche: 1 [2434/2802 (87%)]\tLoss: 0.158097\n",
      "Train Epoche: 1 [2435/2802 (87%)]\tLoss: 6.518055\n",
      "Train Epoche: 1 [2436/2802 (87%)]\tLoss: 5.022347\n",
      "Train Epoche: 1 [2437/2802 (87%)]\tLoss: 14.805803\n",
      "Train Epoche: 1 [2438/2802 (87%)]\tLoss: 15.958843\n",
      "Train Epoche: 1 [2439/2802 (87%)]\tLoss: 1.197888\n",
      "Train Epoche: 1 [2440/2802 (87%)]\tLoss: 0.174112\n",
      "Train Epoche: 1 [2441/2802 (87%)]\tLoss: 1.314282\n",
      "Train Epoche: 1 [2442/2802 (87%)]\tLoss: 2.161805\n",
      "Train Epoche: 1 [2443/2802 (87%)]\tLoss: 0.005534\n",
      "Train Epoche: 1 [2444/2802 (87%)]\tLoss: 0.248464\n",
      "Train Epoche: 1 [2445/2802 (87%)]\tLoss: 0.014173\n",
      "Train Epoche: 1 [2446/2802 (87%)]\tLoss: 1.036589\n",
      "Train Epoche: 1 [2447/2802 (87%)]\tLoss: 3.720551\n",
      "Train Epoche: 1 [2448/2802 (87%)]\tLoss: 4.443454\n",
      "Train Epoche: 1 [2449/2802 (87%)]\tLoss: 5.223645\n",
      "Train Epoche: 1 [2450/2802 (87%)]\tLoss: 0.027469\n",
      "Train Epoche: 1 [2451/2802 (87%)]\tLoss: 1.669424\n",
      "Train Epoche: 1 [2452/2802 (88%)]\tLoss: 4.814474\n",
      "Train Epoche: 1 [2453/2802 (88%)]\tLoss: 27.838421\n",
      "Train Epoche: 1 [2454/2802 (88%)]\tLoss: 69.762871\n",
      "Train Epoche: 1 [2455/2802 (88%)]\tLoss: 116.542366\n",
      "Train Epoche: 1 [2456/2802 (88%)]\tLoss: 4.005423\n",
      "Train Epoche: 1 [2457/2802 (88%)]\tLoss: 2.882543\n",
      "Train Epoche: 1 [2458/2802 (88%)]\tLoss: 0.103667\n",
      "Train Epoche: 1 [2459/2802 (88%)]\tLoss: 7.462385\n",
      "Train Epoche: 1 [2460/2802 (88%)]\tLoss: 12.887958\n",
      "Train Epoche: 1 [2461/2802 (88%)]\tLoss: 1.390473\n",
      "Train Epoche: 1 [2462/2802 (88%)]\tLoss: 0.327631\n",
      "Train Epoche: 1 [2463/2802 (88%)]\tLoss: 6.413933\n",
      "Train Epoche: 1 [2464/2802 (88%)]\tLoss: 19.252619\n",
      "Train Epoche: 1 [2465/2802 (88%)]\tLoss: 24.129915\n",
      "Train Epoche: 1 [2466/2802 (88%)]\tLoss: 0.255880\n",
      "Train Epoche: 1 [2467/2802 (88%)]\tLoss: 6.575254\n",
      "Train Epoche: 1 [2468/2802 (88%)]\tLoss: 1.943140\n",
      "Train Epoche: 1 [2469/2802 (88%)]\tLoss: 0.284759\n",
      "Train Epoche: 1 [2470/2802 (88%)]\tLoss: 1.193777\n",
      "Train Epoche: 1 [2471/2802 (88%)]\tLoss: 0.000564\n",
      "Train Epoche: 1 [2472/2802 (88%)]\tLoss: 10.098772\n",
      "Train Epoche: 1 [2473/2802 (88%)]\tLoss: 2.366747\n",
      "Train Epoche: 1 [2474/2802 (88%)]\tLoss: 0.193177\n",
      "Train Epoche: 1 [2475/2802 (88%)]\tLoss: 0.025586\n",
      "Train Epoche: 1 [2476/2802 (88%)]\tLoss: 15.507369\n",
      "Train Epoche: 1 [2477/2802 (88%)]\tLoss: 17.142067\n",
      "Train Epoche: 1 [2478/2802 (88%)]\tLoss: 0.200416\n",
      "Train Epoche: 1 [2479/2802 (88%)]\tLoss: 4.061045\n",
      "Train Epoche: 1 [2480/2802 (89%)]\tLoss: 2.252208\n",
      "Train Epoche: 1 [2481/2802 (89%)]\tLoss: 0.859706\n",
      "Train Epoche: 1 [2482/2802 (89%)]\tLoss: 43.120934\n",
      "Train Epoche: 1 [2483/2802 (89%)]\tLoss: 1.366117\n",
      "Train Epoche: 1 [2484/2802 (89%)]\tLoss: 25.475550\n",
      "Train Epoche: 1 [2485/2802 (89%)]\tLoss: 0.376093\n",
      "Train Epoche: 1 [2486/2802 (89%)]\tLoss: 19.748112\n",
      "Train Epoche: 1 [2487/2802 (89%)]\tLoss: 33.136372\n",
      "Train Epoche: 1 [2488/2802 (89%)]\tLoss: 0.748149\n",
      "Train Epoche: 1 [2489/2802 (89%)]\tLoss: 68.271820\n",
      "Train Epoche: 1 [2490/2802 (89%)]\tLoss: 0.195600\n",
      "Train Epoche: 1 [2491/2802 (89%)]\tLoss: 5.332767\n",
      "Train Epoche: 1 [2492/2802 (89%)]\tLoss: 0.000100\n",
      "Train Epoche: 1 [2493/2802 (89%)]\tLoss: 0.274097\n",
      "Train Epoche: 1 [2494/2802 (89%)]\tLoss: 1.273249\n",
      "Train Epoche: 1 [2495/2802 (89%)]\tLoss: 3.325868\n",
      "Train Epoche: 1 [2496/2802 (89%)]\tLoss: 3.327085\n",
      "Train Epoche: 1 [2497/2802 (89%)]\tLoss: 37.254963\n",
      "Train Epoche: 1 [2498/2802 (89%)]\tLoss: 1.088515\n",
      "Train Epoche: 1 [2499/2802 (89%)]\tLoss: 0.002067\n",
      "Train Epoche: 1 [2500/2802 (89%)]\tLoss: 7.672807\n",
      "Train Epoche: 1 [2501/2802 (89%)]\tLoss: 56.988678\n",
      "Train Epoche: 1 [2502/2802 (89%)]\tLoss: 57.181034\n",
      "Train Epoche: 1 [2503/2802 (89%)]\tLoss: 0.504183\n",
      "Train Epoche: 1 [2504/2802 (89%)]\tLoss: 1.858380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [2505/2802 (89%)]\tLoss: 22.402678\n",
      "Train Epoche: 1 [2506/2802 (89%)]\tLoss: 5.619779\n",
      "Train Epoche: 1 [2507/2802 (89%)]\tLoss: 0.038025\n",
      "Train Epoche: 1 [2508/2802 (90%)]\tLoss: 1.027418\n",
      "Train Epoche: 1 [2509/2802 (90%)]\tLoss: 12.155175\n",
      "Train Epoche: 1 [2510/2802 (90%)]\tLoss: 17.550159\n",
      "Train Epoche: 1 [2511/2802 (90%)]\tLoss: 1.872405\n",
      "Train Epoche: 1 [2512/2802 (90%)]\tLoss: 25.057875\n",
      "Train Epoche: 1 [2513/2802 (90%)]\tLoss: 108.115089\n",
      "Train Epoche: 1 [2514/2802 (90%)]\tLoss: 4.382567\n",
      "Train Epoche: 1 [2515/2802 (90%)]\tLoss: 7.819118\n",
      "Train Epoche: 1 [2516/2802 (90%)]\tLoss: 0.657509\n",
      "Train Epoche: 1 [2517/2802 (90%)]\tLoss: 0.231675\n",
      "Train Epoche: 1 [2518/2802 (90%)]\tLoss: 10.168645\n",
      "Train Epoche: 1 [2519/2802 (90%)]\tLoss: 3.383383\n",
      "Train Epoche: 1 [2520/2802 (90%)]\tLoss: 0.400214\n",
      "Train Epoche: 1 [2521/2802 (90%)]\tLoss: 0.809757\n",
      "Train Epoche: 1 [2522/2802 (90%)]\tLoss: 1.603078\n",
      "Train Epoche: 1 [2523/2802 (90%)]\tLoss: 10.486441\n",
      "Train Epoche: 1 [2524/2802 (90%)]\tLoss: 11.103729\n",
      "Train Epoche: 1 [2525/2802 (90%)]\tLoss: 17.486502\n",
      "Train Epoche: 1 [2526/2802 (90%)]\tLoss: 4.730410\n",
      "Train Epoche: 1 [2527/2802 (90%)]\tLoss: 105.460960\n",
      "Train Epoche: 1 [2528/2802 (90%)]\tLoss: 5.622095\n",
      "Train Epoche: 1 [2529/2802 (90%)]\tLoss: 26.546095\n",
      "Train Epoche: 1 [2530/2802 (90%)]\tLoss: 5.023745\n",
      "Train Epoche: 1 [2531/2802 (90%)]\tLoss: 26.020657\n",
      "Train Epoche: 1 [2532/2802 (90%)]\tLoss: 0.086649\n",
      "Train Epoche: 1 [2533/2802 (90%)]\tLoss: 22.782429\n",
      "Train Epoche: 1 [2534/2802 (90%)]\tLoss: 7.719784\n",
      "Train Epoche: 1 [2535/2802 (90%)]\tLoss: 3.984939\n",
      "Train Epoche: 1 [2536/2802 (91%)]\tLoss: 0.545023\n",
      "Train Epoche: 1 [2537/2802 (91%)]\tLoss: 61.052235\n",
      "Train Epoche: 1 [2538/2802 (91%)]\tLoss: 10.109291\n",
      "Train Epoche: 1 [2539/2802 (91%)]\tLoss: 0.622594\n",
      "Train Epoche: 1 [2540/2802 (91%)]\tLoss: 1.178339\n",
      "Train Epoche: 1 [2541/2802 (91%)]\tLoss: 10.666799\n",
      "Train Epoche: 1 [2542/2802 (91%)]\tLoss: 4.652882\n",
      "Train Epoche: 1 [2543/2802 (91%)]\tLoss: 40.455105\n",
      "Train Epoche: 1 [2544/2802 (91%)]\tLoss: 14.700694\n",
      "Train Epoche: 1 [2545/2802 (91%)]\tLoss: 0.141163\n",
      "Train Epoche: 1 [2546/2802 (91%)]\tLoss: 1.261620\n",
      "Train Epoche: 1 [2547/2802 (91%)]\tLoss: 185.198883\n",
      "Train Epoche: 1 [2548/2802 (91%)]\tLoss: 1.509401\n",
      "Train Epoche: 1 [2549/2802 (91%)]\tLoss: 0.584032\n",
      "Train Epoche: 1 [2550/2802 (91%)]\tLoss: 71.678917\n",
      "Train Epoche: 1 [2551/2802 (91%)]\tLoss: 28.042358\n",
      "Train Epoche: 1 [2552/2802 (91%)]\tLoss: 1.017303\n",
      "Train Epoche: 1 [2553/2802 (91%)]\tLoss: 19.795080\n",
      "Train Epoche: 1 [2554/2802 (91%)]\tLoss: 0.071968\n",
      "Train Epoche: 1 [2555/2802 (91%)]\tLoss: 1.467222\n",
      "Train Epoche: 1 [2556/2802 (91%)]\tLoss: 0.454313\n",
      "Train Epoche: 1 [2557/2802 (91%)]\tLoss: 38.479778\n",
      "Train Epoche: 1 [2558/2802 (91%)]\tLoss: 1.066512\n",
      "Train Epoche: 1 [2559/2802 (91%)]\tLoss: 30.545931\n",
      "Train Epoche: 1 [2560/2802 (91%)]\tLoss: 3.082855\n",
      "Train Epoche: 1 [2561/2802 (91%)]\tLoss: 61.213863\n",
      "Train Epoche: 1 [2562/2802 (91%)]\tLoss: 13.069303\n",
      "Train Epoche: 1 [2563/2802 (91%)]\tLoss: 5.448267\n",
      "Train Epoche: 1 [2564/2802 (92%)]\tLoss: 19.207123\n",
      "Train Epoche: 1 [2565/2802 (92%)]\tLoss: 39.737324\n",
      "Train Epoche: 1 [2566/2802 (92%)]\tLoss: 0.426455\n",
      "Train Epoche: 1 [2567/2802 (92%)]\tLoss: 1.070666\n",
      "Train Epoche: 1 [2568/2802 (92%)]\tLoss: 0.574958\n",
      "Train Epoche: 1 [2569/2802 (92%)]\tLoss: 0.181753\n",
      "Train Epoche: 1 [2570/2802 (92%)]\tLoss: 0.778668\n",
      "Train Epoche: 1 [2571/2802 (92%)]\tLoss: 15.608557\n",
      "Train Epoche: 1 [2572/2802 (92%)]\tLoss: 7.740169\n",
      "Train Epoche: 1 [2573/2802 (92%)]\tLoss: 0.709091\n",
      "Train Epoche: 1 [2574/2802 (92%)]\tLoss: 5.677989\n",
      "Train Epoche: 1 [2575/2802 (92%)]\tLoss: 4.756791\n",
      "Train Epoche: 1 [2576/2802 (92%)]\tLoss: 0.144614\n",
      "Train Epoche: 1 [2577/2802 (92%)]\tLoss: 254.805176\n",
      "Train Epoche: 1 [2578/2802 (92%)]\tLoss: 164.139755\n",
      "Train Epoche: 1 [2579/2802 (92%)]\tLoss: 0.108933\n",
      "Train Epoche: 1 [2580/2802 (92%)]\tLoss: 15.528152\n",
      "Train Epoche: 1 [2581/2802 (92%)]\tLoss: 89.678612\n",
      "Train Epoche: 1 [2582/2802 (92%)]\tLoss: 15.658345\n",
      "Train Epoche: 1 [2583/2802 (92%)]\tLoss: 0.756805\n",
      "Train Epoche: 1 [2584/2802 (92%)]\tLoss: 3.370456\n",
      "Train Epoche: 1 [2585/2802 (92%)]\tLoss: 2.255630\n",
      "Train Epoche: 1 [2586/2802 (92%)]\tLoss: 154.017120\n",
      "Train Epoche: 1 [2587/2802 (92%)]\tLoss: 12.088672\n",
      "Train Epoche: 1 [2588/2802 (92%)]\tLoss: 82.344284\n",
      "Train Epoche: 1 [2589/2802 (92%)]\tLoss: 11.834483\n",
      "Train Epoche: 1 [2590/2802 (92%)]\tLoss: 1.178064\n",
      "Train Epoche: 1 [2591/2802 (92%)]\tLoss: 0.287975\n",
      "Train Epoche: 1 [2592/2802 (93%)]\tLoss: 57.903690\n",
      "Train Epoche: 1 [2593/2802 (93%)]\tLoss: 13.902888\n",
      "Train Epoche: 1 [2594/2802 (93%)]\tLoss: 0.662732\n",
      "Train Epoche: 1 [2595/2802 (93%)]\tLoss: 0.002656\n",
      "Train Epoche: 1 [2596/2802 (93%)]\tLoss: 14.687782\n",
      "Train Epoche: 1 [2597/2802 (93%)]\tLoss: 11.381036\n",
      "Train Epoche: 1 [2598/2802 (93%)]\tLoss: 31.038073\n",
      "Train Epoche: 1 [2599/2802 (93%)]\tLoss: 6.553051\n",
      "Train Epoche: 1 [2600/2802 (93%)]\tLoss: 0.099433\n",
      "Train Epoche: 1 [2601/2802 (93%)]\tLoss: 17.703606\n",
      "Train Epoche: 1 [2602/2802 (93%)]\tLoss: 5.710069\n",
      "Train Epoche: 1 [2603/2802 (93%)]\tLoss: 33.126930\n",
      "Train Epoche: 1 [2604/2802 (93%)]\tLoss: 2.755099\n",
      "Train Epoche: 1 [2605/2802 (93%)]\tLoss: 324.771973\n",
      "Train Epoche: 1 [2606/2802 (93%)]\tLoss: 36.663307\n",
      "Train Epoche: 1 [2607/2802 (93%)]\tLoss: 1.202568\n",
      "Train Epoche: 1 [2608/2802 (93%)]\tLoss: 2.363912\n",
      "Train Epoche: 1 [2609/2802 (93%)]\tLoss: 0.471299\n",
      "Train Epoche: 1 [2610/2802 (93%)]\tLoss: 0.745445\n",
      "Train Epoche: 1 [2611/2802 (93%)]\tLoss: 107.945114\n",
      "Train Epoche: 1 [2612/2802 (93%)]\tLoss: 0.129021\n",
      "Train Epoche: 1 [2613/2802 (93%)]\tLoss: 0.029234\n",
      "Train Epoche: 1 [2614/2802 (93%)]\tLoss: 21.103914\n",
      "Train Epoche: 1 [2615/2802 (93%)]\tLoss: 2.239940\n",
      "Train Epoche: 1 [2616/2802 (93%)]\tLoss: 12.164227\n",
      "Train Epoche: 1 [2617/2802 (93%)]\tLoss: 1.050081\n",
      "Train Epoche: 1 [2618/2802 (93%)]\tLoss: 94.339691\n",
      "Train Epoche: 1 [2619/2802 (93%)]\tLoss: 19.222708\n",
      "Train Epoche: 1 [2620/2802 (94%)]\tLoss: 26.880373\n",
      "Train Epoche: 1 [2621/2802 (94%)]\tLoss: 0.739034\n",
      "Train Epoche: 1 [2622/2802 (94%)]\tLoss: 0.803409\n",
      "Train Epoche: 1 [2623/2802 (94%)]\tLoss: 0.398690\n",
      "Train Epoche: 1 [2624/2802 (94%)]\tLoss: 9.781958\n",
      "Train Epoche: 1 [2625/2802 (94%)]\tLoss: 3.397217\n",
      "Train Epoche: 1 [2626/2802 (94%)]\tLoss: 4.954997\n",
      "Train Epoche: 1 [2627/2802 (94%)]\tLoss: 0.102696\n",
      "Train Epoche: 1 [2628/2802 (94%)]\tLoss: 26.996346\n",
      "Train Epoche: 1 [2629/2802 (94%)]\tLoss: 4.175429\n",
      "Train Epoche: 1 [2630/2802 (94%)]\tLoss: 67.092827\n",
      "Train Epoche: 1 [2631/2802 (94%)]\tLoss: 0.729574\n",
      "Train Epoche: 1 [2632/2802 (94%)]\tLoss: 10.657107\n",
      "Train Epoche: 1 [2633/2802 (94%)]\tLoss: 1.854921\n",
      "Train Epoche: 1 [2634/2802 (94%)]\tLoss: 0.464165\n",
      "Train Epoche: 1 [2635/2802 (94%)]\tLoss: 0.115218\n",
      "Train Epoche: 1 [2636/2802 (94%)]\tLoss: 0.475593\n",
      "Train Epoche: 1 [2637/2802 (94%)]\tLoss: 8.833697\n",
      "Train Epoche: 1 [2638/2802 (94%)]\tLoss: 26.858236\n",
      "Train Epoche: 1 [2639/2802 (94%)]\tLoss: 0.081641\n",
      "Train Epoche: 1 [2640/2802 (94%)]\tLoss: 0.262481\n",
      "Train Epoche: 1 [2641/2802 (94%)]\tLoss: 0.385473\n",
      "Train Epoche: 1 [2642/2802 (94%)]\tLoss: 4.201308\n",
      "Train Epoche: 1 [2643/2802 (94%)]\tLoss: 5.972622\n",
      "Train Epoche: 1 [2644/2802 (94%)]\tLoss: 29.173119\n",
      "Train Epoche: 1 [2645/2802 (94%)]\tLoss: 23.024441\n",
      "Train Epoche: 1 [2646/2802 (94%)]\tLoss: 1.454014\n",
      "Train Epoche: 1 [2647/2802 (94%)]\tLoss: 15.089403\n",
      "Train Epoche: 1 [2648/2802 (95%)]\tLoss: 3.997546\n",
      "Train Epoche: 1 [2649/2802 (95%)]\tLoss: 9.125898\n",
      "Train Epoche: 1 [2650/2802 (95%)]\tLoss: 52.337120\n",
      "Train Epoche: 1 [2651/2802 (95%)]\tLoss: 0.088588\n",
      "Train Epoche: 1 [2652/2802 (95%)]\tLoss: 3.285974\n",
      "Train Epoche: 1 [2653/2802 (95%)]\tLoss: 7.558635\n",
      "Train Epoche: 1 [2654/2802 (95%)]\tLoss: 7.135329\n",
      "Train Epoche: 1 [2655/2802 (95%)]\tLoss: 2.071302\n",
      "Train Epoche: 1 [2656/2802 (95%)]\tLoss: 2.880435\n",
      "Train Epoche: 1 [2657/2802 (95%)]\tLoss: 1.818619\n",
      "Train Epoche: 1 [2658/2802 (95%)]\tLoss: 0.171130\n",
      "Train Epoche: 1 [2659/2802 (95%)]\tLoss: 2.639577\n",
      "Train Epoche: 1 [2660/2802 (95%)]\tLoss: 0.015698\n",
      "Train Epoche: 1 [2661/2802 (95%)]\tLoss: 1.148228\n",
      "Train Epoche: 1 [2662/2802 (95%)]\tLoss: 76.546494\n",
      "Train Epoche: 1 [2663/2802 (95%)]\tLoss: 0.012066\n",
      "Train Epoche: 1 [2664/2802 (95%)]\tLoss: 0.016091\n",
      "Train Epoche: 1 [2665/2802 (95%)]\tLoss: 20.801842\n",
      "Train Epoche: 1 [2666/2802 (95%)]\tLoss: 6.428231\n",
      "Train Epoche: 1 [2667/2802 (95%)]\tLoss: 0.410198\n",
      "Train Epoche: 1 [2668/2802 (95%)]\tLoss: 137.071793\n",
      "Train Epoche: 1 [2669/2802 (95%)]\tLoss: 1.305405\n",
      "Train Epoche: 1 [2670/2802 (95%)]\tLoss: 1.183737\n",
      "Train Epoche: 1 [2671/2802 (95%)]\tLoss: 2.752706\n",
      "Train Epoche: 1 [2672/2802 (95%)]\tLoss: 242.587723\n",
      "Train Epoche: 1 [2673/2802 (95%)]\tLoss: 0.974590\n",
      "Train Epoche: 1 [2674/2802 (95%)]\tLoss: 0.066697\n",
      "Train Epoche: 1 [2675/2802 (95%)]\tLoss: 300.819427\n",
      "Train Epoche: 1 [2676/2802 (96%)]\tLoss: 9.407346\n",
      "Train Epoche: 1 [2677/2802 (96%)]\tLoss: 0.307358\n",
      "Train Epoche: 1 [2678/2802 (96%)]\tLoss: 0.000005\n",
      "Train Epoche: 1 [2679/2802 (96%)]\tLoss: 0.000332\n",
      "Train Epoche: 1 [2680/2802 (96%)]\tLoss: 1.960867\n",
      "Train Epoche: 1 [2681/2802 (96%)]\tLoss: 0.120954\n",
      "Train Epoche: 1 [2682/2802 (96%)]\tLoss: 0.386355\n",
      "Train Epoche: 1 [2683/2802 (96%)]\tLoss: 8.395678\n",
      "Train Epoche: 1 [2684/2802 (96%)]\tLoss: 93.324348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [2685/2802 (96%)]\tLoss: 1.219189\n",
      "Train Epoche: 1 [2686/2802 (96%)]\tLoss: 0.287439\n",
      "Train Epoche: 1 [2687/2802 (96%)]\tLoss: 0.011751\n",
      "Train Epoche: 1 [2688/2802 (96%)]\tLoss: 1.363603\n",
      "Train Epoche: 1 [2689/2802 (96%)]\tLoss: 4.192852\n",
      "Train Epoche: 1 [2690/2802 (96%)]\tLoss: 58.913418\n",
      "Train Epoche: 1 [2691/2802 (96%)]\tLoss: 5.640435\n",
      "Train Epoche: 1 [2692/2802 (96%)]\tLoss: 124.614937\n",
      "Train Epoche: 1 [2693/2802 (96%)]\tLoss: 26.721397\n",
      "Train Epoche: 1 [2694/2802 (96%)]\tLoss: 8.604746\n",
      "Train Epoche: 1 [2695/2802 (96%)]\tLoss: 24.147289\n",
      "Train Epoche: 1 [2696/2802 (96%)]\tLoss: 7.374929\n",
      "Train Epoche: 1 [2697/2802 (96%)]\tLoss: 4.036176\n",
      "Train Epoche: 1 [2698/2802 (96%)]\tLoss: 6.485688\n",
      "Train Epoche: 1 [2699/2802 (96%)]\tLoss: 25.071758\n",
      "Train Epoche: 1 [2700/2802 (96%)]\tLoss: 3.526362\n",
      "Train Epoche: 1 [2701/2802 (96%)]\tLoss: 0.945697\n",
      "Train Epoche: 1 [2702/2802 (96%)]\tLoss: 0.331589\n",
      "Train Epoche: 1 [2703/2802 (96%)]\tLoss: 6.189213\n",
      "Train Epoche: 1 [2704/2802 (97%)]\tLoss: 3.374702\n",
      "Train Epoche: 1 [2705/2802 (97%)]\tLoss: 2.719642\n",
      "Train Epoche: 1 [2706/2802 (97%)]\tLoss: 2.657732\n",
      "Train Epoche: 1 [2707/2802 (97%)]\tLoss: 26.279840\n",
      "Train Epoche: 1 [2708/2802 (97%)]\tLoss: 17.509974\n",
      "Train Epoche: 1 [2709/2802 (97%)]\tLoss: 6.970282\n",
      "Train Epoche: 1 [2710/2802 (97%)]\tLoss: 151.531509\n",
      "Train Epoche: 1 [2711/2802 (97%)]\tLoss: 5.537896\n",
      "Train Epoche: 1 [2712/2802 (97%)]\tLoss: 11.754352\n",
      "Train Epoche: 1 [2713/2802 (97%)]\tLoss: 12.799062\n",
      "Train Epoche: 1 [2714/2802 (97%)]\tLoss: 2.621030\n",
      "Train Epoche: 1 [2715/2802 (97%)]\tLoss: 0.371489\n",
      "Train Epoche: 1 [2716/2802 (97%)]\tLoss: 7.931693\n",
      "Train Epoche: 1 [2717/2802 (97%)]\tLoss: 103.288162\n",
      "Train Epoche: 1 [2718/2802 (97%)]\tLoss: 1.862950\n",
      "Train Epoche: 1 [2719/2802 (97%)]\tLoss: 1.624355\n",
      "Train Epoche: 1 [2720/2802 (97%)]\tLoss: 6.551515\n",
      "Train Epoche: 1 [2721/2802 (97%)]\tLoss: 63.224911\n",
      "Train Epoche: 1 [2722/2802 (97%)]\tLoss: 0.887722\n",
      "Train Epoche: 1 [2723/2802 (97%)]\tLoss: 9.743295\n",
      "Train Epoche: 1 [2724/2802 (97%)]\tLoss: 2.152829\n",
      "Train Epoche: 1 [2725/2802 (97%)]\tLoss: 0.331767\n",
      "Train Epoche: 1 [2726/2802 (97%)]\tLoss: 29.529570\n",
      "Train Epoche: 1 [2727/2802 (97%)]\tLoss: 27.967140\n",
      "Train Epoche: 1 [2728/2802 (97%)]\tLoss: 0.528306\n",
      "Train Epoche: 1 [2729/2802 (97%)]\tLoss: 6.143148\n",
      "Train Epoche: 1 [2730/2802 (97%)]\tLoss: 23.917774\n",
      "Train Epoche: 1 [2731/2802 (97%)]\tLoss: 2.730448\n",
      "Train Epoche: 1 [2732/2802 (98%)]\tLoss: 4.273906\n",
      "Train Epoche: 1 [2733/2802 (98%)]\tLoss: 0.113713\n",
      "Train Epoche: 1 [2734/2802 (98%)]\tLoss: 0.486976\n",
      "Train Epoche: 1 [2735/2802 (98%)]\tLoss: 1.295675\n",
      "Train Epoche: 1 [2736/2802 (98%)]\tLoss: 0.433494\n",
      "Train Epoche: 1 [2737/2802 (98%)]\tLoss: 8.158926\n",
      "Train Epoche: 1 [2738/2802 (98%)]\tLoss: 1.326582\n",
      "Train Epoche: 1 [2739/2802 (98%)]\tLoss: 8.689558\n",
      "Train Epoche: 1 [2740/2802 (98%)]\tLoss: 39.828709\n",
      "Train Epoche: 1 [2741/2802 (98%)]\tLoss: 0.230961\n",
      "Train Epoche: 1 [2742/2802 (98%)]\tLoss: 8.328807\n",
      "Train Epoche: 1 [2743/2802 (98%)]\tLoss: 17.288136\n",
      "Train Epoche: 1 [2744/2802 (98%)]\tLoss: 328.080322\n",
      "Train Epoche: 1 [2745/2802 (98%)]\tLoss: 2.286707\n",
      "Train Epoche: 1 [2746/2802 (98%)]\tLoss: 0.276252\n",
      "Train Epoche: 1 [2747/2802 (98%)]\tLoss: 3.798733\n",
      "Train Epoche: 1 [2748/2802 (98%)]\tLoss: 8.798398\n",
      "Train Epoche: 1 [2749/2802 (98%)]\tLoss: 3.530195\n",
      "Train Epoche: 1 [2750/2802 (98%)]\tLoss: 0.655648\n",
      "Train Epoche: 1 [2751/2802 (98%)]\tLoss: 4.172903\n",
      "Train Epoche: 1 [2752/2802 (98%)]\tLoss: 2.591562\n",
      "Train Epoche: 1 [2753/2802 (98%)]\tLoss: 10.617257\n",
      "Train Epoche: 1 [2754/2802 (98%)]\tLoss: 0.043647\n",
      "Train Epoche: 1 [2755/2802 (98%)]\tLoss: 1.879712\n",
      "Train Epoche: 1 [2756/2802 (98%)]\tLoss: 0.160974\n",
      "Train Epoche: 1 [2757/2802 (98%)]\tLoss: 26.276957\n",
      "Train Epoche: 1 [2758/2802 (98%)]\tLoss: 121.645386\n",
      "Train Epoche: 1 [2759/2802 (98%)]\tLoss: 8.808687\n",
      "Train Epoche: 1 [2760/2802 (99%)]\tLoss: 0.133328\n",
      "Train Epoche: 1 [2761/2802 (99%)]\tLoss: 2.267035\n",
      "Train Epoche: 1 [2762/2802 (99%)]\tLoss: 1.091809\n",
      "Train Epoche: 1 [2763/2802 (99%)]\tLoss: 0.034942\n",
      "Train Epoche: 1 [2764/2802 (99%)]\tLoss: 97.634865\n",
      "Train Epoche: 1 [2765/2802 (99%)]\tLoss: 0.033426\n",
      "Train Epoche: 1 [2766/2802 (99%)]\tLoss: 0.177372\n",
      "Train Epoche: 1 [2767/2802 (99%)]\tLoss: 1.208696\n",
      "Train Epoche: 1 [2768/2802 (99%)]\tLoss: 0.985576\n",
      "Train Epoche: 1 [2769/2802 (99%)]\tLoss: 0.888999\n",
      "Train Epoche: 1 [2770/2802 (99%)]\tLoss: 1.407951\n",
      "Train Epoche: 1 [2771/2802 (99%)]\tLoss: 107.125465\n",
      "Train Epoche: 1 [2772/2802 (99%)]\tLoss: 1.356815\n",
      "Train Epoche: 1 [2773/2802 (99%)]\tLoss: 0.997718\n",
      "Train Epoche: 1 [2774/2802 (99%)]\tLoss: 0.143824\n",
      "Train Epoche: 1 [2775/2802 (99%)]\tLoss: 9.576937\n",
      "Train Epoche: 1 [2776/2802 (99%)]\tLoss: 15.509464\n",
      "Train Epoche: 1 [2777/2802 (99%)]\tLoss: 30.857965\n",
      "Train Epoche: 1 [2778/2802 (99%)]\tLoss: 0.387443\n",
      "Train Epoche: 1 [2779/2802 (99%)]\tLoss: 1.347924\n",
      "Train Epoche: 1 [2780/2802 (99%)]\tLoss: 0.399421\n",
      "Train Epoche: 1 [2781/2802 (99%)]\tLoss: 2.450433\n",
      "Train Epoche: 1 [2782/2802 (99%)]\tLoss: 4.932630\n",
      "Train Epoche: 1 [2783/2802 (99%)]\tLoss: 2.338953\n",
      "Train Epoche: 1 [2784/2802 (99%)]\tLoss: 4.963320\n",
      "Train Epoche: 1 [2785/2802 (99%)]\tLoss: 9.198574\n",
      "Train Epoche: 1 [2786/2802 (99%)]\tLoss: 3.419432\n",
      "Train Epoche: 1 [2787/2802 (99%)]\tLoss: 8.095555\n",
      "Train Epoche: 1 [2788/2802 (100%)]\tLoss: 0.006781\n",
      "Train Epoche: 1 [2789/2802 (100%)]\tLoss: 2.758199\n",
      "Train Epoche: 1 [2790/2802 (100%)]\tLoss: 0.243533\n",
      "Train Epoche: 1 [2791/2802 (100%)]\tLoss: 82.641747\n",
      "Train Epoche: 1 [2792/2802 (100%)]\tLoss: 9.873412\n",
      "Train Epoche: 1 [2793/2802 (100%)]\tLoss: 0.228775\n",
      "Train Epoche: 1 [2794/2802 (100%)]\tLoss: 1.486286\n",
      "Train Epoche: 1 [2795/2802 (100%)]\tLoss: 0.506826\n",
      "Train Epoche: 1 [2796/2802 (100%)]\tLoss: 47.147366\n",
      "Train Epoche: 1 [2797/2802 (100%)]\tLoss: 163.133682\n",
      "Train Epoche: 1 [2798/2802 (100%)]\tLoss: 0.341405\n",
      "Train Epoche: 1 [2799/2802 (100%)]\tLoss: 6.328893\n",
      "Train Epoche: 1 [2800/2802 (100%)]\tLoss: 4.440367\n",
      "Train Epoche: 1 [2801/2802 (100%)]\tLoss: 4.671924\n",
      "Train Epoche: 2 [0/2802 (0%)]\tLoss: 0.208210\n",
      "Train Epoche: 2 [1/2802 (0%)]\tLoss: 1.148776\n",
      "Train Epoche: 2 [2/2802 (0%)]\tLoss: 1.117679\n",
      "Train Epoche: 2 [3/2802 (0%)]\tLoss: 8.330959\n",
      "Train Epoche: 2 [4/2802 (0%)]\tLoss: 4.157465\n",
      "Train Epoche: 2 [5/2802 (0%)]\tLoss: 82.980415\n",
      "Train Epoche: 2 [6/2802 (0%)]\tLoss: 135.600021\n",
      "Train Epoche: 2 [7/2802 (0%)]\tLoss: 12.131407\n",
      "Train Epoche: 2 [8/2802 (0%)]\tLoss: 10.760106\n",
      "Train Epoche: 2 [9/2802 (0%)]\tLoss: 2.594335\n",
      "Train Epoche: 2 [10/2802 (0%)]\tLoss: 7.675016\n",
      "Train Epoche: 2 [11/2802 (0%)]\tLoss: 1.586248\n",
      "Train Epoche: 2 [12/2802 (0%)]\tLoss: 29.824535\n",
      "Train Epoche: 2 [13/2802 (0%)]\tLoss: 70.469818\n",
      "Train Epoche: 2 [14/2802 (0%)]\tLoss: 0.405726\n",
      "Train Epoche: 2 [15/2802 (1%)]\tLoss: 13.005031\n",
      "Train Epoche: 2 [16/2802 (1%)]\tLoss: 0.703986\n",
      "Train Epoche: 2 [17/2802 (1%)]\tLoss: 284.364075\n",
      "Train Epoche: 2 [18/2802 (1%)]\tLoss: 0.456751\n",
      "Train Epoche: 2 [19/2802 (1%)]\tLoss: 0.049578\n",
      "Train Epoche: 2 [20/2802 (1%)]\tLoss: 9.396310\n",
      "Train Epoche: 2 [21/2802 (1%)]\tLoss: 12.291321\n",
      "Train Epoche: 2 [22/2802 (1%)]\tLoss: 3.336133\n",
      "Train Epoche: 2 [23/2802 (1%)]\tLoss: 17.582104\n",
      "Train Epoche: 2 [24/2802 (1%)]\tLoss: 31.015070\n",
      "Train Epoche: 2 [25/2802 (1%)]\tLoss: 47.339298\n",
      "Train Epoche: 2 [26/2802 (1%)]\tLoss: 13.124990\n",
      "Train Epoche: 2 [27/2802 (1%)]\tLoss: 28.052481\n",
      "Train Epoche: 2 [28/2802 (1%)]\tLoss: 6.115654\n",
      "Train Epoche: 2 [29/2802 (1%)]\tLoss: 5.604566\n",
      "Train Epoche: 2 [30/2802 (1%)]\tLoss: 1.183817\n",
      "Train Epoche: 2 [31/2802 (1%)]\tLoss: 10.242580\n",
      "Train Epoche: 2 [32/2802 (1%)]\tLoss: 30.766172\n",
      "Train Epoche: 2 [33/2802 (1%)]\tLoss: 0.008098\n",
      "Train Epoche: 2 [34/2802 (1%)]\tLoss: 2.585044\n",
      "Train Epoche: 2 [35/2802 (1%)]\tLoss: 0.188085\n",
      "Train Epoche: 2 [36/2802 (1%)]\tLoss: 0.002431\n",
      "Train Epoche: 2 [37/2802 (1%)]\tLoss: 0.673280\n",
      "Train Epoche: 2 [38/2802 (1%)]\tLoss: 6.070895\n",
      "Train Epoche: 2 [39/2802 (1%)]\tLoss: 5.567339\n",
      "Train Epoche: 2 [40/2802 (1%)]\tLoss: 219.286392\n",
      "Train Epoche: 2 [41/2802 (1%)]\tLoss: 9.337234\n",
      "Train Epoche: 2 [42/2802 (1%)]\tLoss: 19.330282\n",
      "Train Epoche: 2 [43/2802 (2%)]\tLoss: 16.924904\n",
      "Train Epoche: 2 [44/2802 (2%)]\tLoss: 2.393097\n",
      "Train Epoche: 2 [45/2802 (2%)]\tLoss: 0.281188\n",
      "Train Epoche: 2 [46/2802 (2%)]\tLoss: 3.347490\n",
      "Train Epoche: 2 [47/2802 (2%)]\tLoss: 2.286868\n",
      "Train Epoche: 2 [48/2802 (2%)]\tLoss: 25.958221\n",
      "Train Epoche: 2 [49/2802 (2%)]\tLoss: 1.826278\n",
      "Train Epoche: 2 [50/2802 (2%)]\tLoss: 1.258567\n",
      "Train Epoche: 2 [51/2802 (2%)]\tLoss: 0.770583\n",
      "Train Epoche: 2 [52/2802 (2%)]\tLoss: 38.630188\n",
      "Train Epoche: 2 [53/2802 (2%)]\tLoss: 12.060626\n",
      "Train Epoche: 2 [54/2802 (2%)]\tLoss: 7.654696\n",
      "Train Epoche: 2 [55/2802 (2%)]\tLoss: 15.693476\n",
      "Train Epoche: 2 [56/2802 (2%)]\tLoss: 73.094727\n",
      "Train Epoche: 2 [57/2802 (2%)]\tLoss: 5.043009\n",
      "Train Epoche: 2 [58/2802 (2%)]\tLoss: 3.816305\n",
      "Train Epoche: 2 [59/2802 (2%)]\tLoss: 0.451420\n",
      "Train Epoche: 2 [60/2802 (2%)]\tLoss: 40.164143\n",
      "Train Epoche: 2 [61/2802 (2%)]\tLoss: 12.603165\n",
      "Train Epoche: 2 [62/2802 (2%)]\tLoss: 11.428031\n",
      "Train Epoche: 2 [63/2802 (2%)]\tLoss: 1.325358\n",
      "Train Epoche: 2 [64/2802 (2%)]\tLoss: 18.374107\n",
      "Train Epoche: 2 [65/2802 (2%)]\tLoss: 0.131246\n",
      "Train Epoche: 2 [66/2802 (2%)]\tLoss: 1.376879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [67/2802 (2%)]\tLoss: 26.309631\n",
      "Train Epoche: 2 [68/2802 (2%)]\tLoss: 0.583375\n",
      "Train Epoche: 2 [69/2802 (2%)]\tLoss: 4.760427\n",
      "Train Epoche: 2 [70/2802 (2%)]\tLoss: 0.026115\n",
      "Train Epoche: 2 [71/2802 (3%)]\tLoss: 2.722282\n",
      "Train Epoche: 2 [72/2802 (3%)]\tLoss: 1.576978\n",
      "Train Epoche: 2 [73/2802 (3%)]\tLoss: 0.015184\n",
      "Train Epoche: 2 [74/2802 (3%)]\tLoss: 199.316315\n",
      "Train Epoche: 2 [75/2802 (3%)]\tLoss: 2.962966\n",
      "Train Epoche: 2 [76/2802 (3%)]\tLoss: 0.848992\n",
      "Train Epoche: 2 [77/2802 (3%)]\tLoss: 55.118958\n",
      "Train Epoche: 2 [78/2802 (3%)]\tLoss: 0.224614\n",
      "Train Epoche: 2 [79/2802 (3%)]\tLoss: 0.035971\n",
      "Train Epoche: 2 [80/2802 (3%)]\tLoss: 16.592358\n",
      "Train Epoche: 2 [81/2802 (3%)]\tLoss: 42.007427\n",
      "Train Epoche: 2 [82/2802 (3%)]\tLoss: 15.456973\n",
      "Train Epoche: 2 [83/2802 (3%)]\tLoss: 0.689228\n",
      "Train Epoche: 2 [84/2802 (3%)]\tLoss: 3.244819\n",
      "Train Epoche: 2 [85/2802 (3%)]\tLoss: 2.928051\n",
      "Train Epoche: 2 [86/2802 (3%)]\tLoss: 1.286866\n",
      "Train Epoche: 2 [87/2802 (3%)]\tLoss: 47.296104\n",
      "Train Epoche: 2 [88/2802 (3%)]\tLoss: 1.482598\n",
      "Train Epoche: 2 [89/2802 (3%)]\tLoss: 0.000052\n",
      "Train Epoche: 2 [90/2802 (3%)]\tLoss: 36.129795\n",
      "Train Epoche: 2 [91/2802 (3%)]\tLoss: 18.986677\n",
      "Train Epoche: 2 [92/2802 (3%)]\tLoss: 10.881852\n",
      "Train Epoche: 2 [93/2802 (3%)]\tLoss: 370.064117\n",
      "Train Epoche: 2 [94/2802 (3%)]\tLoss: 0.003520\n",
      "Train Epoche: 2 [95/2802 (3%)]\tLoss: 0.571592\n",
      "Train Epoche: 2 [96/2802 (3%)]\tLoss: 2.884094\n",
      "Train Epoche: 2 [97/2802 (3%)]\tLoss: 3.833907\n",
      "Train Epoche: 2 [98/2802 (3%)]\tLoss: 27.914141\n",
      "Train Epoche: 2 [99/2802 (4%)]\tLoss: 92.380638\n",
      "Train Epoche: 2 [100/2802 (4%)]\tLoss: 0.011879\n",
      "Train Epoche: 2 [101/2802 (4%)]\tLoss: 5.849588\n",
      "Train Epoche: 2 [102/2802 (4%)]\tLoss: 248.732590\n",
      "Train Epoche: 2 [103/2802 (4%)]\tLoss: 19.557499\n",
      "Train Epoche: 2 [104/2802 (4%)]\tLoss: 0.519868\n",
      "Train Epoche: 2 [105/2802 (4%)]\tLoss: 3.540826\n",
      "Train Epoche: 2 [106/2802 (4%)]\tLoss: 3.147811\n",
      "Train Epoche: 2 [107/2802 (4%)]\tLoss: 14.234469\n",
      "Train Epoche: 2 [108/2802 (4%)]\tLoss: 0.937459\n",
      "Train Epoche: 2 [109/2802 (4%)]\tLoss: 194.900528\n",
      "Train Epoche: 2 [110/2802 (4%)]\tLoss: 0.653412\n",
      "Train Epoche: 2 [111/2802 (4%)]\tLoss: 33.297108\n",
      "Train Epoche: 2 [112/2802 (4%)]\tLoss: 64.219322\n",
      "Train Epoche: 2 [113/2802 (4%)]\tLoss: 1.000649\n",
      "Train Epoche: 2 [114/2802 (4%)]\tLoss: 2.068060\n",
      "Train Epoche: 2 [115/2802 (4%)]\tLoss: 23.236609\n",
      "Train Epoche: 2 [116/2802 (4%)]\tLoss: 6.731340\n",
      "Train Epoche: 2 [117/2802 (4%)]\tLoss: 2.874367\n",
      "Train Epoche: 2 [118/2802 (4%)]\tLoss: 32.074474\n",
      "Train Epoche: 2 [119/2802 (4%)]\tLoss: 30.110847\n",
      "Train Epoche: 2 [120/2802 (4%)]\tLoss: 1.044576\n",
      "Train Epoche: 2 [121/2802 (4%)]\tLoss: 0.991454\n",
      "Train Epoche: 2 [122/2802 (4%)]\tLoss: 4.769714\n",
      "Train Epoche: 2 [123/2802 (4%)]\tLoss: 4.127185\n",
      "Train Epoche: 2 [124/2802 (4%)]\tLoss: 16.714445\n",
      "Train Epoche: 2 [125/2802 (4%)]\tLoss: 0.722754\n",
      "Train Epoche: 2 [126/2802 (4%)]\tLoss: 20.929523\n",
      "Train Epoche: 2 [127/2802 (5%)]\tLoss: 18.225550\n",
      "Train Epoche: 2 [128/2802 (5%)]\tLoss: 8.571956\n",
      "Train Epoche: 2 [129/2802 (5%)]\tLoss: 8.963485\n",
      "Train Epoche: 2 [130/2802 (5%)]\tLoss: 65.911697\n",
      "Train Epoche: 2 [131/2802 (5%)]\tLoss: 4.361508\n",
      "Train Epoche: 2 [132/2802 (5%)]\tLoss: 1.610221\n",
      "Train Epoche: 2 [133/2802 (5%)]\tLoss: 4.391412\n",
      "Train Epoche: 2 [134/2802 (5%)]\tLoss: 0.237428\n",
      "Train Epoche: 2 [135/2802 (5%)]\tLoss: 66.399864\n",
      "Train Epoche: 2 [136/2802 (5%)]\tLoss: 12.875019\n",
      "Train Epoche: 2 [137/2802 (5%)]\tLoss: 0.017576\n",
      "Train Epoche: 2 [138/2802 (5%)]\tLoss: 4.000978\n",
      "Train Epoche: 2 [139/2802 (5%)]\tLoss: 23.564909\n",
      "Train Epoche: 2 [140/2802 (5%)]\tLoss: 7.655475\n",
      "Train Epoche: 2 [141/2802 (5%)]\tLoss: 23.925377\n",
      "Train Epoche: 2 [142/2802 (5%)]\tLoss: 0.651168\n",
      "Train Epoche: 2 [143/2802 (5%)]\tLoss: 1.009418\n",
      "Train Epoche: 2 [144/2802 (5%)]\tLoss: 0.014107\n",
      "Train Epoche: 2 [145/2802 (5%)]\tLoss: 12.694206\n",
      "Train Epoche: 2 [146/2802 (5%)]\tLoss: 16.104273\n",
      "Train Epoche: 2 [147/2802 (5%)]\tLoss: 8.809449\n",
      "Train Epoche: 2 [148/2802 (5%)]\tLoss: 0.418138\n",
      "Train Epoche: 2 [149/2802 (5%)]\tLoss: 8.582158\n",
      "Train Epoche: 2 [150/2802 (5%)]\tLoss: 98.595398\n",
      "Train Epoche: 2 [151/2802 (5%)]\tLoss: 48.729874\n",
      "Train Epoche: 2 [152/2802 (5%)]\tLoss: 0.210217\n",
      "Train Epoche: 2 [153/2802 (5%)]\tLoss: 4.506745\n",
      "Train Epoche: 2 [154/2802 (5%)]\tLoss: 0.075149\n",
      "Train Epoche: 2 [155/2802 (6%)]\tLoss: 12.601980\n",
      "Train Epoche: 2 [156/2802 (6%)]\tLoss: 1.255492\n",
      "Train Epoche: 2 [157/2802 (6%)]\tLoss: 0.003354\n",
      "Train Epoche: 2 [158/2802 (6%)]\tLoss: 18.970373\n",
      "Train Epoche: 2 [159/2802 (6%)]\tLoss: 3.361932\n",
      "Train Epoche: 2 [160/2802 (6%)]\tLoss: 18.126747\n",
      "Train Epoche: 2 [161/2802 (6%)]\tLoss: 1.814502\n",
      "Train Epoche: 2 [162/2802 (6%)]\tLoss: 6.455757\n",
      "Train Epoche: 2 [163/2802 (6%)]\tLoss: 0.988190\n",
      "Train Epoche: 2 [164/2802 (6%)]\tLoss: 6.092857\n",
      "Train Epoche: 2 [165/2802 (6%)]\tLoss: 1.557773\n",
      "Train Epoche: 2 [166/2802 (6%)]\tLoss: 1.497058\n",
      "Train Epoche: 2 [167/2802 (6%)]\tLoss: 8.636449\n",
      "Train Epoche: 2 [168/2802 (6%)]\tLoss: 1.715644\n",
      "Train Epoche: 2 [169/2802 (6%)]\tLoss: 0.155569\n",
      "Train Epoche: 2 [170/2802 (6%)]\tLoss: 3.863968\n",
      "Train Epoche: 2 [171/2802 (6%)]\tLoss: 0.477020\n",
      "Train Epoche: 2 [172/2802 (6%)]\tLoss: 11.228676\n",
      "Train Epoche: 2 [173/2802 (6%)]\tLoss: 0.020114\n",
      "Train Epoche: 2 [174/2802 (6%)]\tLoss: 2.243269\n",
      "Train Epoche: 2 [175/2802 (6%)]\tLoss: 0.489763\n",
      "Train Epoche: 2 [176/2802 (6%)]\tLoss: 1.472781\n",
      "Train Epoche: 2 [177/2802 (6%)]\tLoss: 285.856384\n",
      "Train Epoche: 2 [178/2802 (6%)]\tLoss: 4.027896\n",
      "Train Epoche: 2 [179/2802 (6%)]\tLoss: 1.201884\n",
      "Train Epoche: 2 [180/2802 (6%)]\tLoss: 0.269039\n",
      "Train Epoche: 2 [181/2802 (6%)]\tLoss: 0.464022\n",
      "Train Epoche: 2 [182/2802 (6%)]\tLoss: 2.105931\n",
      "Train Epoche: 2 [183/2802 (7%)]\tLoss: 0.013530\n",
      "Train Epoche: 2 [184/2802 (7%)]\tLoss: 1.247692\n",
      "Train Epoche: 2 [185/2802 (7%)]\tLoss: 16.676849\n",
      "Train Epoche: 2 [186/2802 (7%)]\tLoss: 7.106755\n",
      "Train Epoche: 2 [187/2802 (7%)]\tLoss: 1.425413\n",
      "Train Epoche: 2 [188/2802 (7%)]\tLoss: 3.792126\n",
      "Train Epoche: 2 [189/2802 (7%)]\tLoss: 1.023809\n",
      "Train Epoche: 2 [190/2802 (7%)]\tLoss: 1.465140\n",
      "Train Epoche: 2 [191/2802 (7%)]\tLoss: 0.649239\n",
      "Train Epoche: 2 [192/2802 (7%)]\tLoss: 44.296993\n",
      "Train Epoche: 2 [193/2802 (7%)]\tLoss: 54.309124\n",
      "Train Epoche: 2 [194/2802 (7%)]\tLoss: 1.655958\n",
      "Train Epoche: 2 [195/2802 (7%)]\tLoss: 43.860203\n",
      "Train Epoche: 2 [196/2802 (7%)]\tLoss: 5.457950\n",
      "Train Epoche: 2 [197/2802 (7%)]\tLoss: 5.049351\n",
      "Train Epoche: 2 [198/2802 (7%)]\tLoss: 0.849553\n",
      "Train Epoche: 2 [199/2802 (7%)]\tLoss: 3.572191\n",
      "Train Epoche: 2 [200/2802 (7%)]\tLoss: 1.388516\n",
      "Train Epoche: 2 [201/2802 (7%)]\tLoss: 2.152931\n",
      "Train Epoche: 2 [202/2802 (7%)]\tLoss: 9.842612\n",
      "Train Epoche: 2 [203/2802 (7%)]\tLoss: 164.570328\n",
      "Train Epoche: 2 [204/2802 (7%)]\tLoss: 3.180893\n",
      "Train Epoche: 2 [205/2802 (7%)]\tLoss: 0.089026\n",
      "Train Epoche: 2 [206/2802 (7%)]\tLoss: 43.474270\n",
      "Train Epoche: 2 [207/2802 (7%)]\tLoss: 4.511828\n",
      "Train Epoche: 2 [208/2802 (7%)]\tLoss: 4.498678\n",
      "Train Epoche: 2 [209/2802 (7%)]\tLoss: 11.647551\n",
      "Train Epoche: 2 [210/2802 (7%)]\tLoss: 44.569492\n",
      "Train Epoche: 2 [211/2802 (8%)]\tLoss: 0.668771\n",
      "Train Epoche: 2 [212/2802 (8%)]\tLoss: 0.453910\n",
      "Train Epoche: 2 [213/2802 (8%)]\tLoss: 1.512941\n",
      "Train Epoche: 2 [214/2802 (8%)]\tLoss: 4.084936\n",
      "Train Epoche: 2 [215/2802 (8%)]\tLoss: 49.175140\n",
      "Train Epoche: 2 [216/2802 (8%)]\tLoss: 6.010703\n",
      "Train Epoche: 2 [217/2802 (8%)]\tLoss: 1.855983\n",
      "Train Epoche: 2 [218/2802 (8%)]\tLoss: 2.473861\n",
      "Train Epoche: 2 [219/2802 (8%)]\tLoss: 383.768921\n",
      "Train Epoche: 2 [220/2802 (8%)]\tLoss: 3.876574\n",
      "Train Epoche: 2 [221/2802 (8%)]\tLoss: 5.674287\n",
      "Train Epoche: 2 [222/2802 (8%)]\tLoss: 3.912619\n",
      "Train Epoche: 2 [223/2802 (8%)]\tLoss: 31.717447\n",
      "Train Epoche: 2 [224/2802 (8%)]\tLoss: 116.707520\n",
      "Train Epoche: 2 [225/2802 (8%)]\tLoss: 104.055405\n",
      "Train Epoche: 2 [226/2802 (8%)]\tLoss: 28.693192\n",
      "Train Epoche: 2 [227/2802 (8%)]\tLoss: 1.176710\n",
      "Train Epoche: 2 [228/2802 (8%)]\tLoss: 0.001419\n",
      "Train Epoche: 2 [229/2802 (8%)]\tLoss: 1.733148\n",
      "Train Epoche: 2 [230/2802 (8%)]\tLoss: 12.861054\n",
      "Train Epoche: 2 [231/2802 (8%)]\tLoss: 7.714772\n",
      "Train Epoche: 2 [232/2802 (8%)]\tLoss: 6.876343\n",
      "Train Epoche: 2 [233/2802 (8%)]\tLoss: 3.482361\n",
      "Train Epoche: 2 [234/2802 (8%)]\tLoss: 27.663069\n",
      "Train Epoche: 2 [235/2802 (8%)]\tLoss: 14.871832\n",
      "Train Epoche: 2 [236/2802 (8%)]\tLoss: 0.181052\n",
      "Train Epoche: 2 [237/2802 (8%)]\tLoss: 9.625956\n",
      "Train Epoche: 2 [238/2802 (8%)]\tLoss: 0.281815\n",
      "Train Epoche: 2 [239/2802 (9%)]\tLoss: 32.038654\n",
      "Train Epoche: 2 [240/2802 (9%)]\tLoss: 34.381210\n",
      "Train Epoche: 2 [241/2802 (9%)]\tLoss: 1.429951\n",
      "Train Epoche: 2 [242/2802 (9%)]\tLoss: 27.631216\n",
      "Train Epoche: 2 [243/2802 (9%)]\tLoss: 1.132646\n",
      "Train Epoche: 2 [244/2802 (9%)]\tLoss: 10.039929\n",
      "Train Epoche: 2 [245/2802 (9%)]\tLoss: 4.952834\n",
      "Train Epoche: 2 [246/2802 (9%)]\tLoss: 0.153019\n",
      "Train Epoche: 2 [247/2802 (9%)]\tLoss: 8.244411\n",
      "Train Epoche: 2 [248/2802 (9%)]\tLoss: 1.390846\n",
      "Train Epoche: 2 [249/2802 (9%)]\tLoss: 19.241058\n",
      "Train Epoche: 2 [250/2802 (9%)]\tLoss: 1.453004\n",
      "Train Epoche: 2 [251/2802 (9%)]\tLoss: 12.422738\n",
      "Train Epoche: 2 [252/2802 (9%)]\tLoss: 0.240798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [253/2802 (9%)]\tLoss: 4.551861\n",
      "Train Epoche: 2 [254/2802 (9%)]\tLoss: 0.060109\n",
      "Train Epoche: 2 [255/2802 (9%)]\tLoss: 6.363945\n",
      "Train Epoche: 2 [256/2802 (9%)]\tLoss: 0.509649\n",
      "Train Epoche: 2 [257/2802 (9%)]\tLoss: 20.521002\n",
      "Train Epoche: 2 [258/2802 (9%)]\tLoss: 1.345592\n",
      "Train Epoche: 2 [259/2802 (9%)]\tLoss: 4.498780\n",
      "Train Epoche: 2 [260/2802 (9%)]\tLoss: 0.006405\n",
      "Train Epoche: 2 [261/2802 (9%)]\tLoss: 7.869009\n",
      "Train Epoche: 2 [262/2802 (9%)]\tLoss: 0.073531\n",
      "Train Epoche: 2 [263/2802 (9%)]\tLoss: 0.740931\n",
      "Train Epoche: 2 [264/2802 (9%)]\tLoss: 0.165020\n",
      "Train Epoche: 2 [265/2802 (9%)]\tLoss: 0.800250\n",
      "Train Epoche: 2 [266/2802 (9%)]\tLoss: 35.770126\n",
      "Train Epoche: 2 [267/2802 (10%)]\tLoss: 5.126474\n",
      "Train Epoche: 2 [268/2802 (10%)]\tLoss: 2.575947\n",
      "Train Epoche: 2 [269/2802 (10%)]\tLoss: 7.016742\n",
      "Train Epoche: 2 [270/2802 (10%)]\tLoss: 14.632907\n",
      "Train Epoche: 2 [271/2802 (10%)]\tLoss: 0.000203\n",
      "Train Epoche: 2 [272/2802 (10%)]\tLoss: 4.980953\n",
      "Train Epoche: 2 [273/2802 (10%)]\tLoss: 115.213661\n",
      "Train Epoche: 2 [274/2802 (10%)]\tLoss: 1.982617\n",
      "Train Epoche: 2 [275/2802 (10%)]\tLoss: 5.677723\n",
      "Train Epoche: 2 [276/2802 (10%)]\tLoss: 11.055792\n",
      "Train Epoche: 2 [277/2802 (10%)]\tLoss: 0.113505\n",
      "Train Epoche: 2 [278/2802 (10%)]\tLoss: 7.321075\n",
      "Train Epoche: 2 [279/2802 (10%)]\tLoss: 23.959406\n",
      "Train Epoche: 2 [280/2802 (10%)]\tLoss: 0.498893\n",
      "Train Epoche: 2 [281/2802 (10%)]\tLoss: 4.834978\n",
      "Train Epoche: 2 [282/2802 (10%)]\tLoss: 0.356532\n",
      "Train Epoche: 2 [283/2802 (10%)]\tLoss: 116.386688\n",
      "Train Epoche: 2 [284/2802 (10%)]\tLoss: 267.783203\n",
      "Train Epoche: 2 [285/2802 (10%)]\tLoss: 0.128575\n",
      "Train Epoche: 2 [286/2802 (10%)]\tLoss: 76.097130\n",
      "Train Epoche: 2 [287/2802 (10%)]\tLoss: 3.122546\n",
      "Train Epoche: 2 [288/2802 (10%)]\tLoss: 17.998125\n",
      "Train Epoche: 2 [289/2802 (10%)]\tLoss: 54.520565\n",
      "Train Epoche: 2 [290/2802 (10%)]\tLoss: 3.696500\n",
      "Train Epoche: 2 [291/2802 (10%)]\tLoss: 27.242599\n",
      "Train Epoche: 2 [292/2802 (10%)]\tLoss: 10.043218\n",
      "Train Epoche: 2 [293/2802 (10%)]\tLoss: 14.708396\n",
      "Train Epoche: 2 [294/2802 (10%)]\tLoss: 11.686813\n",
      "Train Epoche: 2 [295/2802 (11%)]\tLoss: 63.212051\n",
      "Train Epoche: 2 [296/2802 (11%)]\tLoss: 36.778122\n",
      "Train Epoche: 2 [297/2802 (11%)]\tLoss: 44.445454\n",
      "Train Epoche: 2 [298/2802 (11%)]\tLoss: 25.636076\n",
      "Train Epoche: 2 [299/2802 (11%)]\tLoss: 6.922686\n",
      "Train Epoche: 2 [300/2802 (11%)]\tLoss: 29.714693\n",
      "Train Epoche: 2 [301/2802 (11%)]\tLoss: 8.100176\n",
      "Train Epoche: 2 [302/2802 (11%)]\tLoss: 0.198959\n",
      "Train Epoche: 2 [303/2802 (11%)]\tLoss: 5.474995\n",
      "Train Epoche: 2 [304/2802 (11%)]\tLoss: 25.428331\n",
      "Train Epoche: 2 [305/2802 (11%)]\tLoss: 8.920907\n",
      "Train Epoche: 2 [306/2802 (11%)]\tLoss: 1.613765\n",
      "Train Epoche: 2 [307/2802 (11%)]\tLoss: 8.870354\n",
      "Train Epoche: 2 [308/2802 (11%)]\tLoss: 15.217392\n",
      "Train Epoche: 2 [309/2802 (11%)]\tLoss: 0.229569\n",
      "Train Epoche: 2 [310/2802 (11%)]\tLoss: 1.367615\n",
      "Train Epoche: 2 [311/2802 (11%)]\tLoss: 5.456809\n",
      "Train Epoche: 2 [312/2802 (11%)]\tLoss: 133.143005\n",
      "Train Epoche: 2 [313/2802 (11%)]\tLoss: 15.299023\n",
      "Train Epoche: 2 [314/2802 (11%)]\tLoss: 3.175355\n",
      "Train Epoche: 2 [315/2802 (11%)]\tLoss: 29.114103\n",
      "Train Epoche: 2 [316/2802 (11%)]\tLoss: 0.047200\n",
      "Train Epoche: 2 [317/2802 (11%)]\tLoss: 0.820396\n",
      "Train Epoche: 2 [318/2802 (11%)]\tLoss: 0.253374\n",
      "Train Epoche: 2 [319/2802 (11%)]\tLoss: 1.031097\n",
      "Train Epoche: 2 [320/2802 (11%)]\tLoss: 2.412579\n",
      "Train Epoche: 2 [321/2802 (11%)]\tLoss: 0.011366\n",
      "Train Epoche: 2 [322/2802 (11%)]\tLoss: 6.175010\n",
      "Train Epoche: 2 [323/2802 (12%)]\tLoss: 41.659958\n",
      "Train Epoche: 2 [324/2802 (12%)]\tLoss: 1.531740\n",
      "Train Epoche: 2 [325/2802 (12%)]\tLoss: 29.518427\n",
      "Train Epoche: 2 [326/2802 (12%)]\tLoss: 0.106564\n",
      "Train Epoche: 2 [327/2802 (12%)]\tLoss: 0.820131\n",
      "Train Epoche: 2 [328/2802 (12%)]\tLoss: 5.273247\n",
      "Train Epoche: 2 [329/2802 (12%)]\tLoss: 394.889313\n",
      "Train Epoche: 2 [330/2802 (12%)]\tLoss: 0.423712\n",
      "Train Epoche: 2 [331/2802 (12%)]\tLoss: 0.752194\n",
      "Train Epoche: 2 [332/2802 (12%)]\tLoss: 0.099678\n",
      "Train Epoche: 2 [333/2802 (12%)]\tLoss: 2.698821\n",
      "Train Epoche: 2 [334/2802 (12%)]\tLoss: 1.266011\n",
      "Train Epoche: 2 [335/2802 (12%)]\tLoss: 1.197934\n",
      "Train Epoche: 2 [336/2802 (12%)]\tLoss: 4.077891\n",
      "Train Epoche: 2 [337/2802 (12%)]\tLoss: 1.143088\n",
      "Train Epoche: 2 [338/2802 (12%)]\tLoss: 6.076066\n",
      "Train Epoche: 2 [339/2802 (12%)]\tLoss: 75.620934\n",
      "Train Epoche: 2 [340/2802 (12%)]\tLoss: 17.539391\n",
      "Train Epoche: 2 [341/2802 (12%)]\tLoss: 2.140414\n",
      "Train Epoche: 2 [342/2802 (12%)]\tLoss: 0.252579\n",
      "Train Epoche: 2 [343/2802 (12%)]\tLoss: 0.673014\n",
      "Train Epoche: 2 [344/2802 (12%)]\tLoss: 0.322556\n",
      "Train Epoche: 2 [345/2802 (12%)]\tLoss: 0.069303\n",
      "Train Epoche: 2 [346/2802 (12%)]\tLoss: 0.745189\n",
      "Train Epoche: 2 [347/2802 (12%)]\tLoss: 3.814343\n",
      "Train Epoche: 2 [348/2802 (12%)]\tLoss: 27.734009\n",
      "Train Epoche: 2 [349/2802 (12%)]\tLoss: 1.779586\n",
      "Train Epoche: 2 [350/2802 (12%)]\tLoss: 0.426552\n",
      "Train Epoche: 2 [351/2802 (13%)]\tLoss: 33.960548\n",
      "Train Epoche: 2 [352/2802 (13%)]\tLoss: 1.092346\n",
      "Train Epoche: 2 [353/2802 (13%)]\tLoss: 7.911567\n",
      "Train Epoche: 2 [354/2802 (13%)]\tLoss: 3.199151\n",
      "Train Epoche: 2 [355/2802 (13%)]\tLoss: 25.887785\n",
      "Train Epoche: 2 [356/2802 (13%)]\tLoss: 0.227026\n",
      "Train Epoche: 2 [357/2802 (13%)]\tLoss: 0.490215\n",
      "Train Epoche: 2 [358/2802 (13%)]\tLoss: 0.007190\n",
      "Train Epoche: 2 [359/2802 (13%)]\tLoss: 25.239351\n",
      "Train Epoche: 2 [360/2802 (13%)]\tLoss: 39.707920\n",
      "Train Epoche: 2 [361/2802 (13%)]\tLoss: 33.543484\n",
      "Train Epoche: 2 [362/2802 (13%)]\tLoss: 13.839807\n",
      "Train Epoche: 2 [363/2802 (13%)]\tLoss: 2.491902\n",
      "Train Epoche: 2 [364/2802 (13%)]\tLoss: 3.345634\n",
      "Train Epoche: 2 [365/2802 (13%)]\tLoss: 6.742310\n",
      "Train Epoche: 2 [366/2802 (13%)]\tLoss: 15.983669\n",
      "Train Epoche: 2 [367/2802 (13%)]\tLoss: 0.134703\n",
      "Train Epoche: 2 [368/2802 (13%)]\tLoss: 327.012543\n",
      "Train Epoche: 2 [369/2802 (13%)]\tLoss: 3.605187\n",
      "Train Epoche: 2 [370/2802 (13%)]\tLoss: 1.941762\n",
      "Train Epoche: 2 [371/2802 (13%)]\tLoss: 2.452201\n",
      "Train Epoche: 2 [372/2802 (13%)]\tLoss: 3.646799\n",
      "Train Epoche: 2 [373/2802 (13%)]\tLoss: 4.729782\n",
      "Train Epoche: 2 [374/2802 (13%)]\tLoss: 1.261311\n",
      "Train Epoche: 2 [375/2802 (13%)]\tLoss: 11.420655\n",
      "Train Epoche: 2 [376/2802 (13%)]\tLoss: 5.799589\n",
      "Train Epoche: 2 [377/2802 (13%)]\tLoss: 0.029975\n",
      "Train Epoche: 2 [378/2802 (13%)]\tLoss: 0.912034\n",
      "Train Epoche: 2 [379/2802 (14%)]\tLoss: 155.430511\n",
      "Train Epoche: 2 [380/2802 (14%)]\tLoss: 39.320774\n",
      "Train Epoche: 2 [381/2802 (14%)]\tLoss: 76.874290\n",
      "Train Epoche: 2 [382/2802 (14%)]\tLoss: 0.067023\n",
      "Train Epoche: 2 [383/2802 (14%)]\tLoss: 1.725591\n",
      "Train Epoche: 2 [384/2802 (14%)]\tLoss: 5.405496\n",
      "Train Epoche: 2 [385/2802 (14%)]\tLoss: 3.484107\n",
      "Train Epoche: 2 [386/2802 (14%)]\tLoss: 0.379937\n",
      "Train Epoche: 2 [387/2802 (14%)]\tLoss: 3.554180\n",
      "Train Epoche: 2 [388/2802 (14%)]\tLoss: 15.625553\n",
      "Train Epoche: 2 [389/2802 (14%)]\tLoss: 0.142868\n",
      "Train Epoche: 2 [390/2802 (14%)]\tLoss: 1.039617\n",
      "Train Epoche: 2 [391/2802 (14%)]\tLoss: 0.309157\n",
      "Train Epoche: 2 [392/2802 (14%)]\tLoss: 8.125383\n",
      "Train Epoche: 2 [393/2802 (14%)]\tLoss: 137.261047\n",
      "Train Epoche: 2 [394/2802 (14%)]\tLoss: 229.261795\n",
      "Train Epoche: 2 [395/2802 (14%)]\tLoss: 1.397819\n",
      "Train Epoche: 2 [396/2802 (14%)]\tLoss: 3.101714\n",
      "Train Epoche: 2 [397/2802 (14%)]\tLoss: 5.738070\n",
      "Train Epoche: 2 [398/2802 (14%)]\tLoss: 303.574036\n",
      "Train Epoche: 2 [399/2802 (14%)]\tLoss: 8.891391\n",
      "Train Epoche: 2 [400/2802 (14%)]\tLoss: 4.424730\n",
      "Train Epoche: 2 [401/2802 (14%)]\tLoss: 6.303458\n",
      "Train Epoche: 2 [402/2802 (14%)]\tLoss: 0.975818\n",
      "Train Epoche: 2 [403/2802 (14%)]\tLoss: 3.935114\n",
      "Train Epoche: 2 [404/2802 (14%)]\tLoss: 17.150770\n",
      "Train Epoche: 2 [405/2802 (14%)]\tLoss: 3.481896\n",
      "Train Epoche: 2 [406/2802 (14%)]\tLoss: 4.632658\n",
      "Train Epoche: 2 [407/2802 (15%)]\tLoss: 0.875873\n",
      "Train Epoche: 2 [408/2802 (15%)]\tLoss: 25.240376\n",
      "Train Epoche: 2 [409/2802 (15%)]\tLoss: 66.862366\n",
      "Train Epoche: 2 [410/2802 (15%)]\tLoss: 68.172943\n",
      "Train Epoche: 2 [411/2802 (15%)]\tLoss: 6.292439\n",
      "Train Epoche: 2 [412/2802 (15%)]\tLoss: 26.636110\n",
      "Train Epoche: 2 [413/2802 (15%)]\tLoss: 0.119985\n",
      "Train Epoche: 2 [414/2802 (15%)]\tLoss: 1.777122\n",
      "Train Epoche: 2 [415/2802 (15%)]\tLoss: 9.767276\n",
      "Train Epoche: 2 [416/2802 (15%)]\tLoss: 0.183747\n",
      "Train Epoche: 2 [417/2802 (15%)]\tLoss: 0.246609\n",
      "Train Epoche: 2 [418/2802 (15%)]\tLoss: 0.217410\n",
      "Train Epoche: 2 [419/2802 (15%)]\tLoss: 211.210663\n",
      "Train Epoche: 2 [420/2802 (15%)]\tLoss: 203.830612\n",
      "Train Epoche: 2 [421/2802 (15%)]\tLoss: 0.527870\n",
      "Train Epoche: 2 [422/2802 (15%)]\tLoss: 20.312738\n",
      "Train Epoche: 2 [423/2802 (15%)]\tLoss: 0.760157\n",
      "Train Epoche: 2 [424/2802 (15%)]\tLoss: 1.191502\n",
      "Train Epoche: 2 [425/2802 (15%)]\tLoss: 2.232194\n",
      "Train Epoche: 2 [426/2802 (15%)]\tLoss: 0.377596\n",
      "Train Epoche: 2 [427/2802 (15%)]\tLoss: 1.203056\n",
      "Train Epoche: 2 [428/2802 (15%)]\tLoss: 28.756317\n",
      "Train Epoche: 2 [429/2802 (15%)]\tLoss: 2.306524\n",
      "Train Epoche: 2 [430/2802 (15%)]\tLoss: 0.878320\n",
      "Train Epoche: 2 [431/2802 (15%)]\tLoss: 62.331242\n",
      "Train Epoche: 2 [432/2802 (15%)]\tLoss: 3.840755\n",
      "Train Epoche: 2 [433/2802 (15%)]\tLoss: 1.701652\n",
      "Train Epoche: 2 [434/2802 (15%)]\tLoss: 0.038106\n",
      "Train Epoche: 2 [435/2802 (16%)]\tLoss: 68.780525\n",
      "Train Epoche: 2 [436/2802 (16%)]\tLoss: 0.008948\n",
      "Train Epoche: 2 [437/2802 (16%)]\tLoss: 0.245204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [438/2802 (16%)]\tLoss: 0.123317\n",
      "Train Epoche: 2 [439/2802 (16%)]\tLoss: 1.307737\n",
      "Train Epoche: 2 [440/2802 (16%)]\tLoss: 0.861897\n",
      "Train Epoche: 2 [441/2802 (16%)]\tLoss: 1.546513\n",
      "Train Epoche: 2 [442/2802 (16%)]\tLoss: 43.235401\n",
      "Train Epoche: 2 [443/2802 (16%)]\tLoss: 3.433168\n",
      "Train Epoche: 2 [444/2802 (16%)]\tLoss: 0.006040\n",
      "Train Epoche: 2 [445/2802 (16%)]\tLoss: 10.673752\n",
      "Train Epoche: 2 [446/2802 (16%)]\tLoss: 0.612146\n",
      "Train Epoche: 2 [447/2802 (16%)]\tLoss: 97.146469\n",
      "Train Epoche: 2 [448/2802 (16%)]\tLoss: 5.233784\n",
      "Train Epoche: 2 [449/2802 (16%)]\tLoss: 26.946005\n",
      "Train Epoche: 2 [450/2802 (16%)]\tLoss: 0.820062\n",
      "Train Epoche: 2 [451/2802 (16%)]\tLoss: 62.619976\n",
      "Train Epoche: 2 [452/2802 (16%)]\tLoss: 23.149052\n",
      "Train Epoche: 2 [453/2802 (16%)]\tLoss: 3.156212\n",
      "Train Epoche: 2 [454/2802 (16%)]\tLoss: 5.326708\n",
      "Train Epoche: 2 [455/2802 (16%)]\tLoss: 7.593962\n",
      "Train Epoche: 2 [456/2802 (16%)]\tLoss: 4.136017\n",
      "Train Epoche: 2 [457/2802 (16%)]\tLoss: 10.665876\n",
      "Train Epoche: 2 [458/2802 (16%)]\tLoss: 0.643094\n",
      "Train Epoche: 2 [459/2802 (16%)]\tLoss: 2.265455\n",
      "Train Epoche: 2 [460/2802 (16%)]\tLoss: 1.986241\n",
      "Train Epoche: 2 [461/2802 (16%)]\tLoss: 0.874125\n",
      "Train Epoche: 2 [462/2802 (16%)]\tLoss: 0.010052\n",
      "Train Epoche: 2 [463/2802 (17%)]\tLoss: 2.277555\n",
      "Train Epoche: 2 [464/2802 (17%)]\tLoss: 0.077650\n",
      "Train Epoche: 2 [465/2802 (17%)]\tLoss: 0.498946\n",
      "Train Epoche: 2 [466/2802 (17%)]\tLoss: 8.811632\n",
      "Train Epoche: 2 [467/2802 (17%)]\tLoss: 0.619451\n",
      "Train Epoche: 2 [468/2802 (17%)]\tLoss: 14.516176\n",
      "Train Epoche: 2 [469/2802 (17%)]\tLoss: 0.471125\n",
      "Train Epoche: 2 [470/2802 (17%)]\tLoss: 220.670609\n",
      "Train Epoche: 2 [471/2802 (17%)]\tLoss: 32.169224\n",
      "Train Epoche: 2 [472/2802 (17%)]\tLoss: 73.103012\n",
      "Train Epoche: 2 [473/2802 (17%)]\tLoss: 38.173145\n",
      "Train Epoche: 2 [474/2802 (17%)]\tLoss: 14.851258\n",
      "Train Epoche: 2 [475/2802 (17%)]\tLoss: 84.027222\n",
      "Train Epoche: 2 [476/2802 (17%)]\tLoss: 0.199537\n",
      "Train Epoche: 2 [477/2802 (17%)]\tLoss: 2.405171\n",
      "Train Epoche: 2 [478/2802 (17%)]\tLoss: 2.710115\n",
      "Train Epoche: 2 [479/2802 (17%)]\tLoss: 0.022093\n",
      "Train Epoche: 2 [480/2802 (17%)]\tLoss: 13.928368\n",
      "Train Epoche: 2 [481/2802 (17%)]\tLoss: 5.108292\n",
      "Train Epoche: 2 [482/2802 (17%)]\tLoss: 3.416988\n",
      "Train Epoche: 2 [483/2802 (17%)]\tLoss: 0.620635\n",
      "Train Epoche: 2 [484/2802 (17%)]\tLoss: 4.029418\n",
      "Train Epoche: 2 [485/2802 (17%)]\tLoss: 1.080862\n",
      "Train Epoche: 2 [486/2802 (17%)]\tLoss: 0.627545\n",
      "Train Epoche: 2 [487/2802 (17%)]\tLoss: 78.319572\n",
      "Train Epoche: 2 [488/2802 (17%)]\tLoss: 43.078751\n",
      "Train Epoche: 2 [489/2802 (17%)]\tLoss: 0.004491\n",
      "Train Epoche: 2 [490/2802 (17%)]\tLoss: 1.627210\n",
      "Train Epoche: 2 [491/2802 (18%)]\tLoss: 1.029922\n",
      "Train Epoche: 2 [492/2802 (18%)]\tLoss: 0.002827\n",
      "Train Epoche: 2 [493/2802 (18%)]\tLoss: 1.237896\n",
      "Train Epoche: 2 [494/2802 (18%)]\tLoss: 6.741532\n",
      "Train Epoche: 2 [495/2802 (18%)]\tLoss: 43.371758\n",
      "Train Epoche: 2 [496/2802 (18%)]\tLoss: 0.142773\n",
      "Train Epoche: 2 [497/2802 (18%)]\tLoss: 4.633848\n",
      "Train Epoche: 2 [498/2802 (18%)]\tLoss: 0.000409\n",
      "Train Epoche: 2 [499/2802 (18%)]\tLoss: 35.194126\n",
      "Train Epoche: 2 [500/2802 (18%)]\tLoss: 6.501624\n",
      "Train Epoche: 2 [501/2802 (18%)]\tLoss: 13.874818\n",
      "Train Epoche: 2 [502/2802 (18%)]\tLoss: 43.542370\n",
      "Train Epoche: 2 [503/2802 (18%)]\tLoss: 58.860210\n",
      "Train Epoche: 2 [504/2802 (18%)]\tLoss: 3.301584\n",
      "Train Epoche: 2 [505/2802 (18%)]\tLoss: 0.866726\n",
      "Train Epoche: 2 [506/2802 (18%)]\tLoss: 2.021289\n",
      "Train Epoche: 2 [507/2802 (18%)]\tLoss: 28.427261\n",
      "Train Epoche: 2 [508/2802 (18%)]\tLoss: 4.433436\n",
      "Train Epoche: 2 [509/2802 (18%)]\tLoss: 24.135040\n",
      "Train Epoche: 2 [510/2802 (18%)]\tLoss: 7.994745\n",
      "Train Epoche: 2 [511/2802 (18%)]\tLoss: 3.873536\n",
      "Train Epoche: 2 [512/2802 (18%)]\tLoss: 3.397097\n",
      "Train Epoche: 2 [513/2802 (18%)]\tLoss: 48.259769\n",
      "Train Epoche: 2 [514/2802 (18%)]\tLoss: 5.491546\n",
      "Train Epoche: 2 [515/2802 (18%)]\tLoss: 1.604124\n",
      "Train Epoche: 2 [516/2802 (18%)]\tLoss: 4.950406\n",
      "Train Epoche: 2 [517/2802 (18%)]\tLoss: 1.502762\n",
      "Train Epoche: 2 [518/2802 (18%)]\tLoss: 7.886244\n",
      "Train Epoche: 2 [519/2802 (19%)]\tLoss: 15.202793\n",
      "Train Epoche: 2 [520/2802 (19%)]\tLoss: 216.338623\n",
      "Train Epoche: 2 [521/2802 (19%)]\tLoss: 8.023685\n",
      "Train Epoche: 2 [522/2802 (19%)]\tLoss: 0.191366\n",
      "Train Epoche: 2 [523/2802 (19%)]\tLoss: 25.972507\n",
      "Train Epoche: 2 [524/2802 (19%)]\tLoss: 2.802646\n",
      "Train Epoche: 2 [525/2802 (19%)]\tLoss: 61.302074\n",
      "Train Epoche: 2 [526/2802 (19%)]\tLoss: 0.538726\n",
      "Train Epoche: 2 [527/2802 (19%)]\tLoss: 0.149794\n",
      "Train Epoche: 2 [528/2802 (19%)]\tLoss: 2.489130\n",
      "Train Epoche: 2 [529/2802 (19%)]\tLoss: 8.347754\n",
      "Train Epoche: 2 [530/2802 (19%)]\tLoss: 29.771208\n",
      "Train Epoche: 2 [531/2802 (19%)]\tLoss: 220.417725\n",
      "Train Epoche: 2 [532/2802 (19%)]\tLoss: 221.970764\n",
      "Train Epoche: 2 [533/2802 (19%)]\tLoss: 0.830566\n",
      "Train Epoche: 2 [534/2802 (19%)]\tLoss: 1.207358\n",
      "Train Epoche: 2 [535/2802 (19%)]\tLoss: 3.845377\n",
      "Train Epoche: 2 [536/2802 (19%)]\tLoss: 0.135472\n",
      "Train Epoche: 2 [537/2802 (19%)]\tLoss: 0.627741\n",
      "Train Epoche: 2 [538/2802 (19%)]\tLoss: 10.390985\n",
      "Train Epoche: 2 [539/2802 (19%)]\tLoss: 2.283057\n",
      "Train Epoche: 2 [540/2802 (19%)]\tLoss: 7.315788\n",
      "Train Epoche: 2 [541/2802 (19%)]\tLoss: 3.548133\n",
      "Train Epoche: 2 [542/2802 (19%)]\tLoss: 0.199223\n",
      "Train Epoche: 2 [543/2802 (19%)]\tLoss: 2.808419\n",
      "Train Epoche: 2 [544/2802 (19%)]\tLoss: 5.592669\n",
      "Train Epoche: 2 [545/2802 (19%)]\tLoss: 18.015928\n",
      "Train Epoche: 2 [546/2802 (19%)]\tLoss: 0.005560\n",
      "Train Epoche: 2 [547/2802 (20%)]\tLoss: 59.946857\n",
      "Train Epoche: 2 [548/2802 (20%)]\tLoss: 11.687087\n",
      "Train Epoche: 2 [549/2802 (20%)]\tLoss: 0.001226\n",
      "Train Epoche: 2 [550/2802 (20%)]\tLoss: 0.004986\n",
      "Train Epoche: 2 [551/2802 (20%)]\tLoss: 9.247190\n",
      "Train Epoche: 2 [552/2802 (20%)]\tLoss: 111.819588\n",
      "Train Epoche: 2 [553/2802 (20%)]\tLoss: 3.562382\n",
      "Train Epoche: 2 [554/2802 (20%)]\tLoss: 1.478853\n",
      "Train Epoche: 2 [555/2802 (20%)]\tLoss: 0.058752\n",
      "Train Epoche: 2 [556/2802 (20%)]\tLoss: 1.558913\n",
      "Train Epoche: 2 [557/2802 (20%)]\tLoss: 1.088733\n",
      "Train Epoche: 2 [558/2802 (20%)]\tLoss: 6.442723\n",
      "Train Epoche: 2 [559/2802 (20%)]\tLoss: 4.820427\n",
      "Train Epoche: 2 [560/2802 (20%)]\tLoss: 4.409265\n",
      "Train Epoche: 2 [561/2802 (20%)]\tLoss: 6.457047\n",
      "Train Epoche: 2 [562/2802 (20%)]\tLoss: 19.236450\n",
      "Train Epoche: 2 [563/2802 (20%)]\tLoss: 0.046449\n",
      "Train Epoche: 2 [564/2802 (20%)]\tLoss: 0.167745\n",
      "Train Epoche: 2 [565/2802 (20%)]\tLoss: 7.633718\n",
      "Train Epoche: 2 [566/2802 (20%)]\tLoss: 3.101765\n",
      "Train Epoche: 2 [567/2802 (20%)]\tLoss: 5.209311\n",
      "Train Epoche: 2 [568/2802 (20%)]\tLoss: 1.160774\n",
      "Train Epoche: 2 [569/2802 (20%)]\tLoss: 11.809520\n",
      "Train Epoche: 2 [570/2802 (20%)]\tLoss: 16.333738\n",
      "Train Epoche: 2 [571/2802 (20%)]\tLoss: 2.265803\n",
      "Train Epoche: 2 [572/2802 (20%)]\tLoss: 5.356322\n",
      "Train Epoche: 2 [573/2802 (20%)]\tLoss: 5.755142\n",
      "Train Epoche: 2 [574/2802 (20%)]\tLoss: 1.136303\n",
      "Train Epoche: 2 [575/2802 (21%)]\tLoss: 10.306556\n",
      "Train Epoche: 2 [576/2802 (21%)]\tLoss: 15.281194\n",
      "Train Epoche: 2 [577/2802 (21%)]\tLoss: 6.975368\n",
      "Train Epoche: 2 [578/2802 (21%)]\tLoss: 1.796065\n",
      "Train Epoche: 2 [579/2802 (21%)]\tLoss: 23.579838\n",
      "Train Epoche: 2 [580/2802 (21%)]\tLoss: 9.471994\n",
      "Train Epoche: 2 [581/2802 (21%)]\tLoss: 1.546612\n",
      "Train Epoche: 2 [582/2802 (21%)]\tLoss: 46.788563\n",
      "Train Epoche: 2 [583/2802 (21%)]\tLoss: 10.497497\n",
      "Train Epoche: 2 [584/2802 (21%)]\tLoss: 1.484148\n",
      "Train Epoche: 2 [585/2802 (21%)]\tLoss: 1.815096\n",
      "Train Epoche: 2 [586/2802 (21%)]\tLoss: 7.397489\n",
      "Train Epoche: 2 [587/2802 (21%)]\tLoss: 0.021987\n",
      "Train Epoche: 2 [588/2802 (21%)]\tLoss: 10.331376\n",
      "Train Epoche: 2 [589/2802 (21%)]\tLoss: 1.630241\n",
      "Train Epoche: 2 [590/2802 (21%)]\tLoss: 28.285742\n",
      "Train Epoche: 2 [591/2802 (21%)]\tLoss: 14.852523\n",
      "Train Epoche: 2 [592/2802 (21%)]\tLoss: 0.313967\n",
      "Train Epoche: 2 [593/2802 (21%)]\tLoss: 3.404652\n",
      "Train Epoche: 2 [594/2802 (21%)]\tLoss: 3.373244\n",
      "Train Epoche: 2 [595/2802 (21%)]\tLoss: 4.619405\n",
      "Train Epoche: 2 [596/2802 (21%)]\tLoss: 11.075800\n",
      "Train Epoche: 2 [597/2802 (21%)]\tLoss: 76.897118\n",
      "Train Epoche: 2 [598/2802 (21%)]\tLoss: 16.334768\n",
      "Train Epoche: 2 [599/2802 (21%)]\tLoss: 2.664977\n",
      "Train Epoche: 2 [600/2802 (21%)]\tLoss: 30.685154\n",
      "Train Epoche: 2 [601/2802 (21%)]\tLoss: 0.333944\n",
      "Train Epoche: 2 [602/2802 (21%)]\tLoss: 0.687096\n",
      "Train Epoche: 2 [603/2802 (22%)]\tLoss: 0.139528\n",
      "Train Epoche: 2 [604/2802 (22%)]\tLoss: 0.109668\n",
      "Train Epoche: 2 [605/2802 (22%)]\tLoss: 244.701385\n",
      "Train Epoche: 2 [606/2802 (22%)]\tLoss: 0.538979\n",
      "Train Epoche: 2 [607/2802 (22%)]\tLoss: 0.422221\n",
      "Train Epoche: 2 [608/2802 (22%)]\tLoss: 0.000299\n",
      "Train Epoche: 2 [609/2802 (22%)]\tLoss: 169.125244\n",
      "Train Epoche: 2 [610/2802 (22%)]\tLoss: 4.303707\n",
      "Train Epoche: 2 [611/2802 (22%)]\tLoss: 27.529726\n",
      "Train Epoche: 2 [612/2802 (22%)]\tLoss: 0.352663\n",
      "Train Epoche: 2 [613/2802 (22%)]\tLoss: 0.294129\n",
      "Train Epoche: 2 [614/2802 (22%)]\tLoss: 2.171669\n",
      "Train Epoche: 2 [615/2802 (22%)]\tLoss: 1.943870\n",
      "Train Epoche: 2 [616/2802 (22%)]\tLoss: 3.782223\n",
      "Train Epoche: 2 [617/2802 (22%)]\tLoss: 2.114101\n",
      "Train Epoche: 2 [618/2802 (22%)]\tLoss: 34.003963\n",
      "Train Epoche: 2 [619/2802 (22%)]\tLoss: 1.501370\n",
      "Train Epoche: 2 [620/2802 (22%)]\tLoss: 10.054954\n",
      "Train Epoche: 2 [621/2802 (22%)]\tLoss: 12.070362\n",
      "Train Epoche: 2 [622/2802 (22%)]\tLoss: 2.792211\n",
      "Train Epoche: 2 [623/2802 (22%)]\tLoss: 299.427521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [624/2802 (22%)]\tLoss: 150.079697\n",
      "Train Epoche: 2 [625/2802 (22%)]\tLoss: 31.651848\n",
      "Train Epoche: 2 [626/2802 (22%)]\tLoss: 21.986320\n",
      "Train Epoche: 2 [627/2802 (22%)]\tLoss: 36.346741\n",
      "Train Epoche: 2 [628/2802 (22%)]\tLoss: 5.139874\n",
      "Train Epoche: 2 [629/2802 (22%)]\tLoss: 272.731079\n",
      "Train Epoche: 2 [630/2802 (22%)]\tLoss: 13.347725\n",
      "Train Epoche: 2 [631/2802 (23%)]\tLoss: 11.375458\n",
      "Train Epoche: 2 [632/2802 (23%)]\tLoss: 4.006881\n",
      "Train Epoche: 2 [633/2802 (23%)]\tLoss: 7.563374\n",
      "Train Epoche: 2 [634/2802 (23%)]\tLoss: 0.967578\n",
      "Train Epoche: 2 [635/2802 (23%)]\tLoss: 12.043387\n",
      "Train Epoche: 2 [636/2802 (23%)]\tLoss: 4.679126\n",
      "Train Epoche: 2 [637/2802 (23%)]\tLoss: 22.769812\n",
      "Train Epoche: 2 [638/2802 (23%)]\tLoss: 0.858657\n",
      "Train Epoche: 2 [639/2802 (23%)]\tLoss: 6.060075\n",
      "Train Epoche: 2 [640/2802 (23%)]\tLoss: 0.053781\n",
      "Train Epoche: 2 [641/2802 (23%)]\tLoss: 0.724560\n",
      "Train Epoche: 2 [642/2802 (23%)]\tLoss: 5.815898\n",
      "Train Epoche: 2 [643/2802 (23%)]\tLoss: 12.356400\n",
      "Train Epoche: 2 [644/2802 (23%)]\tLoss: 14.352341\n",
      "Train Epoche: 2 [645/2802 (23%)]\tLoss: 25.217911\n",
      "Train Epoche: 2 [646/2802 (23%)]\tLoss: 1.255897\n",
      "Train Epoche: 2 [647/2802 (23%)]\tLoss: 0.340739\n",
      "Train Epoche: 2 [648/2802 (23%)]\tLoss: 146.455292\n",
      "Train Epoche: 2 [649/2802 (23%)]\tLoss: 5.124797\n",
      "Train Epoche: 2 [650/2802 (23%)]\tLoss: 113.437683\n",
      "Train Epoche: 2 [651/2802 (23%)]\tLoss: 7.119263\n",
      "Train Epoche: 2 [652/2802 (23%)]\tLoss: 0.457777\n",
      "Train Epoche: 2 [653/2802 (23%)]\tLoss: 22.023272\n",
      "Train Epoche: 2 [654/2802 (23%)]\tLoss: 7.399608\n",
      "Train Epoche: 2 [655/2802 (23%)]\tLoss: 37.522484\n",
      "Train Epoche: 2 [656/2802 (23%)]\tLoss: 58.007629\n",
      "Train Epoche: 2 [657/2802 (23%)]\tLoss: 79.619629\n",
      "Train Epoche: 2 [658/2802 (23%)]\tLoss: 8.226048\n",
      "Train Epoche: 2 [659/2802 (24%)]\tLoss: 33.831524\n",
      "Train Epoche: 2 [660/2802 (24%)]\tLoss: 0.000976\n",
      "Train Epoche: 2 [661/2802 (24%)]\tLoss: 1.693843\n",
      "Train Epoche: 2 [662/2802 (24%)]\tLoss: 83.650291\n",
      "Train Epoche: 2 [663/2802 (24%)]\tLoss: 3.054531\n",
      "Train Epoche: 2 [664/2802 (24%)]\tLoss: 17.268145\n",
      "Train Epoche: 2 [665/2802 (24%)]\tLoss: 20.321852\n",
      "Train Epoche: 2 [666/2802 (24%)]\tLoss: 0.374524\n",
      "Train Epoche: 2 [667/2802 (24%)]\tLoss: 0.121559\n",
      "Train Epoche: 2 [668/2802 (24%)]\tLoss: 38.157085\n",
      "Train Epoche: 2 [669/2802 (24%)]\tLoss: 35.457413\n",
      "Train Epoche: 2 [670/2802 (24%)]\tLoss: 17.055187\n",
      "Train Epoche: 2 [671/2802 (24%)]\tLoss: 74.851501\n",
      "Train Epoche: 2 [672/2802 (24%)]\tLoss: 269.052643\n",
      "Train Epoche: 2 [673/2802 (24%)]\tLoss: 20.858110\n",
      "Train Epoche: 2 [674/2802 (24%)]\tLoss: 0.898961\n",
      "Train Epoche: 2 [675/2802 (24%)]\tLoss: 0.761811\n",
      "Train Epoche: 2 [676/2802 (24%)]\tLoss: 4.457368\n",
      "Train Epoche: 2 [677/2802 (24%)]\tLoss: 10.587478\n",
      "Train Epoche: 2 [678/2802 (24%)]\tLoss: 1.628125\n",
      "Train Epoche: 2 [679/2802 (24%)]\tLoss: 6.431191\n",
      "Train Epoche: 2 [680/2802 (24%)]\tLoss: 20.295652\n",
      "Train Epoche: 2 [681/2802 (24%)]\tLoss: 4.517011\n",
      "Train Epoche: 2 [682/2802 (24%)]\tLoss: 1.613584\n",
      "Train Epoche: 2 [683/2802 (24%)]\tLoss: 1.583144\n",
      "Train Epoche: 2 [684/2802 (24%)]\tLoss: 2.771509\n",
      "Train Epoche: 2 [685/2802 (24%)]\tLoss: 0.258831\n",
      "Train Epoche: 2 [686/2802 (24%)]\tLoss: 2.087341\n",
      "Train Epoche: 2 [687/2802 (25%)]\tLoss: 9.948752\n",
      "Train Epoche: 2 [688/2802 (25%)]\tLoss: 1.113476\n",
      "Train Epoche: 2 [689/2802 (25%)]\tLoss: 24.917547\n",
      "Train Epoche: 2 [690/2802 (25%)]\tLoss: 4.086787\n",
      "Train Epoche: 2 [691/2802 (25%)]\tLoss: 0.184318\n",
      "Train Epoche: 2 [692/2802 (25%)]\tLoss: 1.459513\n",
      "Train Epoche: 2 [693/2802 (25%)]\tLoss: 11.581629\n",
      "Train Epoche: 2 [694/2802 (25%)]\tLoss: 3.555493\n",
      "Train Epoche: 2 [695/2802 (25%)]\tLoss: 13.686125\n",
      "Train Epoche: 2 [696/2802 (25%)]\tLoss: 6.332986\n",
      "Train Epoche: 2 [697/2802 (25%)]\tLoss: 7.350082\n",
      "Train Epoche: 2 [698/2802 (25%)]\tLoss: 29.401682\n",
      "Train Epoche: 2 [699/2802 (25%)]\tLoss: 1.558397\n",
      "Train Epoche: 2 [700/2802 (25%)]\tLoss: 0.015734\n",
      "Train Epoche: 2 [701/2802 (25%)]\tLoss: 3.688001\n",
      "Train Epoche: 2 [702/2802 (25%)]\tLoss: 45.146214\n",
      "Train Epoche: 2 [703/2802 (25%)]\tLoss: 0.003440\n",
      "Train Epoche: 2 [704/2802 (25%)]\tLoss: 59.675854\n",
      "Train Epoche: 2 [705/2802 (25%)]\tLoss: 0.682828\n",
      "Train Epoche: 2 [706/2802 (25%)]\tLoss: 183.031372\n",
      "Train Epoche: 2 [707/2802 (25%)]\tLoss: 8.887907\n",
      "Train Epoche: 2 [708/2802 (25%)]\tLoss: 3.119607\n",
      "Train Epoche: 2 [709/2802 (25%)]\tLoss: 17.537357\n",
      "Train Epoche: 2 [710/2802 (25%)]\tLoss: 2.555633\n",
      "Train Epoche: 2 [711/2802 (25%)]\tLoss: 10.437469\n",
      "Train Epoche: 2 [712/2802 (25%)]\tLoss: 0.068966\n",
      "Train Epoche: 2 [713/2802 (25%)]\tLoss: 0.622021\n",
      "Train Epoche: 2 [714/2802 (25%)]\tLoss: 2.567441\n",
      "Train Epoche: 2 [715/2802 (26%)]\tLoss: 0.775058\n",
      "Train Epoche: 2 [716/2802 (26%)]\tLoss: 20.753672\n",
      "Train Epoche: 2 [717/2802 (26%)]\tLoss: 32.150955\n",
      "Train Epoche: 2 [718/2802 (26%)]\tLoss: 0.223015\n",
      "Train Epoche: 2 [719/2802 (26%)]\tLoss: 4.181936\n",
      "Train Epoche: 2 [720/2802 (26%)]\tLoss: 6.459838\n",
      "Train Epoche: 2 [721/2802 (26%)]\tLoss: 7.665225\n",
      "Train Epoche: 2 [722/2802 (26%)]\tLoss: 0.213926\n",
      "Train Epoche: 2 [723/2802 (26%)]\tLoss: 66.662239\n",
      "Train Epoche: 2 [724/2802 (26%)]\tLoss: 14.048270\n",
      "Train Epoche: 2 [725/2802 (26%)]\tLoss: 49.474827\n",
      "Train Epoche: 2 [726/2802 (26%)]\tLoss: 1.675035\n",
      "Train Epoche: 2 [727/2802 (26%)]\tLoss: 73.624954\n",
      "Train Epoche: 2 [728/2802 (26%)]\tLoss: 1.349745\n",
      "Train Epoche: 2 [729/2802 (26%)]\tLoss: 1.533643\n",
      "Train Epoche: 2 [730/2802 (26%)]\tLoss: 0.005308\n",
      "Train Epoche: 2 [731/2802 (26%)]\tLoss: 12.043093\n",
      "Train Epoche: 2 [732/2802 (26%)]\tLoss: 2.261312\n",
      "Train Epoche: 2 [733/2802 (26%)]\tLoss: 5.600128\n",
      "Train Epoche: 2 [734/2802 (26%)]\tLoss: 12.016118\n",
      "Train Epoche: 2 [735/2802 (26%)]\tLoss: 3.641514\n",
      "Train Epoche: 2 [736/2802 (26%)]\tLoss: 2.744060\n",
      "Train Epoche: 2 [737/2802 (26%)]\tLoss: 46.502979\n",
      "Train Epoche: 2 [738/2802 (26%)]\tLoss: 31.768105\n",
      "Train Epoche: 2 [739/2802 (26%)]\tLoss: 14.650880\n",
      "Train Epoche: 2 [740/2802 (26%)]\tLoss: 21.435514\n",
      "Train Epoche: 2 [741/2802 (26%)]\tLoss: 1.025318\n",
      "Train Epoche: 2 [742/2802 (26%)]\tLoss: 2.718484\n",
      "Train Epoche: 2 [743/2802 (27%)]\tLoss: 11.457142\n",
      "Train Epoche: 2 [744/2802 (27%)]\tLoss: 12.044489\n",
      "Train Epoche: 2 [745/2802 (27%)]\tLoss: 18.111309\n",
      "Train Epoche: 2 [746/2802 (27%)]\tLoss: 159.208817\n",
      "Train Epoche: 2 [747/2802 (27%)]\tLoss: 11.099426\n",
      "Train Epoche: 2 [748/2802 (27%)]\tLoss: 1.149009\n",
      "Train Epoche: 2 [749/2802 (27%)]\tLoss: 2.415678\n",
      "Train Epoche: 2 [750/2802 (27%)]\tLoss: 4.412533\n",
      "Train Epoche: 2 [751/2802 (27%)]\tLoss: 7.615272\n",
      "Train Epoche: 2 [752/2802 (27%)]\tLoss: 0.601402\n",
      "Train Epoche: 2 [753/2802 (27%)]\tLoss: 7.392245\n",
      "Train Epoche: 2 [754/2802 (27%)]\tLoss: 0.452902\n",
      "Train Epoche: 2 [755/2802 (27%)]\tLoss: 2.791863\n",
      "Train Epoche: 2 [756/2802 (27%)]\tLoss: 0.154324\n",
      "Train Epoche: 2 [757/2802 (27%)]\tLoss: 1.180885\n",
      "Train Epoche: 2 [758/2802 (27%)]\tLoss: 1.388329\n",
      "Train Epoche: 2 [759/2802 (27%)]\tLoss: 5.790914\n",
      "Train Epoche: 2 [760/2802 (27%)]\tLoss: 21.144608\n",
      "Train Epoche: 2 [761/2802 (27%)]\tLoss: 8.378891\n",
      "Train Epoche: 2 [762/2802 (27%)]\tLoss: 0.045495\n",
      "Train Epoche: 2 [763/2802 (27%)]\tLoss: 0.351599\n",
      "Train Epoche: 2 [764/2802 (27%)]\tLoss: 4.718214\n",
      "Train Epoche: 2 [765/2802 (27%)]\tLoss: 45.895477\n",
      "Train Epoche: 2 [766/2802 (27%)]\tLoss: 39.340248\n",
      "Train Epoche: 2 [767/2802 (27%)]\tLoss: 1.286866\n",
      "Train Epoche: 2 [768/2802 (27%)]\tLoss: 5.367222\n",
      "Train Epoche: 2 [769/2802 (27%)]\tLoss: 16.993774\n",
      "Train Epoche: 2 [770/2802 (27%)]\tLoss: 9.037083\n",
      "Train Epoche: 2 [771/2802 (28%)]\tLoss: 15.215227\n",
      "Train Epoche: 2 [772/2802 (28%)]\tLoss: 1.020056\n",
      "Train Epoche: 2 [773/2802 (28%)]\tLoss: 1.306142\n",
      "Train Epoche: 2 [774/2802 (28%)]\tLoss: 8.838897\n",
      "Train Epoche: 2 [775/2802 (28%)]\tLoss: 10.450487\n",
      "Train Epoche: 2 [776/2802 (28%)]\tLoss: 0.139795\n",
      "Train Epoche: 2 [777/2802 (28%)]\tLoss: 16.892105\n",
      "Train Epoche: 2 [778/2802 (28%)]\tLoss: 1.636505\n",
      "Train Epoche: 2 [779/2802 (28%)]\tLoss: 1.589214\n",
      "Train Epoche: 2 [780/2802 (28%)]\tLoss: 2.529936\n",
      "Train Epoche: 2 [781/2802 (28%)]\tLoss: 0.202690\n",
      "Train Epoche: 2 [782/2802 (28%)]\tLoss: 80.604736\n",
      "Train Epoche: 2 [783/2802 (28%)]\tLoss: 21.886213\n",
      "Train Epoche: 2 [784/2802 (28%)]\tLoss: 0.021799\n",
      "Train Epoche: 2 [785/2802 (28%)]\tLoss: 12.950725\n",
      "Train Epoche: 2 [786/2802 (28%)]\tLoss: 0.534632\n",
      "Train Epoche: 2 [787/2802 (28%)]\tLoss: 127.913826\n",
      "Train Epoche: 2 [788/2802 (28%)]\tLoss: 0.331733\n",
      "Train Epoche: 2 [789/2802 (28%)]\tLoss: 0.103776\n",
      "Train Epoche: 2 [790/2802 (28%)]\tLoss: 0.431821\n",
      "Train Epoche: 2 [791/2802 (28%)]\tLoss: 13.242730\n",
      "Train Epoche: 2 [792/2802 (28%)]\tLoss: 12.038863\n",
      "Train Epoche: 2 [793/2802 (28%)]\tLoss: 0.000085\n",
      "Train Epoche: 2 [794/2802 (28%)]\tLoss: 50.567867\n",
      "Train Epoche: 2 [795/2802 (28%)]\tLoss: 0.124007\n",
      "Train Epoche: 2 [796/2802 (28%)]\tLoss: 0.038792\n",
      "Train Epoche: 2 [797/2802 (28%)]\tLoss: 7.512350\n",
      "Train Epoche: 2 [798/2802 (28%)]\tLoss: 0.000929\n",
      "Train Epoche: 2 [799/2802 (29%)]\tLoss: 2.179313\n",
      "Train Epoche: 2 [800/2802 (29%)]\tLoss: 0.054362\n",
      "Train Epoche: 2 [801/2802 (29%)]\tLoss: 19.306307\n",
      "Train Epoche: 2 [802/2802 (29%)]\tLoss: 30.856375\n",
      "Train Epoche: 2 [803/2802 (29%)]\tLoss: 2.914763\n",
      "Train Epoche: 2 [804/2802 (29%)]\tLoss: 1.521020\n",
      "Train Epoche: 2 [805/2802 (29%)]\tLoss: 8.098211\n",
      "Train Epoche: 2 [806/2802 (29%)]\tLoss: 3.949084\n",
      "Train Epoche: 2 [807/2802 (29%)]\tLoss: 66.409019\n",
      "Train Epoche: 2 [808/2802 (29%)]\tLoss: 1.014848\n",
      "Train Epoche: 2 [809/2802 (29%)]\tLoss: 0.000265\n",
      "Train Epoche: 2 [810/2802 (29%)]\tLoss: 15.377423\n",
      "Train Epoche: 2 [811/2802 (29%)]\tLoss: 0.035599\n",
      "Train Epoche: 2 [812/2802 (29%)]\tLoss: 36.481586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [813/2802 (29%)]\tLoss: 2.339087\n",
      "Train Epoche: 2 [814/2802 (29%)]\tLoss: 8.975132\n",
      "Train Epoche: 2 [815/2802 (29%)]\tLoss: 37.669044\n",
      "Train Epoche: 2 [816/2802 (29%)]\tLoss: 19.015644\n",
      "Train Epoche: 2 [817/2802 (29%)]\tLoss: 6.045685\n",
      "Train Epoche: 2 [818/2802 (29%)]\tLoss: 0.008029\n",
      "Train Epoche: 2 [819/2802 (29%)]\tLoss: 23.175249\n",
      "Train Epoche: 2 [820/2802 (29%)]\tLoss: 9.531246\n",
      "Train Epoche: 2 [821/2802 (29%)]\tLoss: 6.612928\n",
      "Train Epoche: 2 [822/2802 (29%)]\tLoss: 1.493621\n",
      "Train Epoche: 2 [823/2802 (29%)]\tLoss: 1.270793\n",
      "Train Epoche: 2 [824/2802 (29%)]\tLoss: 47.121754\n",
      "Train Epoche: 2 [825/2802 (29%)]\tLoss: 9.788566\n",
      "Train Epoche: 2 [826/2802 (29%)]\tLoss: 67.443611\n",
      "Train Epoche: 2 [827/2802 (30%)]\tLoss: 0.000417\n",
      "Train Epoche: 2 [828/2802 (30%)]\tLoss: 25.222948\n",
      "Train Epoche: 2 [829/2802 (30%)]\tLoss: 5.607903\n",
      "Train Epoche: 2 [830/2802 (30%)]\tLoss: 9.060584\n",
      "Train Epoche: 2 [831/2802 (30%)]\tLoss: 31.889915\n",
      "Train Epoche: 2 [832/2802 (30%)]\tLoss: 31.153814\n",
      "Train Epoche: 2 [833/2802 (30%)]\tLoss: 11.707525\n",
      "Train Epoche: 2 [834/2802 (30%)]\tLoss: 3.605807\n",
      "Train Epoche: 2 [835/2802 (30%)]\tLoss: 5.920587\n",
      "Train Epoche: 2 [836/2802 (30%)]\tLoss: 7.580534\n",
      "Train Epoche: 2 [837/2802 (30%)]\tLoss: 10.204172\n",
      "Train Epoche: 2 [838/2802 (30%)]\tLoss: 105.773026\n",
      "Train Epoche: 2 [839/2802 (30%)]\tLoss: 43.368343\n",
      "Train Epoche: 2 [840/2802 (30%)]\tLoss: 18.714096\n",
      "Train Epoche: 2 [841/2802 (30%)]\tLoss: 5.283804\n",
      "Train Epoche: 2 [842/2802 (30%)]\tLoss: 3.148589\n",
      "Train Epoche: 2 [843/2802 (30%)]\tLoss: 6.115307\n",
      "Train Epoche: 2 [844/2802 (30%)]\tLoss: 1.515617\n",
      "Train Epoche: 2 [845/2802 (30%)]\tLoss: 9.804681\n",
      "Train Epoche: 2 [846/2802 (30%)]\tLoss: 42.792271\n",
      "Train Epoche: 2 [847/2802 (30%)]\tLoss: 121.747063\n",
      "Train Epoche: 2 [848/2802 (30%)]\tLoss: 11.357279\n",
      "Train Epoche: 2 [849/2802 (30%)]\tLoss: 3.369266\n",
      "Train Epoche: 2 [850/2802 (30%)]\tLoss: 12.223805\n",
      "Train Epoche: 2 [851/2802 (30%)]\tLoss: 0.765885\n",
      "Train Epoche: 2 [852/2802 (30%)]\tLoss: 59.831150\n",
      "Train Epoche: 2 [853/2802 (30%)]\tLoss: 10.909075\n",
      "Train Epoche: 2 [854/2802 (30%)]\tLoss: 11.625316\n",
      "Train Epoche: 2 [855/2802 (31%)]\tLoss: 17.769543\n",
      "Train Epoche: 2 [856/2802 (31%)]\tLoss: 2.116131\n",
      "Train Epoche: 2 [857/2802 (31%)]\tLoss: 10.505458\n",
      "Train Epoche: 2 [858/2802 (31%)]\tLoss: 0.795806\n",
      "Train Epoche: 2 [859/2802 (31%)]\tLoss: 91.898849\n",
      "Train Epoche: 2 [860/2802 (31%)]\tLoss: 0.037891\n",
      "Train Epoche: 2 [861/2802 (31%)]\tLoss: 7.256132\n",
      "Train Epoche: 2 [862/2802 (31%)]\tLoss: 0.208296\n",
      "Train Epoche: 2 [863/2802 (31%)]\tLoss: 4.744033\n",
      "Train Epoche: 2 [864/2802 (31%)]\tLoss: 0.380611\n",
      "Train Epoche: 2 [865/2802 (31%)]\tLoss: 0.295276\n",
      "Train Epoche: 2 [866/2802 (31%)]\tLoss: 0.268294\n",
      "Train Epoche: 2 [867/2802 (31%)]\tLoss: 2.327516\n",
      "Train Epoche: 2 [868/2802 (31%)]\tLoss: 6.397425\n",
      "Train Epoche: 2 [869/2802 (31%)]\tLoss: 0.543625\n",
      "Train Epoche: 2 [870/2802 (31%)]\tLoss: 11.347658\n",
      "Train Epoche: 2 [871/2802 (31%)]\tLoss: 0.156882\n",
      "Train Epoche: 2 [872/2802 (31%)]\tLoss: 4.616280\n",
      "Train Epoche: 2 [873/2802 (31%)]\tLoss: 4.312192\n",
      "Train Epoche: 2 [874/2802 (31%)]\tLoss: 12.604885\n",
      "Train Epoche: 2 [875/2802 (31%)]\tLoss: 107.825432\n",
      "Train Epoche: 2 [876/2802 (31%)]\tLoss: 131.799454\n",
      "Train Epoche: 2 [877/2802 (31%)]\tLoss: 45.090061\n",
      "Train Epoche: 2 [878/2802 (31%)]\tLoss: 1.167723\n",
      "Train Epoche: 2 [879/2802 (31%)]\tLoss: 8.307089\n",
      "Train Epoche: 2 [880/2802 (31%)]\tLoss: 7.373405\n",
      "Train Epoche: 2 [881/2802 (31%)]\tLoss: 2.068596\n",
      "Train Epoche: 2 [882/2802 (31%)]\tLoss: 38.582130\n",
      "Train Epoche: 2 [883/2802 (32%)]\tLoss: 22.445669\n",
      "Train Epoche: 2 [884/2802 (32%)]\tLoss: 1.439330\n",
      "Train Epoche: 2 [885/2802 (32%)]\tLoss: 0.244293\n",
      "Train Epoche: 2 [886/2802 (32%)]\tLoss: 0.347089\n",
      "Train Epoche: 2 [887/2802 (32%)]\tLoss: 0.220425\n",
      "Train Epoche: 2 [888/2802 (32%)]\tLoss: 12.056123\n",
      "Train Epoche: 2 [889/2802 (32%)]\tLoss: 1.022786\n",
      "Train Epoche: 2 [890/2802 (32%)]\tLoss: 13.444832\n",
      "Train Epoche: 2 [891/2802 (32%)]\tLoss: 25.951157\n",
      "Train Epoche: 2 [892/2802 (32%)]\tLoss: 198.785645\n",
      "Train Epoche: 2 [893/2802 (32%)]\tLoss: 1.294112\n",
      "Train Epoche: 2 [894/2802 (32%)]\tLoss: 0.600895\n",
      "Train Epoche: 2 [895/2802 (32%)]\tLoss: 0.063591\n",
      "Train Epoche: 2 [896/2802 (32%)]\tLoss: 25.198298\n",
      "Train Epoche: 2 [897/2802 (32%)]\tLoss: 186.457062\n",
      "Train Epoche: 2 [898/2802 (32%)]\tLoss: 0.425526\n",
      "Train Epoche: 2 [899/2802 (32%)]\tLoss: 0.754524\n",
      "Train Epoche: 2 [900/2802 (32%)]\tLoss: 29.858431\n",
      "Train Epoche: 2 [901/2802 (32%)]\tLoss: 18.981840\n",
      "Train Epoche: 2 [902/2802 (32%)]\tLoss: 0.960650\n",
      "Train Epoche: 2 [903/2802 (32%)]\tLoss: 24.054111\n",
      "Train Epoche: 2 [904/2802 (32%)]\tLoss: 10.138755\n",
      "Train Epoche: 2 [905/2802 (32%)]\tLoss: 0.989631\n",
      "Train Epoche: 2 [906/2802 (32%)]\tLoss: 205.833679\n",
      "Train Epoche: 2 [907/2802 (32%)]\tLoss: 1.371436\n",
      "Train Epoche: 2 [908/2802 (32%)]\tLoss: 87.929428\n",
      "Train Epoche: 2 [909/2802 (32%)]\tLoss: 4.698153\n",
      "Train Epoche: 2 [910/2802 (32%)]\tLoss: 4.818045\n",
      "Train Epoche: 2 [911/2802 (33%)]\tLoss: 0.146715\n",
      "Train Epoche: 2 [912/2802 (33%)]\tLoss: 28.231209\n",
      "Train Epoche: 2 [913/2802 (33%)]\tLoss: 7.267805\n",
      "Train Epoche: 2 [914/2802 (33%)]\tLoss: 0.924367\n",
      "Train Epoche: 2 [915/2802 (33%)]\tLoss: 0.441175\n",
      "Train Epoche: 2 [916/2802 (33%)]\tLoss: 80.441246\n",
      "Train Epoche: 2 [917/2802 (33%)]\tLoss: 1.688947\n",
      "Train Epoche: 2 [918/2802 (33%)]\tLoss: 0.284569\n",
      "Train Epoche: 2 [919/2802 (33%)]\tLoss: 0.019789\n",
      "Train Epoche: 2 [920/2802 (33%)]\tLoss: 9.539642\n",
      "Train Epoche: 2 [921/2802 (33%)]\tLoss: 0.000020\n",
      "Train Epoche: 2 [922/2802 (33%)]\tLoss: 18.731897\n",
      "Train Epoche: 2 [923/2802 (33%)]\tLoss: 0.440135\n",
      "Train Epoche: 2 [924/2802 (33%)]\tLoss: 3.973300\n",
      "Train Epoche: 2 [925/2802 (33%)]\tLoss: 5.149156\n",
      "Train Epoche: 2 [926/2802 (33%)]\tLoss: 0.241760\n",
      "Train Epoche: 2 [927/2802 (33%)]\tLoss: 4.234416\n",
      "Train Epoche: 2 [928/2802 (33%)]\tLoss: 0.702073\n",
      "Train Epoche: 2 [929/2802 (33%)]\tLoss: 0.120510\n",
      "Train Epoche: 2 [930/2802 (33%)]\tLoss: 0.010055\n",
      "Train Epoche: 2 [931/2802 (33%)]\tLoss: 12.429401\n",
      "Train Epoche: 2 [932/2802 (33%)]\tLoss: 166.236893\n",
      "Train Epoche: 2 [933/2802 (33%)]\tLoss: 16.648548\n",
      "Train Epoche: 2 [934/2802 (33%)]\tLoss: 0.712172\n",
      "Train Epoche: 2 [935/2802 (33%)]\tLoss: 51.023643\n",
      "Train Epoche: 2 [936/2802 (33%)]\tLoss: 68.012375\n",
      "Train Epoche: 2 [937/2802 (33%)]\tLoss: 72.596550\n",
      "Train Epoche: 2 [938/2802 (33%)]\tLoss: 8.635878\n",
      "Train Epoche: 2 [939/2802 (34%)]\tLoss: 0.042476\n",
      "Train Epoche: 2 [940/2802 (34%)]\tLoss: 0.743524\n",
      "Train Epoche: 2 [941/2802 (34%)]\tLoss: 11.325399\n",
      "Train Epoche: 2 [942/2802 (34%)]\tLoss: 5.725573\n",
      "Train Epoche: 2 [943/2802 (34%)]\tLoss: 10.025775\n",
      "Train Epoche: 2 [944/2802 (34%)]\tLoss: 0.824035\n",
      "Train Epoche: 2 [945/2802 (34%)]\tLoss: 32.159801\n",
      "Train Epoche: 2 [946/2802 (34%)]\tLoss: 357.040863\n",
      "Train Epoche: 2 [947/2802 (34%)]\tLoss: 2.273205\n",
      "Train Epoche: 2 [948/2802 (34%)]\tLoss: 2.214093\n",
      "Train Epoche: 2 [949/2802 (34%)]\tLoss: 1.135681\n",
      "Train Epoche: 2 [950/2802 (34%)]\tLoss: 0.402373\n",
      "Train Epoche: 2 [951/2802 (34%)]\tLoss: 13.054706\n",
      "Train Epoche: 2 [952/2802 (34%)]\tLoss: 3.636277\n",
      "Train Epoche: 2 [953/2802 (34%)]\tLoss: 9.149226\n",
      "Train Epoche: 2 [954/2802 (34%)]\tLoss: 3.201721\n",
      "Train Epoche: 2 [955/2802 (34%)]\tLoss: 2.305064\n",
      "Train Epoche: 2 [956/2802 (34%)]\tLoss: 0.245698\n",
      "Train Epoche: 2 [957/2802 (34%)]\tLoss: 0.117102\n",
      "Train Epoche: 2 [958/2802 (34%)]\tLoss: 11.693661\n",
      "Train Epoche: 2 [959/2802 (34%)]\tLoss: 2.401327\n",
      "Train Epoche: 2 [960/2802 (34%)]\tLoss: 1.327309\n",
      "Train Epoche: 2 [961/2802 (34%)]\tLoss: 1.861279\n",
      "Train Epoche: 2 [962/2802 (34%)]\tLoss: 20.994696\n",
      "Train Epoche: 2 [963/2802 (34%)]\tLoss: 1.027594\n",
      "Train Epoche: 2 [964/2802 (34%)]\tLoss: 5.864414\n",
      "Train Epoche: 2 [965/2802 (34%)]\tLoss: 8.326558\n",
      "Train Epoche: 2 [966/2802 (34%)]\tLoss: 10.398816\n",
      "Train Epoche: 2 [967/2802 (35%)]\tLoss: 28.553829\n",
      "Train Epoche: 2 [968/2802 (35%)]\tLoss: 2.368933\n",
      "Train Epoche: 2 [969/2802 (35%)]\tLoss: 0.444375\n",
      "Train Epoche: 2 [970/2802 (35%)]\tLoss: 0.122534\n",
      "Train Epoche: 2 [971/2802 (35%)]\tLoss: 27.545820\n",
      "Train Epoche: 2 [972/2802 (35%)]\tLoss: 1.715163\n",
      "Train Epoche: 2 [973/2802 (35%)]\tLoss: 25.371260\n",
      "Train Epoche: 2 [974/2802 (35%)]\tLoss: 11.320977\n",
      "Train Epoche: 2 [975/2802 (35%)]\tLoss: 0.381423\n",
      "Train Epoche: 2 [976/2802 (35%)]\tLoss: 0.172427\n",
      "Train Epoche: 2 [977/2802 (35%)]\tLoss: 3.149060\n",
      "Train Epoche: 2 [978/2802 (35%)]\tLoss: 0.090629\n",
      "Train Epoche: 2 [979/2802 (35%)]\tLoss: 1.692690\n",
      "Train Epoche: 2 [980/2802 (35%)]\tLoss: 2.620181\n",
      "Train Epoche: 2 [981/2802 (35%)]\tLoss: 1.223221\n",
      "Train Epoche: 2 [982/2802 (35%)]\tLoss: 7.067813\n",
      "Train Epoche: 2 [983/2802 (35%)]\tLoss: 4.778550\n",
      "Train Epoche: 2 [984/2802 (35%)]\tLoss: 0.385540\n",
      "Train Epoche: 2 [985/2802 (35%)]\tLoss: 2.966523\n",
      "Train Epoche: 2 [986/2802 (35%)]\tLoss: 0.013621\n",
      "Train Epoche: 2 [987/2802 (35%)]\tLoss: 17.438467\n",
      "Train Epoche: 2 [988/2802 (35%)]\tLoss: 4.712589\n",
      "Train Epoche: 2 [989/2802 (35%)]\tLoss: 9.923261\n",
      "Train Epoche: 2 [990/2802 (35%)]\tLoss: 7.976998\n",
      "Train Epoche: 2 [991/2802 (35%)]\tLoss: 0.000176\n",
      "Train Epoche: 2 [992/2802 (35%)]\tLoss: 0.438635\n",
      "Train Epoche: 2 [993/2802 (35%)]\tLoss: 0.193961\n",
      "Train Epoche: 2 [994/2802 (35%)]\tLoss: 6.311534\n",
      "Train Epoche: 2 [995/2802 (36%)]\tLoss: 10.143945\n",
      "Train Epoche: 2 [996/2802 (36%)]\tLoss: 4.707005\n",
      "Train Epoche: 2 [997/2802 (36%)]\tLoss: 30.335537\n",
      "Train Epoche: 2 [998/2802 (36%)]\tLoss: 15.998077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [999/2802 (36%)]\tLoss: 2.118680\n",
      "Train Epoche: 2 [1000/2802 (36%)]\tLoss: 0.324111\n",
      "Train Epoche: 2 [1001/2802 (36%)]\tLoss: 46.501312\n",
      "Train Epoche: 2 [1002/2802 (36%)]\tLoss: 5.104119\n",
      "Train Epoche: 2 [1003/2802 (36%)]\tLoss: 27.444527\n",
      "Train Epoche: 2 [1004/2802 (36%)]\tLoss: 0.593343\n",
      "Train Epoche: 2 [1005/2802 (36%)]\tLoss: 25.346432\n",
      "Train Epoche: 2 [1006/2802 (36%)]\tLoss: 66.063736\n",
      "Train Epoche: 2 [1007/2802 (36%)]\tLoss: 14.783129\n",
      "Train Epoche: 2 [1008/2802 (36%)]\tLoss: 0.010999\n",
      "Train Epoche: 2 [1009/2802 (36%)]\tLoss: 2.737541\n",
      "Train Epoche: 2 [1010/2802 (36%)]\tLoss: 14.058967\n",
      "Train Epoche: 2 [1011/2802 (36%)]\tLoss: 8.668561\n",
      "Train Epoche: 2 [1012/2802 (36%)]\tLoss: 0.256737\n",
      "Train Epoche: 2 [1013/2802 (36%)]\tLoss: 0.089900\n",
      "Train Epoche: 2 [1014/2802 (36%)]\tLoss: 1.157983\n",
      "Train Epoche: 2 [1015/2802 (36%)]\tLoss: 18.904852\n",
      "Train Epoche: 2 [1016/2802 (36%)]\tLoss: 4.685210\n",
      "Train Epoche: 2 [1017/2802 (36%)]\tLoss: 27.623602\n",
      "Train Epoche: 2 [1018/2802 (36%)]\tLoss: 0.066419\n",
      "Train Epoche: 2 [1019/2802 (36%)]\tLoss: 6.213157\n",
      "Train Epoche: 2 [1020/2802 (36%)]\tLoss: 1.135784\n",
      "Train Epoche: 2 [1021/2802 (36%)]\tLoss: 3.085935\n",
      "Train Epoche: 2 [1022/2802 (36%)]\tLoss: 0.072238\n",
      "Train Epoche: 2 [1023/2802 (37%)]\tLoss: 39.622234\n",
      "Train Epoche: 2 [1024/2802 (37%)]\tLoss: 0.206869\n",
      "Train Epoche: 2 [1025/2802 (37%)]\tLoss: 2.441821\n",
      "Train Epoche: 2 [1026/2802 (37%)]\tLoss: 112.203156\n",
      "Train Epoche: 2 [1027/2802 (37%)]\tLoss: 2.158025\n",
      "Train Epoche: 2 [1028/2802 (37%)]\tLoss: 2.062553\n",
      "Train Epoche: 2 [1029/2802 (37%)]\tLoss: 0.363430\n",
      "Train Epoche: 2 [1030/2802 (37%)]\tLoss: 1.964680\n",
      "Train Epoche: 2 [1031/2802 (37%)]\tLoss: 2.381942\n",
      "Train Epoche: 2 [1032/2802 (37%)]\tLoss: 1.364800\n",
      "Train Epoche: 2 [1033/2802 (37%)]\tLoss: 0.002827\n",
      "Train Epoche: 2 [1034/2802 (37%)]\tLoss: 18.755507\n",
      "Train Epoche: 2 [1035/2802 (37%)]\tLoss: 9.108000\n",
      "Train Epoche: 2 [1036/2802 (37%)]\tLoss: 1.017340\n",
      "Train Epoche: 2 [1037/2802 (37%)]\tLoss: 0.053285\n",
      "Train Epoche: 2 [1038/2802 (37%)]\tLoss: 4.141348\n",
      "Train Epoche: 2 [1039/2802 (37%)]\tLoss: 0.233092\n",
      "Train Epoche: 2 [1040/2802 (37%)]\tLoss: 37.456230\n",
      "Train Epoche: 2 [1041/2802 (37%)]\tLoss: 379.473206\n",
      "Train Epoche: 2 [1042/2802 (37%)]\tLoss: 0.096085\n",
      "Train Epoche: 2 [1043/2802 (37%)]\tLoss: 32.799854\n",
      "Train Epoche: 2 [1044/2802 (37%)]\tLoss: 4.391923\n",
      "Train Epoche: 2 [1045/2802 (37%)]\tLoss: 281.720612\n",
      "Train Epoche: 2 [1046/2802 (37%)]\tLoss: 2.545285\n",
      "Train Epoche: 2 [1047/2802 (37%)]\tLoss: 0.042695\n",
      "Train Epoche: 2 [1048/2802 (37%)]\tLoss: 0.134402\n",
      "Train Epoche: 2 [1049/2802 (37%)]\tLoss: 2.019825\n",
      "Train Epoche: 2 [1050/2802 (37%)]\tLoss: 0.056717\n",
      "Train Epoche: 2 [1051/2802 (38%)]\tLoss: 0.750488\n",
      "Train Epoche: 2 [1052/2802 (38%)]\tLoss: 0.706815\n",
      "Train Epoche: 2 [1053/2802 (38%)]\tLoss: 1.217934\n",
      "Train Epoche: 2 [1054/2802 (38%)]\tLoss: 0.097328\n",
      "Train Epoche: 2 [1055/2802 (38%)]\tLoss: 45.619553\n",
      "Train Epoche: 2 [1056/2802 (38%)]\tLoss: 11.611087\n",
      "Train Epoche: 2 [1057/2802 (38%)]\tLoss: 101.645447\n",
      "Train Epoche: 2 [1058/2802 (38%)]\tLoss: 0.459776\n",
      "Train Epoche: 2 [1059/2802 (38%)]\tLoss: 15.689819\n",
      "Train Epoche: 2 [1060/2802 (38%)]\tLoss: 24.975859\n",
      "Train Epoche: 2 [1061/2802 (38%)]\tLoss: 15.271159\n",
      "Train Epoche: 2 [1062/2802 (38%)]\tLoss: 0.416819\n",
      "Train Epoche: 2 [1063/2802 (38%)]\tLoss: 31.500908\n",
      "Train Epoche: 2 [1064/2802 (38%)]\tLoss: 14.594584\n",
      "Train Epoche: 2 [1065/2802 (38%)]\tLoss: 0.210032\n",
      "Train Epoche: 2 [1066/2802 (38%)]\tLoss: 1.095703\n",
      "Train Epoche: 2 [1067/2802 (38%)]\tLoss: 17.137131\n",
      "Train Epoche: 2 [1068/2802 (38%)]\tLoss: 1.875188\n",
      "Train Epoche: 2 [1069/2802 (38%)]\tLoss: 1.559277\n",
      "Train Epoche: 2 [1070/2802 (38%)]\tLoss: 0.401119\n",
      "Train Epoche: 2 [1071/2802 (38%)]\tLoss: 6.959619\n",
      "Train Epoche: 2 [1072/2802 (38%)]\tLoss: 0.613004\n",
      "Train Epoche: 2 [1073/2802 (38%)]\tLoss: 29.105165\n",
      "Train Epoche: 2 [1074/2802 (38%)]\tLoss: 0.499394\n",
      "Train Epoche: 2 [1075/2802 (38%)]\tLoss: 3.635476\n",
      "Train Epoche: 2 [1076/2802 (38%)]\tLoss: 0.119727\n",
      "Train Epoche: 2 [1077/2802 (38%)]\tLoss: 10.465698\n",
      "Train Epoche: 2 [1078/2802 (38%)]\tLoss: 1.441427\n",
      "Train Epoche: 2 [1079/2802 (39%)]\tLoss: 8.593511\n",
      "Train Epoche: 2 [1080/2802 (39%)]\tLoss: 0.698023\n",
      "Train Epoche: 2 [1081/2802 (39%)]\tLoss: 3.935122\n",
      "Train Epoche: 2 [1082/2802 (39%)]\tLoss: 5.782106\n",
      "Train Epoche: 2 [1083/2802 (39%)]\tLoss: 3.703975\n",
      "Train Epoche: 2 [1084/2802 (39%)]\tLoss: 2.321821\n",
      "Train Epoche: 2 [1085/2802 (39%)]\tLoss: 3.273417\n",
      "Train Epoche: 2 [1086/2802 (39%)]\tLoss: 0.692419\n",
      "Train Epoche: 2 [1087/2802 (39%)]\tLoss: 16.098387\n",
      "Train Epoche: 2 [1088/2802 (39%)]\tLoss: 3.035226\n",
      "Train Epoche: 2 [1089/2802 (39%)]\tLoss: 1.632579\n",
      "Train Epoche: 2 [1090/2802 (39%)]\tLoss: 3.969503\n",
      "Train Epoche: 2 [1091/2802 (39%)]\tLoss: 2.711925\n",
      "Train Epoche: 2 [1092/2802 (39%)]\tLoss: 69.097939\n",
      "Train Epoche: 2 [1093/2802 (39%)]\tLoss: 4.617870\n",
      "Train Epoche: 2 [1094/2802 (39%)]\tLoss: 0.000022\n",
      "Train Epoche: 2 [1095/2802 (39%)]\tLoss: 7.333737\n",
      "Train Epoche: 2 [1096/2802 (39%)]\tLoss: 39.966629\n",
      "Train Epoche: 2 [1097/2802 (39%)]\tLoss: 11.909952\n",
      "Train Epoche: 2 [1098/2802 (39%)]\tLoss: 65.776009\n",
      "Train Epoche: 2 [1099/2802 (39%)]\tLoss: 203.230118\n",
      "Train Epoche: 2 [1100/2802 (39%)]\tLoss: 12.596557\n",
      "Train Epoche: 2 [1101/2802 (39%)]\tLoss: 2.703912\n",
      "Train Epoche: 2 [1102/2802 (39%)]\tLoss: 13.426108\n",
      "Train Epoche: 2 [1103/2802 (39%)]\tLoss: 2.951542\n",
      "Train Epoche: 2 [1104/2802 (39%)]\tLoss: 2.137339\n",
      "Train Epoche: 2 [1105/2802 (39%)]\tLoss: 5.011897\n",
      "Train Epoche: 2 [1106/2802 (39%)]\tLoss: 4.884100\n",
      "Train Epoche: 2 [1107/2802 (40%)]\tLoss: 2.951643\n",
      "Train Epoche: 2 [1108/2802 (40%)]\tLoss: 2.980058\n",
      "Train Epoche: 2 [1109/2802 (40%)]\tLoss: 0.285833\n",
      "Train Epoche: 2 [1110/2802 (40%)]\tLoss: 0.219403\n",
      "Train Epoche: 2 [1111/2802 (40%)]\tLoss: 4.800003\n",
      "Train Epoche: 2 [1112/2802 (40%)]\tLoss: 3.014154\n",
      "Train Epoche: 2 [1113/2802 (40%)]\tLoss: 48.082657\n",
      "Train Epoche: 2 [1114/2802 (40%)]\tLoss: 2.782726\n",
      "Train Epoche: 2 [1115/2802 (40%)]\tLoss: 0.488467\n",
      "Train Epoche: 2 [1116/2802 (40%)]\tLoss: 63.893860\n",
      "Train Epoche: 2 [1117/2802 (40%)]\tLoss: 1.779367\n",
      "Train Epoche: 2 [1118/2802 (40%)]\tLoss: 8.581250\n",
      "Train Epoche: 2 [1119/2802 (40%)]\tLoss: 14.091440\n",
      "Train Epoche: 2 [1120/2802 (40%)]\tLoss: 44.979702\n",
      "Train Epoche: 2 [1121/2802 (40%)]\tLoss: 8.515154\n",
      "Train Epoche: 2 [1122/2802 (40%)]\tLoss: 14.356185\n",
      "Train Epoche: 2 [1123/2802 (40%)]\tLoss: 2.489533\n",
      "Train Epoche: 2 [1124/2802 (40%)]\tLoss: 25.211571\n",
      "Train Epoche: 2 [1125/2802 (40%)]\tLoss: 14.256764\n",
      "Train Epoche: 2 [1126/2802 (40%)]\tLoss: 0.936307\n",
      "Train Epoche: 2 [1127/2802 (40%)]\tLoss: 5.999085\n",
      "Train Epoche: 2 [1128/2802 (40%)]\tLoss: 10.951558\n",
      "Train Epoche: 2 [1129/2802 (40%)]\tLoss: 1.043015\n",
      "Train Epoche: 2 [1130/2802 (40%)]\tLoss: 3.885827\n",
      "Train Epoche: 2 [1131/2802 (40%)]\tLoss: 23.102446\n",
      "Train Epoche: 2 [1132/2802 (40%)]\tLoss: 0.308537\n",
      "Train Epoche: 2 [1133/2802 (40%)]\tLoss: 0.006917\n",
      "Train Epoche: 2 [1134/2802 (40%)]\tLoss: 0.276797\n",
      "Train Epoche: 2 [1135/2802 (41%)]\tLoss: 0.262760\n",
      "Train Epoche: 2 [1136/2802 (41%)]\tLoss: 12.097075\n",
      "Train Epoche: 2 [1137/2802 (41%)]\tLoss: 1.520512\n",
      "Train Epoche: 2 [1138/2802 (41%)]\tLoss: 3.182072\n",
      "Train Epoche: 2 [1139/2802 (41%)]\tLoss: 0.278639\n",
      "Train Epoche: 2 [1140/2802 (41%)]\tLoss: 14.107010\n",
      "Train Epoche: 2 [1141/2802 (41%)]\tLoss: 2.770112\n",
      "Train Epoche: 2 [1142/2802 (41%)]\tLoss: 21.668215\n",
      "Train Epoche: 2 [1143/2802 (41%)]\tLoss: 1.437263\n",
      "Train Epoche: 2 [1144/2802 (41%)]\tLoss: 0.813671\n",
      "Train Epoche: 2 [1145/2802 (41%)]\tLoss: 15.933511\n",
      "Train Epoche: 2 [1146/2802 (41%)]\tLoss: 3.964830\n",
      "Train Epoche: 2 [1147/2802 (41%)]\tLoss: 0.031762\n",
      "Train Epoche: 2 [1148/2802 (41%)]\tLoss: 6.738358\n",
      "Train Epoche: 2 [1149/2802 (41%)]\tLoss: 1.636259\n",
      "Train Epoche: 2 [1150/2802 (41%)]\tLoss: 4.641998\n",
      "Train Epoche: 2 [1151/2802 (41%)]\tLoss: 22.477942\n",
      "Train Epoche: 2 [1152/2802 (41%)]\tLoss: 0.018640\n",
      "Train Epoche: 2 [1153/2802 (41%)]\tLoss: 26.937645\n",
      "Train Epoche: 2 [1154/2802 (41%)]\tLoss: 1.731500\n",
      "Train Epoche: 2 [1155/2802 (41%)]\tLoss: 0.063856\n",
      "Train Epoche: 2 [1156/2802 (41%)]\tLoss: 0.275116\n",
      "Train Epoche: 2 [1157/2802 (41%)]\tLoss: 9.917656\n",
      "Train Epoche: 2 [1158/2802 (41%)]\tLoss: 11.117856\n",
      "Train Epoche: 2 [1159/2802 (41%)]\tLoss: 2.190391\n",
      "Train Epoche: 2 [1160/2802 (41%)]\tLoss: 174.945587\n",
      "Train Epoche: 2 [1161/2802 (41%)]\tLoss: 16.861507\n",
      "Train Epoche: 2 [1162/2802 (41%)]\tLoss: 3.718296\n",
      "Train Epoche: 2 [1163/2802 (42%)]\tLoss: 3.769932\n",
      "Train Epoche: 2 [1164/2802 (42%)]\tLoss: 15.178823\n",
      "Train Epoche: 2 [1165/2802 (42%)]\tLoss: 52.379269\n",
      "Train Epoche: 2 [1166/2802 (42%)]\tLoss: 0.471151\n",
      "Train Epoche: 2 [1167/2802 (42%)]\tLoss: 0.687473\n",
      "Train Epoche: 2 [1168/2802 (42%)]\tLoss: 31.160286\n",
      "Train Epoche: 2 [1169/2802 (42%)]\tLoss: 1.323119\n",
      "Train Epoche: 2 [1170/2802 (42%)]\tLoss: 13.384522\n",
      "Train Epoche: 2 [1171/2802 (42%)]\tLoss: 1.083305\n",
      "Train Epoche: 2 [1172/2802 (42%)]\tLoss: 9.286458\n",
      "Train Epoche: 2 [1173/2802 (42%)]\tLoss: 0.429590\n",
      "Train Epoche: 2 [1174/2802 (42%)]\tLoss: 71.118073\n",
      "Train Epoche: 2 [1175/2802 (42%)]\tLoss: 2.442294\n",
      "Train Epoche: 2 [1176/2802 (42%)]\tLoss: 4.034034\n",
      "Train Epoche: 2 [1177/2802 (42%)]\tLoss: 19.780819\n",
      "Train Epoche: 2 [1178/2802 (42%)]\tLoss: 4.702075\n",
      "Train Epoche: 2 [1179/2802 (42%)]\tLoss: 131.024918\n",
      "Train Epoche: 2 [1180/2802 (42%)]\tLoss: 0.833419\n",
      "Train Epoche: 2 [1181/2802 (42%)]\tLoss: 20.381809\n",
      "Train Epoche: 2 [1182/2802 (42%)]\tLoss: 29.869719\n",
      "Train Epoche: 2 [1183/2802 (42%)]\tLoss: 1.089739\n",
      "Train Epoche: 2 [1184/2802 (42%)]\tLoss: 2.681617\n",
      "Train Epoche: 2 [1185/2802 (42%)]\tLoss: 45.270203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1186/2802 (42%)]\tLoss: 0.992091\n",
      "Train Epoche: 2 [1187/2802 (42%)]\tLoss: 0.008480\n",
      "Train Epoche: 2 [1188/2802 (42%)]\tLoss: 0.041293\n",
      "Train Epoche: 2 [1189/2802 (42%)]\tLoss: 17.779840\n",
      "Train Epoche: 2 [1190/2802 (42%)]\tLoss: 2.195505\n",
      "Train Epoche: 2 [1191/2802 (43%)]\tLoss: 0.158189\n",
      "Train Epoche: 2 [1192/2802 (43%)]\tLoss: 5.065279\n",
      "Train Epoche: 2 [1193/2802 (43%)]\tLoss: 0.001851\n",
      "Train Epoche: 2 [1194/2802 (43%)]\tLoss: 0.076219\n",
      "Train Epoche: 2 [1195/2802 (43%)]\tLoss: 18.008577\n",
      "Train Epoche: 2 [1196/2802 (43%)]\tLoss: 10.137067\n",
      "Train Epoche: 2 [1197/2802 (43%)]\tLoss: 0.235925\n",
      "Train Epoche: 2 [1198/2802 (43%)]\tLoss: 1.139760\n",
      "Train Epoche: 2 [1199/2802 (43%)]\tLoss: 5.881677\n",
      "Train Epoche: 2 [1200/2802 (43%)]\tLoss: 0.332649\n",
      "Train Epoche: 2 [1201/2802 (43%)]\tLoss: 107.992897\n",
      "Train Epoche: 2 [1202/2802 (43%)]\tLoss: 0.096656\n",
      "Train Epoche: 2 [1203/2802 (43%)]\tLoss: 0.560498\n",
      "Train Epoche: 2 [1204/2802 (43%)]\tLoss: 0.122824\n",
      "Train Epoche: 2 [1205/2802 (43%)]\tLoss: 0.019616\n",
      "Train Epoche: 2 [1206/2802 (43%)]\tLoss: 17.125147\n",
      "Train Epoche: 2 [1207/2802 (43%)]\tLoss: 312.900635\n",
      "Train Epoche: 2 [1208/2802 (43%)]\tLoss: 12.466777\n",
      "Train Epoche: 2 [1209/2802 (43%)]\tLoss: 1.384638\n",
      "Train Epoche: 2 [1210/2802 (43%)]\tLoss: 9.607017\n",
      "Train Epoche: 2 [1211/2802 (43%)]\tLoss: 9.287147\n",
      "Train Epoche: 2 [1212/2802 (43%)]\tLoss: 192.725815\n",
      "Train Epoche: 2 [1213/2802 (43%)]\tLoss: 19.678602\n",
      "Train Epoche: 2 [1214/2802 (43%)]\tLoss: 0.475935\n",
      "Train Epoche: 2 [1215/2802 (43%)]\tLoss: 31.237823\n",
      "Train Epoche: 2 [1216/2802 (43%)]\tLoss: 16.496361\n",
      "Train Epoche: 2 [1217/2802 (43%)]\tLoss: 0.245035\n",
      "Train Epoche: 2 [1218/2802 (43%)]\tLoss: 3.381888\n",
      "Train Epoche: 2 [1219/2802 (44%)]\tLoss: 4.378102\n",
      "Train Epoche: 2 [1220/2802 (44%)]\tLoss: 0.205416\n",
      "Train Epoche: 2 [1221/2802 (44%)]\tLoss: 59.090984\n",
      "Train Epoche: 2 [1222/2802 (44%)]\tLoss: 4.827867\n",
      "Train Epoche: 2 [1223/2802 (44%)]\tLoss: 15.536367\n",
      "Train Epoche: 2 [1224/2802 (44%)]\tLoss: 2.493821\n",
      "Train Epoche: 2 [1225/2802 (44%)]\tLoss: 1.954819\n",
      "Train Epoche: 2 [1226/2802 (44%)]\tLoss: 4.132876\n",
      "Train Epoche: 2 [1227/2802 (44%)]\tLoss: 4.518771\n",
      "Train Epoche: 2 [1228/2802 (44%)]\tLoss: 0.017583\n",
      "Train Epoche: 2 [1229/2802 (44%)]\tLoss: 13.402189\n",
      "Train Epoche: 2 [1230/2802 (44%)]\tLoss: 4.023709\n",
      "Train Epoche: 2 [1231/2802 (44%)]\tLoss: 46.117455\n",
      "Train Epoche: 2 [1232/2802 (44%)]\tLoss: 0.020964\n",
      "Train Epoche: 2 [1233/2802 (44%)]\tLoss: 1.165492\n",
      "Train Epoche: 2 [1234/2802 (44%)]\tLoss: 20.011478\n",
      "Train Epoche: 2 [1235/2802 (44%)]\tLoss: 9.558320\n",
      "Train Epoche: 2 [1236/2802 (44%)]\tLoss: 0.270179\n",
      "Train Epoche: 2 [1237/2802 (44%)]\tLoss: 52.177921\n",
      "Train Epoche: 2 [1238/2802 (44%)]\tLoss: 0.157674\n",
      "Train Epoche: 2 [1239/2802 (44%)]\tLoss: 19.462090\n",
      "Train Epoche: 2 [1240/2802 (44%)]\tLoss: 81.494690\n",
      "Train Epoche: 2 [1241/2802 (44%)]\tLoss: 5.414670\n",
      "Train Epoche: 2 [1242/2802 (44%)]\tLoss: 8.801346\n",
      "Train Epoche: 2 [1243/2802 (44%)]\tLoss: 0.516305\n",
      "Train Epoche: 2 [1244/2802 (44%)]\tLoss: 6.267254\n",
      "Train Epoche: 2 [1245/2802 (44%)]\tLoss: 14.491079\n",
      "Train Epoche: 2 [1246/2802 (44%)]\tLoss: 0.887252\n",
      "Train Epoche: 2 [1247/2802 (45%)]\tLoss: 1.724086\n",
      "Train Epoche: 2 [1248/2802 (45%)]\tLoss: 3.826896\n",
      "Train Epoche: 2 [1249/2802 (45%)]\tLoss: 10.066455\n",
      "Train Epoche: 2 [1250/2802 (45%)]\tLoss: 0.512023\n",
      "Train Epoche: 2 [1251/2802 (45%)]\tLoss: 4.162460\n",
      "Train Epoche: 2 [1252/2802 (45%)]\tLoss: 1.300946\n",
      "Train Epoche: 2 [1253/2802 (45%)]\tLoss: 6.419039\n",
      "Train Epoche: 2 [1254/2802 (45%)]\tLoss: 2.373423\n",
      "Train Epoche: 2 [1255/2802 (45%)]\tLoss: 21.347513\n",
      "Train Epoche: 2 [1256/2802 (45%)]\tLoss: 0.065919\n",
      "Train Epoche: 2 [1257/2802 (45%)]\tLoss: 0.664067\n",
      "Train Epoche: 2 [1258/2802 (45%)]\tLoss: 1.728108\n",
      "Train Epoche: 2 [1259/2802 (45%)]\tLoss: 20.718014\n",
      "Train Epoche: 2 [1260/2802 (45%)]\tLoss: 0.227398\n",
      "Train Epoche: 2 [1261/2802 (45%)]\tLoss: 0.749538\n",
      "Train Epoche: 2 [1262/2802 (45%)]\tLoss: 57.380753\n",
      "Train Epoche: 2 [1263/2802 (45%)]\tLoss: 17.822559\n",
      "Train Epoche: 2 [1264/2802 (45%)]\tLoss: 6.105640\n",
      "Train Epoche: 2 [1265/2802 (45%)]\tLoss: 7.209662\n",
      "Train Epoche: 2 [1266/2802 (45%)]\tLoss: 0.908891\n",
      "Train Epoche: 2 [1267/2802 (45%)]\tLoss: 11.573932\n",
      "Train Epoche: 2 [1268/2802 (45%)]\tLoss: 2.899410\n",
      "Train Epoche: 2 [1269/2802 (45%)]\tLoss: 0.003662\n",
      "Train Epoche: 2 [1270/2802 (45%)]\tLoss: 1.192090\n",
      "Train Epoche: 2 [1271/2802 (45%)]\tLoss: 1.385166\n",
      "Train Epoche: 2 [1272/2802 (45%)]\tLoss: 0.751209\n",
      "Train Epoche: 2 [1273/2802 (45%)]\tLoss: 10.275405\n",
      "Train Epoche: 2 [1274/2802 (45%)]\tLoss: 12.373102\n",
      "Train Epoche: 2 [1275/2802 (46%)]\tLoss: 0.441026\n",
      "Train Epoche: 2 [1276/2802 (46%)]\tLoss: 0.583887\n",
      "Train Epoche: 2 [1277/2802 (46%)]\tLoss: 2.307593\n",
      "Train Epoche: 2 [1278/2802 (46%)]\tLoss: 0.053602\n",
      "Train Epoche: 2 [1279/2802 (46%)]\tLoss: 16.083849\n",
      "Train Epoche: 2 [1280/2802 (46%)]\tLoss: 24.964766\n",
      "Train Epoche: 2 [1281/2802 (46%)]\tLoss: 0.051979\n",
      "Train Epoche: 2 [1282/2802 (46%)]\tLoss: 56.412292\n",
      "Train Epoche: 2 [1283/2802 (46%)]\tLoss: 0.075094\n",
      "Train Epoche: 2 [1284/2802 (46%)]\tLoss: 102.816261\n",
      "Train Epoche: 2 [1285/2802 (46%)]\tLoss: 2.571593\n",
      "Train Epoche: 2 [1286/2802 (46%)]\tLoss: 19.418468\n",
      "Train Epoche: 2 [1287/2802 (46%)]\tLoss: 5.853838\n",
      "Train Epoche: 2 [1288/2802 (46%)]\tLoss: 16.141666\n",
      "Train Epoche: 2 [1289/2802 (46%)]\tLoss: 0.003700\n",
      "Train Epoche: 2 [1290/2802 (46%)]\tLoss: 0.471054\n",
      "Train Epoche: 2 [1291/2802 (46%)]\tLoss: 0.237027\n",
      "Train Epoche: 2 [1292/2802 (46%)]\tLoss: 64.044762\n",
      "Train Epoche: 2 [1293/2802 (46%)]\tLoss: 45.395206\n",
      "Train Epoche: 2 [1294/2802 (46%)]\tLoss: 0.120233\n",
      "Train Epoche: 2 [1295/2802 (46%)]\tLoss: 0.155908\n",
      "Train Epoche: 2 [1296/2802 (46%)]\tLoss: 2.997876\n",
      "Train Epoche: 2 [1297/2802 (46%)]\tLoss: 0.345741\n",
      "Train Epoche: 2 [1298/2802 (46%)]\tLoss: 94.131279\n",
      "Train Epoche: 2 [1299/2802 (46%)]\tLoss: 1.190948\n",
      "Train Epoche: 2 [1300/2802 (46%)]\tLoss: 14.571188\n",
      "Train Epoche: 2 [1301/2802 (46%)]\tLoss: 1.211697\n",
      "Train Epoche: 2 [1302/2802 (46%)]\tLoss: 17.418310\n",
      "Train Epoche: 2 [1303/2802 (47%)]\tLoss: 11.155811\n",
      "Train Epoche: 2 [1304/2802 (47%)]\tLoss: 0.408453\n",
      "Train Epoche: 2 [1305/2802 (47%)]\tLoss: 0.026063\n",
      "Train Epoche: 2 [1306/2802 (47%)]\tLoss: 2.219892\n",
      "Train Epoche: 2 [1307/2802 (47%)]\tLoss: 0.000005\n",
      "Train Epoche: 2 [1308/2802 (47%)]\tLoss: 4.802857\n",
      "Train Epoche: 2 [1309/2802 (47%)]\tLoss: 0.378087\n",
      "Train Epoche: 2 [1310/2802 (47%)]\tLoss: 1.149265\n",
      "Train Epoche: 2 [1311/2802 (47%)]\tLoss: 1.143159\n",
      "Train Epoche: 2 [1312/2802 (47%)]\tLoss: 17.157461\n",
      "Train Epoche: 2 [1313/2802 (47%)]\tLoss: 0.399504\n",
      "Train Epoche: 2 [1314/2802 (47%)]\tLoss: 0.081526\n",
      "Train Epoche: 2 [1315/2802 (47%)]\tLoss: 1.427625\n",
      "Train Epoche: 2 [1316/2802 (47%)]\tLoss: 0.095242\n",
      "Train Epoche: 2 [1317/2802 (47%)]\tLoss: 0.707518\n",
      "Train Epoche: 2 [1318/2802 (47%)]\tLoss: 1.638515\n",
      "Train Epoche: 2 [1319/2802 (47%)]\tLoss: 0.383146\n",
      "Train Epoche: 2 [1320/2802 (47%)]\tLoss: 0.815542\n",
      "Train Epoche: 2 [1321/2802 (47%)]\tLoss: 0.060929\n",
      "Train Epoche: 2 [1322/2802 (47%)]\tLoss: 80.356110\n",
      "Train Epoche: 2 [1323/2802 (47%)]\tLoss: 1.556771\n",
      "Train Epoche: 2 [1324/2802 (47%)]\tLoss: 21.792248\n",
      "Train Epoche: 2 [1325/2802 (47%)]\tLoss: 0.013030\n",
      "Train Epoche: 2 [1326/2802 (47%)]\tLoss: 65.331070\n",
      "Train Epoche: 2 [1327/2802 (47%)]\tLoss: 1.114229\n",
      "Train Epoche: 2 [1328/2802 (47%)]\tLoss: 0.332902\n",
      "Train Epoche: 2 [1329/2802 (47%)]\tLoss: 23.855364\n",
      "Train Epoche: 2 [1330/2802 (47%)]\tLoss: 0.317706\n",
      "Train Epoche: 2 [1331/2802 (48%)]\tLoss: 3.055911\n",
      "Train Epoche: 2 [1332/2802 (48%)]\tLoss: 95.229050\n",
      "Train Epoche: 2 [1333/2802 (48%)]\tLoss: 1.048736\n",
      "Train Epoche: 2 [1334/2802 (48%)]\tLoss: 0.605357\n",
      "Train Epoche: 2 [1335/2802 (48%)]\tLoss: 356.071259\n",
      "Train Epoche: 2 [1336/2802 (48%)]\tLoss: 0.509109\n",
      "Train Epoche: 2 [1337/2802 (48%)]\tLoss: 2.798599\n",
      "Train Epoche: 2 [1338/2802 (48%)]\tLoss: 102.521805\n",
      "Train Epoche: 2 [1339/2802 (48%)]\tLoss: 0.889779\n",
      "Train Epoche: 2 [1340/2802 (48%)]\tLoss: 27.241871\n",
      "Train Epoche: 2 [1341/2802 (48%)]\tLoss: 7.770679\n",
      "Train Epoche: 2 [1342/2802 (48%)]\tLoss: 45.389103\n",
      "Train Epoche: 2 [1343/2802 (48%)]\tLoss: 0.667787\n",
      "Train Epoche: 2 [1344/2802 (48%)]\tLoss: 9.215792\n",
      "Train Epoche: 2 [1345/2802 (48%)]\tLoss: 14.011148\n",
      "Train Epoche: 2 [1346/2802 (48%)]\tLoss: 37.075535\n",
      "Train Epoche: 2 [1347/2802 (48%)]\tLoss: 11.155480\n",
      "Train Epoche: 2 [1348/2802 (48%)]\tLoss: 3.704623\n",
      "Train Epoche: 2 [1349/2802 (48%)]\tLoss: 13.811800\n",
      "Train Epoche: 2 [1350/2802 (48%)]\tLoss: 42.997902\n",
      "Train Epoche: 2 [1351/2802 (48%)]\tLoss: 6.549135\n",
      "Train Epoche: 2 [1352/2802 (48%)]\tLoss: 19.907902\n",
      "Train Epoche: 2 [1353/2802 (48%)]\tLoss: 14.099059\n",
      "Train Epoche: 2 [1354/2802 (48%)]\tLoss: 68.185684\n",
      "Train Epoche: 2 [1355/2802 (48%)]\tLoss: 30.194332\n",
      "Train Epoche: 2 [1356/2802 (48%)]\tLoss: 0.032473\n",
      "Train Epoche: 2 [1357/2802 (48%)]\tLoss: 39.658779\n",
      "Train Epoche: 2 [1358/2802 (48%)]\tLoss: 0.018882\n",
      "Train Epoche: 2 [1359/2802 (49%)]\tLoss: 2.770842\n",
      "Train Epoche: 2 [1360/2802 (49%)]\tLoss: 5.391633\n",
      "Train Epoche: 2 [1361/2802 (49%)]\tLoss: 33.793961\n",
      "Train Epoche: 2 [1362/2802 (49%)]\tLoss: 0.001703\n",
      "Train Epoche: 2 [1363/2802 (49%)]\tLoss: 23.396624\n",
      "Train Epoche: 2 [1364/2802 (49%)]\tLoss: 40.460915\n",
      "Train Epoche: 2 [1365/2802 (49%)]\tLoss: 0.146306\n",
      "Train Epoche: 2 [1366/2802 (49%)]\tLoss: 17.286879\n",
      "Train Epoche: 2 [1367/2802 (49%)]\tLoss: 3.887989\n",
      "Train Epoche: 2 [1368/2802 (49%)]\tLoss: 1.053498\n",
      "Train Epoche: 2 [1369/2802 (49%)]\tLoss: 0.560142\n",
      "Train Epoche: 2 [1370/2802 (49%)]\tLoss: 10.576118\n",
      "Train Epoche: 2 [1371/2802 (49%)]\tLoss: 1.564203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1372/2802 (49%)]\tLoss: 1.109422\n",
      "Train Epoche: 2 [1373/2802 (49%)]\tLoss: 5.239554\n",
      "Train Epoche: 2 [1374/2802 (49%)]\tLoss: 49.213818\n",
      "Train Epoche: 2 [1375/2802 (49%)]\tLoss: 0.631081\n",
      "Train Epoche: 2 [1376/2802 (49%)]\tLoss: 3.026199\n",
      "Train Epoche: 2 [1377/2802 (49%)]\tLoss: 4.570222\n",
      "Train Epoche: 2 [1378/2802 (49%)]\tLoss: 1.853064\n",
      "Train Epoche: 2 [1379/2802 (49%)]\tLoss: 0.520742\n",
      "Train Epoche: 2 [1380/2802 (49%)]\tLoss: 0.717807\n",
      "Train Epoche: 2 [1381/2802 (49%)]\tLoss: 53.710419\n",
      "Train Epoche: 2 [1382/2802 (49%)]\tLoss: 157.533600\n",
      "Train Epoche: 2 [1383/2802 (49%)]\tLoss: 4.984759\n",
      "Train Epoche: 2 [1384/2802 (49%)]\tLoss: 18.779156\n",
      "Train Epoche: 2 [1385/2802 (49%)]\tLoss: 4.424738\n",
      "Train Epoche: 2 [1386/2802 (49%)]\tLoss: 0.023099\n",
      "Train Epoche: 2 [1387/2802 (50%)]\tLoss: 55.447136\n",
      "Train Epoche: 2 [1388/2802 (50%)]\tLoss: 4.748128\n",
      "Train Epoche: 2 [1389/2802 (50%)]\tLoss: 2.539727\n",
      "Train Epoche: 2 [1390/2802 (50%)]\tLoss: 2.092892\n",
      "Train Epoche: 2 [1391/2802 (50%)]\tLoss: 15.478108\n",
      "Train Epoche: 2 [1392/2802 (50%)]\tLoss: 30.507040\n",
      "Train Epoche: 2 [1393/2802 (50%)]\tLoss: 43.889336\n",
      "Train Epoche: 2 [1394/2802 (50%)]\tLoss: 2.496336\n",
      "Train Epoche: 2 [1395/2802 (50%)]\tLoss: 15.618580\n",
      "Train Epoche: 2 [1396/2802 (50%)]\tLoss: 15.476236\n",
      "Train Epoche: 2 [1397/2802 (50%)]\tLoss: 54.508335\n",
      "Train Epoche: 2 [1398/2802 (50%)]\tLoss: 156.786972\n",
      "Train Epoche: 2 [1399/2802 (50%)]\tLoss: 3.712873\n",
      "Train Epoche: 2 [1400/2802 (50%)]\tLoss: 0.316297\n",
      "Train Epoche: 2 [1401/2802 (50%)]\tLoss: 38.388138\n",
      "Train Epoche: 2 [1402/2802 (50%)]\tLoss: 7.944569\n",
      "Train Epoche: 2 [1403/2802 (50%)]\tLoss: 0.008273\n",
      "Train Epoche: 2 [1404/2802 (50%)]\tLoss: 8.128222\n",
      "Train Epoche: 2 [1405/2802 (50%)]\tLoss: 0.203179\n",
      "Train Epoche: 2 [1406/2802 (50%)]\tLoss: 4.602119\n",
      "Train Epoche: 2 [1407/2802 (50%)]\tLoss: 3.231331\n",
      "Train Epoche: 2 [1408/2802 (50%)]\tLoss: 0.313300\n",
      "Train Epoche: 2 [1409/2802 (50%)]\tLoss: 0.913581\n",
      "Train Epoche: 2 [1410/2802 (50%)]\tLoss: 4.765670\n",
      "Train Epoche: 2 [1411/2802 (50%)]\tLoss: 113.943962\n",
      "Train Epoche: 2 [1412/2802 (50%)]\tLoss: 0.969978\n",
      "Train Epoche: 2 [1413/2802 (50%)]\tLoss: 0.867586\n",
      "Train Epoche: 2 [1414/2802 (50%)]\tLoss: 31.649424\n",
      "Train Epoche: 2 [1415/2802 (50%)]\tLoss: 0.124807\n",
      "Train Epoche: 2 [1416/2802 (51%)]\tLoss: 0.441964\n",
      "Train Epoche: 2 [1417/2802 (51%)]\tLoss: 0.045256\n",
      "Train Epoche: 2 [1418/2802 (51%)]\tLoss: 10.157366\n",
      "Train Epoche: 2 [1419/2802 (51%)]\tLoss: 20.600180\n",
      "Train Epoche: 2 [1420/2802 (51%)]\tLoss: 0.001357\n",
      "Train Epoche: 2 [1421/2802 (51%)]\tLoss: 3.821229\n",
      "Train Epoche: 2 [1422/2802 (51%)]\tLoss: 122.419930\n",
      "Train Epoche: 2 [1423/2802 (51%)]\tLoss: 0.000480\n",
      "Train Epoche: 2 [1424/2802 (51%)]\tLoss: 0.188812\n",
      "Train Epoche: 2 [1425/2802 (51%)]\tLoss: 18.460766\n",
      "Train Epoche: 2 [1426/2802 (51%)]\tLoss: 6.297756\n",
      "Train Epoche: 2 [1427/2802 (51%)]\tLoss: 2.244630\n",
      "Train Epoche: 2 [1428/2802 (51%)]\tLoss: 2.592427\n",
      "Train Epoche: 2 [1429/2802 (51%)]\tLoss: 34.662418\n",
      "Train Epoche: 2 [1430/2802 (51%)]\tLoss: 23.571659\n",
      "Train Epoche: 2 [1431/2802 (51%)]\tLoss: 13.210925\n",
      "Train Epoche: 2 [1432/2802 (51%)]\tLoss: 57.494545\n",
      "Train Epoche: 2 [1433/2802 (51%)]\tLoss: 26.301649\n",
      "Train Epoche: 2 [1434/2802 (51%)]\tLoss: 2.452664\n",
      "Train Epoche: 2 [1435/2802 (51%)]\tLoss: 7.984011\n",
      "Train Epoche: 2 [1436/2802 (51%)]\tLoss: 4.667334\n",
      "Train Epoche: 2 [1437/2802 (51%)]\tLoss: 9.032485\n",
      "Train Epoche: 2 [1438/2802 (51%)]\tLoss: 1.307229\n",
      "Train Epoche: 2 [1439/2802 (51%)]\tLoss: 37.311924\n",
      "Train Epoche: 2 [1440/2802 (51%)]\tLoss: 1.547566\n",
      "Train Epoche: 2 [1441/2802 (51%)]\tLoss: 0.766137\n",
      "Train Epoche: 2 [1442/2802 (51%)]\tLoss: 2.783844\n",
      "Train Epoche: 2 [1443/2802 (51%)]\tLoss: 8.285059\n",
      "Train Epoche: 2 [1444/2802 (52%)]\tLoss: 11.623611\n",
      "Train Epoche: 2 [1445/2802 (52%)]\tLoss: 1.148933\n",
      "Train Epoche: 2 [1446/2802 (52%)]\tLoss: 5.564034\n",
      "Train Epoche: 2 [1447/2802 (52%)]\tLoss: 2.126741\n",
      "Train Epoche: 2 [1448/2802 (52%)]\tLoss: 61.119167\n",
      "Train Epoche: 2 [1449/2802 (52%)]\tLoss: 0.029716\n",
      "Train Epoche: 2 [1450/2802 (52%)]\tLoss: 2.386177\n",
      "Train Epoche: 2 [1451/2802 (52%)]\tLoss: 4.597669\n",
      "Train Epoche: 2 [1452/2802 (52%)]\tLoss: 0.033617\n",
      "Train Epoche: 2 [1453/2802 (52%)]\tLoss: 3.671478\n",
      "Train Epoche: 2 [1454/2802 (52%)]\tLoss: 12.415794\n",
      "Train Epoche: 2 [1455/2802 (52%)]\tLoss: 0.089716\n",
      "Train Epoche: 2 [1456/2802 (52%)]\tLoss: 0.463579\n",
      "Train Epoche: 2 [1457/2802 (52%)]\tLoss: 0.542366\n",
      "Train Epoche: 2 [1458/2802 (52%)]\tLoss: 10.026599\n",
      "Train Epoche: 2 [1459/2802 (52%)]\tLoss: 1.846760\n",
      "Train Epoche: 2 [1460/2802 (52%)]\tLoss: 11.597667\n",
      "Train Epoche: 2 [1461/2802 (52%)]\tLoss: 6.545358\n",
      "Train Epoche: 2 [1462/2802 (52%)]\tLoss: 1.834841\n",
      "Train Epoche: 2 [1463/2802 (52%)]\tLoss: 0.083412\n",
      "Train Epoche: 2 [1464/2802 (52%)]\tLoss: 2.662727\n",
      "Train Epoche: 2 [1465/2802 (52%)]\tLoss: 4.990682\n",
      "Train Epoche: 2 [1466/2802 (52%)]\tLoss: 1.266599\n",
      "Train Epoche: 2 [1467/2802 (52%)]\tLoss: 0.079417\n",
      "Train Epoche: 2 [1468/2802 (52%)]\tLoss: 3.763791\n",
      "Train Epoche: 2 [1469/2802 (52%)]\tLoss: 241.728577\n",
      "Train Epoche: 2 [1470/2802 (52%)]\tLoss: 0.498185\n",
      "Train Epoche: 2 [1471/2802 (52%)]\tLoss: 4.859746\n",
      "Train Epoche: 2 [1472/2802 (53%)]\tLoss: 1.087867\n",
      "Train Epoche: 2 [1473/2802 (53%)]\tLoss: 4.790857\n",
      "Train Epoche: 2 [1474/2802 (53%)]\tLoss: 0.190096\n",
      "Train Epoche: 2 [1475/2802 (53%)]\tLoss: 1.056862\n",
      "Train Epoche: 2 [1476/2802 (53%)]\tLoss: 0.479253\n",
      "Train Epoche: 2 [1477/2802 (53%)]\tLoss: 12.448129\n",
      "Train Epoche: 2 [1478/2802 (53%)]\tLoss: 19.308300\n",
      "Train Epoche: 2 [1479/2802 (53%)]\tLoss: 0.122900\n",
      "Train Epoche: 2 [1480/2802 (53%)]\tLoss: 1.039111\n",
      "Train Epoche: 2 [1481/2802 (53%)]\tLoss: 10.587297\n",
      "Train Epoche: 2 [1482/2802 (53%)]\tLoss: 1.654844\n",
      "Train Epoche: 2 [1483/2802 (53%)]\tLoss: 5.719484\n",
      "Train Epoche: 2 [1484/2802 (53%)]\tLoss: 7.867920\n",
      "Train Epoche: 2 [1485/2802 (53%)]\tLoss: 2.418816\n",
      "Train Epoche: 2 [1486/2802 (53%)]\tLoss: 101.846420\n",
      "Train Epoche: 2 [1487/2802 (53%)]\tLoss: 24.268574\n",
      "Train Epoche: 2 [1488/2802 (53%)]\tLoss: 1.220996\n",
      "Train Epoche: 2 [1489/2802 (53%)]\tLoss: 48.161144\n",
      "Train Epoche: 2 [1490/2802 (53%)]\tLoss: 1.471590\n",
      "Train Epoche: 2 [1491/2802 (53%)]\tLoss: 0.051774\n",
      "Train Epoche: 2 [1492/2802 (53%)]\tLoss: 9.589490\n",
      "Train Epoche: 2 [1493/2802 (53%)]\tLoss: 6.944483\n",
      "Train Epoche: 2 [1494/2802 (53%)]\tLoss: 44.492146\n",
      "Train Epoche: 2 [1495/2802 (53%)]\tLoss: 0.091997\n",
      "Train Epoche: 2 [1496/2802 (53%)]\tLoss: 2.638078\n",
      "Train Epoche: 2 [1497/2802 (53%)]\tLoss: 16.477909\n",
      "Train Epoche: 2 [1498/2802 (53%)]\tLoss: 2.173627\n",
      "Train Epoche: 2 [1499/2802 (53%)]\tLoss: 13.260192\n",
      "Train Epoche: 2 [1500/2802 (54%)]\tLoss: 2.771993\n",
      "Train Epoche: 2 [1501/2802 (54%)]\tLoss: 0.138760\n",
      "Train Epoche: 2 [1502/2802 (54%)]\tLoss: 4.535804\n",
      "Train Epoche: 2 [1503/2802 (54%)]\tLoss: 10.932699\n",
      "Train Epoche: 2 [1504/2802 (54%)]\tLoss: 26.390230\n",
      "Train Epoche: 2 [1505/2802 (54%)]\tLoss: 2.974662\n",
      "Train Epoche: 2 [1506/2802 (54%)]\tLoss: 0.594097\n",
      "Train Epoche: 2 [1507/2802 (54%)]\tLoss: 0.197307\n",
      "Train Epoche: 2 [1508/2802 (54%)]\tLoss: 13.563769\n",
      "Train Epoche: 2 [1509/2802 (54%)]\tLoss: 26.165661\n",
      "Train Epoche: 2 [1510/2802 (54%)]\tLoss: 0.749869\n",
      "Train Epoche: 2 [1511/2802 (54%)]\tLoss: 11.712919\n",
      "Train Epoche: 2 [1512/2802 (54%)]\tLoss: 18.596785\n",
      "Train Epoche: 2 [1513/2802 (54%)]\tLoss: 0.199380\n",
      "Train Epoche: 2 [1514/2802 (54%)]\tLoss: 1.345120\n",
      "Train Epoche: 2 [1515/2802 (54%)]\tLoss: 44.179836\n",
      "Train Epoche: 2 [1516/2802 (54%)]\tLoss: 0.100860\n",
      "Train Epoche: 2 [1517/2802 (54%)]\tLoss: 17.205048\n",
      "Train Epoche: 2 [1518/2802 (54%)]\tLoss: 6.565048\n",
      "Train Epoche: 2 [1519/2802 (54%)]\tLoss: 1.798930\n",
      "Train Epoche: 2 [1520/2802 (54%)]\tLoss: 0.504091\n",
      "Train Epoche: 2 [1521/2802 (54%)]\tLoss: 5.924003\n",
      "Train Epoche: 2 [1522/2802 (54%)]\tLoss: 8.717466\n",
      "Train Epoche: 2 [1523/2802 (54%)]\tLoss: 76.413956\n",
      "Train Epoche: 2 [1524/2802 (54%)]\tLoss: 11.663300\n",
      "Train Epoche: 2 [1525/2802 (54%)]\tLoss: 1.309902\n",
      "Train Epoche: 2 [1526/2802 (54%)]\tLoss: 9.556336\n",
      "Train Epoche: 2 [1527/2802 (54%)]\tLoss: 3.902631\n",
      "Train Epoche: 2 [1528/2802 (55%)]\tLoss: 5.386488\n",
      "Train Epoche: 2 [1529/2802 (55%)]\tLoss: 13.135399\n",
      "Train Epoche: 2 [1530/2802 (55%)]\tLoss: 5.700754\n",
      "Train Epoche: 2 [1531/2802 (55%)]\tLoss: 4.586751\n",
      "Train Epoche: 2 [1532/2802 (55%)]\tLoss: 9.212770\n",
      "Train Epoche: 2 [1533/2802 (55%)]\tLoss: 3.169032\n",
      "Train Epoche: 2 [1534/2802 (55%)]\tLoss: 2.264931\n",
      "Train Epoche: 2 [1535/2802 (55%)]\tLoss: 59.605000\n",
      "Train Epoche: 2 [1536/2802 (55%)]\tLoss: 7.394060\n",
      "Train Epoche: 2 [1537/2802 (55%)]\tLoss: 1.202878\n",
      "Train Epoche: 2 [1538/2802 (55%)]\tLoss: 11.140279\n",
      "Train Epoche: 2 [1539/2802 (55%)]\tLoss: 1.049172\n",
      "Train Epoche: 2 [1540/2802 (55%)]\tLoss: 22.868687\n",
      "Train Epoche: 2 [1541/2802 (55%)]\tLoss: 6.542772\n",
      "Train Epoche: 2 [1542/2802 (55%)]\tLoss: 2.080350\n",
      "Train Epoche: 2 [1543/2802 (55%)]\tLoss: 1.516393\n",
      "Train Epoche: 2 [1544/2802 (55%)]\tLoss: 1.346786\n",
      "Train Epoche: 2 [1545/2802 (55%)]\tLoss: 1.693940\n",
      "Train Epoche: 2 [1546/2802 (55%)]\tLoss: 0.209958\n",
      "Train Epoche: 2 [1547/2802 (55%)]\tLoss: 0.464677\n",
      "Train Epoche: 2 [1548/2802 (55%)]\tLoss: 132.874161\n",
      "Train Epoche: 2 [1549/2802 (55%)]\tLoss: 12.051831\n",
      "Train Epoche: 2 [1550/2802 (55%)]\tLoss: 1.617017\n",
      "Train Epoche: 2 [1551/2802 (55%)]\tLoss: 2.002479\n",
      "Train Epoche: 2 [1552/2802 (55%)]\tLoss: 17.981529\n",
      "Train Epoche: 2 [1553/2802 (55%)]\tLoss: 0.163680\n",
      "Train Epoche: 2 [1554/2802 (55%)]\tLoss: 0.191185\n",
      "Train Epoche: 2 [1555/2802 (55%)]\tLoss: 126.579712\n",
      "Train Epoche: 2 [1556/2802 (56%)]\tLoss: 81.778732\n",
      "Train Epoche: 2 [1557/2802 (56%)]\tLoss: 4.612326\n",
      "Train Epoche: 2 [1558/2802 (56%)]\tLoss: 20.546221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1559/2802 (56%)]\tLoss: 7.211291\n",
      "Train Epoche: 2 [1560/2802 (56%)]\tLoss: 0.107634\n",
      "Train Epoche: 2 [1561/2802 (56%)]\tLoss: 0.482548\n",
      "Train Epoche: 2 [1562/2802 (56%)]\tLoss: 3.372161\n",
      "Train Epoche: 2 [1563/2802 (56%)]\tLoss: 0.368594\n",
      "Train Epoche: 2 [1564/2802 (56%)]\tLoss: 5.176066\n",
      "Train Epoche: 2 [1565/2802 (56%)]\tLoss: 1.070173\n",
      "Train Epoche: 2 [1566/2802 (56%)]\tLoss: 0.235034\n",
      "Train Epoche: 2 [1567/2802 (56%)]\tLoss: 5.092224\n",
      "Train Epoche: 2 [1568/2802 (56%)]\tLoss: 0.621058\n",
      "Train Epoche: 2 [1569/2802 (56%)]\tLoss: 3.153516\n",
      "Train Epoche: 2 [1570/2802 (56%)]\tLoss: 54.911900\n",
      "Train Epoche: 2 [1571/2802 (56%)]\tLoss: 65.386826\n",
      "Train Epoche: 2 [1572/2802 (56%)]\tLoss: 1.351019\n",
      "Train Epoche: 2 [1573/2802 (56%)]\tLoss: 3.970941\n",
      "Train Epoche: 2 [1574/2802 (56%)]\tLoss: 2.786102\n",
      "Train Epoche: 2 [1575/2802 (56%)]\tLoss: 0.759486\n",
      "Train Epoche: 2 [1576/2802 (56%)]\tLoss: 14.177949\n",
      "Train Epoche: 2 [1577/2802 (56%)]\tLoss: 36.734760\n",
      "Train Epoche: 2 [1578/2802 (56%)]\tLoss: 265.972351\n",
      "Train Epoche: 2 [1579/2802 (56%)]\tLoss: 3.417063\n",
      "Train Epoche: 2 [1580/2802 (56%)]\tLoss: 2.823035\n",
      "Train Epoche: 2 [1581/2802 (56%)]\tLoss: 1.824309\n",
      "Train Epoche: 2 [1582/2802 (56%)]\tLoss: 19.653067\n",
      "Train Epoche: 2 [1583/2802 (56%)]\tLoss: 38.450748\n",
      "Train Epoche: 2 [1584/2802 (57%)]\tLoss: 3.209377\n",
      "Train Epoche: 2 [1585/2802 (57%)]\tLoss: 55.761658\n",
      "Train Epoche: 2 [1586/2802 (57%)]\tLoss: 0.063504\n",
      "Train Epoche: 2 [1587/2802 (57%)]\tLoss: 1.221145\n",
      "Train Epoche: 2 [1588/2802 (57%)]\tLoss: 9.889085\n",
      "Train Epoche: 2 [1589/2802 (57%)]\tLoss: 2.300659\n",
      "Train Epoche: 2 [1590/2802 (57%)]\tLoss: 33.646473\n",
      "Train Epoche: 2 [1591/2802 (57%)]\tLoss: 0.009122\n",
      "Train Epoche: 2 [1592/2802 (57%)]\tLoss: 12.447893\n",
      "Train Epoche: 2 [1593/2802 (57%)]\tLoss: 2.458468\n",
      "Train Epoche: 2 [1594/2802 (57%)]\tLoss: 1.392423\n",
      "Train Epoche: 2 [1595/2802 (57%)]\tLoss: 118.070953\n",
      "Train Epoche: 2 [1596/2802 (57%)]\tLoss: 20.930998\n",
      "Train Epoche: 2 [1597/2802 (57%)]\tLoss: 13.149801\n",
      "Train Epoche: 2 [1598/2802 (57%)]\tLoss: 0.034825\n",
      "Train Epoche: 2 [1599/2802 (57%)]\tLoss: 38.352242\n",
      "Train Epoche: 2 [1600/2802 (57%)]\tLoss: 0.075639\n",
      "Train Epoche: 2 [1601/2802 (57%)]\tLoss: 0.084774\n",
      "Train Epoche: 2 [1602/2802 (57%)]\tLoss: 6.429348\n",
      "Train Epoche: 2 [1603/2802 (57%)]\tLoss: 24.247459\n",
      "Train Epoche: 2 [1604/2802 (57%)]\tLoss: 0.304446\n",
      "Train Epoche: 2 [1605/2802 (57%)]\tLoss: 6.972523\n",
      "Train Epoche: 2 [1606/2802 (57%)]\tLoss: 5.946870\n",
      "Train Epoche: 2 [1607/2802 (57%)]\tLoss: 1.374638\n",
      "Train Epoche: 2 [1608/2802 (57%)]\tLoss: 3.831741\n",
      "Train Epoche: 2 [1609/2802 (57%)]\tLoss: 10.475734\n",
      "Train Epoche: 2 [1610/2802 (57%)]\tLoss: 31.551939\n",
      "Train Epoche: 2 [1611/2802 (57%)]\tLoss: 23.207848\n",
      "Train Epoche: 2 [1612/2802 (58%)]\tLoss: 0.050733\n",
      "Train Epoche: 2 [1613/2802 (58%)]\tLoss: 0.514059\n",
      "Train Epoche: 2 [1614/2802 (58%)]\tLoss: 3.858228\n",
      "Train Epoche: 2 [1615/2802 (58%)]\tLoss: 2.526009\n",
      "Train Epoche: 2 [1616/2802 (58%)]\tLoss: 0.000141\n",
      "Train Epoche: 2 [1617/2802 (58%)]\tLoss: 3.111322\n",
      "Train Epoche: 2 [1618/2802 (58%)]\tLoss: 0.043032\n",
      "Train Epoche: 2 [1619/2802 (58%)]\tLoss: 1.395806\n",
      "Train Epoche: 2 [1620/2802 (58%)]\tLoss: 0.800231\n",
      "Train Epoche: 2 [1621/2802 (58%)]\tLoss: 3.792538\n",
      "Train Epoche: 2 [1622/2802 (58%)]\tLoss: 0.011801\n",
      "Train Epoche: 2 [1623/2802 (58%)]\tLoss: 0.644777\n",
      "Train Epoche: 2 [1624/2802 (58%)]\tLoss: 0.887839\n",
      "Train Epoche: 2 [1625/2802 (58%)]\tLoss: 3.666074\n",
      "Train Epoche: 2 [1626/2802 (58%)]\tLoss: 1.224432\n",
      "Train Epoche: 2 [1627/2802 (58%)]\tLoss: 18.038134\n",
      "Train Epoche: 2 [1628/2802 (58%)]\tLoss: 0.694158\n",
      "Train Epoche: 2 [1629/2802 (58%)]\tLoss: 2.708606\n",
      "Train Epoche: 2 [1630/2802 (58%)]\tLoss: 0.020258\n",
      "Train Epoche: 2 [1631/2802 (58%)]\tLoss: 47.751392\n",
      "Train Epoche: 2 [1632/2802 (58%)]\tLoss: 9.176628\n",
      "Train Epoche: 2 [1633/2802 (58%)]\tLoss: 0.923232\n",
      "Train Epoche: 2 [1634/2802 (58%)]\tLoss: 8.369390\n",
      "Train Epoche: 2 [1635/2802 (58%)]\tLoss: 107.924309\n",
      "Train Epoche: 2 [1636/2802 (58%)]\tLoss: 43.073029\n",
      "Train Epoche: 2 [1637/2802 (58%)]\tLoss: 0.792371\n",
      "Train Epoche: 2 [1638/2802 (58%)]\tLoss: 1.207432\n",
      "Train Epoche: 2 [1639/2802 (58%)]\tLoss: 0.023850\n",
      "Train Epoche: 2 [1640/2802 (59%)]\tLoss: 152.673065\n",
      "Train Epoche: 2 [1641/2802 (59%)]\tLoss: 0.060393\n",
      "Train Epoche: 2 [1642/2802 (59%)]\tLoss: 194.904495\n",
      "Train Epoche: 2 [1643/2802 (59%)]\tLoss: 9.206367\n",
      "Train Epoche: 2 [1644/2802 (59%)]\tLoss: 21.818371\n",
      "Train Epoche: 2 [1645/2802 (59%)]\tLoss: 18.195833\n",
      "Train Epoche: 2 [1646/2802 (59%)]\tLoss: 2.908967\n",
      "Train Epoche: 2 [1647/2802 (59%)]\tLoss: 1.888226\n",
      "Train Epoche: 2 [1648/2802 (59%)]\tLoss: 3.004014\n",
      "Train Epoche: 2 [1649/2802 (59%)]\tLoss: 14.639350\n",
      "Train Epoche: 2 [1650/2802 (59%)]\tLoss: 2.234155\n",
      "Train Epoche: 2 [1651/2802 (59%)]\tLoss: 4.559927\n",
      "Train Epoche: 2 [1652/2802 (59%)]\tLoss: 2.126749\n",
      "Train Epoche: 2 [1653/2802 (59%)]\tLoss: 0.103368\n",
      "Train Epoche: 2 [1654/2802 (59%)]\tLoss: 25.015471\n",
      "Train Epoche: 2 [1655/2802 (59%)]\tLoss: 1.321616\n",
      "Train Epoche: 2 [1656/2802 (59%)]\tLoss: 0.585058\n",
      "Train Epoche: 2 [1657/2802 (59%)]\tLoss: 2.218778\n",
      "Train Epoche: 2 [1658/2802 (59%)]\tLoss: 45.542881\n",
      "Train Epoche: 2 [1659/2802 (59%)]\tLoss: 2.757345\n",
      "Train Epoche: 2 [1660/2802 (59%)]\tLoss: 5.188641\n",
      "Train Epoche: 2 [1661/2802 (59%)]\tLoss: 240.115646\n",
      "Train Epoche: 2 [1662/2802 (59%)]\tLoss: 23.419878\n",
      "Train Epoche: 2 [1663/2802 (59%)]\tLoss: 2.691334\n",
      "Train Epoche: 2 [1664/2802 (59%)]\tLoss: 226.369873\n",
      "Train Epoche: 2 [1665/2802 (59%)]\tLoss: 1.385284\n",
      "Train Epoche: 2 [1666/2802 (59%)]\tLoss: 32.451576\n",
      "Train Epoche: 2 [1667/2802 (59%)]\tLoss: 19.274708\n",
      "Train Epoche: 2 [1668/2802 (60%)]\tLoss: 70.619514\n",
      "Train Epoche: 2 [1669/2802 (60%)]\tLoss: 0.212796\n",
      "Train Epoche: 2 [1670/2802 (60%)]\tLoss: 4.436477\n",
      "Train Epoche: 2 [1671/2802 (60%)]\tLoss: 0.813282\n",
      "Train Epoche: 2 [1672/2802 (60%)]\tLoss: 22.734289\n",
      "Train Epoche: 2 [1673/2802 (60%)]\tLoss: 4.147955\n",
      "Train Epoche: 2 [1674/2802 (60%)]\tLoss: 6.908847\n",
      "Train Epoche: 2 [1675/2802 (60%)]\tLoss: 42.368309\n",
      "Train Epoche: 2 [1676/2802 (60%)]\tLoss: 0.000065\n",
      "Train Epoche: 2 [1677/2802 (60%)]\tLoss: 0.167599\n",
      "Train Epoche: 2 [1678/2802 (60%)]\tLoss: 25.556171\n",
      "Train Epoche: 2 [1679/2802 (60%)]\tLoss: 1.423250\n",
      "Train Epoche: 2 [1680/2802 (60%)]\tLoss: 22.814213\n",
      "Train Epoche: 2 [1681/2802 (60%)]\tLoss: 39.753403\n",
      "Train Epoche: 2 [1682/2802 (60%)]\tLoss: 1.616436\n",
      "Train Epoche: 2 [1683/2802 (60%)]\tLoss: 77.960846\n",
      "Train Epoche: 2 [1684/2802 (60%)]\tLoss: 4.675995\n",
      "Train Epoche: 2 [1685/2802 (60%)]\tLoss: 1.071943\n",
      "Train Epoche: 2 [1686/2802 (60%)]\tLoss: 57.784519\n",
      "Train Epoche: 2 [1687/2802 (60%)]\tLoss: 1.370611\n",
      "Train Epoche: 2 [1688/2802 (60%)]\tLoss: 4.070006\n",
      "Train Epoche: 2 [1689/2802 (60%)]\tLoss: 0.494954\n",
      "Train Epoche: 2 [1690/2802 (60%)]\tLoss: 0.842177\n",
      "Train Epoche: 2 [1691/2802 (60%)]\tLoss: 5.207143\n",
      "Train Epoche: 2 [1692/2802 (60%)]\tLoss: 3.472819\n",
      "Train Epoche: 2 [1693/2802 (60%)]\tLoss: 10.224422\n",
      "Train Epoche: 2 [1694/2802 (60%)]\tLoss: 10.886791\n",
      "Train Epoche: 2 [1695/2802 (60%)]\tLoss: 3.662672\n",
      "Train Epoche: 2 [1696/2802 (61%)]\tLoss: 7.525915\n",
      "Train Epoche: 2 [1697/2802 (61%)]\tLoss: 2.071226\n",
      "Train Epoche: 2 [1698/2802 (61%)]\tLoss: 2.175259\n",
      "Train Epoche: 2 [1699/2802 (61%)]\tLoss: 0.856725\n",
      "Train Epoche: 2 [1700/2802 (61%)]\tLoss: 2.655223\n",
      "Train Epoche: 2 [1701/2802 (61%)]\tLoss: 11.129605\n",
      "Train Epoche: 2 [1702/2802 (61%)]\tLoss: 2.822555\n",
      "Train Epoche: 2 [1703/2802 (61%)]\tLoss: 18.978333\n",
      "Train Epoche: 2 [1704/2802 (61%)]\tLoss: 15.180257\n",
      "Train Epoche: 2 [1705/2802 (61%)]\tLoss: 0.058683\n",
      "Train Epoche: 2 [1706/2802 (61%)]\tLoss: 0.592079\n",
      "Train Epoche: 2 [1707/2802 (61%)]\tLoss: 8.795197\n",
      "Train Epoche: 2 [1708/2802 (61%)]\tLoss: 0.255551\n",
      "Train Epoche: 2 [1709/2802 (61%)]\tLoss: 4.243574\n",
      "Train Epoche: 2 [1710/2802 (61%)]\tLoss: 6.613124\n",
      "Train Epoche: 2 [1711/2802 (61%)]\tLoss: 6.235464\n",
      "Train Epoche: 2 [1712/2802 (61%)]\tLoss: 9.024696\n",
      "Train Epoche: 2 [1713/2802 (61%)]\tLoss: 3.108361\n",
      "Train Epoche: 2 [1714/2802 (61%)]\tLoss: 8.600870\n",
      "Train Epoche: 2 [1715/2802 (61%)]\tLoss: 5.806031\n",
      "Train Epoche: 2 [1716/2802 (61%)]\tLoss: 14.853405\n",
      "Train Epoche: 2 [1717/2802 (61%)]\tLoss: 0.120917\n",
      "Train Epoche: 2 [1718/2802 (61%)]\tLoss: 4.141411\n",
      "Train Epoche: 2 [1719/2802 (61%)]\tLoss: 39.945385\n",
      "Train Epoche: 2 [1720/2802 (61%)]\tLoss: 0.789314\n",
      "Train Epoche: 2 [1721/2802 (61%)]\tLoss: 20.659626\n",
      "Train Epoche: 2 [1722/2802 (61%)]\tLoss: 18.983215\n",
      "Train Epoche: 2 [1723/2802 (61%)]\tLoss: 2.350583\n",
      "Train Epoche: 2 [1724/2802 (62%)]\tLoss: 11.242338\n",
      "Train Epoche: 2 [1725/2802 (62%)]\tLoss: 59.011940\n",
      "Train Epoche: 2 [1726/2802 (62%)]\tLoss: 0.161433\n",
      "Train Epoche: 2 [1727/2802 (62%)]\tLoss: 5.856636\n",
      "Train Epoche: 2 [1728/2802 (62%)]\tLoss: 37.703575\n",
      "Train Epoche: 2 [1729/2802 (62%)]\tLoss: 0.666557\n",
      "Train Epoche: 2 [1730/2802 (62%)]\tLoss: 1.218740\n",
      "Train Epoche: 2 [1731/2802 (62%)]\tLoss: 0.468488\n",
      "Train Epoche: 2 [1732/2802 (62%)]\tLoss: 0.754977\n",
      "Train Epoche: 2 [1733/2802 (62%)]\tLoss: 0.895064\n",
      "Train Epoche: 2 [1734/2802 (62%)]\tLoss: 2.997360\n",
      "Train Epoche: 2 [1735/2802 (62%)]\tLoss: 58.323437\n",
      "Train Epoche: 2 [1736/2802 (62%)]\tLoss: 30.050236\n",
      "Train Epoche: 2 [1737/2802 (62%)]\tLoss: 74.969223\n",
      "Train Epoche: 2 [1738/2802 (62%)]\tLoss: 5.475253\n",
      "Train Epoche: 2 [1739/2802 (62%)]\tLoss: 16.977959\n",
      "Train Epoche: 2 [1740/2802 (62%)]\tLoss: 0.612638\n",
      "Train Epoche: 2 [1741/2802 (62%)]\tLoss: 0.001596\n",
      "Train Epoche: 2 [1742/2802 (62%)]\tLoss: 0.979836\n",
      "Train Epoche: 2 [1743/2802 (62%)]\tLoss: 0.002310\n",
      "Train Epoche: 2 [1744/2802 (62%)]\tLoss: 1.059558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1745/2802 (62%)]\tLoss: 0.191448\n",
      "Train Epoche: 2 [1746/2802 (62%)]\tLoss: 0.654878\n",
      "Train Epoche: 2 [1747/2802 (62%)]\tLoss: 12.524558\n",
      "Train Epoche: 2 [1748/2802 (62%)]\tLoss: 0.077976\n",
      "Train Epoche: 2 [1749/2802 (62%)]\tLoss: 5.676578\n",
      "Train Epoche: 2 [1750/2802 (62%)]\tLoss: 8.734488\n",
      "Train Epoche: 2 [1751/2802 (62%)]\tLoss: 22.734730\n",
      "Train Epoche: 2 [1752/2802 (63%)]\tLoss: 2.383904\n",
      "Train Epoche: 2 [1753/2802 (63%)]\tLoss: 32.571339\n",
      "Train Epoche: 2 [1754/2802 (63%)]\tLoss: 16.421541\n",
      "Train Epoche: 2 [1755/2802 (63%)]\tLoss: 0.021439\n",
      "Train Epoche: 2 [1756/2802 (63%)]\tLoss: 3.900564\n",
      "Train Epoche: 2 [1757/2802 (63%)]\tLoss: 0.853566\n",
      "Train Epoche: 2 [1758/2802 (63%)]\tLoss: 0.497000\n",
      "Train Epoche: 2 [1759/2802 (63%)]\tLoss: 5.789961\n",
      "Train Epoche: 2 [1760/2802 (63%)]\tLoss: 3.172077\n",
      "Train Epoche: 2 [1761/2802 (63%)]\tLoss: 46.880756\n",
      "Train Epoche: 2 [1762/2802 (63%)]\tLoss: 1.155909\n",
      "Train Epoche: 2 [1763/2802 (63%)]\tLoss: 4.895644\n",
      "Train Epoche: 2 [1764/2802 (63%)]\tLoss: 76.992035\n",
      "Train Epoche: 2 [1765/2802 (63%)]\tLoss: 126.257645\n",
      "Train Epoche: 2 [1766/2802 (63%)]\tLoss: 0.163893\n",
      "Train Epoche: 2 [1767/2802 (63%)]\tLoss: 7.917405\n",
      "Train Epoche: 2 [1768/2802 (63%)]\tLoss: 2.301605\n",
      "Train Epoche: 2 [1769/2802 (63%)]\tLoss: 0.590102\n",
      "Train Epoche: 2 [1770/2802 (63%)]\tLoss: 16.922880\n",
      "Train Epoche: 2 [1771/2802 (63%)]\tLoss: 8.641663\n",
      "Train Epoche: 2 [1772/2802 (63%)]\tLoss: 35.561230\n",
      "Train Epoche: 2 [1773/2802 (63%)]\tLoss: 38.176399\n",
      "Train Epoche: 2 [1774/2802 (63%)]\tLoss: 0.552693\n",
      "Train Epoche: 2 [1775/2802 (63%)]\tLoss: 4.842213\n",
      "Train Epoche: 2 [1776/2802 (63%)]\tLoss: 3.379677\n",
      "Train Epoche: 2 [1777/2802 (63%)]\tLoss: 2.182834\n",
      "Train Epoche: 2 [1778/2802 (63%)]\tLoss: 0.895781\n",
      "Train Epoche: 2 [1779/2802 (63%)]\tLoss: 0.018072\n",
      "Train Epoche: 2 [1780/2802 (64%)]\tLoss: 0.265316\n",
      "Train Epoche: 2 [1781/2802 (64%)]\tLoss: 27.681890\n",
      "Train Epoche: 2 [1782/2802 (64%)]\tLoss: 1.961473\n",
      "Train Epoche: 2 [1783/2802 (64%)]\tLoss: 7.538077\n",
      "Train Epoche: 2 [1784/2802 (64%)]\tLoss: 18.386339\n",
      "Train Epoche: 2 [1785/2802 (64%)]\tLoss: 4.417627\n",
      "Train Epoche: 2 [1786/2802 (64%)]\tLoss: 43.242989\n",
      "Train Epoche: 2 [1787/2802 (64%)]\tLoss: 7.031184\n",
      "Train Epoche: 2 [1788/2802 (64%)]\tLoss: 26.322958\n",
      "Train Epoche: 2 [1789/2802 (64%)]\tLoss: 3.617765\n",
      "Train Epoche: 2 [1790/2802 (64%)]\tLoss: 25.350897\n",
      "Train Epoche: 2 [1791/2802 (64%)]\tLoss: 9.573290\n",
      "Train Epoche: 2 [1792/2802 (64%)]\tLoss: 36.772526\n",
      "Train Epoche: 2 [1793/2802 (64%)]\tLoss: 0.003434\n",
      "Train Epoche: 2 [1794/2802 (64%)]\tLoss: 9.787775\n",
      "Train Epoche: 2 [1795/2802 (64%)]\tLoss: 1.546218\n",
      "Train Epoche: 2 [1796/2802 (64%)]\tLoss: 297.195557\n",
      "Train Epoche: 2 [1797/2802 (64%)]\tLoss: 3.586185\n",
      "Train Epoche: 2 [1798/2802 (64%)]\tLoss: 0.700835\n",
      "Train Epoche: 2 [1799/2802 (64%)]\tLoss: 161.260559\n",
      "Train Epoche: 2 [1800/2802 (64%)]\tLoss: 9.060429\n",
      "Train Epoche: 2 [1801/2802 (64%)]\tLoss: 25.968319\n",
      "Train Epoche: 2 [1802/2802 (64%)]\tLoss: 5.344573\n",
      "Train Epoche: 2 [1803/2802 (64%)]\tLoss: 106.069099\n",
      "Train Epoche: 2 [1804/2802 (64%)]\tLoss: 5.001624\n",
      "Train Epoche: 2 [1805/2802 (64%)]\tLoss: 0.757555\n",
      "Train Epoche: 2 [1806/2802 (64%)]\tLoss: 5.731474\n",
      "Train Epoche: 2 [1807/2802 (64%)]\tLoss: 0.147905\n",
      "Train Epoche: 2 [1808/2802 (65%)]\tLoss: 8.279575\n",
      "Train Epoche: 2 [1809/2802 (65%)]\tLoss: 32.343739\n",
      "Train Epoche: 2 [1810/2802 (65%)]\tLoss: 3.965079\n",
      "Train Epoche: 2 [1811/2802 (65%)]\tLoss: 4.855890\n",
      "Train Epoche: 2 [1812/2802 (65%)]\tLoss: 0.146116\n",
      "Train Epoche: 2 [1813/2802 (65%)]\tLoss: 3.766570\n",
      "Train Epoche: 2 [1814/2802 (65%)]\tLoss: 5.063109\n",
      "Train Epoche: 2 [1815/2802 (65%)]\tLoss: 34.117458\n",
      "Train Epoche: 2 [1816/2802 (65%)]\tLoss: 1.180572\n",
      "Train Epoche: 2 [1817/2802 (65%)]\tLoss: 0.002664\n",
      "Train Epoche: 2 [1818/2802 (65%)]\tLoss: 0.741829\n",
      "Train Epoche: 2 [1819/2802 (65%)]\tLoss: 1.990883\n",
      "Train Epoche: 2 [1820/2802 (65%)]\tLoss: 1.366473\n",
      "Train Epoche: 2 [1821/2802 (65%)]\tLoss: 134.108917\n",
      "Train Epoche: 2 [1822/2802 (65%)]\tLoss: 6.774480\n",
      "Train Epoche: 2 [1823/2802 (65%)]\tLoss: 20.867611\n",
      "Train Epoche: 2 [1824/2802 (65%)]\tLoss: 0.210638\n",
      "Train Epoche: 2 [1825/2802 (65%)]\tLoss: 3.229894\n",
      "Train Epoche: 2 [1826/2802 (65%)]\tLoss: 12.694014\n",
      "Train Epoche: 2 [1827/2802 (65%)]\tLoss: 0.364699\n",
      "Train Epoche: 2 [1828/2802 (65%)]\tLoss: 25.219347\n",
      "Train Epoche: 2 [1829/2802 (65%)]\tLoss: 209.707947\n",
      "Train Epoche: 2 [1830/2802 (65%)]\tLoss: 9.347635\n",
      "Train Epoche: 2 [1831/2802 (65%)]\tLoss: 0.012181\n",
      "Train Epoche: 2 [1832/2802 (65%)]\tLoss: 10.140759\n",
      "Train Epoche: 2 [1833/2802 (65%)]\tLoss: 6.574380\n",
      "Train Epoche: 2 [1834/2802 (65%)]\tLoss: 2.259933\n",
      "Train Epoche: 2 [1835/2802 (65%)]\tLoss: 0.151936\n",
      "Train Epoche: 2 [1836/2802 (66%)]\tLoss: 1.286360\n",
      "Train Epoche: 2 [1837/2802 (66%)]\tLoss: 28.520298\n",
      "Train Epoche: 2 [1838/2802 (66%)]\tLoss: 0.003369\n",
      "Train Epoche: 2 [1839/2802 (66%)]\tLoss: 4.396682\n",
      "Train Epoche: 2 [1840/2802 (66%)]\tLoss: 4.629563\n",
      "Train Epoche: 2 [1841/2802 (66%)]\tLoss: 11.006016\n",
      "Train Epoche: 2 [1842/2802 (66%)]\tLoss: 3.447740\n",
      "Train Epoche: 2 [1843/2802 (66%)]\tLoss: 7.836557\n",
      "Train Epoche: 2 [1844/2802 (66%)]\tLoss: 0.596155\n",
      "Train Epoche: 2 [1845/2802 (66%)]\tLoss: 0.007730\n",
      "Train Epoche: 2 [1846/2802 (66%)]\tLoss: 2.413899\n",
      "Train Epoche: 2 [1847/2802 (66%)]\tLoss: 1.778217\n",
      "Train Epoche: 2 [1848/2802 (66%)]\tLoss: 14.809949\n",
      "Train Epoche: 2 [1849/2802 (66%)]\tLoss: 2.606253\n",
      "Train Epoche: 2 [1850/2802 (66%)]\tLoss: 0.471538\n",
      "Train Epoche: 2 [1851/2802 (66%)]\tLoss: 295.736359\n",
      "Train Epoche: 2 [1852/2802 (66%)]\tLoss: 1.580538\n",
      "Train Epoche: 2 [1853/2802 (66%)]\tLoss: 9.769511\n",
      "Train Epoche: 2 [1854/2802 (66%)]\tLoss: 20.939814\n",
      "Train Epoche: 2 [1855/2802 (66%)]\tLoss: 1.266556\n",
      "Train Epoche: 2 [1856/2802 (66%)]\tLoss: 1.053911\n",
      "Train Epoche: 2 [1857/2802 (66%)]\tLoss: 20.137804\n",
      "Train Epoche: 2 [1858/2802 (66%)]\tLoss: 4.925992\n",
      "Train Epoche: 2 [1859/2802 (66%)]\tLoss: 0.452197\n",
      "Train Epoche: 2 [1860/2802 (66%)]\tLoss: 12.954016\n",
      "Train Epoche: 2 [1861/2802 (66%)]\tLoss: 1.867437\n",
      "Train Epoche: 2 [1862/2802 (66%)]\tLoss: 87.597290\n",
      "Train Epoche: 2 [1863/2802 (66%)]\tLoss: 2.062717\n",
      "Train Epoche: 2 [1864/2802 (67%)]\tLoss: 26.793530\n",
      "Train Epoche: 2 [1865/2802 (67%)]\tLoss: 4.928016\n",
      "Train Epoche: 2 [1866/2802 (67%)]\tLoss: 14.459208\n",
      "Train Epoche: 2 [1867/2802 (67%)]\tLoss: 0.904196\n",
      "Train Epoche: 2 [1868/2802 (67%)]\tLoss: 0.000233\n",
      "Train Epoche: 2 [1869/2802 (67%)]\tLoss: 9.446025\n",
      "Train Epoche: 2 [1870/2802 (67%)]\tLoss: 48.945061\n",
      "Train Epoche: 2 [1871/2802 (67%)]\tLoss: 0.173717\n",
      "Train Epoche: 2 [1872/2802 (67%)]\tLoss: 1.822135\n",
      "Train Epoche: 2 [1873/2802 (67%)]\tLoss: 23.046238\n",
      "Train Epoche: 2 [1874/2802 (67%)]\tLoss: 0.028578\n",
      "Train Epoche: 2 [1875/2802 (67%)]\tLoss: 0.004077\n",
      "Train Epoche: 2 [1876/2802 (67%)]\tLoss: 0.445320\n",
      "Train Epoche: 2 [1877/2802 (67%)]\tLoss: 48.059727\n",
      "Train Epoche: 2 [1878/2802 (67%)]\tLoss: 2.707144\n",
      "Train Epoche: 2 [1879/2802 (67%)]\tLoss: 1.340605\n",
      "Train Epoche: 2 [1880/2802 (67%)]\tLoss: 10.852614\n",
      "Train Epoche: 2 [1881/2802 (67%)]\tLoss: 0.402419\n",
      "Train Epoche: 2 [1882/2802 (67%)]\tLoss: 4.647161\n",
      "Train Epoche: 2 [1883/2802 (67%)]\tLoss: 0.210072\n",
      "Train Epoche: 2 [1884/2802 (67%)]\tLoss: 3.552083\n",
      "Train Epoche: 2 [1885/2802 (67%)]\tLoss: 0.098904\n",
      "Train Epoche: 2 [1886/2802 (67%)]\tLoss: 12.936829\n",
      "Train Epoche: 2 [1887/2802 (67%)]\tLoss: 43.922562\n",
      "Train Epoche: 2 [1888/2802 (67%)]\tLoss: 0.776713\n",
      "Train Epoche: 2 [1889/2802 (67%)]\tLoss: 0.431932\n",
      "Train Epoche: 2 [1890/2802 (67%)]\tLoss: 0.066582\n",
      "Train Epoche: 2 [1891/2802 (67%)]\tLoss: 1.172455\n",
      "Train Epoche: 2 [1892/2802 (68%)]\tLoss: 0.000051\n",
      "Train Epoche: 2 [1893/2802 (68%)]\tLoss: 3.492817\n",
      "Train Epoche: 2 [1894/2802 (68%)]\tLoss: 0.046404\n",
      "Train Epoche: 2 [1895/2802 (68%)]\tLoss: 6.243047\n",
      "Train Epoche: 2 [1896/2802 (68%)]\tLoss: 5.908453\n",
      "Train Epoche: 2 [1897/2802 (68%)]\tLoss: 5.542248\n",
      "Train Epoche: 2 [1898/2802 (68%)]\tLoss: 3.361135\n",
      "Train Epoche: 2 [1899/2802 (68%)]\tLoss: 0.059238\n",
      "Train Epoche: 2 [1900/2802 (68%)]\tLoss: 20.995163\n",
      "Train Epoche: 2 [1901/2802 (68%)]\tLoss: 2.911176\n",
      "Train Epoche: 2 [1902/2802 (68%)]\tLoss: 2.273312\n",
      "Train Epoche: 2 [1903/2802 (68%)]\tLoss: 3.130863\n",
      "Train Epoche: 2 [1904/2802 (68%)]\tLoss: 1.604647\n",
      "Train Epoche: 2 [1905/2802 (68%)]\tLoss: 83.220032\n",
      "Train Epoche: 2 [1906/2802 (68%)]\tLoss: 6.849481\n",
      "Train Epoche: 2 [1907/2802 (68%)]\tLoss: 0.890834\n",
      "Train Epoche: 2 [1908/2802 (68%)]\tLoss: 74.527557\n",
      "Train Epoche: 2 [1909/2802 (68%)]\tLoss: 0.013925\n",
      "Train Epoche: 2 [1910/2802 (68%)]\tLoss: 10.017279\n",
      "Train Epoche: 2 [1911/2802 (68%)]\tLoss: 0.669266\n",
      "Train Epoche: 2 [1912/2802 (68%)]\tLoss: 68.579895\n",
      "Train Epoche: 2 [1913/2802 (68%)]\tLoss: 30.573399\n",
      "Train Epoche: 2 [1914/2802 (68%)]\tLoss: 13.845733\n",
      "Train Epoche: 2 [1915/2802 (68%)]\tLoss: 0.013639\n",
      "Train Epoche: 2 [1916/2802 (68%)]\tLoss: 48.648129\n",
      "Train Epoche: 2 [1917/2802 (68%)]\tLoss: 13.372962\n",
      "Train Epoche: 2 [1918/2802 (68%)]\tLoss: 2.731343\n",
      "Train Epoche: 2 [1919/2802 (68%)]\tLoss: 1.240170\n",
      "Train Epoche: 2 [1920/2802 (69%)]\tLoss: 8.890045\n",
      "Train Epoche: 2 [1921/2802 (69%)]\tLoss: 1.312034\n",
      "Train Epoche: 2 [1922/2802 (69%)]\tLoss: 7.649505\n",
      "Train Epoche: 2 [1923/2802 (69%)]\tLoss: 4.711136\n",
      "Train Epoche: 2 [1924/2802 (69%)]\tLoss: 115.969147\n",
      "Train Epoche: 2 [1925/2802 (69%)]\tLoss: 0.985648\n",
      "Train Epoche: 2 [1926/2802 (69%)]\tLoss: 40.099159\n",
      "Train Epoche: 2 [1927/2802 (69%)]\tLoss: 0.880816\n",
      "Train Epoche: 2 [1928/2802 (69%)]\tLoss: 3.593528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1929/2802 (69%)]\tLoss: 49.452747\n",
      "Train Epoche: 2 [1930/2802 (69%)]\tLoss: 0.003203\n",
      "Train Epoche: 2 [1931/2802 (69%)]\tLoss: 23.801382\n",
      "Train Epoche: 2 [1932/2802 (69%)]\tLoss: 1.745764\n",
      "Train Epoche: 2 [1933/2802 (69%)]\tLoss: 5.672849\n",
      "Train Epoche: 2 [1934/2802 (69%)]\tLoss: 2.044246\n",
      "Train Epoche: 2 [1935/2802 (69%)]\tLoss: 7.904819\n",
      "Train Epoche: 2 [1936/2802 (69%)]\tLoss: 4.175923\n",
      "Train Epoche: 2 [1937/2802 (69%)]\tLoss: 64.666092\n",
      "Train Epoche: 2 [1938/2802 (69%)]\tLoss: 24.385481\n",
      "Train Epoche: 2 [1939/2802 (69%)]\tLoss: 0.033916\n",
      "Train Epoche: 2 [1940/2802 (69%)]\tLoss: 0.260528\n",
      "Train Epoche: 2 [1941/2802 (69%)]\tLoss: 6.431467\n",
      "Train Epoche: 2 [1942/2802 (69%)]\tLoss: 0.032342\n",
      "Train Epoche: 2 [1943/2802 (69%)]\tLoss: 7.746092\n",
      "Train Epoche: 2 [1944/2802 (69%)]\tLoss: 11.427495\n",
      "Train Epoche: 2 [1945/2802 (69%)]\tLoss: 5.153482\n",
      "Train Epoche: 2 [1946/2802 (69%)]\tLoss: 1.174379\n",
      "Train Epoche: 2 [1947/2802 (69%)]\tLoss: 23.896576\n",
      "Train Epoche: 2 [1948/2802 (70%)]\tLoss: 23.786062\n",
      "Train Epoche: 2 [1949/2802 (70%)]\tLoss: 14.095550\n",
      "Train Epoche: 2 [1950/2802 (70%)]\tLoss: 2.861613\n",
      "Train Epoche: 2 [1951/2802 (70%)]\tLoss: 70.634949\n",
      "Train Epoche: 2 [1952/2802 (70%)]\tLoss: 8.064331\n",
      "Train Epoche: 2 [1953/2802 (70%)]\tLoss: 1.215325\n",
      "Train Epoche: 2 [1954/2802 (70%)]\tLoss: 2.232242\n",
      "Train Epoche: 2 [1955/2802 (70%)]\tLoss: 7.298406\n",
      "Train Epoche: 2 [1956/2802 (70%)]\tLoss: 0.035289\n",
      "Train Epoche: 2 [1957/2802 (70%)]\tLoss: 131.726044\n",
      "Train Epoche: 2 [1958/2802 (70%)]\tLoss: 70.169090\n",
      "Train Epoche: 2 [1959/2802 (70%)]\tLoss: 0.154824\n",
      "Train Epoche: 2 [1960/2802 (70%)]\tLoss: 0.494499\n",
      "Train Epoche: 2 [1961/2802 (70%)]\tLoss: 0.051549\n",
      "Train Epoche: 2 [1962/2802 (70%)]\tLoss: 3.580646\n",
      "Train Epoche: 2 [1963/2802 (70%)]\tLoss: 3.657887\n",
      "Train Epoche: 2 [1964/2802 (70%)]\tLoss: 4.874610\n",
      "Train Epoche: 2 [1965/2802 (70%)]\tLoss: 2.866277\n",
      "Train Epoche: 2 [1966/2802 (70%)]\tLoss: 0.201852\n",
      "Train Epoche: 2 [1967/2802 (70%)]\tLoss: 0.612695\n",
      "Train Epoche: 2 [1968/2802 (70%)]\tLoss: 17.574522\n",
      "Train Epoche: 2 [1969/2802 (70%)]\tLoss: 27.338074\n",
      "Train Epoche: 2 [1970/2802 (70%)]\tLoss: 2.805936\n",
      "Train Epoche: 2 [1971/2802 (70%)]\tLoss: 52.187031\n",
      "Train Epoche: 2 [1972/2802 (70%)]\tLoss: 0.013199\n",
      "Train Epoche: 2 [1973/2802 (70%)]\tLoss: 1.705480\n",
      "Train Epoche: 2 [1974/2802 (70%)]\tLoss: 4.087059\n",
      "Train Epoche: 2 [1975/2802 (70%)]\tLoss: 27.013533\n",
      "Train Epoche: 2 [1976/2802 (71%)]\tLoss: 2.329870\n",
      "Train Epoche: 2 [1977/2802 (71%)]\tLoss: 1.948391\n",
      "Train Epoche: 2 [1978/2802 (71%)]\tLoss: 1.435960\n",
      "Train Epoche: 2 [1979/2802 (71%)]\tLoss: 14.931029\n",
      "Train Epoche: 2 [1980/2802 (71%)]\tLoss: 10.166091\n",
      "Train Epoche: 2 [1981/2802 (71%)]\tLoss: 2.221228\n",
      "Train Epoche: 2 [1982/2802 (71%)]\tLoss: 17.226501\n",
      "Train Epoche: 2 [1983/2802 (71%)]\tLoss: 0.278560\n",
      "Train Epoche: 2 [1984/2802 (71%)]\tLoss: 8.283368\n",
      "Train Epoche: 2 [1985/2802 (71%)]\tLoss: 5.582642\n",
      "Train Epoche: 2 [1986/2802 (71%)]\tLoss: 280.432098\n",
      "Train Epoche: 2 [1987/2802 (71%)]\tLoss: 101.054596\n",
      "Train Epoche: 2 [1988/2802 (71%)]\tLoss: 2.321722\n",
      "Train Epoche: 2 [1989/2802 (71%)]\tLoss: 0.437364\n",
      "Train Epoche: 2 [1990/2802 (71%)]\tLoss: 0.393664\n",
      "Train Epoche: 2 [1991/2802 (71%)]\tLoss: 1.431589\n",
      "Train Epoche: 2 [1992/2802 (71%)]\tLoss: 0.124035\n",
      "Train Epoche: 2 [1993/2802 (71%)]\tLoss: 1.419023\n",
      "Train Epoche: 2 [1994/2802 (71%)]\tLoss: 5.910326\n",
      "Train Epoche: 2 [1995/2802 (71%)]\tLoss: 0.000850\n",
      "Train Epoche: 2 [1996/2802 (71%)]\tLoss: 0.042021\n",
      "Train Epoche: 2 [1997/2802 (71%)]\tLoss: 11.760192\n",
      "Train Epoche: 2 [1998/2802 (71%)]\tLoss: 3.605673\n",
      "Train Epoche: 2 [1999/2802 (71%)]\tLoss: 16.513586\n",
      "Train Epoche: 2 [2000/2802 (71%)]\tLoss: 82.339508\n",
      "Train Epoche: 2 [2001/2802 (71%)]\tLoss: 0.437683\n",
      "Train Epoche: 2 [2002/2802 (71%)]\tLoss: 2.349333\n",
      "Train Epoche: 2 [2003/2802 (71%)]\tLoss: 0.000554\n",
      "Train Epoche: 2 [2004/2802 (72%)]\tLoss: 14.785941\n",
      "Train Epoche: 2 [2005/2802 (72%)]\tLoss: 4.485928\n",
      "Train Epoche: 2 [2006/2802 (72%)]\tLoss: 0.376496\n",
      "Train Epoche: 2 [2007/2802 (72%)]\tLoss: 1.884844\n",
      "Train Epoche: 2 [2008/2802 (72%)]\tLoss: 19.780809\n",
      "Train Epoche: 2 [2009/2802 (72%)]\tLoss: 2.565916\n",
      "Train Epoche: 2 [2010/2802 (72%)]\tLoss: 4.350666\n",
      "Train Epoche: 2 [2011/2802 (72%)]\tLoss: 0.050826\n",
      "Train Epoche: 2 [2012/2802 (72%)]\tLoss: 4.130107\n",
      "Train Epoche: 2 [2013/2802 (72%)]\tLoss: 3.919839\n",
      "Train Epoche: 2 [2014/2802 (72%)]\tLoss: 0.528137\n",
      "Train Epoche: 2 [2015/2802 (72%)]\tLoss: 36.633816\n",
      "Train Epoche: 2 [2016/2802 (72%)]\tLoss: 3.564906\n",
      "Train Epoche: 2 [2017/2802 (72%)]\tLoss: 62.498413\n",
      "Train Epoche: 2 [2018/2802 (72%)]\tLoss: 0.456367\n",
      "Train Epoche: 2 [2019/2802 (72%)]\tLoss: 28.679943\n",
      "Train Epoche: 2 [2020/2802 (72%)]\tLoss: 1.538650\n",
      "Train Epoche: 2 [2021/2802 (72%)]\tLoss: 1.437181\n",
      "Train Epoche: 2 [2022/2802 (72%)]\tLoss: 34.845150\n",
      "Train Epoche: 2 [2023/2802 (72%)]\tLoss: 8.843246\n",
      "Train Epoche: 2 [2024/2802 (72%)]\tLoss: 1.015612\n",
      "Train Epoche: 2 [2025/2802 (72%)]\tLoss: 5.491841\n",
      "Train Epoche: 2 [2026/2802 (72%)]\tLoss: 2.787516\n",
      "Train Epoche: 2 [2027/2802 (72%)]\tLoss: 43.142532\n",
      "Train Epoche: 2 [2028/2802 (72%)]\tLoss: 3.779599\n",
      "Train Epoche: 2 [2029/2802 (72%)]\tLoss: 14.481409\n",
      "Train Epoche: 2 [2030/2802 (72%)]\tLoss: 38.962536\n",
      "Train Epoche: 2 [2031/2802 (72%)]\tLoss: 7.325860\n",
      "Train Epoche: 2 [2032/2802 (73%)]\tLoss: 15.397071\n",
      "Train Epoche: 2 [2033/2802 (73%)]\tLoss: 0.060281\n",
      "Train Epoche: 2 [2034/2802 (73%)]\tLoss: 15.937865\n",
      "Train Epoche: 2 [2035/2802 (73%)]\tLoss: 4.269168\n",
      "Train Epoche: 2 [2036/2802 (73%)]\tLoss: 8.216914\n",
      "Train Epoche: 2 [2037/2802 (73%)]\tLoss: 3.139340\n",
      "Train Epoche: 2 [2038/2802 (73%)]\tLoss: 0.954216\n",
      "Train Epoche: 2 [2039/2802 (73%)]\tLoss: 0.620139\n",
      "Train Epoche: 2 [2040/2802 (73%)]\tLoss: 2.736458\n",
      "Train Epoche: 2 [2041/2802 (73%)]\tLoss: 18.402668\n",
      "Train Epoche: 2 [2042/2802 (73%)]\tLoss: 216.172745\n",
      "Train Epoche: 2 [2043/2802 (73%)]\tLoss: 10.023667\n",
      "Train Epoche: 2 [2044/2802 (73%)]\tLoss: 2.141139\n",
      "Train Epoche: 2 [2045/2802 (73%)]\tLoss: 5.887591\n",
      "Train Epoche: 2 [2046/2802 (73%)]\tLoss: 3.892867\n",
      "Train Epoche: 2 [2047/2802 (73%)]\tLoss: 25.208046\n",
      "Train Epoche: 2 [2048/2802 (73%)]\tLoss: 0.711711\n",
      "Train Epoche: 2 [2049/2802 (73%)]\tLoss: 3.262610\n",
      "Train Epoche: 2 [2050/2802 (73%)]\tLoss: 19.572121\n",
      "Train Epoche: 2 [2051/2802 (73%)]\tLoss: 18.796003\n",
      "Train Epoche: 2 [2052/2802 (73%)]\tLoss: 5.182794\n",
      "Train Epoche: 2 [2053/2802 (73%)]\tLoss: 6.582080\n",
      "Train Epoche: 2 [2054/2802 (73%)]\tLoss: 11.976398\n",
      "Train Epoche: 2 [2055/2802 (73%)]\tLoss: 29.254139\n",
      "Train Epoche: 2 [2056/2802 (73%)]\tLoss: 13.912370\n",
      "Train Epoche: 2 [2057/2802 (73%)]\tLoss: 0.027502\n",
      "Train Epoche: 2 [2058/2802 (73%)]\tLoss: 0.272710\n",
      "Train Epoche: 2 [2059/2802 (73%)]\tLoss: 1.265174\n",
      "Train Epoche: 2 [2060/2802 (74%)]\tLoss: 1.580687\n",
      "Train Epoche: 2 [2061/2802 (74%)]\tLoss: 32.715839\n",
      "Train Epoche: 2 [2062/2802 (74%)]\tLoss: 192.369751\n",
      "Train Epoche: 2 [2063/2802 (74%)]\tLoss: 0.116421\n",
      "Train Epoche: 2 [2064/2802 (74%)]\tLoss: 5.133117\n",
      "Train Epoche: 2 [2065/2802 (74%)]\tLoss: 3.948645\n",
      "Train Epoche: 2 [2066/2802 (74%)]\tLoss: 0.436441\n",
      "Train Epoche: 2 [2067/2802 (74%)]\tLoss: 2.963770\n",
      "Train Epoche: 2 [2068/2802 (74%)]\tLoss: 9.167374\n",
      "Train Epoche: 2 [2069/2802 (74%)]\tLoss: 12.813873\n",
      "Train Epoche: 2 [2070/2802 (74%)]\tLoss: 0.403776\n",
      "Train Epoche: 2 [2071/2802 (74%)]\tLoss: 20.817860\n",
      "Train Epoche: 2 [2072/2802 (74%)]\tLoss: 0.012284\n",
      "Train Epoche: 2 [2073/2802 (74%)]\tLoss: 2.244813\n",
      "Train Epoche: 2 [2074/2802 (74%)]\tLoss: 49.203728\n",
      "Train Epoche: 2 [2075/2802 (74%)]\tLoss: 0.036564\n",
      "Train Epoche: 2 [2076/2802 (74%)]\tLoss: 15.428626\n",
      "Train Epoche: 2 [2077/2802 (74%)]\tLoss: 7.943601\n",
      "Train Epoche: 2 [2078/2802 (74%)]\tLoss: 2.819864\n",
      "Train Epoche: 2 [2079/2802 (74%)]\tLoss: 0.010099\n",
      "Train Epoche: 2 [2080/2802 (74%)]\tLoss: 2.602886\n",
      "Train Epoche: 2 [2081/2802 (74%)]\tLoss: 0.389608\n",
      "Train Epoche: 2 [2082/2802 (74%)]\tLoss: 2.006134\n",
      "Train Epoche: 2 [2083/2802 (74%)]\tLoss: 12.411508\n",
      "Train Epoche: 2 [2084/2802 (74%)]\tLoss: 0.343653\n",
      "Train Epoche: 2 [2085/2802 (74%)]\tLoss: 15.299041\n",
      "Train Epoche: 2 [2086/2802 (74%)]\tLoss: 0.276838\n",
      "Train Epoche: 2 [2087/2802 (74%)]\tLoss: 11.059068\n",
      "Train Epoche: 2 [2088/2802 (75%)]\tLoss: 8.144381\n",
      "Train Epoche: 2 [2089/2802 (75%)]\tLoss: 0.041370\n",
      "Train Epoche: 2 [2090/2802 (75%)]\tLoss: 6.379660\n",
      "Train Epoche: 2 [2091/2802 (75%)]\tLoss: 21.543394\n",
      "Train Epoche: 2 [2092/2802 (75%)]\tLoss: 5.481145\n",
      "Train Epoche: 2 [2093/2802 (75%)]\tLoss: 2.627829\n",
      "Train Epoche: 2 [2094/2802 (75%)]\tLoss: 1.375670\n",
      "Train Epoche: 2 [2095/2802 (75%)]\tLoss: 0.658346\n",
      "Train Epoche: 2 [2096/2802 (75%)]\tLoss: 2.962851\n",
      "Train Epoche: 2 [2097/2802 (75%)]\tLoss: 0.741290\n",
      "Train Epoche: 2 [2098/2802 (75%)]\tLoss: 1.517104\n",
      "Train Epoche: 2 [2099/2802 (75%)]\tLoss: 1.441911\n",
      "Train Epoche: 2 [2100/2802 (75%)]\tLoss: 0.076099\n",
      "Train Epoche: 2 [2101/2802 (75%)]\tLoss: 0.330480\n",
      "Train Epoche: 2 [2102/2802 (75%)]\tLoss: 0.196606\n",
      "Train Epoche: 2 [2103/2802 (75%)]\tLoss: 0.134940\n",
      "Train Epoche: 2 [2104/2802 (75%)]\tLoss: 54.386547\n",
      "Train Epoche: 2 [2105/2802 (75%)]\tLoss: 12.460670\n",
      "Train Epoche: 2 [2106/2802 (75%)]\tLoss: 0.108981\n",
      "Train Epoche: 2 [2107/2802 (75%)]\tLoss: 18.486336\n",
      "Train Epoche: 2 [2108/2802 (75%)]\tLoss: 226.531952\n",
      "Train Epoche: 2 [2109/2802 (75%)]\tLoss: 36.137024\n",
      "Train Epoche: 2 [2110/2802 (75%)]\tLoss: 4.497720\n",
      "Train Epoche: 2 [2111/2802 (75%)]\tLoss: 4.170994\n",
      "Train Epoche: 2 [2112/2802 (75%)]\tLoss: 5.454269\n",
      "Train Epoche: 2 [2113/2802 (75%)]\tLoss: 1.618276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [2114/2802 (75%)]\tLoss: 0.788265\n",
      "Train Epoche: 2 [2115/2802 (75%)]\tLoss: 32.722427\n",
      "Train Epoche: 2 [2116/2802 (76%)]\tLoss: 0.228266\n",
      "Train Epoche: 2 [2117/2802 (76%)]\tLoss: 0.215094\n",
      "Train Epoche: 2 [2118/2802 (76%)]\tLoss: 4.882395\n",
      "Train Epoche: 2 [2119/2802 (76%)]\tLoss: 9.099620\n",
      "Train Epoche: 2 [2120/2802 (76%)]\tLoss: 4.260439\n",
      "Train Epoche: 2 [2121/2802 (76%)]\tLoss: 10.028595\n",
      "Train Epoche: 2 [2122/2802 (76%)]\tLoss: 3.542869\n",
      "Train Epoche: 2 [2123/2802 (76%)]\tLoss: 21.110653\n",
      "Train Epoche: 2 [2124/2802 (76%)]\tLoss: 85.271744\n",
      "Train Epoche: 2 [2125/2802 (76%)]\tLoss: 10.382907\n",
      "Train Epoche: 2 [2126/2802 (76%)]\tLoss: 1.523756\n",
      "Train Epoche: 2 [2127/2802 (76%)]\tLoss: 13.358636\n",
      "Train Epoche: 2 [2128/2802 (76%)]\tLoss: 0.577031\n",
      "Train Epoche: 2 [2129/2802 (76%)]\tLoss: 9.938354\n",
      "Train Epoche: 2 [2130/2802 (76%)]\tLoss: 0.062966\n",
      "Train Epoche: 2 [2131/2802 (76%)]\tLoss: 2.530275\n",
      "Train Epoche: 2 [2132/2802 (76%)]\tLoss: 0.023587\n",
      "Train Epoche: 2 [2133/2802 (76%)]\tLoss: 12.323891\n",
      "Train Epoche: 2 [2134/2802 (76%)]\tLoss: 305.092194\n",
      "Train Epoche: 2 [2135/2802 (76%)]\tLoss: 0.979221\n",
      "Train Epoche: 2 [2136/2802 (76%)]\tLoss: 3.651684\n",
      "Train Epoche: 2 [2137/2802 (76%)]\tLoss: 0.000986\n",
      "Train Epoche: 2 [2138/2802 (76%)]\tLoss: 0.040736\n",
      "Train Epoche: 2 [2139/2802 (76%)]\tLoss: 1.832345\n",
      "Train Epoche: 2 [2140/2802 (76%)]\tLoss: 2.897251\n",
      "Train Epoche: 2 [2141/2802 (76%)]\tLoss: 1.083876\n",
      "Train Epoche: 2 [2142/2802 (76%)]\tLoss: 0.011084\n",
      "Train Epoche: 2 [2143/2802 (76%)]\tLoss: 5.802070\n",
      "Train Epoche: 2 [2144/2802 (77%)]\tLoss: 0.455981\n",
      "Train Epoche: 2 [2145/2802 (77%)]\tLoss: 17.932920\n",
      "Train Epoche: 2 [2146/2802 (77%)]\tLoss: 0.476951\n",
      "Train Epoche: 2 [2147/2802 (77%)]\tLoss: 0.784347\n",
      "Train Epoche: 2 [2148/2802 (77%)]\tLoss: 0.810678\n",
      "Train Epoche: 2 [2149/2802 (77%)]\tLoss: 146.565506\n",
      "Train Epoche: 2 [2150/2802 (77%)]\tLoss: 16.070452\n",
      "Train Epoche: 2 [2151/2802 (77%)]\tLoss: 0.421227\n",
      "Train Epoche: 2 [2152/2802 (77%)]\tLoss: 10.996097\n",
      "Train Epoche: 2 [2153/2802 (77%)]\tLoss: 0.024867\n",
      "Train Epoche: 2 [2154/2802 (77%)]\tLoss: 4.639266\n",
      "Train Epoche: 2 [2155/2802 (77%)]\tLoss: 64.516998\n",
      "Train Epoche: 2 [2156/2802 (77%)]\tLoss: 0.277195\n",
      "Train Epoche: 2 [2157/2802 (77%)]\tLoss: 22.799911\n",
      "Train Epoche: 2 [2158/2802 (77%)]\tLoss: 2.224902\n",
      "Train Epoche: 2 [2159/2802 (77%)]\tLoss: 2.771453\n",
      "Train Epoche: 2 [2160/2802 (77%)]\tLoss: 11.765570\n",
      "Train Epoche: 2 [2161/2802 (77%)]\tLoss: 0.052136\n",
      "Train Epoche: 2 [2162/2802 (77%)]\tLoss: 4.854877\n",
      "Train Epoche: 2 [2163/2802 (77%)]\tLoss: 9.676149\n",
      "Train Epoche: 2 [2164/2802 (77%)]\tLoss: 0.054801\n",
      "Train Epoche: 2 [2165/2802 (77%)]\tLoss: 1.257916\n",
      "Train Epoche: 2 [2166/2802 (77%)]\tLoss: 5.169238\n",
      "Train Epoche: 2 [2167/2802 (77%)]\tLoss: 2.499469\n",
      "Train Epoche: 2 [2168/2802 (77%)]\tLoss: 6.952929\n",
      "Train Epoche: 2 [2169/2802 (77%)]\tLoss: 10.232953\n",
      "Train Epoche: 2 [2170/2802 (77%)]\tLoss: 6.351743\n",
      "Train Epoche: 2 [2171/2802 (77%)]\tLoss: 2.497162\n",
      "Train Epoche: 2 [2172/2802 (78%)]\tLoss: 3.159107\n",
      "Train Epoche: 2 [2173/2802 (78%)]\tLoss: 0.148751\n",
      "Train Epoche: 2 [2174/2802 (78%)]\tLoss: 47.047546\n",
      "Train Epoche: 2 [2175/2802 (78%)]\tLoss: 0.012155\n",
      "Train Epoche: 2 [2176/2802 (78%)]\tLoss: 4.349091\n",
      "Train Epoche: 2 [2177/2802 (78%)]\tLoss: 0.012747\n",
      "Train Epoche: 2 [2178/2802 (78%)]\tLoss: 1.267995\n",
      "Train Epoche: 2 [2179/2802 (78%)]\tLoss: 5.741887\n",
      "Train Epoche: 2 [2180/2802 (78%)]\tLoss: 1.060786\n",
      "Train Epoche: 2 [2181/2802 (78%)]\tLoss: 3.576064\n",
      "Train Epoche: 2 [2182/2802 (78%)]\tLoss: 0.211549\n",
      "Train Epoche: 2 [2183/2802 (78%)]\tLoss: 0.129778\n",
      "Train Epoche: 2 [2184/2802 (78%)]\tLoss: 12.132045\n",
      "Train Epoche: 2 [2185/2802 (78%)]\tLoss: 13.038798\n",
      "Train Epoche: 2 [2186/2802 (78%)]\tLoss: 139.175262\n",
      "Train Epoche: 2 [2187/2802 (78%)]\tLoss: 2.198866\n",
      "Train Epoche: 2 [2188/2802 (78%)]\tLoss: 0.972520\n",
      "Train Epoche: 2 [2189/2802 (78%)]\tLoss: 0.341844\n",
      "Train Epoche: 2 [2190/2802 (78%)]\tLoss: 0.079285\n",
      "Train Epoche: 2 [2191/2802 (78%)]\tLoss: 7.473855\n",
      "Train Epoche: 2 [2192/2802 (78%)]\tLoss: 6.317041\n",
      "Train Epoche: 2 [2193/2802 (78%)]\tLoss: 1.569670\n",
      "Train Epoche: 2 [2194/2802 (78%)]\tLoss: 0.058816\n",
      "Train Epoche: 2 [2195/2802 (78%)]\tLoss: 10.697800\n",
      "Train Epoche: 2 [2196/2802 (78%)]\tLoss: 4.889379\n",
      "Train Epoche: 2 [2197/2802 (78%)]\tLoss: 1.936202\n",
      "Train Epoche: 2 [2198/2802 (78%)]\tLoss: 1.865986\n",
      "Train Epoche: 2 [2199/2802 (78%)]\tLoss: 0.015514\n",
      "Train Epoche: 2 [2200/2802 (79%)]\tLoss: 45.874222\n",
      "Train Epoche: 2 [2201/2802 (79%)]\tLoss: 4.268939\n",
      "Train Epoche: 2 [2202/2802 (79%)]\tLoss: 9.533284\n",
      "Train Epoche: 2 [2203/2802 (79%)]\tLoss: 0.016454\n",
      "Train Epoche: 2 [2204/2802 (79%)]\tLoss: 13.588177\n",
      "Train Epoche: 2 [2205/2802 (79%)]\tLoss: 2.970442\n",
      "Train Epoche: 2 [2206/2802 (79%)]\tLoss: 16.062683\n",
      "Train Epoche: 2 [2207/2802 (79%)]\tLoss: 1.844195\n",
      "Train Epoche: 2 [2208/2802 (79%)]\tLoss: 2.664121\n",
      "Train Epoche: 2 [2209/2802 (79%)]\tLoss: 7.535449\n",
      "Train Epoche: 2 [2210/2802 (79%)]\tLoss: 0.009824\n",
      "Train Epoche: 2 [2211/2802 (79%)]\tLoss: 0.493047\n",
      "Train Epoche: 2 [2212/2802 (79%)]\tLoss: 3.366053\n",
      "Train Epoche: 2 [2213/2802 (79%)]\tLoss: 65.034973\n",
      "Train Epoche: 2 [2214/2802 (79%)]\tLoss: 0.004583\n",
      "Train Epoche: 2 [2215/2802 (79%)]\tLoss: 1.192343\n",
      "Train Epoche: 2 [2216/2802 (79%)]\tLoss: 0.031701\n",
      "Train Epoche: 2 [2217/2802 (79%)]\tLoss: 11.906711\n",
      "Train Epoche: 2 [2218/2802 (79%)]\tLoss: 0.349413\n",
      "Train Epoche: 2 [2219/2802 (79%)]\tLoss: 33.401421\n",
      "Train Epoche: 2 [2220/2802 (79%)]\tLoss: 6.200564\n",
      "Train Epoche: 2 [2221/2802 (79%)]\tLoss: 2.647169\n",
      "Train Epoche: 2 [2222/2802 (79%)]\tLoss: 2.076371\n",
      "Train Epoche: 2 [2223/2802 (79%)]\tLoss: 0.961695\n",
      "Train Epoche: 2 [2224/2802 (79%)]\tLoss: 7.064280\n",
      "Train Epoche: 2 [2225/2802 (79%)]\tLoss: 4.031108\n",
      "Train Epoche: 2 [2226/2802 (79%)]\tLoss: 1.714602\n",
      "Train Epoche: 2 [2227/2802 (79%)]\tLoss: 0.071719\n",
      "Train Epoche: 2 [2228/2802 (80%)]\tLoss: 1.600402\n",
      "Train Epoche: 2 [2229/2802 (80%)]\tLoss: 1.424407\n",
      "Train Epoche: 2 [2230/2802 (80%)]\tLoss: 5.542578\n",
      "Train Epoche: 2 [2231/2802 (80%)]\tLoss: 0.130609\n",
      "Train Epoche: 2 [2232/2802 (80%)]\tLoss: 1.162020\n",
      "Train Epoche: 2 [2233/2802 (80%)]\tLoss: 0.320314\n",
      "Train Epoche: 2 [2234/2802 (80%)]\tLoss: 235.212769\n",
      "Train Epoche: 2 [2235/2802 (80%)]\tLoss: 2.993417\n",
      "Train Epoche: 2 [2236/2802 (80%)]\tLoss: 0.027231\n",
      "Train Epoche: 2 [2237/2802 (80%)]\tLoss: 4.342140\n",
      "Train Epoche: 2 [2238/2802 (80%)]\tLoss: 18.881046\n",
      "Train Epoche: 2 [2239/2802 (80%)]\tLoss: 0.250421\n",
      "Train Epoche: 2 [2240/2802 (80%)]\tLoss: 0.624700\n",
      "Train Epoche: 2 [2241/2802 (80%)]\tLoss: 0.169149\n",
      "Train Epoche: 2 [2242/2802 (80%)]\tLoss: 2.755994\n",
      "Train Epoche: 2 [2243/2802 (80%)]\tLoss: 381.031006\n",
      "Train Epoche: 2 [2244/2802 (80%)]\tLoss: 0.001315\n",
      "Train Epoche: 2 [2245/2802 (80%)]\tLoss: 0.332497\n",
      "Train Epoche: 2 [2246/2802 (80%)]\tLoss: 3.622670\n",
      "Train Epoche: 2 [2247/2802 (80%)]\tLoss: 1.846967\n",
      "Train Epoche: 2 [2248/2802 (80%)]\tLoss: 4.528945\n",
      "Train Epoche: 2 [2249/2802 (80%)]\tLoss: 2.310285\n",
      "Train Epoche: 2 [2250/2802 (80%)]\tLoss: 3.110068\n",
      "Train Epoche: 2 [2251/2802 (80%)]\tLoss: 1.756931\n",
      "Train Epoche: 2 [2252/2802 (80%)]\tLoss: 1.233656\n",
      "Train Epoche: 2 [2253/2802 (80%)]\tLoss: 0.357937\n",
      "Train Epoche: 2 [2254/2802 (80%)]\tLoss: 5.011124\n",
      "Train Epoche: 2 [2255/2802 (80%)]\tLoss: 0.149036\n",
      "Train Epoche: 2 [2256/2802 (81%)]\tLoss: 4.351171\n",
      "Train Epoche: 2 [2257/2802 (81%)]\tLoss: 0.621742\n",
      "Train Epoche: 2 [2258/2802 (81%)]\tLoss: 16.138773\n",
      "Train Epoche: 2 [2259/2802 (81%)]\tLoss: 3.969243\n",
      "Train Epoche: 2 [2260/2802 (81%)]\tLoss: 0.007597\n",
      "Train Epoche: 2 [2261/2802 (81%)]\tLoss: 34.733627\n",
      "Train Epoche: 2 [2262/2802 (81%)]\tLoss: 2.531037\n",
      "Train Epoche: 2 [2263/2802 (81%)]\tLoss: 1.098077\n",
      "Train Epoche: 2 [2264/2802 (81%)]\tLoss: 0.160180\n",
      "Train Epoche: 2 [2265/2802 (81%)]\tLoss: 0.100786\n",
      "Train Epoche: 2 [2266/2802 (81%)]\tLoss: 4.236617\n",
      "Train Epoche: 2 [2267/2802 (81%)]\tLoss: 2.921270\n",
      "Train Epoche: 2 [2268/2802 (81%)]\tLoss: 8.012403\n",
      "Train Epoche: 2 [2269/2802 (81%)]\tLoss: 2.194932\n",
      "Train Epoche: 2 [2270/2802 (81%)]\tLoss: 3.948065\n",
      "Train Epoche: 2 [2271/2802 (81%)]\tLoss: 0.004209\n",
      "Train Epoche: 2 [2272/2802 (81%)]\tLoss: 4.905706\n",
      "Train Epoche: 2 [2273/2802 (81%)]\tLoss: 0.515562\n",
      "Train Epoche: 2 [2274/2802 (81%)]\tLoss: 0.206317\n",
      "Train Epoche: 2 [2275/2802 (81%)]\tLoss: 0.876030\n",
      "Train Epoche: 2 [2276/2802 (81%)]\tLoss: 5.694704\n",
      "Train Epoche: 2 [2277/2802 (81%)]\tLoss: 2.573266\n",
      "Train Epoche: 2 [2278/2802 (81%)]\tLoss: 2.915522\n",
      "Train Epoche: 2 [2279/2802 (81%)]\tLoss: 0.925082\n",
      "Train Epoche: 2 [2280/2802 (81%)]\tLoss: 2.365023\n",
      "Train Epoche: 2 [2281/2802 (81%)]\tLoss: 15.564588\n",
      "Train Epoche: 2 [2282/2802 (81%)]\tLoss: 0.914433\n",
      "Train Epoche: 2 [2283/2802 (81%)]\tLoss: 34.710297\n",
      "Train Epoche: 2 [2284/2802 (82%)]\tLoss: 0.595786\n",
      "Train Epoche: 2 [2285/2802 (82%)]\tLoss: 1.103308\n",
      "Train Epoche: 2 [2286/2802 (82%)]\tLoss: 19.178362\n",
      "Train Epoche: 2 [2287/2802 (82%)]\tLoss: 105.332039\n",
      "Train Epoche: 2 [2288/2802 (82%)]\tLoss: 0.825074\n",
      "Train Epoche: 2 [2289/2802 (82%)]\tLoss: 0.144470\n",
      "Train Epoche: 2 [2290/2802 (82%)]\tLoss: 274.393616\n",
      "Train Epoche: 2 [2291/2802 (82%)]\tLoss: 3.242566\n",
      "Train Epoche: 2 [2292/2802 (82%)]\tLoss: 0.050848\n",
      "Train Epoche: 2 [2293/2802 (82%)]\tLoss: 1.441966\n",
      "Train Epoche: 2 [2294/2802 (82%)]\tLoss: 3.071639\n",
      "Train Epoche: 2 [2295/2802 (82%)]\tLoss: 6.745039\n",
      "Train Epoche: 2 [2296/2802 (82%)]\tLoss: 4.851784\n",
      "Train Epoche: 2 [2297/2802 (82%)]\tLoss: 0.070046\n",
      "Train Epoche: 2 [2298/2802 (82%)]\tLoss: 51.051098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [2299/2802 (82%)]\tLoss: 27.930397\n",
      "Train Epoche: 2 [2300/2802 (82%)]\tLoss: 2.288113\n",
      "Train Epoche: 2 [2301/2802 (82%)]\tLoss: 0.336865\n",
      "Train Epoche: 2 [2302/2802 (82%)]\tLoss: 1.757919\n",
      "Train Epoche: 2 [2303/2802 (82%)]\tLoss: 79.693611\n",
      "Train Epoche: 2 [2304/2802 (82%)]\tLoss: 1.903056\n",
      "Train Epoche: 2 [2305/2802 (82%)]\tLoss: 2.537663\n",
      "Train Epoche: 2 [2306/2802 (82%)]\tLoss: 63.343624\n",
      "Train Epoche: 2 [2307/2802 (82%)]\tLoss: 16.378717\n",
      "Train Epoche: 2 [2308/2802 (82%)]\tLoss: 0.090833\n",
      "Train Epoche: 2 [2309/2802 (82%)]\tLoss: 0.563168\n",
      "Train Epoche: 2 [2310/2802 (82%)]\tLoss: 22.257477\n",
      "Train Epoche: 2 [2311/2802 (82%)]\tLoss: 4.071969\n",
      "Train Epoche: 2 [2312/2802 (83%)]\tLoss: 52.020042\n",
      "Train Epoche: 2 [2313/2802 (83%)]\tLoss: 11.531566\n",
      "Train Epoche: 2 [2314/2802 (83%)]\tLoss: 0.281953\n",
      "Train Epoche: 2 [2315/2802 (83%)]\tLoss: 12.185794\n",
      "Train Epoche: 2 [2316/2802 (83%)]\tLoss: 104.231758\n",
      "Train Epoche: 2 [2317/2802 (83%)]\tLoss: 0.348900\n",
      "Train Epoche: 2 [2318/2802 (83%)]\tLoss: 0.103449\n",
      "Train Epoche: 2 [2319/2802 (83%)]\tLoss: 8.125242\n",
      "Train Epoche: 2 [2320/2802 (83%)]\tLoss: 1.717313\n",
      "Train Epoche: 2 [2321/2802 (83%)]\tLoss: 2.485599\n",
      "Train Epoche: 2 [2322/2802 (83%)]\tLoss: 0.024457\n",
      "Train Epoche: 2 [2323/2802 (83%)]\tLoss: 3.922479\n",
      "Train Epoche: 2 [2324/2802 (83%)]\tLoss: 26.144182\n",
      "Train Epoche: 2 [2325/2802 (83%)]\tLoss: 5.422284\n",
      "Train Epoche: 2 [2326/2802 (83%)]\tLoss: 0.813780\n",
      "Train Epoche: 2 [2327/2802 (83%)]\tLoss: 0.136819\n",
      "Train Epoche: 2 [2328/2802 (83%)]\tLoss: 10.750461\n",
      "Train Epoche: 2 [2329/2802 (83%)]\tLoss: 6.712264\n",
      "Train Epoche: 2 [2330/2802 (83%)]\tLoss: 4.452601\n",
      "Train Epoche: 2 [2331/2802 (83%)]\tLoss: 0.066084\n",
      "Train Epoche: 2 [2332/2802 (83%)]\tLoss: 22.965181\n",
      "Train Epoche: 2 [2333/2802 (83%)]\tLoss: 3.256470\n",
      "Train Epoche: 2 [2334/2802 (83%)]\tLoss: 1.250996\n",
      "Train Epoche: 2 [2335/2802 (83%)]\tLoss: 32.953552\n",
      "Train Epoche: 2 [2336/2802 (83%)]\tLoss: 3.641048\n",
      "Train Epoche: 2 [2337/2802 (83%)]\tLoss: 0.009976\n",
      "Train Epoche: 2 [2338/2802 (83%)]\tLoss: 6.462204\n",
      "Train Epoche: 2 [2339/2802 (83%)]\tLoss: 3.020528\n",
      "Train Epoche: 2 [2340/2802 (84%)]\tLoss: 0.792839\n",
      "Train Epoche: 2 [2341/2802 (84%)]\tLoss: 15.694013\n",
      "Train Epoche: 2 [2342/2802 (84%)]\tLoss: 2.028449\n",
      "Train Epoche: 2 [2343/2802 (84%)]\tLoss: 3.675627\n",
      "Train Epoche: 2 [2344/2802 (84%)]\tLoss: 0.387566\n",
      "Train Epoche: 2 [2345/2802 (84%)]\tLoss: 26.264980\n",
      "Train Epoche: 2 [2346/2802 (84%)]\tLoss: 64.966278\n",
      "Train Epoche: 2 [2347/2802 (84%)]\tLoss: 0.260292\n",
      "Train Epoche: 2 [2348/2802 (84%)]\tLoss: 0.079896\n",
      "Train Epoche: 2 [2349/2802 (84%)]\tLoss: 23.327620\n",
      "Train Epoche: 2 [2350/2802 (84%)]\tLoss: 6.451072\n",
      "Train Epoche: 2 [2351/2802 (84%)]\tLoss: 0.687705\n",
      "Train Epoche: 2 [2352/2802 (84%)]\tLoss: 0.001516\n",
      "Train Epoche: 2 [2353/2802 (84%)]\tLoss: 0.058705\n",
      "Train Epoche: 2 [2354/2802 (84%)]\tLoss: 12.338238\n",
      "Train Epoche: 2 [2355/2802 (84%)]\tLoss: 6.683620\n",
      "Train Epoche: 2 [2356/2802 (84%)]\tLoss: 0.181862\n",
      "Train Epoche: 2 [2357/2802 (84%)]\tLoss: 0.041465\n",
      "Train Epoche: 2 [2358/2802 (84%)]\tLoss: 0.749360\n",
      "Train Epoche: 2 [2359/2802 (84%)]\tLoss: 63.271481\n",
      "Train Epoche: 2 [2360/2802 (84%)]\tLoss: 23.704035\n",
      "Train Epoche: 2 [2361/2802 (84%)]\tLoss: 18.665125\n",
      "Train Epoche: 2 [2362/2802 (84%)]\tLoss: 3.758081\n",
      "Train Epoche: 2 [2363/2802 (84%)]\tLoss: 51.689552\n",
      "Train Epoche: 2 [2364/2802 (84%)]\tLoss: 5.794152\n",
      "Train Epoche: 2 [2365/2802 (84%)]\tLoss: 1.834825\n",
      "Train Epoche: 2 [2366/2802 (84%)]\tLoss: 4.615341\n",
      "Train Epoche: 2 [2367/2802 (84%)]\tLoss: 0.056746\n",
      "Train Epoche: 2 [2368/2802 (85%)]\tLoss: 0.935849\n",
      "Train Epoche: 2 [2369/2802 (85%)]\tLoss: 5.790163\n",
      "Train Epoche: 2 [2370/2802 (85%)]\tLoss: 1.114342\n",
      "Train Epoche: 2 [2371/2802 (85%)]\tLoss: 16.418009\n",
      "Train Epoche: 2 [2372/2802 (85%)]\tLoss: 0.176687\n",
      "Train Epoche: 2 [2373/2802 (85%)]\tLoss: 2.394315\n",
      "Train Epoche: 2 [2374/2802 (85%)]\tLoss: 2.780580\n",
      "Train Epoche: 2 [2375/2802 (85%)]\tLoss: 7.184753\n",
      "Train Epoche: 2 [2376/2802 (85%)]\tLoss: 17.038851\n",
      "Train Epoche: 2 [2377/2802 (85%)]\tLoss: 2.143963\n",
      "Train Epoche: 2 [2378/2802 (85%)]\tLoss: 318.026306\n",
      "Train Epoche: 2 [2379/2802 (85%)]\tLoss: 0.625028\n",
      "Train Epoche: 2 [2380/2802 (85%)]\tLoss: 8.164974\n",
      "Train Epoche: 2 [2381/2802 (85%)]\tLoss: 0.061864\n",
      "Train Epoche: 2 [2382/2802 (85%)]\tLoss: 5.323099\n",
      "Train Epoche: 2 [2383/2802 (85%)]\tLoss: 13.968801\n",
      "Train Epoche: 2 [2384/2802 (85%)]\tLoss: 94.606354\n",
      "Train Epoche: 2 [2385/2802 (85%)]\tLoss: 1.688978\n",
      "Train Epoche: 2 [2386/2802 (85%)]\tLoss: 9.095225\n",
      "Train Epoche: 2 [2387/2802 (85%)]\tLoss: 108.590233\n",
      "Train Epoche: 2 [2388/2802 (85%)]\tLoss: 4.722469\n",
      "Train Epoche: 2 [2389/2802 (85%)]\tLoss: 2.983597\n",
      "Train Epoche: 2 [2390/2802 (85%)]\tLoss: 6.310408\n",
      "Train Epoche: 2 [2391/2802 (85%)]\tLoss: 1.708262\n",
      "Train Epoche: 2 [2392/2802 (85%)]\tLoss: 0.154829\n",
      "Train Epoche: 2 [2393/2802 (85%)]\tLoss: 6.661843\n",
      "Train Epoche: 2 [2394/2802 (85%)]\tLoss: 73.079285\n",
      "Train Epoche: 2 [2395/2802 (85%)]\tLoss: 72.457458\n",
      "Train Epoche: 2 [2396/2802 (86%)]\tLoss: 10.426478\n",
      "Train Epoche: 2 [2397/2802 (86%)]\tLoss: 1.468965\n",
      "Train Epoche: 2 [2398/2802 (86%)]\tLoss: 0.116528\n",
      "Train Epoche: 2 [2399/2802 (86%)]\tLoss: 5.381929\n",
      "Train Epoche: 2 [2400/2802 (86%)]\tLoss: 9.423434\n",
      "Train Epoche: 2 [2401/2802 (86%)]\tLoss: 167.558594\n",
      "Train Epoche: 2 [2402/2802 (86%)]\tLoss: 0.461438\n",
      "Train Epoche: 2 [2403/2802 (86%)]\tLoss: 2.842792\n",
      "Train Epoche: 2 [2404/2802 (86%)]\tLoss: 5.502310\n",
      "Train Epoche: 2 [2405/2802 (86%)]\tLoss: 1.807109\n",
      "Train Epoche: 2 [2406/2802 (86%)]\tLoss: 0.398079\n",
      "Train Epoche: 2 [2407/2802 (86%)]\tLoss: 0.937501\n",
      "Train Epoche: 2 [2408/2802 (86%)]\tLoss: 102.560783\n",
      "Train Epoche: 2 [2409/2802 (86%)]\tLoss: 2.626974\n",
      "Train Epoche: 2 [2410/2802 (86%)]\tLoss: 3.633644\n",
      "Train Epoche: 2 [2411/2802 (86%)]\tLoss: 0.455548\n",
      "Train Epoche: 2 [2412/2802 (86%)]\tLoss: 2.149216\n",
      "Train Epoche: 2 [2413/2802 (86%)]\tLoss: 0.287173\n",
      "Train Epoche: 2 [2414/2802 (86%)]\tLoss: 12.036031\n",
      "Train Epoche: 2 [2415/2802 (86%)]\tLoss: 1.925894\n",
      "Train Epoche: 2 [2416/2802 (86%)]\tLoss: 0.438000\n",
      "Train Epoche: 2 [2417/2802 (86%)]\tLoss: 17.439575\n",
      "Train Epoche: 2 [2418/2802 (86%)]\tLoss: 0.571171\n",
      "Train Epoche: 2 [2419/2802 (86%)]\tLoss: 23.467749\n",
      "Train Epoche: 2 [2420/2802 (86%)]\tLoss: 16.343067\n",
      "Train Epoche: 2 [2421/2802 (86%)]\tLoss: 3.319364\n",
      "Train Epoche: 2 [2422/2802 (86%)]\tLoss: 5.771897\n",
      "Train Epoche: 2 [2423/2802 (86%)]\tLoss: 0.711344\n",
      "Train Epoche: 2 [2424/2802 (87%)]\tLoss: 1.568430\n",
      "Train Epoche: 2 [2425/2802 (87%)]\tLoss: 2.724513\n",
      "Train Epoche: 2 [2426/2802 (87%)]\tLoss: 0.489872\n",
      "Train Epoche: 2 [2427/2802 (87%)]\tLoss: 0.173344\n",
      "Train Epoche: 2 [2428/2802 (87%)]\tLoss: 13.271600\n",
      "Train Epoche: 2 [2429/2802 (87%)]\tLoss: 12.612556\n",
      "Train Epoche: 2 [2430/2802 (87%)]\tLoss: 0.342185\n",
      "Train Epoche: 2 [2431/2802 (87%)]\tLoss: 0.029053\n",
      "Train Epoche: 2 [2432/2802 (87%)]\tLoss: 15.086163\n",
      "Train Epoche: 2 [2433/2802 (87%)]\tLoss: 3.814146\n",
      "Train Epoche: 2 [2434/2802 (87%)]\tLoss: 0.454173\n",
      "Train Epoche: 2 [2435/2802 (87%)]\tLoss: 2.322009\n",
      "Train Epoche: 2 [2436/2802 (87%)]\tLoss: 0.038992\n",
      "Train Epoche: 2 [2437/2802 (87%)]\tLoss: 14.380613\n",
      "Train Epoche: 2 [2438/2802 (87%)]\tLoss: 0.004032\n",
      "Train Epoche: 2 [2439/2802 (87%)]\tLoss: 3.842486\n",
      "Train Epoche: 2 [2440/2802 (87%)]\tLoss: 1.154593\n",
      "Train Epoche: 2 [2441/2802 (87%)]\tLoss: 6.520300\n",
      "Train Epoche: 2 [2442/2802 (87%)]\tLoss: 0.517939\n",
      "Train Epoche: 2 [2443/2802 (87%)]\tLoss: 0.340648\n",
      "Train Epoche: 2 [2444/2802 (87%)]\tLoss: 0.030971\n",
      "Train Epoche: 2 [2445/2802 (87%)]\tLoss: 0.848950\n",
      "Train Epoche: 2 [2446/2802 (87%)]\tLoss: 0.487983\n",
      "Train Epoche: 2 [2447/2802 (87%)]\tLoss: 2.159244\n",
      "Train Epoche: 2 [2448/2802 (87%)]\tLoss: 21.107552\n",
      "Train Epoche: 2 [2449/2802 (87%)]\tLoss: 6.805455\n",
      "Train Epoche: 2 [2450/2802 (87%)]\tLoss: 0.684040\n",
      "Train Epoche: 2 [2451/2802 (87%)]\tLoss: 1.492518\n",
      "Train Epoche: 2 [2452/2802 (88%)]\tLoss: 0.286353\n",
      "Train Epoche: 2 [2453/2802 (88%)]\tLoss: 5.492395\n",
      "Train Epoche: 2 [2454/2802 (88%)]\tLoss: 35.524715\n",
      "Train Epoche: 2 [2455/2802 (88%)]\tLoss: 101.739944\n",
      "Train Epoche: 2 [2456/2802 (88%)]\tLoss: 8.149677\n",
      "Train Epoche: 2 [2457/2802 (88%)]\tLoss: 1.451101\n",
      "Train Epoche: 2 [2458/2802 (88%)]\tLoss: 0.296853\n",
      "Train Epoche: 2 [2459/2802 (88%)]\tLoss: 0.198088\n",
      "Train Epoche: 2 [2460/2802 (88%)]\tLoss: 60.655529\n",
      "Train Epoche: 2 [2461/2802 (88%)]\tLoss: 8.384187\n",
      "Train Epoche: 2 [2462/2802 (88%)]\tLoss: 2.009258\n",
      "Train Epoche: 2 [2463/2802 (88%)]\tLoss: 1.668630\n",
      "Train Epoche: 2 [2464/2802 (88%)]\tLoss: 5.756955\n",
      "Train Epoche: 2 [2465/2802 (88%)]\tLoss: 0.092046\n",
      "Train Epoche: 2 [2466/2802 (88%)]\tLoss: 6.339525\n",
      "Train Epoche: 2 [2467/2802 (88%)]\tLoss: 2.277339\n",
      "Train Epoche: 2 [2468/2802 (88%)]\tLoss: 2.152240\n",
      "Train Epoche: 2 [2469/2802 (88%)]\tLoss: 17.905388\n",
      "Train Epoche: 2 [2470/2802 (88%)]\tLoss: 1.642841\n",
      "Train Epoche: 2 [2471/2802 (88%)]\tLoss: 0.115711\n",
      "Train Epoche: 2 [2472/2802 (88%)]\tLoss: 12.285517\n",
      "Train Epoche: 2 [2473/2802 (88%)]\tLoss: 5.675664\n",
      "Train Epoche: 2 [2474/2802 (88%)]\tLoss: 5.917371\n",
      "Train Epoche: 2 [2475/2802 (88%)]\tLoss: 0.210290\n",
      "Train Epoche: 2 [2476/2802 (88%)]\tLoss: 17.354271\n",
      "Train Epoche: 2 [2477/2802 (88%)]\tLoss: 13.549229\n",
      "Train Epoche: 2 [2478/2802 (88%)]\tLoss: 0.078676\n",
      "Train Epoche: 2 [2479/2802 (88%)]\tLoss: 0.460599\n",
      "Train Epoche: 2 [2480/2802 (89%)]\tLoss: 9.966297\n",
      "Train Epoche: 2 [2481/2802 (89%)]\tLoss: 10.724885\n",
      "Train Epoche: 2 [2482/2802 (89%)]\tLoss: 7.670293\n",
      "Train Epoche: 2 [2483/2802 (89%)]\tLoss: 10.949657\n",
      "Train Epoche: 2 [2484/2802 (89%)]\tLoss: 5.729003\n",
      "Train Epoche: 2 [2485/2802 (89%)]\tLoss: 0.382412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [2486/2802 (89%)]\tLoss: 3.586550\n",
      "Train Epoche: 2 [2487/2802 (89%)]\tLoss: 17.331293\n",
      "Train Epoche: 2 [2488/2802 (89%)]\tLoss: 0.633643\n",
      "Train Epoche: 2 [2489/2802 (89%)]\tLoss: 60.128105\n",
      "Train Epoche: 2 [2490/2802 (89%)]\tLoss: 0.336734\n",
      "Train Epoche: 2 [2491/2802 (89%)]\tLoss: 0.111686\n",
      "Train Epoche: 2 [2492/2802 (89%)]\tLoss: 0.061944\n",
      "Train Epoche: 2 [2493/2802 (89%)]\tLoss: 1.434199\n",
      "Train Epoche: 2 [2494/2802 (89%)]\tLoss: 3.331480\n",
      "Train Epoche: 2 [2495/2802 (89%)]\tLoss: 2.986253\n",
      "Train Epoche: 2 [2496/2802 (89%)]\tLoss: 1.444693\n",
      "Train Epoche: 2 [2497/2802 (89%)]\tLoss: 38.008755\n",
      "Train Epoche: 2 [2498/2802 (89%)]\tLoss: 0.103388\n",
      "Train Epoche: 2 [2499/2802 (89%)]\tLoss: 0.055882\n",
      "Train Epoche: 2 [2500/2802 (89%)]\tLoss: 11.385926\n",
      "Train Epoche: 2 [2501/2802 (89%)]\tLoss: 69.824631\n",
      "Train Epoche: 2 [2502/2802 (89%)]\tLoss: 31.574656\n",
      "Train Epoche: 2 [2503/2802 (89%)]\tLoss: 9.309652\n",
      "Train Epoche: 2 [2504/2802 (89%)]\tLoss: 1.907073\n",
      "Train Epoche: 2 [2505/2802 (89%)]\tLoss: 1.257831\n",
      "Train Epoche: 2 [2506/2802 (89%)]\tLoss: 0.587340\n",
      "Train Epoche: 2 [2507/2802 (89%)]\tLoss: 0.246559\n",
      "Train Epoche: 2 [2508/2802 (90%)]\tLoss: 9.188499\n",
      "Train Epoche: 2 [2509/2802 (90%)]\tLoss: 1.000569\n",
      "Train Epoche: 2 [2510/2802 (90%)]\tLoss: 2.710094\n",
      "Train Epoche: 2 [2511/2802 (90%)]\tLoss: 14.693397\n",
      "Train Epoche: 2 [2512/2802 (90%)]\tLoss: 12.022046\n",
      "Train Epoche: 2 [2513/2802 (90%)]\tLoss: 67.148453\n",
      "Train Epoche: 2 [2514/2802 (90%)]\tLoss: 4.054229\n",
      "Train Epoche: 2 [2515/2802 (90%)]\tLoss: 20.133799\n",
      "Train Epoche: 2 [2516/2802 (90%)]\tLoss: 1.230523\n",
      "Train Epoche: 2 [2517/2802 (90%)]\tLoss: 3.157355\n",
      "Train Epoche: 2 [2518/2802 (90%)]\tLoss: 4.739479\n",
      "Train Epoche: 2 [2519/2802 (90%)]\tLoss: 0.235299\n",
      "Train Epoche: 2 [2520/2802 (90%)]\tLoss: 0.028128\n",
      "Train Epoche: 2 [2521/2802 (90%)]\tLoss: 0.264395\n",
      "Train Epoche: 2 [2522/2802 (90%)]\tLoss: 0.673669\n",
      "Train Epoche: 2 [2523/2802 (90%)]\tLoss: 3.879650\n",
      "Train Epoche: 2 [2524/2802 (90%)]\tLoss: 0.693778\n",
      "Train Epoche: 2 [2525/2802 (90%)]\tLoss: 0.999537\n",
      "Train Epoche: 2 [2526/2802 (90%)]\tLoss: 3.361442\n",
      "Train Epoche: 2 [2527/2802 (90%)]\tLoss: 26.078062\n",
      "Train Epoche: 2 [2528/2802 (90%)]\tLoss: 0.077121\n",
      "Train Epoche: 2 [2529/2802 (90%)]\tLoss: 20.837322\n",
      "Train Epoche: 2 [2530/2802 (90%)]\tLoss: 0.258850\n",
      "Train Epoche: 2 [2531/2802 (90%)]\tLoss: 5.334155\n",
      "Train Epoche: 2 [2532/2802 (90%)]\tLoss: 2.004580\n",
      "Train Epoche: 2 [2533/2802 (90%)]\tLoss: 5.694424\n",
      "Train Epoche: 2 [2534/2802 (90%)]\tLoss: 0.016318\n",
      "Train Epoche: 2 [2535/2802 (90%)]\tLoss: 3.304406\n",
      "Train Epoche: 2 [2536/2802 (91%)]\tLoss: 0.016036\n",
      "Train Epoche: 2 [2537/2802 (91%)]\tLoss: 51.038357\n",
      "Train Epoche: 2 [2538/2802 (91%)]\tLoss: 23.792496\n",
      "Train Epoche: 2 [2539/2802 (91%)]\tLoss: 1.639457\n",
      "Train Epoche: 2 [2540/2802 (91%)]\tLoss: 0.222438\n",
      "Train Epoche: 2 [2541/2802 (91%)]\tLoss: 1.751500\n",
      "Train Epoche: 2 [2542/2802 (91%)]\tLoss: 5.260518\n",
      "Train Epoche: 2 [2543/2802 (91%)]\tLoss: 24.615467\n",
      "Train Epoche: 2 [2544/2802 (91%)]\tLoss: 41.216179\n",
      "Train Epoche: 2 [2545/2802 (91%)]\tLoss: 3.546728\n",
      "Train Epoche: 2 [2546/2802 (91%)]\tLoss: 2.609182\n",
      "Train Epoche: 2 [2547/2802 (91%)]\tLoss: 107.236702\n",
      "Train Epoche: 2 [2548/2802 (91%)]\tLoss: 3.265604\n",
      "Train Epoche: 2 [2549/2802 (91%)]\tLoss: 1.660706\n",
      "Train Epoche: 2 [2550/2802 (91%)]\tLoss: 3.760347\n",
      "Train Epoche: 2 [2551/2802 (91%)]\tLoss: 48.600861\n",
      "Train Epoche: 2 [2552/2802 (91%)]\tLoss: 8.577068\n",
      "Train Epoche: 2 [2553/2802 (91%)]\tLoss: 12.543115\n",
      "Train Epoche: 2 [2554/2802 (91%)]\tLoss: 0.002248\n",
      "Train Epoche: 2 [2555/2802 (91%)]\tLoss: 3.541098\n",
      "Train Epoche: 2 [2556/2802 (91%)]\tLoss: 1.134441\n",
      "Train Epoche: 2 [2557/2802 (91%)]\tLoss: 10.245859\n",
      "Train Epoche: 2 [2558/2802 (91%)]\tLoss: 1.632416\n",
      "Train Epoche: 2 [2559/2802 (91%)]\tLoss: 61.797272\n",
      "Train Epoche: 2 [2560/2802 (91%)]\tLoss: 2.574989\n",
      "Train Epoche: 2 [2561/2802 (91%)]\tLoss: 47.984547\n",
      "Train Epoche: 2 [2562/2802 (91%)]\tLoss: 8.798992\n",
      "Train Epoche: 2 [2563/2802 (91%)]\tLoss: 0.261201\n",
      "Train Epoche: 2 [2564/2802 (92%)]\tLoss: 1.284626\n",
      "Train Epoche: 2 [2565/2802 (92%)]\tLoss: 9.036613\n",
      "Train Epoche: 2 [2566/2802 (92%)]\tLoss: 2.106019\n",
      "Train Epoche: 2 [2567/2802 (92%)]\tLoss: 0.246333\n",
      "Train Epoche: 2 [2568/2802 (92%)]\tLoss: 6.253114\n",
      "Train Epoche: 2 [2569/2802 (92%)]\tLoss: 3.301336\n",
      "Train Epoche: 2 [2570/2802 (92%)]\tLoss: 2.035254\n",
      "Train Epoche: 2 [2571/2802 (92%)]\tLoss: 0.979037\n",
      "Train Epoche: 2 [2572/2802 (92%)]\tLoss: 2.591457\n",
      "Train Epoche: 2 [2573/2802 (92%)]\tLoss: 3.685543\n",
      "Train Epoche: 2 [2574/2802 (92%)]\tLoss: 2.049936\n",
      "Train Epoche: 2 [2575/2802 (92%)]\tLoss: 0.000205\n",
      "Train Epoche: 2 [2576/2802 (92%)]\tLoss: 0.003904\n",
      "Train Epoche: 2 [2577/2802 (92%)]\tLoss: 220.194901\n",
      "Train Epoche: 2 [2578/2802 (92%)]\tLoss: 32.798740\n",
      "Train Epoche: 2 [2579/2802 (92%)]\tLoss: 2.225499\n",
      "Train Epoche: 2 [2580/2802 (92%)]\tLoss: 0.521056\n",
      "Train Epoche: 2 [2581/2802 (92%)]\tLoss: 63.641663\n",
      "Train Epoche: 2 [2582/2802 (92%)]\tLoss: 3.188573\n",
      "Train Epoche: 2 [2583/2802 (92%)]\tLoss: 0.004152\n",
      "Train Epoche: 2 [2584/2802 (92%)]\tLoss: 1.630297\n",
      "Train Epoche: 2 [2585/2802 (92%)]\tLoss: 4.301853\n",
      "Train Epoche: 2 [2586/2802 (92%)]\tLoss: 78.536949\n",
      "Train Epoche: 2 [2587/2802 (92%)]\tLoss: 16.334270\n",
      "Train Epoche: 2 [2588/2802 (92%)]\tLoss: 18.438753\n",
      "Train Epoche: 2 [2589/2802 (92%)]\tLoss: 6.628237\n",
      "Train Epoche: 2 [2590/2802 (92%)]\tLoss: 0.113335\n",
      "Train Epoche: 2 [2591/2802 (92%)]\tLoss: 2.657407\n",
      "Train Epoche: 2 [2592/2802 (93%)]\tLoss: 1.023467\n",
      "Train Epoche: 2 [2593/2802 (93%)]\tLoss: 3.548255\n",
      "Train Epoche: 2 [2594/2802 (93%)]\tLoss: 1.816236\n",
      "Train Epoche: 2 [2595/2802 (93%)]\tLoss: 11.312187\n",
      "Train Epoche: 2 [2596/2802 (93%)]\tLoss: 28.350727\n",
      "Train Epoche: 2 [2597/2802 (93%)]\tLoss: 4.686106\n",
      "Train Epoche: 2 [2598/2802 (93%)]\tLoss: 16.197168\n",
      "Train Epoche: 2 [2599/2802 (93%)]\tLoss: 69.056694\n",
      "Train Epoche: 2 [2600/2802 (93%)]\tLoss: 0.290369\n",
      "Train Epoche: 2 [2601/2802 (93%)]\tLoss: 5.879123\n",
      "Train Epoche: 2 [2602/2802 (93%)]\tLoss: 15.241531\n",
      "Train Epoche: 2 [2603/2802 (93%)]\tLoss: 1.877420\n",
      "Train Epoche: 2 [2604/2802 (93%)]\tLoss: 33.170902\n",
      "Train Epoche: 2 [2605/2802 (93%)]\tLoss: 193.343048\n",
      "Train Epoche: 2 [2606/2802 (93%)]\tLoss: 25.957882\n",
      "Train Epoche: 2 [2607/2802 (93%)]\tLoss: 11.994756\n",
      "Train Epoche: 2 [2608/2802 (93%)]\tLoss: 4.695878\n",
      "Train Epoche: 2 [2609/2802 (93%)]\tLoss: 2.133920\n",
      "Train Epoche: 2 [2610/2802 (93%)]\tLoss: 0.619143\n",
      "Train Epoche: 2 [2611/2802 (93%)]\tLoss: 64.722275\n",
      "Train Epoche: 2 [2612/2802 (93%)]\tLoss: 0.161015\n",
      "Train Epoche: 2 [2613/2802 (93%)]\tLoss: 0.264900\n",
      "Train Epoche: 2 [2614/2802 (93%)]\tLoss: 9.779780\n",
      "Train Epoche: 2 [2615/2802 (93%)]\tLoss: 0.000056\n",
      "Train Epoche: 2 [2616/2802 (93%)]\tLoss: 4.100589\n",
      "Train Epoche: 2 [2617/2802 (93%)]\tLoss: 0.176932\n",
      "Train Epoche: 2 [2618/2802 (93%)]\tLoss: 50.435562\n",
      "Train Epoche: 2 [2619/2802 (93%)]\tLoss: 17.759892\n",
      "Train Epoche: 2 [2620/2802 (94%)]\tLoss: 36.872456\n",
      "Train Epoche: 2 [2621/2802 (94%)]\tLoss: 1.159828\n",
      "Train Epoche: 2 [2622/2802 (94%)]\tLoss: 4.491101\n",
      "Train Epoche: 2 [2623/2802 (94%)]\tLoss: 0.304175\n",
      "Train Epoche: 2 [2624/2802 (94%)]\tLoss: 5.840073\n",
      "Train Epoche: 2 [2625/2802 (94%)]\tLoss: 0.003208\n",
      "Train Epoche: 2 [2626/2802 (94%)]\tLoss: 5.696713\n",
      "Train Epoche: 2 [2627/2802 (94%)]\tLoss: 2.394802\n",
      "Train Epoche: 2 [2628/2802 (94%)]\tLoss: 3.971091\n",
      "Train Epoche: 2 [2629/2802 (94%)]\tLoss: 5.848283\n",
      "Train Epoche: 2 [2630/2802 (94%)]\tLoss: 24.300127\n",
      "Train Epoche: 2 [2631/2802 (94%)]\tLoss: 1.242463\n",
      "Train Epoche: 2 [2632/2802 (94%)]\tLoss: 2.361984\n",
      "Train Epoche: 2 [2633/2802 (94%)]\tLoss: 2.274840\n",
      "Train Epoche: 2 [2634/2802 (94%)]\tLoss: 0.057247\n",
      "Train Epoche: 2 [2635/2802 (94%)]\tLoss: 0.049503\n",
      "Train Epoche: 2 [2636/2802 (94%)]\tLoss: 1.479852\n",
      "Train Epoche: 2 [2637/2802 (94%)]\tLoss: 1.649031\n",
      "Train Epoche: 2 [2638/2802 (94%)]\tLoss: 1.590495\n",
      "Train Epoche: 2 [2639/2802 (94%)]\tLoss: 0.008553\n",
      "Train Epoche: 2 [2640/2802 (94%)]\tLoss: 13.385576\n",
      "Train Epoche: 2 [2641/2802 (94%)]\tLoss: 2.573737\n",
      "Train Epoche: 2 [2642/2802 (94%)]\tLoss: 25.895288\n",
      "Train Epoche: 2 [2643/2802 (94%)]\tLoss: 6.408627\n",
      "Train Epoche: 2 [2644/2802 (94%)]\tLoss: 12.204288\n",
      "Train Epoche: 2 [2645/2802 (94%)]\tLoss: 28.459591\n",
      "Train Epoche: 2 [2646/2802 (94%)]\tLoss: 0.001332\n",
      "Train Epoche: 2 [2647/2802 (94%)]\tLoss: 26.715117\n",
      "Train Epoche: 2 [2648/2802 (95%)]\tLoss: 1.454701\n",
      "Train Epoche: 2 [2649/2802 (95%)]\tLoss: 0.708535\n",
      "Train Epoche: 2 [2650/2802 (95%)]\tLoss: 20.094090\n",
      "Train Epoche: 2 [2651/2802 (95%)]\tLoss: 5.874707\n",
      "Train Epoche: 2 [2652/2802 (95%)]\tLoss: 0.820156\n",
      "Train Epoche: 2 [2653/2802 (95%)]\tLoss: 3.487287\n",
      "Train Epoche: 2 [2654/2802 (95%)]\tLoss: 6.790351\n",
      "Train Epoche: 2 [2655/2802 (95%)]\tLoss: 0.426868\n",
      "Train Epoche: 2 [2656/2802 (95%)]\tLoss: 0.383424\n",
      "Train Epoche: 2 [2657/2802 (95%)]\tLoss: 1.420326\n",
      "Train Epoche: 2 [2658/2802 (95%)]\tLoss: 0.027203\n",
      "Train Epoche: 2 [2659/2802 (95%)]\tLoss: 0.010912\n",
      "Train Epoche: 2 [2660/2802 (95%)]\tLoss: 0.334537\n",
      "Train Epoche: 2 [2661/2802 (95%)]\tLoss: 3.746294\n",
      "Train Epoche: 2 [2662/2802 (95%)]\tLoss: 31.962090\n",
      "Train Epoche: 2 [2663/2802 (95%)]\tLoss: 2.520948\n",
      "Train Epoche: 2 [2664/2802 (95%)]\tLoss: 4.952424\n",
      "Train Epoche: 2 [2665/2802 (95%)]\tLoss: 12.381248\n",
      "Train Epoche: 2 [2666/2802 (95%)]\tLoss: 1.330653\n",
      "Train Epoche: 2 [2667/2802 (95%)]\tLoss: 6.883352\n",
      "Train Epoche: 2 [2668/2802 (95%)]\tLoss: 203.498199\n",
      "Train Epoche: 2 [2669/2802 (95%)]\tLoss: 0.274480\n",
      "Train Epoche: 2 [2670/2802 (95%)]\tLoss: 0.723094\n",
      "Train Epoche: 2 [2671/2802 (95%)]\tLoss: 1.404477\n",
      "Train Epoche: 2 [2672/2802 (95%)]\tLoss: 129.261398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [2673/2802 (95%)]\tLoss: 6.037659\n",
      "Train Epoche: 2 [2674/2802 (95%)]\tLoss: 0.838779\n",
      "Train Epoche: 2 [2675/2802 (95%)]\tLoss: 216.365555\n",
      "Train Epoche: 2 [2676/2802 (96%)]\tLoss: 0.036928\n",
      "Train Epoche: 2 [2677/2802 (96%)]\tLoss: 1.125639\n",
      "Train Epoche: 2 [2678/2802 (96%)]\tLoss: 0.012300\n",
      "Train Epoche: 2 [2679/2802 (96%)]\tLoss: 3.984619\n",
      "Train Epoche: 2 [2680/2802 (96%)]\tLoss: 31.498606\n",
      "Train Epoche: 2 [2681/2802 (96%)]\tLoss: 0.404469\n",
      "Train Epoche: 2 [2682/2802 (96%)]\tLoss: 0.380479\n",
      "Train Epoche: 2 [2683/2802 (96%)]\tLoss: 9.199055\n",
      "Train Epoche: 2 [2684/2802 (96%)]\tLoss: 24.703737\n",
      "Train Epoche: 2 [2685/2802 (96%)]\tLoss: 0.245919\n",
      "Train Epoche: 2 [2686/2802 (96%)]\tLoss: 0.406824\n",
      "Train Epoche: 2 [2687/2802 (96%)]\tLoss: 0.055953\n",
      "Train Epoche: 2 [2688/2802 (96%)]\tLoss: 0.006074\n",
      "Train Epoche: 2 [2689/2802 (96%)]\tLoss: 10.415802\n",
      "Train Epoche: 2 [2690/2802 (96%)]\tLoss: 30.621319\n",
      "Train Epoche: 2 [2691/2802 (96%)]\tLoss: 5.939026\n",
      "Train Epoche: 2 [2692/2802 (96%)]\tLoss: 97.209549\n",
      "Train Epoche: 2 [2693/2802 (96%)]\tLoss: 18.981058\n",
      "Train Epoche: 2 [2694/2802 (96%)]\tLoss: 6.503823\n",
      "Train Epoche: 2 [2695/2802 (96%)]\tLoss: 43.873390\n",
      "Train Epoche: 2 [2696/2802 (96%)]\tLoss: 10.530368\n",
      "Train Epoche: 2 [2697/2802 (96%)]\tLoss: 22.648960\n",
      "Train Epoche: 2 [2698/2802 (96%)]\tLoss: 4.512934\n",
      "Train Epoche: 2 [2699/2802 (96%)]\tLoss: 12.589426\n",
      "Train Epoche: 2 [2700/2802 (96%)]\tLoss: 0.706196\n",
      "Train Epoche: 2 [2701/2802 (96%)]\tLoss: 16.163593\n",
      "Train Epoche: 2 [2702/2802 (96%)]\tLoss: 0.036731\n",
      "Train Epoche: 2 [2703/2802 (96%)]\tLoss: 2.928727\n",
      "Train Epoche: 2 [2704/2802 (97%)]\tLoss: 1.865941\n",
      "Train Epoche: 2 [2705/2802 (97%)]\tLoss: 0.968185\n",
      "Train Epoche: 2 [2706/2802 (97%)]\tLoss: 0.335318\n",
      "Train Epoche: 2 [2707/2802 (97%)]\tLoss: 33.983322\n",
      "Train Epoche: 2 [2708/2802 (97%)]\tLoss: 1.844469\n",
      "Train Epoche: 2 [2709/2802 (97%)]\tLoss: 0.536982\n",
      "Train Epoche: 2 [2710/2802 (97%)]\tLoss: 123.464302\n",
      "Train Epoche: 2 [2711/2802 (97%)]\tLoss: 0.550602\n",
      "Train Epoche: 2 [2712/2802 (97%)]\tLoss: 8.666430\n",
      "Train Epoche: 2 [2713/2802 (97%)]\tLoss: 0.217268\n",
      "Train Epoche: 2 [2714/2802 (97%)]\tLoss: 5.823048\n",
      "Train Epoche: 2 [2715/2802 (97%)]\tLoss: 1.001122\n",
      "Train Epoche: 2 [2716/2802 (97%)]\tLoss: 13.849147\n",
      "Train Epoche: 2 [2717/2802 (97%)]\tLoss: 38.144703\n",
      "Train Epoche: 2 [2718/2802 (97%)]\tLoss: 0.851155\n",
      "Train Epoche: 2 [2719/2802 (97%)]\tLoss: 4.758570\n",
      "Train Epoche: 2 [2720/2802 (97%)]\tLoss: 20.516630\n",
      "Train Epoche: 2 [2721/2802 (97%)]\tLoss: 5.884914\n",
      "Train Epoche: 2 [2722/2802 (97%)]\tLoss: 6.744123\n",
      "Train Epoche: 2 [2723/2802 (97%)]\tLoss: 4.479047\n",
      "Train Epoche: 2 [2724/2802 (97%)]\tLoss: 2.347470\n",
      "Train Epoche: 2 [2725/2802 (97%)]\tLoss: 4.294324\n",
      "Train Epoche: 2 [2726/2802 (97%)]\tLoss: 1.120086\n",
      "Train Epoche: 2 [2727/2802 (97%)]\tLoss: 0.054861\n",
      "Train Epoche: 2 [2728/2802 (97%)]\tLoss: 0.106701\n",
      "Train Epoche: 2 [2729/2802 (97%)]\tLoss: 3.500742\n",
      "Train Epoche: 2 [2730/2802 (97%)]\tLoss: 51.044979\n",
      "Train Epoche: 2 [2731/2802 (97%)]\tLoss: 0.005249\n",
      "Train Epoche: 2 [2732/2802 (98%)]\tLoss: 0.923452\n",
      "Train Epoche: 2 [2733/2802 (98%)]\tLoss: 0.012918\n",
      "Train Epoche: 2 [2734/2802 (98%)]\tLoss: 0.538880\n",
      "Train Epoche: 2 [2735/2802 (98%)]\tLoss: 0.023124\n",
      "Train Epoche: 2 [2736/2802 (98%)]\tLoss: 1.975070\n",
      "Train Epoche: 2 [2737/2802 (98%)]\tLoss: 6.637713\n",
      "Train Epoche: 2 [2738/2802 (98%)]\tLoss: 1.472906\n",
      "Train Epoche: 2 [2739/2802 (98%)]\tLoss: 3.217142\n",
      "Train Epoche: 2 [2740/2802 (98%)]\tLoss: 24.566713\n",
      "Train Epoche: 2 [2741/2802 (98%)]\tLoss: 0.120933\n",
      "Train Epoche: 2 [2742/2802 (98%)]\tLoss: 10.834010\n",
      "Train Epoche: 2 [2743/2802 (98%)]\tLoss: 19.068649\n",
      "Train Epoche: 2 [2744/2802 (98%)]\tLoss: 302.918182\n",
      "Train Epoche: 2 [2745/2802 (98%)]\tLoss: 0.183046\n",
      "Train Epoche: 2 [2746/2802 (98%)]\tLoss: 8.267013\n",
      "Train Epoche: 2 [2747/2802 (98%)]\tLoss: 1.253257\n",
      "Train Epoche: 2 [2748/2802 (98%)]\tLoss: 16.637650\n",
      "Train Epoche: 2 [2749/2802 (98%)]\tLoss: 9.249516\n",
      "Train Epoche: 2 [2750/2802 (98%)]\tLoss: 2.274535\n",
      "Train Epoche: 2 [2751/2802 (98%)]\tLoss: 4.628159\n",
      "Train Epoche: 2 [2752/2802 (98%)]\tLoss: 0.023821\n",
      "Train Epoche: 2 [2753/2802 (98%)]\tLoss: 3.638554\n",
      "Train Epoche: 2 [2754/2802 (98%)]\tLoss: 0.485546\n",
      "Train Epoche: 2 [2755/2802 (98%)]\tLoss: 1.955698\n",
      "Train Epoche: 2 [2756/2802 (98%)]\tLoss: 0.043872\n",
      "Train Epoche: 2 [2757/2802 (98%)]\tLoss: 7.739960\n",
      "Train Epoche: 2 [2758/2802 (98%)]\tLoss: 105.491150\n",
      "Train Epoche: 2 [2759/2802 (98%)]\tLoss: 6.689071\n",
      "Train Epoche: 2 [2760/2802 (99%)]\tLoss: 4.886726\n",
      "Train Epoche: 2 [2761/2802 (99%)]\tLoss: 0.420486\n",
      "Train Epoche: 2 [2762/2802 (99%)]\tLoss: 1.635508\n",
      "Train Epoche: 2 [2763/2802 (99%)]\tLoss: 1.131597\n",
      "Train Epoche: 2 [2764/2802 (99%)]\tLoss: 61.179142\n",
      "Train Epoche: 2 [2765/2802 (99%)]\tLoss: 12.060017\n",
      "Train Epoche: 2 [2766/2802 (99%)]\tLoss: 1.040669\n",
      "Train Epoche: 2 [2767/2802 (99%)]\tLoss: 0.023344\n",
      "Train Epoche: 2 [2768/2802 (99%)]\tLoss: 29.866966\n",
      "Train Epoche: 2 [2769/2802 (99%)]\tLoss: 0.027439\n",
      "Train Epoche: 2 [2770/2802 (99%)]\tLoss: 5.222737\n",
      "Train Epoche: 2 [2771/2802 (99%)]\tLoss: 89.174355\n",
      "Train Epoche: 2 [2772/2802 (99%)]\tLoss: 0.174527\n",
      "Train Epoche: 2 [2773/2802 (99%)]\tLoss: 0.169312\n",
      "Train Epoche: 2 [2774/2802 (99%)]\tLoss: 0.088106\n",
      "Train Epoche: 2 [2775/2802 (99%)]\tLoss: 0.717202\n",
      "Train Epoche: 2 [2776/2802 (99%)]\tLoss: 3.284379\n",
      "Train Epoche: 2 [2777/2802 (99%)]\tLoss: 10.743320\n",
      "Train Epoche: 2 [2778/2802 (99%)]\tLoss: 6.347062\n",
      "Train Epoche: 2 [2779/2802 (99%)]\tLoss: 0.837144\n",
      "Train Epoche: 2 [2780/2802 (99%)]\tLoss: 5.514924\n",
      "Train Epoche: 2 [2781/2802 (99%)]\tLoss: 5.335192\n",
      "Train Epoche: 2 [2782/2802 (99%)]\tLoss: 0.028594\n",
      "Train Epoche: 2 [2783/2802 (99%)]\tLoss: 3.539864\n",
      "Train Epoche: 2 [2784/2802 (99%)]\tLoss: 0.009694\n",
      "Train Epoche: 2 [2785/2802 (99%)]\tLoss: 0.621328\n",
      "Train Epoche: 2 [2786/2802 (99%)]\tLoss: 0.033136\n",
      "Train Epoche: 2 [2787/2802 (99%)]\tLoss: 15.616048\n",
      "Train Epoche: 2 [2788/2802 (100%)]\tLoss: 0.997761\n",
      "Train Epoche: 2 [2789/2802 (100%)]\tLoss: 16.892982\n",
      "Train Epoche: 2 [2790/2802 (100%)]\tLoss: 4.249871\n",
      "Train Epoche: 2 [2791/2802 (100%)]\tLoss: 0.187744\n",
      "Train Epoche: 2 [2792/2802 (100%)]\tLoss: 0.414287\n",
      "Train Epoche: 2 [2793/2802 (100%)]\tLoss: 1.390767\n",
      "Train Epoche: 2 [2794/2802 (100%)]\tLoss: 1.210685\n",
      "Train Epoche: 2 [2795/2802 (100%)]\tLoss: 1.039102\n",
      "Train Epoche: 2 [2796/2802 (100%)]\tLoss: 5.305309\n",
      "Train Epoche: 2 [2797/2802 (100%)]\tLoss: 75.353813\n",
      "Train Epoche: 2 [2798/2802 (100%)]\tLoss: 0.379204\n",
      "Train Epoche: 2 [2799/2802 (100%)]\tLoss: 6.464729\n",
      "Train Epoche: 2 [2800/2802 (100%)]\tLoss: 0.544338\n",
      "Train Epoche: 2 [2801/2802 (100%)]\tLoss: 3.099528\n"
     ]
    }
   ],
   "source": [
    "max_epochs = h.opt_combination['epochen']\n",
    "lr = h.opt_combination['lr']\n",
    "\n",
    "cuda = input('Cuda? [y/n]: ')\n",
    "model = Netz()\n",
    "if cuda.lower() == 'y':\n",
    "    model.cuda() \n",
    "\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)     \n",
    "\n",
    "def train_cuda(epoch):\n",
    "    model.train()\n",
    "    batch_id = 0\n",
    "    for data, target in train:\n",
    "        #data = data.cuda()\n",
    "        target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "        shape = target.size()[1]\n",
    "        target = target.resize(shape,1).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        #print(\"Out: \", out, out.size())\n",
    "        #print(\"Target: \", target, target.size())\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "            epoch, batch_id *len(data), len(train),\n",
    "        100. * batch_id / len(train), loss.item()))\n",
    "        batch_id +=1\n",
    "        \n",
    "    #random.shuffle(train_data)\n",
    "def train_(epoch):\n",
    "    model.train()\n",
    "    batch_id = 0\n",
    "    for data, target in train:\n",
    "        #data = data.cuda()\n",
    "        target = torch.Tensor(target).unsqueeze(0)#.cuda()\n",
    "        shape = target.size()[1]\n",
    "        target = target.resize(shape,1)#.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        #print(\"Out: \", out, out.size())\n",
    "        #print(\"Target: \", target, target.size())\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "            epoch, batch_id *len(data), len(train),\n",
    "        100. * batch_id / len(train), loss.item()))\n",
    "        batch_id +=1\n",
    "        \n",
    "    #random.shuffle(train_data)\n",
    "if cuda.lower() == 'y':\n",
    "    for epoch in range(1,max_epochs):\n",
    "        train_cuda(epoch) \n",
    "else:\n",
    "    for epoch in range(1,max_epochs):\n",
    "        train_(epoch)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_times_cuda(test_final):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    help_dict = {}\n",
    "    for key in test_final.keys():\n",
    "        #print(key)\n",
    "        help_dict = {}\n",
    "        for data, target in test_final[key]:\n",
    "            model.eval()\n",
    "            #files.listdir(path)\n",
    "            data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1).cuda()\n",
    "            out = model(data).cpu()\n",
    "            #print(out)\n",
    "            out = out.detach().numpy()\n",
    "            #out = np.round(out)\n",
    "            target = target.cpu()\n",
    "            target = target.detach().numpy()\n",
    "            #print(data)\n",
    "            #print(data[\"driverId\"])\n",
    "            total += abs(out - target[0][0])\n",
    "            #print(\"current_position: \", data[0][0].item())\n",
    "            #print(\"Output: \", out)\n",
    "            #print(\"Target: \", target)\n",
    "            help_dict[target[0][0]] = out\n",
    "            #print(\"Difference: \", out - target)\n",
    "            count+=1\n",
    "        #help_dict = sorted(help_dict.items(), key=operator.itemgetter(1))\n",
    "        result_dict[key] = help_dict\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "def test_times(test_final):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    help_dict = {}\n",
    "    for key in test_final.keys():\n",
    "        #print(key)\n",
    "        help_dict = {}\n",
    "        for data, target in test_final[key]:\n",
    "            model.eval()\n",
    "            #files.listdir(path)\n",
    "            #data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1)#.cuda()\n",
    "            out = model(data)#.cpu()\n",
    "            #print(out)\n",
    "            out = out.detach().numpy()\n",
    "            #out = np.round(out)\n",
    "            #target = target.cpu()\n",
    "            target = target.detach().numpy()\n",
    "            #print(data)\n",
    "            #print(data[\"driverId\"])\n",
    "            total += abs(out - target[0][0])\n",
    "            #print(\"current_position: \", data[0][0].item())\n",
    "            #print(\"Output: \", out)\n",
    "            #print(\"Target: \", target)\n",
    "            help_dict[target[0][0]] = out\n",
    "            #print(\"Difference: \", out - target)\n",
    "            count+=1\n",
    "        #help_dict = sorted(help_dict.items(), key=operator.itemgetter(1))\n",
    "        result_dict[key] = help_dict\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "if cuda.lower() =='y':\n",
    "    total_results = test_times_cuda(test)\n",
    "else:\n",
    "    total_results = test_times(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dfs = {}\n",
    "for raceId, dict in total_results.items():    \n",
    "    #Auswerten der Vorhersagen aus den Outputdaten des Modells\n",
    "    A = [x[0][0] for x in list((dict.values()))]\n",
    "    y = list(dict.keys())\n",
    "    t = pd.DataFrame(columns = ['target', 'prediction'])\n",
    "    t['target'] = y\n",
    "    t['prediction'] = A\n",
    "    #sortieren des DataFrames nach den vorhergesagten Positionen, aufsteigend\n",
    "    end = sqldf.sqldf('''select * from t order by prediction ASC''')\n",
    "    end.reset_index(inplace = True)\n",
    "    #Da DF nun nach prediction aufsteigend sortiert ist, kann veränderter Index als predictete Position gesetzt werden\n",
    "    end.rename(columns = {'index': 'predicted_position'},inplace = True)\n",
    "    #zur Übersichtlichkeit wird nun der endgültige DF nach den richtigen Positionen (target) sortiert (aufsteigend)\n",
    "    end = sqldf.sqldf('''select * from end order by target ASC''')\n",
    "    end['predicted_position'] = end['predicted_position']+1\n",
    "    #umstellen der Spaltenreihenfolge\n",
    "    end = end[['target', 'predicted_position', 'prediction']]\n",
    "    result_dfs[raceId] = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaceId: 852\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    1.794732\n",
      "1      2.0                   2    2.883790\n",
      "2      3.0                   7    7.683998\n",
      "3      4.0                   3    3.230210\n",
      "4      5.0                   9    9.997762\n",
      "5      6.0                   4    3.871595\n",
      "6      7.0                   6    7.010408\n",
      "7      8.0                   5    4.295087\n",
      "8      9.0                   8    8.238929\n",
      "9     10.0                  13   12.445101\n",
      "10    11.0                  11   11.902659\n",
      "11    12.0                  12   12.144609\n",
      "12    13.0                  14   12.523632\n",
      "13    14.0                  16   15.820935\n",
      "14    15.0                  15   15.770011\n",
      "15    16.0                  10   11.520939\n",
      "16    17.0                  17   17.298086\n",
      "17    18.0                  18   18.589184\n",
      "18    19.0                  19   18.904297\n",
      "19    24.0                  20   21.121977 \n",
      "\n",
      "RaceId: 913\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    2.283848\n",
      "1      2.0                   2    3.189545\n",
      "2      3.0                   4    3.822863\n",
      "3      4.0                   3    3.221581\n",
      "4      5.0                   5    5.889320\n",
      "5      6.0                   8    8.850107\n",
      "6      7.0                  13   12.575513\n",
      "7      8.0                   6    6.355873\n",
      "8      9.0                  10   10.368895\n",
      "9     10.0                   9    9.084997\n",
      "10    11.0                   7    7.197251\n",
      "11    12.0                  14   14.021576\n",
      "12    13.0                  12   12.099751\n",
      "13    14.0                  11   11.575739\n",
      "14    15.0                  16   16.204899\n",
      "15    16.0                  15   14.738927\n",
      "16    17.0                  17   17.338926\n",
      "17    22.0                  18   18.104399 \n",
      "\n",
      "RaceId: 937\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    1.914385\n",
      "1      2.0                   2    2.514713\n",
      "2      3.0                   3    5.131259\n",
      "3      4.0                   5    5.890182\n",
      "4      5.0                   7    7.970378\n",
      "5      6.0                   4    5.726040\n",
      "6      7.0                   8    8.374369\n",
      "7      8.0                   9    9.178917\n",
      "8      9.0                  12   11.774027\n",
      "9     10.0                  10   10.104074\n",
      "10    11.0                  11   11.248029\n",
      "11    12.0                   6    7.488482\n",
      "12    13.0                  13   13.675494\n",
      "13    14.0                  15   14.480755\n",
      "14    15.0                  14   14.169433\n",
      "15    16.0                  16   14.755373\n",
      "16    20.0                  17   21.404211 \n",
      "\n",
      "RaceId: 951\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    2.059582\n",
      "1      2.0                   3    3.461728\n",
      "2      3.0                   2    3.351088\n",
      "3      4.0                   4    3.804996\n",
      "4      5.0                   5    4.489640\n",
      "5      6.0                   8    9.811393\n",
      "6      7.0                  11   11.630501\n",
      "7      8.0                  12   12.007916\n",
      "8      9.0                   6    6.940333\n",
      "9     10.0                  15   14.087950\n",
      "10    11.0                   7    8.957441\n",
      "11    12.0                  13   12.405666\n",
      "12    13.0                  14   13.851814\n",
      "13    14.0                  10   11.354946\n",
      "14    15.0                  16   14.947116\n",
      "15    16.0                  17   15.569329\n",
      "16    17.0                  19   17.503277\n",
      "17    18.0                  18   15.918743\n",
      "18    22.0                   9   10.315487 \n",
      "\n",
      "RaceId: 977\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    2.001312\n",
      "1      2.0                   2    2.608047\n",
      "2      3.0                   4    3.694444\n",
      "3      4.0                   5    4.740567\n",
      "4      5.0                   3    3.082364\n",
      "5      6.0                   6    5.816886\n",
      "6      7.0                   8    8.319505\n",
      "7      8.0                   7    7.516834\n",
      "8      9.0                   9   10.717010\n",
      "9     10.0                  10   11.070556\n",
      "10    11.0                  11   12.054624\n",
      "11    12.0                  13   13.642472\n",
      "12    13.0                  14   13.653093\n",
      "13    14.0                  12   13.443542\n",
      "14    15.0                  15   15.293180\n",
      "15    16.0                  16   15.869542\n",
      "16    20.0                  17   17.754364 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in result_dfs.items():\n",
    "    print('RaceId:',key)\n",
    "    print(value,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nogo_columns_hannah_kacke = [#'grid',\n",
    "                #'race_completion',\n",
    "                'lap_position','circuitId','lap_number',\n",
    "                'podium_position', 'raceId',\n",
    "                'grandprix_name', 'driver_fullname',\n",
    "               'constructor_name', 'total_laps',\n",
    "               'status_clean', 'constructorId',\n",
    "                'total_milliseconds', 'driverId'\n",
    "               'lap_in_milliseconds','year', 'stop_binary','constructorId_1.0',\n",
    "                 'constructorId_3.0',\n",
    "                 'constructorId_4.0',\n",
    "                 'constructorId_5.0',\n",
    "                 'constructorId_6.0',\n",
    "                 'constructorId_9.0',\n",
    "                 'constructorId_10.0',\n",
    "                 'constructorId_15.0',\n",
    "                 'constructorId_131.0',\n",
    "                 'constructorId_164.0',\n",
    "                 'constructorId_166.0',\n",
    "                 'constructorId_205.0',\n",
    "                 'constructorId_206.0',\n",
    "                 'constructorId_207.0',\n",
    "                 'constructorId_208.0',\n",
    "                 'constructorId_209.0',\n",
    "                 'constructorId_210.0']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
