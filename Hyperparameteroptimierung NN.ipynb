{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import pandasql as sqldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel dieses Notebooks ist es die Hyperparameter Lernrate und Epochenanzahl für das gegebene Neuronale Netz zu optimieren. Hierfür wird eine Klasse verwendet, die in gegebenen Intervallen verschiedene Kombinationen dieser Parameter geordnet ausprobiert (kein random search) und das beste Resultat, basierend auf übergebenen Trainingsdaten zurück gibt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory = 'sliced_data'):\n",
    "    '''\n",
    "        Funktion, die die aufbereiteten/vorbereiteten Daten aus existierenden CSV Dateien \n",
    "        einliest und je nach Vollständigkeit in zwei Dictionaries abspeichert,\n",
    "        geschlüsselt nach der jeweiligen RaceId\n",
    "    '''\n",
    "    #Ids von Rennen, die als Regenrennen identifiziert wurden\n",
    "    rain_id = [847,861,879,910,914,934,942,953,957,967,950,982]\n",
    "    if os.path.exists(directory):\n",
    "        csv_filenames = []\n",
    "        #auslesen aller csv file dateinamen aus formula 1 datensatz und abspeichern in liste\n",
    "        for filename in os.listdir(os.getcwd()+'/'+directory):\n",
    "            typ = filename.split('.')[-1]\n",
    "            name = filename.split('.')[0]\n",
    "            if typ == 'csv':\n",
    "                csv_filenames.append(filename)\n",
    "        sliced_races = {}\n",
    "        #einlesen und abspeichern als dataframe aller dateien\n",
    "        for file in csv_filenames:\n",
    "            try:\n",
    "                df = pd.read_csv(directory+'/'+file, engine = 'python', sep = ';', decimal = '.')\n",
    "                del df['Unnamed: 0']\n",
    "            except Exception as e:\n",
    "                df = pd.read_csv(directory+'/'+file, engine = 'c', sep = ';', decimal = '.')\n",
    "                del df['Unnamed: 0']\n",
    "                print(e)\n",
    "            #print(df.head())\n",
    "            f = int(file.split('_')[-1].split('.')[0])\n",
    "            df[\"rain\"] = 0\n",
    "            #bias wird hinzugefügt\n",
    "            #df['bias'] = 1\n",
    "            #setzen der regenkomponente auf 1 für regenrennen\n",
    "            if list(df[\"raceId\"])[0] in rain_id:\n",
    "                df[\"rain\"] = 1\n",
    "            sliced_races[f] = df\n",
    "        print('Einlesen der sliced Dateien erfolgreich')\n",
    "    else:\n",
    "        raise ('sliced Dateien können nicht eingelesen werden, da kein entsprechendes Verzeichnis existiert!')\n",
    "        \n",
    "    if os.path.exists('split_data'):\n",
    "        csv_filenames = []\n",
    "        #auslesen aller csv file dateinamen aus formula 1 datensatz und abspeichern in liste\n",
    "        for filename in os.listdir(os.getcwd()+'/split_data'):\n",
    "            typ = filename.split('.')[-1]\n",
    "            name = filename.split('.')[0]\n",
    "            if typ == 'csv':\n",
    "                csv_filenames.append(filename)\n",
    "        split_by_race = {}\n",
    "        #einlesen und abspeichern als dataframe aller dateien\n",
    "        for file in csv_filenames:\n",
    "            try:\n",
    "                df = pd.read_csv('split_data/'+file, engine = 'python', sep = ';', decimal = '.')\n",
    "                del df['Unnamed: 0']\n",
    "            except Exception as e:\n",
    "                df = pd.read_csv('split_data/'+file, engine = 'c', sep = ';', decimal = '.')\n",
    "                del df['Unnamed: 0']\n",
    "                print(e)\n",
    "            f = int(file.split('_')[-1].split('.')[0])\n",
    "            #hinzufügen eines bias\n",
    "            #df['bias'] = 1\n",
    "            split_by_race[f] = df\n",
    "        print('Einlesen der split Dateien erfolgreich')\n",
    "    else:\n",
    "        raise('split Dateien können nicht eingelesen werden, da kein entsprechendes Verzeichnis existiert!')\n",
    "        \n",
    "    return sliced_races, split_by_race\n",
    "\n",
    "\n",
    "def train_dev_test(data_dict, train_p = 0.7, dev_p = 0.2, test_p = 0.1, nogo_columns = []):\n",
    "    \n",
    "    if round(train_p+dev_p+test_p,1) !=1.0:\n",
    "        raise ValueError ('No valid train/dev/test distribution')\n",
    "    \n",
    "    '''\n",
    "        Daten werde in einem Dictionary übergeben, Dataframes werden in dieser \n",
    "        Funktion geshuffled und dann in einen Traindatensatz und in einen \n",
    "        Testdatensatz aufgeteilt.\n",
    "    '''\n",
    "    #aufteilen in train, dev, test counter\n",
    "    train_count = round(len(data_dict.keys())*train_p, 0)\n",
    "    dev_count = train_count+round(len(data_dict.keys())*dev_p,0)\n",
    "    test_count = len(data_dict.keys())-(train_count+dev_count)\n",
    "    \n",
    "    #shufflen der übergebenen Daten\n",
    "    keys = list(data_dict.keys())\n",
    "    random.shuffle(keys)\n",
    "    data_shuffled = {}\n",
    "    for key in keys:\n",
    "        data_shuffled[key] = data_dict[key]\n",
    "        \n",
    "    #erzeugen separater train,dev,test dictionaries\n",
    "    train = {}\n",
    "    dev = {}\n",
    "    test = {}\n",
    "    c = 0\n",
    "    \n",
    "    #daten sollen nicht in tensoren umgewandelt werden\n",
    "    for id, df in data_shuffled.items():\n",
    "        #entfernen nicht gewollter spalten aus dataframe\n",
    "        cols = [col for col in df.columns if col not in nogo_columns]\n",
    "        df = df[cols]\n",
    "        if c < train_count:\n",
    "            train[id] = df\n",
    "        elif c >= train_count and c < dev_count:\n",
    "            dev[id] = df\n",
    "        else:\n",
    "            test[id] = df\n",
    "        c += 1\n",
    "                \n",
    "    return train, dev, test\n",
    "\n",
    "def to_tensor(train_data, dev_data, test_data, nogo_columns = []):\n",
    "    train = []\n",
    "    train_ = {}\n",
    "    dev = []\n",
    "    dev_ = {}\n",
    "    test = []\n",
    "    test_ = {}\n",
    "    \n",
    "    for id, race in train_data.items():\n",
    "        for did in race.driverId.unique():\n",
    "            temp = race.where(race.driverId == did).dropna(how = \"all\")\n",
    "            stop_sum = np.sum(temp['stop_binary'])#anzahl boxenstops\n",
    "            temp['sum_milliseconds_pro_lap'] = temp['sum_milliseconds_pro_lap']/60000\n",
    "            temp_y = list(temp[\"podium_position\"])#list((temp[\"total_milliseconds\"]/60000))\n",
    "            #temp_y = temp_y[0]\n",
    "            cols = [col for col in temp.columns if col not in nogo_columns]\n",
    "            temp_x = temp[cols]\n",
    "            #stops=temp_x.sum(axis = 0)[2] #Addierte anzahl an stops bis zu dem Zeitpunkt\n",
    "            temp_x = temp_x.tail(1)\n",
    "            temp_x['stop_binary'] = stop_sum\n",
    "            x_tensor = torch.tensor(temp_x[cols].values)#torch.tensor(temp_x.values)\n",
    "            #temp_x = x_tensor.float()\n",
    "            train.append((x_tensor, [temp_y[0]]))\n",
    "        train_[id] = train\n",
    "        train = []\n",
    "    for id, race in dev_data.items():\n",
    "        for did in race.driverId.unique():\n",
    "            temp = race.where(race.driverId == did).dropna(how = \"all\")\n",
    "            stop_sum = np.sum(temp['stop_binary'])#anzahl boxenstops\n",
    "            temp['sum_milliseconds_pro_lap'] = temp['sum_milliseconds_pro_lap']/60000\n",
    "            temp_y = list(temp[\"podium_position\"])#list((temp[\"total_milliseconds\"]/60000))\n",
    "            #temp_y = temp_y[0]\n",
    "            cols = [col for col in temp.columns if col not in nogo_columns]\n",
    "            temp_x = temp[cols]\n",
    "            #stops=temp_x.sum(axis = 0)[2] #Addierte anzahl an stops bis zu dem Zeitpunkt\n",
    "            temp_x = temp_x.tail(1)\n",
    "            temp_x['stop_binary'] = stop_sum\n",
    "            x_tensor = torch.tensor(temp_x[temp_x.columns].values)\n",
    "            #temp_x = x_tensor.float()\n",
    "            dev.append((x_tensor, [temp_y[0]]))\n",
    "        dev_[id] = dev\n",
    "        dev = []\n",
    "    for id, race in test_data.items():\n",
    "        for did in race.driverId.unique():\n",
    "            temp = race.where(race.driverId == did).dropna(how = \"all\")\n",
    "            stop_sum = np.sum(temp['stop_binary'])#anzahl boxenstops\n",
    "            temp['sum_milliseconds_pro_lap'] = temp['sum_milliseconds_pro_lap']/60000\n",
    "            temp_y = list(temp[\"podium_position\"])#list((temp[\"total_milliseconds\"]/60000))\n",
    "            #temp_y = temp_y[0]\n",
    "            cols = [col for col in temp.columns if col not in nogo_columns]\n",
    "            temp_x = temp[cols]\n",
    "            #stops=temp_x.sum(axis = 0)[2] #Addierte anzahl an stops bis zu dem Zeitpunkt\n",
    "            temp_x = temp_x.tail(1)\n",
    "            temp_x['stop_binary'] = stop_sum\n",
    "            x_tensor = torch.tensor(temp_x[temp_x.columns].values)\n",
    "            #temp_x = x_tensor.float()\n",
    "            test.append((x_tensor, [temp_y[0]]))\n",
    "        test_[id] = test\n",
    "        test = []\n",
    "        \n",
    "    return train_,dev_,test_\n",
    "        \n",
    "#torch.tensor(df.values)\n",
    "def train_test (data_dict, test_num = 5, nogo_columns = []):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    test_final = {}\n",
    "    temp_y_podium = []\n",
    "    test_races = list(data_dict.keys())\n",
    "    random.shuffle(test_races)\n",
    "    test_races = test_races[0:test_num]\n",
    "    for key, value in data_dict.items():\n",
    "        helper = key\n",
    "        for did in value.driverId.unique():\n",
    "            temp = value.where(value.driverId == did).dropna(how = \"all\")\n",
    "            if list(temp[\"podium_position\"])[0] < 0: #Top x finish positions\n",
    "                pp = 1\n",
    "            else:\n",
    "                if key in test_races:\n",
    "                    temp['sum_milliseconds_pro_lap'] = temp['sum_milliseconds_pro_lap']/60000\n",
    "                    temp_y = list(temp[\"podium_position\"])#list((temp[\"total_milliseconds\"]/60000))\n",
    "                    #temp_y = temp_y[0]\n",
    "                    cols = [col for col in temp.columns if col not in nogo_columns]\n",
    "                    temp_x = temp[cols]\n",
    "                    stops=temp_x.sum(axis = 0)[2] #Addierte anzahl an stops bis zu dem Zeitpunkt\n",
    "                    temp_x = temp_x.tail(1)\n",
    "                    temp_x['stop_binary'] = stops\n",
    "                    x_tensor = torch.tensor(temp_x[temp_x.columns].values)\n",
    "                    #temp_x = x_tensor.float()\n",
    "                    test_data.append((x_tensor, [temp_y[0]]))\n",
    "                else:\n",
    "                    temp['sum_milliseconds_pro_lap'] = temp['sum_milliseconds_pro_lap']/60000\n",
    "                    temp_y = list(temp[\"podium_position\"])#list((temp[\"total_milliseconds\"]/60000))\n",
    "                    #temp_y = temp_y[0]\n",
    "                    cols = [col for col in temp.columns if col not in nogo_columns]\n",
    "                    temp_x = temp[cols]\n",
    "                    stops=temp_x.sum(axis = 0)[2] #Addierte anzahl an stops bis zu dem Zeitpunkt\n",
    "                    temp_x = temp_x.tail(1)\n",
    "                    temp_x['stop_binary'] = stops\n",
    "                    x_tensor = torch.tensor(temp_x[temp_x.columns].values)\n",
    "                    #temp_x = x_tensor.float()\n",
    "                    train_data.append((x_tensor, [temp_y[0]]))\n",
    "        if key in test_races:\n",
    "            test_final[key]=test_data\n",
    "        test_data = []\n",
    "    random.shuffle(train_data)\n",
    "    #random.shuffle(test_data)\n",
    "    #test_data = train_data[len(train_data)-100:]\n",
    "    train_data = train_data#[0:len(train_data)-100]\n",
    "            \n",
    "            #break\n",
    "            #for i, row in temp.iterrows():\n",
    "            \n",
    "    return train_data, test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen der Dateien als Dataframes, welche in zwei Dictionaries gespeichert werden:<br>\n",
    "- Attribut directory verweißt auf den Ordnernamen der sliced Dateien, Default ist 'sliced_data'\n",
    "- für die split_by_race Daten sollte kein separates Directory angegeben werden müssen, da es keinen Unterschied zwischen den Rennen gibt! (Nur bei Slicing: Wie viele Prozent des Rennens möchte ich? ergibt ein anderes Directory Sinn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einlesen der sliced Dateien erfolgreich\n",
      "Einlesen der split Dateien erfolgreich\n"
     ]
    }
   ],
   "source": [
    "sliced_races, split_by_race = load_data(directory = 'sliced_data_50_neueForm')\n",
    "#temp_x = load_data(directory = 'sliced_data_50_neueForm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufteilen des Datensatzes in einen Trainings-, einen Development- und einen Testteil mit Hilfe der Funktion train_dev_test(). Standardmäßig wird der Datensatz zu 70% in Trainings-, zu 20% in Dev- und zu 10% in Testdaten aufgeteilt. Die Verteilung kann mit den Parametern train_p,dev_p,test_p die der Funktion train_dev_test() übergeben werden können, angepasst werden. Wichtig ist bloß, dass insgesamt train_p,dev_p und test_p 1 ergeben. Die Datenbeispiele aus sliced_races werden vor dem Aufteilen geshuffled. Die Funktion train_dev_test() gibt die Datenbeispiele dann wieder in Dictionary Form zurück, wobei jeder RaceId ein DataFrame mit Renndaten zugeordnet wird. Im nächsten Schritt werden die drei Datensammlungen der Funktion to_tensor() übergeben, die nicht gewünschte Spalten aus den Datensätzen entfernt (nogo_columns) und diese dann in Tensoren abspeichert, die wiederrum  ihrem Targetvalue (podiums_position) zugeordnet werden, um die Überprüfung der Vorhersagen später zu vereinfachen. Tupel mit Datensatz und Targetvalue werden dann wieder in dem jeweiligen Dictionary abgespeichert und ihrer RaceId zugeordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nogo_columns = ['year', 'podium_position', 'raceId','lap_number','total_laps','driverId',\n",
    "                'grandprix_name', 'driver_fullname',\n",
    "               'constructor_name', #'total_laps',\n",
    "               #'status_clean', 'constructorId',\n",
    "                'total_milliseconds',\n",
    "               'lap_in_milliseconds',\n",
    "               'driverId_1.0',\n",
    "                 'driverId_2.0',\n",
    "                 'driverId_3.0',\n",
    "                 'driverId_4.0',\n",
    "                 'driverId_5.0',\n",
    "                 'driverId_8.0',\n",
    "                 'driverId_10.0',\n",
    "                 'driverId_13.0',\n",
    "                 'driverId_15.0',\n",
    "                 'driverId_16.0',\n",
    "                 'driverId_17.0',\n",
    "                 'driverId_18.0',\n",
    "                 'driverId_20.0',\n",
    "                 'driverId_22.0',\n",
    "                 'driverId_24.0',\n",
    "                 'driverId_30.0',\n",
    "                 'driverId_37.0',\n",
    "                 'driverId_39.0',\n",
    "                 'driverId_67.0',\n",
    "                 'driverId_153.0',\n",
    "                 'driverId_154.0',\n",
    "                 'driverId_155.0',\n",
    "                 'driverId_807.0',\n",
    "                 'driverId_808.0',\n",
    "                 'driverId_811.0',\n",
    "                 'driverId_812.0',\n",
    "                 'driverId_813.0',\n",
    "                 'driverId_814.0',\n",
    "                 'driverId_815.0',\n",
    "                 'driverId_816.0',\n",
    "                 'driverId_817.0',\n",
    "                 'driverId_818.0',\n",
    "                 'driverId_819.0',\n",
    "                 'driverId_820.0',\n",
    "                 'driverId_821.0',\n",
    "                 'driverId_822.0',\n",
    "                 'driverId_823.0',\n",
    "                 'driverId_824.0',\n",
    "                 'driverId_825.0',\n",
    "                 'driverId_826.0',\n",
    "                 'driverId_827.0',\n",
    "                 'driverId_828.0',\n",
    "                 'driverId_829.0',\n",
    "                 'driverId_830.0',\n",
    "                 'driverId_831.0',\n",
    "                 'driverId_832.0',\n",
    "                 'driverId_833.0',\n",
    "                 'driverId_834.0',\n",
    "                 'driverId_835.0',\n",
    "                 'driverId_836.0',\n",
    "                 'driverId_837.0',\n",
    "                 'driverId_838.0',\n",
    "                 'driverId_839.0',\n",
    "                 'driverId_840.0',\n",
    "                 'driverId_841.0',\n",
    "                 'driverId_842.0',\n",
    "                 'driverId_843.0',\n",
    "                   \"status_clean\"]\n",
    "train, dev, test = train_dev_test(sliced_races)\n",
    "train_T, dev_T, test_T = to_tensor(train, dev, test, nogo_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 96\n",
      "Dev: 27\n",
      "Test: 14\n",
      "Rennen insgesamt: 137\n",
      "=========================\n",
      "Tensoren:\n",
      "Train: 96\n",
      "Dev: 27\n",
      "Test: 14\n",
      "Rennen insgesamt: 137\n"
     ]
    }
   ],
   "source": [
    "print('Train:',len(train))\n",
    "print('Dev:',len(dev))\n",
    "print('Test:',len(test))\n",
    "print('Rennen insgesamt:', len(train)+len(dev)+len(test))\n",
    "print(25*'=')\n",
    "print('Tensoren:')\n",
    "print('Train:',len(train_T))\n",
    "print('Dev:',len(dev_T))\n",
    "print('Test:',len(test_T))\n",
    "print('Rennen insgesamt:', len(train_T)+len(dev_T)+len(test_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definieren von notwendigen Klassen: \n",
    "    \n",
    "Zuerst wird die Klasse für das Neuronale Netz definiert, danach die Klasse für die Optimierung der Hyperparameter Lernrate und Epochen Anzahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetzDynamic(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_information):\n",
    "        super(NetzDynamic,self).__init__()\n",
    "        self.__layer_information = layer_information\n",
    "        #definieren von einer Moduleliste, die die Layer enthalten wird\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        #erzeugen der in layer_information übergebeben Layer und hinzufügen zu layers Moduleliste\n",
    "        for specs in self.__layer_information.values():\n",
    "            type_ = specs[0]\n",
    "            in_ = specs[1]\n",
    "            out_ = specs[2]\n",
    "            if type_ == 'linear':\n",
    "                self.layers.append(nn.Linear(in_,out_))\n",
    "            if type_ == 'dropout':\n",
    "                self.layers.append(nn.Dropout())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #definieren eines dynamischen forward pass, activation function richtet sich nach dictionary key\n",
    "        lay_idx = 0\n",
    "        for activation, specs in self.__layer_information.items():\n",
    "            \n",
    "            if activation.startswith('relu'):\n",
    "                layer = self.layers[lay_idx] #auswählen des relevanten Layers aus layers ModuleList\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = F.relu(x.float())\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = F.relu(x.float())\n",
    "                    x = layer(x.float())\n",
    "                    \n",
    "            if activation.startswith('sigmoid'):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = torch.sigmoid(x.float())\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = torch.sigmoid(x.float())\n",
    "                    x = layer(x.float())\n",
    "                    \n",
    "            if activation.startswith('tanh'):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = torch.tanh(x.float())\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = torch.tanh(x.float())\n",
    "                    x = layer(x.float())\n",
    "                    \n",
    "            #if activation.startswith('last'):\n",
    "            #    layer = self.layers[lay_idx]\n",
    "            #    if specs[0] == 'dropout':\n",
    "            #        x = layer(x)\n",
    "            #    else:\n",
    "            #        x = layer(x.float()) \n",
    "            if activation.startswith(\"first\"):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float())\n",
    "            if activation.startswith(\"no_activation\"):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float()) \n",
    "            lay_idx += 1\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetzDynamic_andereFunktionen(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_information):\n",
    "        super(NetzDynamic,self).__init__()\n",
    "        self.__layer_information = layer_information\n",
    "        #definieren von einer Moduleliste, die die Layer enthalten wird\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        #erzeugen der in layer_information übergebeben Layer und hinzufügen zu layers Moduleliste\n",
    "        for specs in self.__layer_information.values():\n",
    "            type_ = specs[0]\n",
    "            in_ = specs[1]\n",
    "            out_ = specs[2]\n",
    "            if type_ == 'linear':\n",
    "                self.layers.append(nn.Linear(in_,out_))\n",
    "            if type_ == 'dropout':\n",
    "                self.layers.append(nn.Dropout())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #definieren eines dynamischen forward pass, activation function richtet sich nach dictionary key\n",
    "        lay_idx = 0\n",
    "        for activation, specs in self.__layer_information.items():\n",
    "            \n",
    "            if activation.startswith('relu'):\n",
    "                layer = self.layers[lay_idx] #auswählen des relevanten Layers aus layers ModuleList\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = F.relu(x.float())\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = F.relu(x.float())\n",
    "                    x = layer(x.float())\n",
    "                    \n",
    "            if activation.startswith('sigmoid'):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = torch.sigmoid(x.float())\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = torch.sigmoid(x.float())\n",
    "                    x = layer(x.float())\n",
    "                    \n",
    "            if activation.startswith('tanh'):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = torch.tanh(x.float())\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = torch.tanh(x.float())\n",
    "                    x = layer(x.float())\n",
    "                    \n",
    "            #if activation.startswith('last'):\n",
    "            #    layer = self.layers[lay_idx]\n",
    "            #    if specs[0] == 'dropout':\n",
    "            #        x = layer(x)\n",
    "            #    else:\n",
    "            #        x = layer(x.float()) \n",
    "            if activation.startswith(\"first\"):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float())\n",
    "            if activation.startswith(\"no_activation\"):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float()) \n",
    "            lay_idx += 1\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetzDynamic_falsch(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_information):\n",
    "        super(NetzDynamic,self).__init__()\n",
    "        self.__layer_information = layer_information\n",
    "        #definieren von einer Moduleliste, die die Layer enthalten wird\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        #erzeugen der in layer_information übergebeben Layer und hinzufügen zu layers Moduleliste\n",
    "        for specs in self.__layer_information.values():\n",
    "            type_ = specs[0]\n",
    "            in_ = specs[1]\n",
    "            out_ = specs[2]\n",
    "            if type_ == 'linear':\n",
    "                self.layers.append(nn.Linear(in_,out_))\n",
    "            if type_ == 'dropout':\n",
    "                self.layers.append(nn.Dropout())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #definieren eines dynamischen forward pass, activation function richtet sich nach dictionary key\n",
    "        lay_idx = 0\n",
    "        for activation, specs in self.__layer_information.items():\n",
    "            \n",
    "            if activation.startswith('relu'):\n",
    "                layer = self.layers[lay_idx] #auswählen des relevanten Layers aus layers ModuleList\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float())\n",
    "                    x = F.relu(x.float())\n",
    "            if activation.startswith('sigmoid'):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float())\n",
    "                    x = torch.sigmoid(x.float())\n",
    "            if activation.startswith('tanh'):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float())\n",
    "                    x = torch.tanh(x.float())\n",
    "            if activation.startswith('last'):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float()) \n",
    "            if activation.startswith(\"first\"):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float())\n",
    "            if activation.startswith(\"no_activation\"):\n",
    "                layer = self.layers[lay_idx]\n",
    "                if specs[0] == 'dropout':\n",
    "                    x = layer(x)\n",
    "                else:\n",
    "                    x = layer(x.float()) \n",
    "            lay_idx += 1\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relu linear dropout linear --> in linear layer nach dropout keine activation!!!\n",
    "\n",
    "**Baustelle**:\n",
    "Es werden Netze zufällig erzeugt, mit jeweils einer festen Anzahl von Layern. Von diesen werden Variationen erzeugt. Ziel ist es zu sagen \"alle Netze mit 5 Layern waren besser als alle Netze mit nur 4 Layern\", um somit ein besseres Gefühl dafür zu bekommen, welche Layeranzahl am besten funktioniert. Außerdem werden die Netze mit einheitlichen Activation Funktionen erzeugt, sie haben also entweder nur ReLu, nur Tanh oder nur Sigmoid als Aktivierungsfunktion. Diese Netze sind direkt miteinander vergleichbar. Alle Netze haben ein Dropout Layer als drittes Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 1\n",
    "drop_layer = 3\n",
    "start_val = 10\n",
    "out_val = 1\n",
    "activations = [\"relu\", \"sigmoid\", \"tanh\"]\n",
    "neuron_count = (30,200)\n",
    "variations = 4\n",
    "all_netz = []\n",
    "x = 0\n",
    "for layer in range(7,11):\n",
    "    c = 0\n",
    "    while c < variations:\n",
    "        netz = {}\n",
    "        for l in range(layer):\n",
    "            x+=1\n",
    "            specs_list = []\n",
    "            if l == 0:\n",
    "                in_ = start_val\n",
    "                out_ = random.randint(neuron_count[0],neuron_count[1])\n",
    "                key = \"first\"\n",
    "                layer_type = \"linear\"\n",
    "                specs_list.append(layer_type)\n",
    "                specs_list.append(in_)\n",
    "                specs_list.append(out_)\n",
    "            elif l == drop_layer-1:\n",
    "                key = \"no_activation\"+str(x)\n",
    "                layer_type = \"dropout\"\n",
    "                layer_before = netz[list(netz.keys())[l-2]]\n",
    "                out_ = netz[list(netz.keys())[l-2]][2]\n",
    "                specs_list.append(layer_type)\n",
    "                specs_list.append(in_)\n",
    "                specs_list.append(out_)\n",
    "            elif l == drop_layer:\n",
    "                key = \"no_activation\"+str(x)\n",
    "                layer_type = \"linear\"\n",
    "                layer_before = netz[list(netz.keys())[l-2]]\n",
    "                in_ = netz[list(netz.keys())[l-2]][2]\n",
    "                out_ = random.randint(neuron_count[0],neuron_count[1])\n",
    "                specs_list.append(layer_type)\n",
    "                specs_list.append(in_)\n",
    "                specs_list.append(out_)\n",
    "            elif l == layer -1:\n",
    "                in_ = netz[list(netz.keys())[l-1]][2]\n",
    "                out_ = out_val\n",
    "                key = \"last\"\n",
    "                layer_type = \"linear\"\n",
    "                specs_list.append(layer_type)\n",
    "                specs_list.append(in_)\n",
    "                specs_list.append(out_)\n",
    "            else:\n",
    "                key = \"relu\"+str(x)\n",
    "                layer_type = \"linear\"\n",
    "                layer_before = netz[list(netz.keys())[l-1]]\n",
    "                in_ = netz[list(netz.keys())[l-1]][2]\n",
    "                out_ = random.randint(neuron_count[0],neuron_count[1])\n",
    "                specs_list.append(layer_type)\n",
    "                specs_list.append(in_)\n",
    "                specs_list.append(out_)\n",
    "                \n",
    "            netz[key]=specs_list   \n",
    "        all_netz.append(netz)\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#netze werden kopiert, nur die activation functions werden verändert\n",
    "temp_alle_netze = []\n",
    "for netz in all_netz:\n",
    "    x = 0\n",
    "    for activation in [\"sigmoid\", \"tanh\"]:\n",
    "        netz_ = {}\n",
    "        for acts, layer_specs in netz.items():\n",
    "            if acts in [\"first\", \"last\"] or acts.startswith(\"no_activation\"):\n",
    "                netz_[acts] = layer_specs\n",
    "            else:\n",
    "                key = activation+str(x)\n",
    "                netz_[key] = layer_specs\n",
    "                x +=1\n",
    "        temp_alle_netze.append(netz_)\n",
    "all_netz = all_netz+temp_alle_netze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netze = {}\n",
    "infos = []\n",
    "max_epochs = 6\n",
    "r = 0\n",
    "for instruction in all_netz:\n",
    "    #Netz_D = NetzDynamic(instruction)\n",
    "    k = str(len(instruction.keys()))+'_l_'+list(instruction.keys())[1]+str(r)\n",
    "    r+=1\n",
    "    #print(k)\n",
    "    netze[k] = instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HP_Layer_Optimizer(object):\n",
    "    \n",
    "    def __init__ (self, \n",
    "                  layer_range = (2,8), #range mindestanzahl layer, max anzahl layer\n",
    "                  input_start = 53, #anzahl inputneuronen für das Inputlayer\n",
    "                  output_last = 1, #anzahl outputneuronen für das letzte layer\n",
    "                  in_out_range = (30,200), #range in der sich die anzahl der Neuronen für die Hiddenlayer bewegen soll\n",
    "                  random_activation = False,#boolean, die angibt ob activation functions random gepicked werden sollen\n",
    "                  types = ['linear', 'dropout'],#layertypen, von denen gewählt wird\n",
    "                  activations = ['relu', 'sigmoid', 'tanh'],#activation functions\n",
    "                  activation_dist = [0.5,0.25,0.25],#wenn random_activation == False wird hier die Häufigkeit für jede Activation übergeben\n",
    "                  dropout_num = 1,#anzahl von dropoutlayern pro NN\n",
    "                  cuda = True,#soll mit cuda gearbeitet werden?\n",
    "                  lr = 0.0001,#lernrate für adam optimizer\n",
    "                  max_epochs = 5,#anzahl der Trainingsepochen\n",
    "                  create_combinations = True, #sollen von der Klasse Netze erzeugt werden, oder sollen diese von außen in die Klasse gegeben werden?\n",
    "                  create_variations = False,#soll von einer Layeranzahl mehr als ein Netz erzeugt werden?\n",
    "                  num_variations = 4 #wenn Variationen erzeugt werden sollen wird eine festgelegte Zahl Netze mit 4 Layer, eine festgelegte Anzahl mit 5 Layern usw. erzeugt\n",
    "                  ):\n",
    "        \n",
    "        self.__model = None #model, welches trainiert und getestet wird, wird hier zwischengespeichert\n",
    "        self.__layer_range = layer_range #min und max anzahl von layern in Tupelform\n",
    "        self.__input_start = input_start#Inputgröße für Startlayer/Inputlayer\n",
    "        self.__output_last = output_last#Outputgröße für letztes Layer / Outputlayer\n",
    "        self.__in_out_range = in_out_range#range in der sich die Anzahl der In- und Outputs für die Hiddenlayer bewegen soll (Tupelform)\n",
    "        self.__layer_types = types#liste welche Layertypen enthält (bspw. linear oder dropout)\n",
    "        self.__dropout_number = dropout_num#anzahl von layern, die pro Netz ein dropout Layer sein sollen\n",
    "        self.__random_activation = random_activation#Boolean, ob die activation zufällig gewählt werden soll\n",
    "        self.__create_variations = create_variations\n",
    "        self.__number_of_variations = num_variations\n",
    "        \n",
    "        self.__combination_results = {}#dictionary, die dem schlüssel zu einer NN Kombi einen MAE zuordnet\n",
    "        self.model_specs_combinations = {}#dictionary, welches die jeweiligen NN Kombinationen enthält (einem Schlüssel zugeordnet)\n",
    "        self.train_data = None#trainingsdaten, die dem Optimizer übergeben werden für die Modelle\n",
    "        self.test_data = None#developmentdaten, um die Modelle zu testen und Aussagen über die besten Kombinationen zu treffen\n",
    "        self.lr = lr#lernrate für Modell OPtimizer (default Adam)\n",
    "        self.max_epochs = max_epochs#Anzahl der Trainingsepochen\n",
    "        self.cuda = cuda#Boolean, ob mit cuda gearbeitet werden soll oder nicht\n",
    "        self.opt_combination = {}#dictionary enthält optimale kombination aus möglichen kombinationen\n",
    "        \n",
    "        self.__activations = None#entweder Liste (wenn activations random ausgewählt werden sollen), oder dictionary, \n",
    "        #wenn activation functions mit einer bestimmten Häufigkeit verwendet werden sollen\n",
    "        if self.__random_activation:#random pick von activation functions\n",
    "            self.__activations = activations\n",
    "        else:#activation function sollen mit einer gewissen häufigkeit ausgewählt werden\n",
    "            k = {}\n",
    "            for a in range(len(types)):#zuordnen einer häufihkeit aus activation_dist liste zu jeder activation function aus types\n",
    "                act = types[a]\n",
    "                dist = activation_dist[a]\n",
    "                k[act] = dist\n",
    "            self.__activations = k\n",
    "            \n",
    "        #aufrufen der Funktion, die anhand der übergebenen Parameter Modellkombinationen erzeugt\n",
    "        if create_combinations:\n",
    "            self.__create_combinations()\n",
    "        else:\n",
    "            print('NN Kombinationen müssen in Dictionary Form selbst übergeben werden')\n",
    "        \n",
    "    def __train(self, epoch, optimizer):\n",
    "        '''\n",
    "            funktion übernimmt das Training von dem in self.__model\n",
    "            zwischengespeicherten NN. \n",
    "            Epoch: Jetzige Epoche in der trainiert wird (für coolen print Befehl wichtig)\n",
    "            Optimizer: Optimizer mit dem Parameter des NN aus self.__model optimiert werden\n",
    "        '''\n",
    "        if self.cuda:\n",
    "            self.__model.cuda()\n",
    "            self.__model.train()\n",
    "            batch_id = 0\n",
    "            for key in self.train_data.keys():\n",
    "                for data, target in self.train_data[key]:\n",
    "                    data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1).cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    out = self.__model(data)\n",
    "                    #print(\"Out: \", out, out.size())\n",
    "                    #print(\"Target: \", target, target.size())\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss = criterion(out, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch, batch_id *len(data), len(self.train_data),\n",
    "                    100. * batch_id / len(self.train_data), loss.item()))\n",
    "                    batch_id +=1\n",
    "        else:\n",
    "            self.__model.train()\n",
    "            batch_id = 0\n",
    "            for key in self.train_data.keys():\n",
    "                for data, target in self.train_data[key]:\n",
    "                    #data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)#.cuda()\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1)#.cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    out = self.__model(data)\n",
    "                    #print(\"Out: \", out, out.size())\n",
    "                    #print(\"Target: \", target, target.size())\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss = criterion(out, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch, batch_id *len(data), len(self.train_data),\n",
    "                    100. * batch_id / len(self.train_data), loss.item()))\n",
    "                    batch_id +=1\n",
    "            \n",
    "    def __test(self):\n",
    "        '''\n",
    "            Funktion, die das Testen des Models aus self.__model auf den übergebenen Dev-Daten \n",
    "            (self.test_data) übernimmt und den gesamt MAE für das jeweilige Modell berechnet\n",
    "        '''\n",
    "        total = 0\n",
    "        count = 0\n",
    "        result_dict = {}\n",
    "        result = pd.DataFrame(columns = ['target','prediction'])\n",
    "        help_dict = {}\n",
    "        if self.cuda:\n",
    "            self.__model.cuda()\n",
    "            for key in self.test_data.keys():\n",
    "                #print(key)\n",
    "                help_dict = {}\n",
    "                for data, target in self.test_data[key]:\n",
    "                    self.__model.eval()\n",
    "                    #files.listdir(path)\n",
    "                    data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1).cuda()\n",
    "                    out = self.__model(data).cpu()\n",
    "                    #print(out)\n",
    "                    out = out.detach().numpy()\n",
    "                    #out = np.round(out)\n",
    "                    target = target.cpu()\n",
    "                    target = target.detach().numpy()\n",
    "                    #print(data)\n",
    "                    #print(data[\"driverId\"])\n",
    "                    total += abs(out - target[0][0])\n",
    "                    #print(\"current_position: \", data[0][0].item())\n",
    "                    #print(\"Output: \", out)\n",
    "                    #print(\"Target: \", target)\n",
    "                    help_dict[target[0][0]] = out\n",
    "                    #print(\"Difference: \", out - target)\n",
    "                    count+=1\n",
    "                #Auslesen der predicteten Werte A und der zugehörigen targets y\n",
    "                A = [x[0][0] for x in list((help_dict.values()))]\n",
    "                y = list(help_dict.keys())\n",
    "                \n",
    "                #Anfügen der Werte an Result\n",
    "                t = pd.DataFrame()\n",
    "                t['target'] = y\n",
    "                t['prediction_value'] = A\n",
    "                t = sqldf.sqldf('''select * from t order by prediction_value ASC''')\n",
    "                t.reset_index(inplace = True)\n",
    "                t.rename(columns = {'index':'prediction'}, inplace = True)\n",
    "                t['prediction'] = t['prediction']+1\n",
    "                \n",
    "                \n",
    "                result = result.append(t, sort = True)\n",
    "                \n",
    "        else:\n",
    "            for raceId in self.test_data.keys():\n",
    "                #print(key)\n",
    "                help_dict = {}\n",
    "                for data, target in self.test_data[raceId]:\n",
    "                    self.__model.eval()\n",
    "                    #files.listdir(path)\n",
    "                    #data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1)#.cuda()\n",
    "                    out = self.__model(data)#.cpu()\n",
    "                    #print(out)\n",
    "                    out = out.detach().numpy()\n",
    "                    #out = np.round(out)\n",
    "                    #target = target.cpu()\n",
    "                    target = target.detach().numpy()\n",
    "                    #print(data)\n",
    "                    #print(data[\"driverId\"])\n",
    "                    total += abs(out - target[0][0])\n",
    "                    #print(\"current_position: \", data[0][0].item())\n",
    "                    #print(\"Output: \", out)\n",
    "                    #print(\"Target: \", target)\n",
    "                    help_dict[target[0][0]] = out\n",
    "                    #print(\"Difference: \", out - target)\n",
    "                    count+=1\n",
    "                #Auslesen der predicteten Werte A und der zugehörigen targets y\n",
    "                A = [x[0][0] for x in list((help_dict.values()))]\n",
    "                y = list(help_dict.keys())\n",
    "                \n",
    "                #Anfügen der Werte an Result\n",
    "                t = pd.DataFrame()\n",
    "                t['target'] = y\n",
    "                t['prediction_value'] = A\n",
    "                t = sqldf.sqldf('''select * from t order by prediction_value ASC''')\n",
    "                t.reset_index(inplace = True)\n",
    "                t.rename(columns = {'index':'prediction'}, inplace = True)\n",
    "                t['prediction'] = t['prediction']+1\n",
    "                \n",
    "                result = result.append(t, sort = True)\n",
    "        return result\n",
    "    \n",
    "    def get_all_information(self):\n",
    "        \n",
    "        print('All Model Combinations with encoding:\\n', self.model_specs_combinations)\n",
    "        print('Model Results:\\n', self.__combination_results)\n",
    "        print('Optimale Kombination:\\n', self.opt_combination)\n",
    "        \n",
    "    def validate_combinations(self):\n",
    "        \n",
    "        for key, combination in self.model_specs_combinations.items():\n",
    "            print(combination)\n",
    "            self.__model = NetzDynamic(combination)\n",
    "            optimizer = optim.Adam(self.__model.parameters(), lr = self.lr)\n",
    "            #trainieren des modells\n",
    "            for epoch in range(1,self.max_epochs):\n",
    "                self.__train(epoch, optimizer)  \n",
    "            \n",
    "            result = self.__test()\n",
    "            A = result.prediction.tolist()\n",
    "            y = result.target.tolist()\n",
    "            mae = MAE(A,y)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.__combination_results[key] = mae\n",
    "            \n",
    "        #finden der besten kombination nach minimalstem Error (MAE)\n",
    "        key_min = min(self.__combination_results.keys(), key=(lambda k: self.__combination_results[k]))\n",
    "        best_combination = self.model_specs_combinations[key_min]\n",
    "        best_combination['mae'] = self.__combination_results[key_min]\n",
    "        self.opt_combination = best_combination\n",
    "        #self.__combination_overview[key] = specifics\n",
    "        \n",
    "        \n",
    "    def __create_combinations(self):\n",
    "        '''\n",
    "            Funktion erzeugt Dictionarys, mit NN Modellspezifikationen, die\n",
    "            später gegeneinander getestet werden sollen. Kombinationen werden\n",
    "            in dem Dictionary self.model_specs_combinations unter einem \n",
    "            Schlüssel abgespeichert\n",
    "        '''\n",
    "        \n",
    "        if self.__create_variations:\n",
    "            '''\n",
    "            es werden mehr als ein Netz von einer bestimmten Layeranzahl erzeugt:\n",
    "            wenn die Layeranzahl Range zwischen 7 und 9 liegt, self.__create_variations = True ist und\n",
    "            die Variable self.__number_of_variations = 4 ist werden 4 Netze mit 7 Layern und 4 Netze\n",
    "            mit 8 Layern erzeugt. Diese können dann gegeneinander getestet werden und geben eine bessere\n",
    "            Übersicht über eine gute Layeranzahl.\n",
    "            '''\n",
    "            min_layer = self.__layer_range[0]\n",
    "            max_layer = self.__layer_range[1]\n",
    "            if min_layer == max_layer:\n",
    "                max_layer += 1\n",
    "            for layer in range(min_layer, max_layer): \n",
    "                variation_counter = 0\n",
    "                while variation_counter < self.__number_of_variations:\n",
    "                    variation_counter +=1 \n",
    "                    dropout_counter = 0\n",
    "                    act_count = 0\n",
    "                    specs_dict = {}\n",
    "                    middle = layer//2\n",
    "                    for l in range(layer):\n",
    "                        layer_specs = []\n",
    "                        if self.__random_activation:#random activation pick ist aktiviert\n",
    "                            act = random.choice(self.__activations)\n",
    "                            key = act+str(act_count)\n",
    "                            act_count += 1\n",
    "                            if dropout_counter == self.__dropout_number:\n",
    "                                #es wurden schon ausreichend dropout layer erzeugt\n",
    "                                l_ = random.choice([x for x in self.__layer_types if x not in ['dropout']])\n",
    "                            else:\n",
    "                                l_ = random.choice(self.__layer_types)\n",
    "                                if l_ == 'dropout':\n",
    "                                    dropout_counter +=1\n",
    "                                    \n",
    "                            if l == 0:\n",
    "                                in_ = self.__input_start\n",
    "                                if l_ == 'dropout':\n",
    "                                    dropout_counter = dropout_counter-1\n",
    "                                    l_ = 'linear'\n",
    "                                range_start = self.__in_out_range[0]\n",
    "                                range_end = self.__in_out_range[1]\n",
    "                                out_ = random.randint(range_start, range_end)\n",
    "                                layer_specs.append(l_)\n",
    "                                layer_specs.append(in_)\n",
    "                                layer_specs.append(out_)\n",
    "                            else:\n",
    "                                layer_before = specs_dict[list(specs_dict.keys())[-1]]\n",
    "                                out_alt = specs_dict[list(specs_dict.keys())[-1]][2]\n",
    "                                l_type = layer_before[0]\n",
    "                                i_ltype = -2\n",
    "                                while l_type == 'dropout':#überprüfen ob vorhergegangenes Layer ein dropout layer war\n",
    "                                    #sobald das vorhergegangene nicht-dropoutlayer gefunden wurde, wird output größe übernommen\n",
    "                                    layer_before = specs_dict[list(specs_dict.keys())[i_ltype]]\n",
    "                                    l_type = layer_before[0]\n",
    "                                    out_alt = specs_dict[list(specs_dict.keys())[i_ltype]][2]\n",
    "                                    i_ltype = i_ltype -1\n",
    "                                if l <= middle: #in der ertsen hälfte nimmt output zu\n",
    "                                    in_ = out_alt\n",
    "                                    range_start = out_alt\n",
    "                                    range_end = self.__in_out_range[1]\n",
    "                                    out_ = random.randint(range_start, range_end)\n",
    "                                else:#in zweiter hälfte der layer nimmt output wieder ab\n",
    "                                    in_ = out_alt\n",
    "                                    range_start = self.__in_out_range[0]\n",
    "                                    range_end = out_alt\n",
    "                                    out_ = random.randint(range_start, range_end)\n",
    "                                layer_specs.append(l_)\n",
    "                                layer_specs.append(in_)\n",
    "                                layer_specs.append(out_)\n",
    "                            if l == layer-1:\n",
    "                                l_ = 'linear'\n",
    "                                layer_specs = [l_,in_,self.__output_last]\n",
    "                                specs_dict['last'] = layer_specs#layer wird ohne activation gespeichert und als letztes Layer des NN gekennzeichnet\n",
    "                                \n",
    "                            elif l == 0:\n",
    "                                specs_dict['first'] = layer_specs\n",
    "                            else:\n",
    "                                #if l_ == 'dropout':#keine activation function bei dropout layern\n",
    "                                #    key = 'no_activation'+str(act_count)\n",
    "                                if specs_dict[list(specs_dict.keys())[-1]][0] == 'dropout':# keine activation function bei layern direkt nach einem dropout layer\n",
    "                                    key = 'no_activation'+str(act_count)\n",
    "                                specs_dict[key] = layer_specs\n",
    "                    key = random.randint(0,10000)\n",
    "                    while key in list(self.model_specs_combinations.keys()):\n",
    "                         key = random.randint(0,10000)\n",
    "                    self.model_specs_combinations[key]= specs_dict   \n",
    "                    print(specs_dict,'\\n')\n",
    "                \n",
    "        else:\n",
    "            min_layer = self.__layer_range[0]\n",
    "            max_layer = self.__layer_range[1]\n",
    "            if min_layer == max_layer:\n",
    "                max_layer += 1\n",
    "            for layer in range(min_layer, max_layer): \n",
    "                \n",
    "                dropout_counter = 0\n",
    "                act_count = 0\n",
    "                specs_dict = {}\n",
    "                middle = layer//2\n",
    "                for l in range(layer):\n",
    "                    layer_specs = []\n",
    "                    if self.__random_activation:#random activation pick ist aktiviert\n",
    "                        act = random.choice(self.__activations)\n",
    "                        key = act+str(act_count)\n",
    "                        act_count += 1\n",
    "                        if dropout_counter == self.__dropout_number:\n",
    "                            #es wurden schon ausreichend dropout layer erzeugt\n",
    "                            l_ = random.choice([x for x in self.__layer_types if x not in ['dropout']])\n",
    "                        else:\n",
    "                            l_ = random.choice(self.__layer_types)\n",
    "                            if l_ == 'dropout':\n",
    "                                dropout_counter +=1\n",
    "                                \n",
    "                        if l == 0:\n",
    "                            in_ = self.__input_start\n",
    "                            if l_ == 'dropout':\n",
    "                                dropout_counter = dropout_counter-1\n",
    "                                l_ = 'linear'\n",
    "                            range_start = self.__in_out_range[0]\n",
    "                            range_end = self.__in_out_range[1]\n",
    "                            out_ = random.randint(range_start, range_end)\n",
    "                            layer_specs.append(l_)\n",
    "                            layer_specs.append(in_)\n",
    "                            layer_specs.append(out_)\n",
    "                        else:\n",
    "                            layer_before = specs_dict[list(specs_dict.keys())[-1]]\n",
    "                            out_alt = specs_dict[list(specs_dict.keys())[-1]][2]\n",
    "                            l_type = layer_before[0]\n",
    "                            i_ltype = -2\n",
    "                            while l_type == 'dropout':#überprüfen ob vorhergegangenes Layer ein dropout layer war\n",
    "                                #sobald das vorhergegangene nicht-dropoutlayer gefunden wurde, wird output größe übernommen\n",
    "                                layer_before = specs_dict[list(specs_dict.keys())[i_ltype]]\n",
    "                                l_type = layer_before[0]\n",
    "                                out_alt = specs_dict[list(specs_dict.keys())[i_ltype]][2]\n",
    "                                i_ltype = i_ltype -1\n",
    "                            if l <= middle: #in der ertsen hälfte nimmt output zu\n",
    "                                in_ = out_alt\n",
    "                                range_start = out_alt\n",
    "                                range_end = self.__in_out_range[1]\n",
    "                                out_ = random.randint(range_start, range_end)\n",
    "                            else:#in zweiter hälfte der layer nimmt output wieder ab\n",
    "                                in_ = out_alt\n",
    "                                range_start = self.__in_out_range[0]\n",
    "                                range_end = out_alt\n",
    "                                out_ = random.randint(range_start, range_end)\n",
    "                            layer_specs.append(l_)\n",
    "                            layer_specs.append(in_)\n",
    "                            layer_specs.append(out_)\n",
    "                        #if l == layer-1:\n",
    "                        #    l_ = 'linear'\n",
    "                        #    layer_specs = [l_,in_,self.__output_last]\n",
    "                        #    specs_dict['last'] = layer_specs#layer wird ohne activation gespeichert und als letztes Layer des NN gekennzeichnet\n",
    "                            \n",
    "                        #elif l == 0:\n",
    "                        if l == 0:\n",
    "                            specs_dict['first'] = layer_specs\n",
    "                        else:\n",
    "                            #if l_ == 'dropout':#keine activation function bei dropout layern\n",
    "                            #    key = 'no_activation'+str(act_count)\n",
    "                            if specs_dict[list(specs_dict.keys())[-1]][0] == 'dropout':# keine activation function bei layern direkt nach einem dropout layer\n",
    "                                key = 'no_activation'+str(act_count)\n",
    "                            specs_dict[key] = layer_specs\n",
    "                key = random.randint(0,10000)\n",
    "                while key in list(self.model_specs_combinations.keys()):\n",
    "                     key = random.randint(0,10000)\n",
    "                self.model_specs_combinations[key]= specs_dict   \n",
    "                print(specs_dict)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first': ['linear', 10, 77], 'relu1': ['linear', 77, 143], 'relu2': ['dropout', 143, 170], 'no_activation4': ['linear', 143, 181], 'relu4': ['linear', 181, 196], 'relu5': ['linear', 196, 95], 'relu6': ['linear', 95, 73], 'last': ['linear', 73, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 42], 'relu1': ['dropout', 42, 180], 'no_activation3': ['linear', 42, 81], 'relu3': ['linear', 81, 192], 'relu4': ['linear', 192, 196], 'relu5': ['linear', 196, 142], 'relu6': ['linear', 142, 96], 'last': ['linear', 96, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 169], 'relu1': ['dropout', 169, 182], 'no_activation3': ['linear', 169, 173], 'relu3': ['linear', 173, 191], 'relu4': ['linear', 191, 197], 'relu5': ['linear', 197, 143], 'relu6': ['linear', 143, 79], 'last': ['linear', 79, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 191], 'relu1': ['linear', 191, 196], 'relu2': ['dropout', 196, 200], 'no_activation4': ['linear', 196, 198], 'relu4': ['linear', 198, 200], 'relu5': ['linear', 200, 61], 'relu6': ['linear', 61, 43], 'last': ['linear', 43, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 100], 'relu1': ['linear', 100, 179], 'relu2': ['linear', 179, 180], 'relu3': ['dropout', 180, 188], 'no_activation5': ['linear', 180, 190], 'relu5': ['linear', 190, 87], 'relu6': ['linear', 87, 42], 'relu7': ['linear', 42, 32], 'last': ['linear', 32, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 195], 'relu1': ['dropout', 195, 200], 'no_activation3': ['linear', 195, 196], 'relu3': ['linear', 196, 200], 'relu4': ['linear', 200, 200], 'relu5': ['linear', 200, 124], 'relu6': ['linear', 124, 93], 'relu7': ['linear', 93, 54], 'last': ['linear', 54, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 61], 'relu1': ['linear', 61, 92], 'relu2': ['linear', 92, 159], 'relu3': ['dropout', 159, 167], 'no_activation5': ['linear', 159, 190], 'relu5': ['linear', 190, 31], 'relu6': ['linear', 31, 31], 'relu7': ['linear', 31, 30], 'last': ['linear', 30, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 111], 'relu1': ['linear', 111, 126], 'relu2': ['dropout', 126, 184], 'no_activation4': ['linear', 126, 170], 'relu4': ['linear', 170, 191], 'relu5': ['linear', 191, 71], 'relu6': ['linear', 71, 45], 'relu7': ['linear', 45, 41], 'last': ['linear', 41, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 45], 'relu1': ['dropout', 45, 142], 'no_activation3': ['linear', 45, 90], 'relu3': ['linear', 90, 185], 'relu4': ['linear', 185, 186], 'relu5': ['linear', 186, 187], 'relu6': ['linear', 187, 81], 'relu7': ['linear', 81, 53], 'relu8': ['linear', 53, 36], 'last': ['linear', 36, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 182], 'relu1': ['linear', 182, 183], 'relu2': ['dropout', 183, 198], 'no_activation4': ['linear', 183, 188], 'relu4': ['linear', 188, 197], 'relu5': ['linear', 197, 200], 'relu6': ['linear', 200, 197], 'relu7': ['linear', 197, 94], 'relu8': ['linear', 94, 46], 'last': ['linear', 46, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 73], 'relu1': ['dropout', 73, 135], 'no_activation3': ['linear', 73, 127], 'relu3': ['linear', 127, 150], 'relu4': ['linear', 150, 193], 'relu5': ['linear', 193, 199], 'relu6': ['linear', 199, 113], 'relu7': ['linear', 113, 72], 'relu8': ['linear', 72, 68], 'last': ['linear', 68, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 137], 'relu1': ['linear', 137, 158], 'relu2': ['dropout', 158, 158], 'no_activation4': ['linear', 158, 173], 'relu4': ['linear', 173, 180], 'relu5': ['linear', 180, 199], 'relu6': ['linear', 199, 134], 'relu7': ['linear', 134, 34], 'relu8': ['linear', 34, 34], 'last': ['linear', 34, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 146], 'relu1': ['linear', 146, 184], 'relu2': ['dropout', 184, 190], 'no_activation4': ['linear', 184, 188], 'relu4': ['linear', 188, 197], 'relu5': ['linear', 197, 197], 'relu6': ['linear', 197, 147], 'relu7': ['linear', 147, 135], 'relu8': ['linear', 135, 121], 'relu9': ['linear', 121, 89], 'last': ['linear', 89, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 104], 'relu1': ['linear', 104, 196], 'relu2': ['dropout', 196, 198], 'no_activation4': ['linear', 196, 196], 'relu4': ['linear', 196, 196], 'relu5': ['linear', 196, 197], 'relu6': ['linear', 197, 122], 'relu7': ['linear', 122, 91], 'relu8': ['linear', 91, 70], 'relu9': ['linear', 70, 40], 'last': ['linear', 40, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 76], 'relu1': ['linear', 76, 138], 'relu2': ['linear', 138, 170], 'relu3': ['dropout', 170, 191], 'no_activation5': ['linear', 170, 183], 'relu5': ['linear', 183, 184], 'relu6': ['linear', 184, 92], 'relu7': ['linear', 92, 33], 'relu8': ['linear', 33, 33], 'relu9': ['linear', 33, 32], 'last': ['linear', 32, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 53], 'relu1': ['dropout', 53, 192], 'no_activation3': ['linear', 53, 57], 'relu3': ['linear', 57, 87], 'relu4': ['linear', 87, 128], 'relu5': ['linear', 128, 149], 'relu6': ['linear', 149, 59], 'relu7': ['linear', 59, 58], 'relu8': ['linear', 58, 54], 'relu9': ['linear', 54, 42], 'last': ['linear', 42, 1]} \n",
      "\n",
      "{'first': ['linear', 10, 77], 'relu1': ['linear', 77, 143], 'relu2': ['dropout', 143, 170], 'no_activation4': ['linear', 143, 181], 'relu4': ['linear', 181, 196], 'relu5': ['linear', 196, 95], 'relu6': ['linear', 95, 73], 'last': ['linear', 73, 1]}\n",
      "Train Epoche: 1 [0/96 (0%)]\tLoss: 398.008942\n",
      "Train Epoche: 1 [1/96 (1%)]\tLoss: 142.713593\n",
      "Train Epoche: 1 [2/96 (2%)]\tLoss: 80.058571\n",
      "Train Epoche: 1 [3/96 (3%)]\tLoss: 120.098457\n",
      "Train Epoche: 1 [4/96 (4%)]\tLoss: 396.483429\n",
      "Train Epoche: 1 [5/96 (5%)]\tLoss: 47.967556\n",
      "Train Epoche: 1 [6/96 (6%)]\tLoss: 98.472084\n",
      "Train Epoche: 1 [7/96 (7%)]\tLoss: 166.686676\n",
      "Train Epoche: 1 [8/96 (8%)]\tLoss: 8.588000\n",
      "Train Epoche: 1 [9/96 (9%)]\tLoss: 3.812596"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 73])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoche: 1 [10/96 (10%)]\tLoss: 62.772888\n",
      "Train Epoche: 1 [11/96 (11%)]\tLoss: 34.767738\n",
      "Train Epoche: 1 [12/96 (12%)]\tLoss: 396.239532\n",
      "Train Epoche: 1 [13/96 (14%)]\tLoss: 15.370191\n",
      "Train Epoche: 1 [14/96 (15%)]\tLoss: 221.052856\n",
      "Train Epoche: 1 [15/96 (16%)]\tLoss: 395.544342\n",
      "Train Epoche: 1 [16/96 (17%)]\tLoss: 0.825584\n",
      "Train Epoche: 1 [17/96 (18%)]\tLoss: 22.772278\n",
      "Train Epoche: 1 [18/96 (19%)]\tLoss: 184.733765\n",
      "Train Epoche: 1 [19/96 (20%)]\tLoss: 240.234512\n",
      "Train Epoche: 1 [20/96 (21%)]\tLoss: 62.465485\n",
      "Train Epoche: 1 [21/96 (22%)]\tLoss: 320.430450\n",
      "Train Epoche: 1 [22/96 (23%)]\tLoss: 480.141479\n",
      "Train Epoche: 1 [23/96 (24%)]\tLoss: 570.207275\n",
      "Train Epoche: 1 [24/96 (25%)]\tLoss: 221.194809\n",
      "Train Epoche: 1 [25/96 (26%)]\tLoss: 193.301254\n",
      "Train Epoche: 1 [26/96 (27%)]\tLoss: 47.421867\n",
      "Train Epoche: 1 [27/96 (28%)]\tLoss: 78.733643\n",
      "Train Epoche: 1 [28/96 (29%)]\tLoss: 139.864731\n",
      "Train Epoche: 1 [29/96 (30%)]\tLoss: 34.447769\n",
      "Train Epoche: 1 [30/96 (31%)]\tLoss: 15.063132\n",
      "Train Epoche: 1 [31/96 (32%)]\tLoss: 0.731383\n",
      "Train Epoche: 1 [32/96 (33%)]\tLoss: 163.704575\n",
      "Train Epoche: 1 [33/96 (34%)]\tLoss: 117.401505\n",
      "Train Epoche: 1 [34/96 (35%)]\tLoss: 20.613081\n",
      "Train Epoche: 1 [35/96 (36%)]\tLoss: 89.781837\n",
      "Train Epoche: 1 [36/96 (38%)]\tLoss: 376.808258\n",
      "Train Epoche: 1 [37/96 (39%)]\tLoss: 409.649963\n",
      "Train Epoche: 1 [38/96 (40%)]\tLoss: 258.041168\n",
      "Train Epoche: 1 [39/96 (41%)]\tLoss: 224.324173\n",
      "Train Epoche: 1 [40/96 (42%)]\tLoss: 2.487913\n",
      "Train Epoche: 1 [41/96 (43%)]\tLoss: 5.542403\n",
      "Train Epoche: 1 [42/96 (44%)]\tLoss: 323.349060\n",
      "Train Epoche: 1 [43/96 (45%)]\tLoss: 512.613953\n",
      "Train Epoche: 1 [44/96 (46%)]\tLoss: 391.789612\n",
      "Train Epoche: 1 [45/96 (47%)]\tLoss: 188.581467\n",
      "Train Epoche: 1 [46/96 (48%)]\tLoss: 59.468910\n",
      "Train Epoche: 1 [47/96 (49%)]\tLoss: 115.155609\n",
      "Train Epoche: 1 [48/96 (50%)]\tLoss: 32.869156\n",
      "Train Epoche: 1 [49/96 (51%)]\tLoss: 161.977005\n",
      "Train Epoche: 1 [50/96 (52%)]\tLoss: 137.116379\n",
      "Train Epoche: 1 [51/96 (53%)]\tLoss: 95.113853\n",
      "Train Epoche: 1 [52/96 (54%)]\tLoss: 13.217649\n",
      "Train Epoche: 1 [53/96 (55%)]\tLoss: 3.062924\n",
      "Train Epoche: 1 [54/96 (56%)]\tLoss: 277.795776\n",
      "Train Epoche: 1 [55/96 (57%)]\tLoss: 75.465591\n",
      "Train Epoche: 1 [56/96 (58%)]\tLoss: 7.109630\n",
      "Train Epoche: 1 [57/96 (59%)]\tLoss: 393.642334\n",
      "Train Epoche: 1 [58/96 (60%)]\tLoss: 242.438202\n",
      "Train Epoche: 1 [59/96 (61%)]\tLoss: 391.703766\n",
      "Train Epoche: 1 [60/96 (62%)]\tLoss: 1.284156\n",
      "Train Epoche: 1 [61/96 (64%)]\tLoss: 14.903150\n",
      "Train Epoche: 1 [62/96 (65%)]\tLoss: 28.731579\n",
      "Train Epoche: 1 [63/96 (66%)]\tLoss: 156.749680\n",
      "Train Epoche: 1 [64/96 (67%)]\tLoss: 12.072047\n",
      "Train Epoche: 1 [65/96 (68%)]\tLoss: 7.248028\n",
      "Train Epoche: 1 [66/96 (69%)]\tLoss: 88.377846\n",
      "Train Epoche: 1 [67/96 (70%)]\tLoss: 158.492371\n",
      "Train Epoche: 1 [68/96 (71%)]\tLoss: 107.863655\n",
      "Train Epoche: 1 [69/96 (72%)]\tLoss: 29.383034\n",
      "Train Epoche: 1 [70/96 (73%)]\tLoss: 40.192776\n",
      "Train Epoche: 1 [71/96 (74%)]\tLoss: 56.215641\n",
      "Train Epoche: 1 [72/96 (75%)]\tLoss: 207.682556\n",
      "Train Epoche: 1 [73/96 (76%)]\tLoss: 19.752182\n",
      "Train Epoche: 1 [74/96 (77%)]\tLoss: 451.821594\n",
      "Train Epoche: 1 [75/96 (78%)]\tLoss: 231.750565\n",
      "Train Epoche: 1 [76/96 (79%)]\tLoss: 175.516281\n",
      "Train Epoche: 1 [77/96 (80%)]\tLoss: 1.818562\n",
      "Train Epoche: 1 [78/96 (81%)]\tLoss: 3.648443\n",
      "Train Epoche: 1 [79/96 (82%)]\tLoss: 176.171356\n",
      "Train Epoche: 1 [80/96 (83%)]\tLoss: 76.065865\n",
      "Train Epoche: 1 [81/96 (84%)]\tLoss: 94.448669\n",
      "Train Epoche: 1 [82/96 (85%)]\tLoss: 36.252586\n",
      "Train Epoche: 1 [83/96 (86%)]\tLoss: 9.247003\n",
      "Train Epoche: 1 [84/96 (88%)]\tLoss: 3.860373\n",
      "Train Epoche: 1 [85/96 (89%)]\tLoss: 442.411438\n",
      "Train Epoche: 1 [86/96 (90%)]\tLoss: 444.632263\n",
      "Train Epoche: 1 [87/96 (91%)]\tLoss: 14.671205\n",
      "Train Epoche: 1 [88/96 (92%)]\tLoss: 75.442177\n",
      "Train Epoche: 1 [89/96 (93%)]\tLoss: 57.408707\n",
      "Train Epoche: 1 [90/96 (94%)]\tLoss: 18.508255\n",
      "Train Epoche: 1 [91/96 (95%)]\tLoss: 407.394653\n",
      "Train Epoche: 1 [92/96 (96%)]\tLoss: 39.861279\n",
      "Train Epoche: 1 [93/96 (97%)]\tLoss: 136.097656\n",
      "Train Epoche: 1 [94/96 (98%)]\tLoss: 294.704742\n",
      "Train Epoche: 1 [95/96 (99%)]\tLoss: 5.819556\n",
      "Train Epoche: 1 [96/96 (100%)]\tLoss: 24.586355\n",
      "Train Epoche: 1 [97/96 (101%)]\tLoss: 118.216408\n",
      "Train Epoche: 1 [98/96 (102%)]\tLoss: 160.574600\n",
      "Train Epoche: 1 [99/96 (103%)]\tLoss: 94.614883\n",
      "Train Epoche: 1 [100/96 (104%)]\tLoss: 57.502354\n",
      "Train Epoche: 1 [101/96 (105%)]\tLoss: 137.863983\n",
      "Train Epoche: 1 [102/96 (106%)]\tLoss: 73.734901\n",
      "Train Epoche: 1 [103/96 (107%)]\tLoss: 106.505608\n",
      "Train Epoche: 1 [104/96 (108%)]\tLoss: 83.685211\n",
      "Train Epoche: 1 [105/96 (109%)]\tLoss: 187.940125\n",
      "Train Epoche: 1 [106/96 (110%)]\tLoss: 138.002823\n",
      "Train Epoche: 1 [107/96 (111%)]\tLoss: 174.589142\n",
      "Train Epoche: 1 [108/96 (112%)]\tLoss: 15.761581\n",
      "Train Epoche: 1 [109/96 (114%)]\tLoss: 1.551453\n",
      "Train Epoche: 1 [110/96 (115%)]\tLoss: 31.901749\n",
      "Train Epoche: 1 [111/96 (116%)]\tLoss: 57.774265\n",
      "Train Epoche: 1 [112/96 (117%)]\tLoss: 24.557827\n",
      "Train Epoche: 1 [113/96 (118%)]\tLoss: 4.030570\n",
      "Train Epoche: 1 [114/96 (119%)]\tLoss: 9.006371\n",
      "Train Epoche: 1 [115/96 (120%)]\tLoss: 360.843018\n",
      "Train Epoche: 1 [116/96 (121%)]\tLoss: 295.761200\n",
      "Train Epoche: 1 [117/96 (122%)]\tLoss: 80.092506\n",
      "Train Epoche: 1 [118/96 (123%)]\tLoss: 18.436419\n",
      "Train Epoche: 1 [119/96 (124%)]\tLoss: 71.904053\n",
      "Train Epoche: 1 [120/96 (125%)]\tLoss: 110.796234\n",
      "Train Epoche: 1 [121/96 (126%)]\tLoss: 60.304836\n",
      "Train Epoche: 1 [122/96 (127%)]\tLoss: 33.207821\n",
      "Train Epoche: 1 [123/96 (128%)]\tLoss: 66.793648\n",
      "Train Epoche: 1 [124/96 (129%)]\tLoss: 60.294418\n",
      "Train Epoche: 1 [125/96 (130%)]\tLoss: 31.642694\n",
      "Train Epoche: 1 [126/96 (131%)]\tLoss: 71.128212\n",
      "Train Epoche: 1 [127/96 (132%)]\tLoss: 40.521572\n",
      "Train Epoche: 1 [128/96 (133%)]\tLoss: 202.264771\n",
      "Train Epoche: 1 [129/96 (134%)]\tLoss: 8.655535\n",
      "Train Epoche: 1 [130/96 (135%)]\tLoss: 269.380096\n",
      "Train Epoche: 1 [131/96 (136%)]\tLoss: 154.280380\n",
      "Train Epoche: 1 [132/96 (138%)]\tLoss: 119.759254\n",
      "Train Epoche: 1 [133/96 (139%)]\tLoss: 2.309961\n",
      "Train Epoche: 1 [134/96 (140%)]\tLoss: 3.501277\n",
      "Train Epoche: 1 [135/96 (141%)]\tLoss: 14.004040\n",
      "Train Epoche: 1 [136/96 (142%)]\tLoss: 17.683010\n",
      "Train Epoche: 1 [137/96 (143%)]\tLoss: 269.910278\n",
      "Train Epoche: 1 [138/96 (144%)]\tLoss: 3.162406\n",
      "Train Epoche: 1 [139/96 (145%)]\tLoss: 169.297409\n",
      "Train Epoche: 1 [140/96 (146%)]\tLoss: 89.750626\n",
      "Train Epoche: 1 [141/96 (147%)]\tLoss: 55.469185\n",
      "Train Epoche: 1 [142/96 (148%)]\tLoss: 124.570702\n",
      "Train Epoche: 1 [143/96 (149%)]\tLoss: 66.554916\n",
      "Train Epoche: 1 [144/96 (150%)]\tLoss: 79.055473\n",
      "Train Epoche: 1 [145/96 (151%)]\tLoss: 3.019279\n",
      "Train Epoche: 1 [146/96 (152%)]\tLoss: 3.867913\n",
      "Train Epoche: 1 [147/96 (153%)]\tLoss: 127.737373\n",
      "Train Epoche: 1 [148/96 (154%)]\tLoss: 281.708435\n",
      "Train Epoche: 1 [149/96 (155%)]\tLoss: 26.551773\n",
      "Train Epoche: 1 [150/96 (156%)]\tLoss: 110.406288\n",
      "Train Epoche: 1 [151/96 (157%)]\tLoss: 5.644299\n",
      "Train Epoche: 1 [152/96 (158%)]\tLoss: 9.138690\n",
      "Train Epoche: 1 [153/96 (159%)]\tLoss: 51.140034\n",
      "Train Epoche: 1 [154/96 (160%)]\tLoss: 40.946465\n",
      "Train Epoche: 1 [155/96 (161%)]\tLoss: 6.713937\n",
      "Train Epoche: 1 [156/96 (162%)]\tLoss: 8.408672\n",
      "Train Epoche: 1 [157/96 (164%)]\tLoss: 21.841080\n",
      "Train Epoche: 1 [158/96 (165%)]\tLoss: 42.895077\n",
      "Train Epoche: 1 [159/96 (166%)]\tLoss: 7.891000\n",
      "Train Epoche: 1 [160/96 (167%)]\tLoss: 21.858776\n",
      "Train Epoche: 1 [161/96 (168%)]\tLoss: 48.447666\n",
      "Train Epoche: 1 [162/96 (169%)]\tLoss: 44.641861\n",
      "Train Epoche: 1 [163/96 (170%)]\tLoss: 45.468231\n",
      "Train Epoche: 1 [164/96 (171%)]\tLoss: 79.805481\n",
      "Train Epoche: 1 [165/96 (172%)]\tLoss: 82.006432\n",
      "Train Epoche: 1 [166/96 (173%)]\tLoss: 32.635586\n",
      "Train Epoche: 1 [167/96 (174%)]\tLoss: 8.653912\n",
      "Train Epoche: 1 [168/96 (175%)]\tLoss: 17.782274\n",
      "Train Epoche: 1 [169/96 (176%)]\tLoss: 57.245037\n",
      "Train Epoche: 1 [170/96 (177%)]\tLoss: 9.904530\n",
      "Train Epoche: 1 [171/96 (178%)]\tLoss: 94.747719\n",
      "Train Epoche: 1 [172/96 (179%)]\tLoss: 63.193821\n",
      "Train Epoche: 1 [173/96 (180%)]\tLoss: 27.651226\n",
      "Train Epoche: 1 [174/96 (181%)]\tLoss: 190.650299\n",
      "Train Epoche: 1 [175/96 (182%)]\tLoss: 6.563737\n",
      "Train Epoche: 1 [176/96 (183%)]\tLoss: 7.863188\n",
      "Train Epoche: 1 [177/96 (184%)]\tLoss: 4.192805\n",
      "Train Epoche: 1 [178/96 (185%)]\tLoss: 6.040732\n",
      "Train Epoche: 1 [179/96 (186%)]\tLoss: 16.242849\n",
      "Train Epoche: 1 [180/96 (188%)]\tLoss: 55.252258\n",
      "Train Epoche: 1 [181/96 (189%)]\tLoss: 21.955263\n",
      "Train Epoche: 1 [182/96 (190%)]\tLoss: 10.503697\n",
      "Train Epoche: 1 [183/96 (191%)]\tLoss: 9.557072\n",
      "Train Epoche: 1 [184/96 (192%)]\tLoss: 123.338585\n",
      "Train Epoche: 1 [185/96 (193%)]\tLoss: 57.464912\n",
      "Train Epoche: 1 [186/96 (194%)]\tLoss: 43.871044\n",
      "Train Epoche: 1 [187/96 (195%)]\tLoss: 75.723969\n",
      "Train Epoche: 1 [188/96 (196%)]\tLoss: 31.993137\n",
      "Train Epoche: 1 [189/96 (197%)]\tLoss: 33.632622\n",
      "Train Epoche: 1 [190/96 (198%)]\tLoss: 12.797557\n",
      "Train Epoche: 1 [191/96 (199%)]\tLoss: 19.767502\n",
      "Train Epoche: 1 [192/96 (200%)]\tLoss: 3.455729\n",
      "Train Epoche: 1 [193/96 (201%)]\tLoss: 19.458544\n",
      "Train Epoche: 1 [194/96 (202%)]\tLoss: 3.087245\n",
      "Train Epoche: 1 [195/96 (203%)]\tLoss: 2.055921\n",
      "Train Epoche: 1 [196/96 (204%)]\tLoss: 151.208862\n",
      "Train Epoche: 1 [197/96 (205%)]\tLoss: 10.393356\n",
      "Train Epoche: 1 [198/96 (206%)]\tLoss: 9.993134\n",
      "Train Epoche: 1 [199/96 (207%)]\tLoss: 4.226743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [200/96 (208%)]\tLoss: 3.236683\n",
      "Train Epoche: 1 [201/96 (209%)]\tLoss: 6.887524\n",
      "Train Epoche: 1 [202/96 (210%)]\tLoss: 62.876732\n",
      "Train Epoche: 1 [203/96 (211%)]\tLoss: 19.031940\n",
      "Train Epoche: 1 [204/96 (212%)]\tLoss: 6.489679\n",
      "Train Epoche: 1 [205/96 (214%)]\tLoss: 68.662766\n",
      "Train Epoche: 1 [206/96 (215%)]\tLoss: 90.342331\n",
      "Train Epoche: 1 [207/96 (216%)]\tLoss: 12.834703\n",
      "Train Epoche: 1 [208/96 (217%)]\tLoss: 9.900310\n",
      "Train Epoche: 1 [209/96 (218%)]\tLoss: 23.381529\n",
      "Train Epoche: 1 [210/96 (219%)]\tLoss: 60.649937\n",
      "Train Epoche: 1 [211/96 (220%)]\tLoss: 32.461937\n",
      "Train Epoche: 1 [212/96 (221%)]\tLoss: 25.472420\n",
      "Train Epoche: 1 [213/96 (222%)]\tLoss: 26.875669\n",
      "Train Epoche: 1 [214/96 (223%)]\tLoss: 97.516403\n",
      "Train Epoche: 1 [215/96 (224%)]\tLoss: 92.421471\n",
      "Train Epoche: 1 [216/96 (225%)]\tLoss: 145.300568\n",
      "Train Epoche: 1 [217/96 (226%)]\tLoss: 118.973099\n",
      "Train Epoche: 1 [218/96 (227%)]\tLoss: 13.402946\n",
      "Train Epoche: 1 [219/96 (228%)]\tLoss: 14.841379\n",
      "Train Epoche: 1 [220/96 (229%)]\tLoss: 39.587990\n",
      "Train Epoche: 1 [221/96 (230%)]\tLoss: 35.741241\n",
      "Train Epoche: 1 [222/96 (231%)]\tLoss: 23.650850\n",
      "Train Epoche: 1 [223/96 (232%)]\tLoss: 159.091248\n",
      "Train Epoche: 1 [224/96 (233%)]\tLoss: 134.735275\n",
      "Train Epoche: 1 [225/96 (234%)]\tLoss: 2.191614\n",
      "Train Epoche: 1 [226/96 (235%)]\tLoss: 12.575030\n",
      "Train Epoche: 1 [227/96 (236%)]\tLoss: 151.452347\n",
      "Train Epoche: 1 [228/96 (238%)]\tLoss: 157.611160\n",
      "Train Epoche: 1 [229/96 (239%)]\tLoss: 28.488449\n",
      "Train Epoche: 1 [230/96 (240%)]\tLoss: 109.876236\n",
      "Train Epoche: 1 [231/96 (241%)]\tLoss: 0.411079\n",
      "Train Epoche: 1 [232/96 (242%)]\tLoss: 34.081295\n",
      "Train Epoche: 1 [233/96 (243%)]\tLoss: 93.350586\n",
      "Train Epoche: 1 [234/96 (244%)]\tLoss: 96.215706\n",
      "Train Epoche: 1 [235/96 (245%)]\tLoss: 5.600712\n",
      "Train Epoche: 1 [236/96 (246%)]\tLoss: 100.741936\n",
      "Train Epoche: 1 [237/96 (247%)]\tLoss: 401.868225\n",
      "Train Epoche: 1 [238/96 (248%)]\tLoss: 51.248386\n",
      "Train Epoche: 1 [239/96 (249%)]\tLoss: 7.445220\n",
      "Train Epoche: 1 [240/96 (250%)]\tLoss: 5.983774\n",
      "Train Epoche: 1 [241/96 (251%)]\tLoss: 29.656317\n",
      "Train Epoche: 1 [242/96 (252%)]\tLoss: 43.259586\n",
      "Train Epoche: 1 [243/96 (253%)]\tLoss: 6.246163\n",
      "Train Epoche: 1 [244/96 (254%)]\tLoss: 1.798756\n",
      "Train Epoche: 1 [245/96 (255%)]\tLoss: 62.558552\n",
      "Train Epoche: 1 [246/96 (256%)]\tLoss: 4.161519\n",
      "Train Epoche: 1 [247/96 (257%)]\tLoss: 1.884173\n",
      "Train Epoche: 1 [248/96 (258%)]\tLoss: 18.033861\n",
      "Train Epoche: 1 [249/96 (259%)]\tLoss: 6.495895\n",
      "Train Epoche: 1 [250/96 (260%)]\tLoss: 45.304943\n",
      "Train Epoche: 1 [251/96 (261%)]\tLoss: 63.726143\n",
      "Train Epoche: 1 [252/96 (262%)]\tLoss: 148.907303\n",
      "Train Epoche: 1 [253/96 (264%)]\tLoss: 82.851273\n",
      "Train Epoche: 1 [254/96 (265%)]\tLoss: 78.477631\n",
      "Train Epoche: 1 [255/96 (266%)]\tLoss: 33.071232\n",
      "Train Epoche: 1 [256/96 (267%)]\tLoss: 61.002102\n",
      "Train Epoche: 1 [257/96 (268%)]\tLoss: 13.339621\n",
      "Train Epoche: 1 [258/96 (269%)]\tLoss: 178.792450\n",
      "Train Epoche: 1 [259/96 (270%)]\tLoss: 173.467575\n",
      "Train Epoche: 1 [260/96 (271%)]\tLoss: 21.222122\n",
      "Train Epoche: 1 [261/96 (272%)]\tLoss: 25.971443\n",
      "Train Epoche: 1 [262/96 (273%)]\tLoss: 2.656537\n",
      "Train Epoche: 1 [263/96 (274%)]\tLoss: 244.175858\n",
      "Train Epoche: 1 [264/96 (275%)]\tLoss: 0.413790\n",
      "Train Epoche: 1 [265/96 (276%)]\tLoss: 0.181603\n",
      "Train Epoche: 1 [266/96 (277%)]\tLoss: 17.652004\n",
      "Train Epoche: 1 [267/96 (278%)]\tLoss: 36.337109\n",
      "Train Epoche: 1 [268/96 (279%)]\tLoss: 211.951447\n",
      "Train Epoche: 1 [269/96 (280%)]\tLoss: 5.825210\n",
      "Train Epoche: 1 [270/96 (281%)]\tLoss: 160.347885\n",
      "Train Epoche: 1 [271/96 (282%)]\tLoss: 78.722878\n",
      "Train Epoche: 1 [272/96 (283%)]\tLoss: 158.175186\n",
      "Train Epoche: 1 [273/96 (284%)]\tLoss: 290.195862\n",
      "Train Epoche: 1 [274/96 (285%)]\tLoss: 14.683030\n",
      "Train Epoche: 1 [275/96 (286%)]\tLoss: 77.241356\n",
      "Train Epoche: 1 [276/96 (288%)]\tLoss: 19.696436\n",
      "Train Epoche: 1 [277/96 (289%)]\tLoss: 57.137794\n",
      "Train Epoche: 1 [278/96 (290%)]\tLoss: 258.719635\n",
      "Train Epoche: 1 [279/96 (291%)]\tLoss: 20.083206\n",
      "Train Epoche: 1 [280/96 (292%)]\tLoss: 10.239001\n",
      "Train Epoche: 1 [281/96 (293%)]\tLoss: 6.187850\n",
      "Train Epoche: 1 [282/96 (294%)]\tLoss: 19.083784\n",
      "Train Epoche: 1 [283/96 (295%)]\tLoss: 6.331746\n",
      "Train Epoche: 1 [284/96 (296%)]\tLoss: 42.605209\n",
      "Train Epoche: 1 [285/96 (297%)]\tLoss: 2.676460\n",
      "Train Epoche: 1 [286/96 (298%)]\tLoss: 10.658171\n",
      "Train Epoche: 1 [287/96 (299%)]\tLoss: 30.984589\n",
      "Train Epoche: 1 [288/96 (300%)]\tLoss: 17.416679\n",
      "Train Epoche: 1 [289/96 (301%)]\tLoss: 40.055252\n",
      "Train Epoche: 1 [290/96 (302%)]\tLoss: 7.977068\n",
      "Train Epoche: 1 [291/96 (303%)]\tLoss: 152.777924\n",
      "Train Epoche: 1 [292/96 (304%)]\tLoss: 38.198536\n",
      "Train Epoche: 1 [293/96 (305%)]\tLoss: 71.541824\n",
      "Train Epoche: 1 [294/96 (306%)]\tLoss: 14.060234\n",
      "Train Epoche: 1 [295/96 (307%)]\tLoss: 8.603238\n",
      "Train Epoche: 1 [296/96 (308%)]\tLoss: 7.126205\n",
      "Train Epoche: 1 [297/96 (309%)]\tLoss: 3.314571\n",
      "Train Epoche: 1 [298/96 (310%)]\tLoss: 7.764393\n",
      "Train Epoche: 1 [299/96 (311%)]\tLoss: 19.464863\n",
      "Train Epoche: 1 [300/96 (312%)]\tLoss: 26.475973\n",
      "Train Epoche: 1 [301/96 (314%)]\tLoss: 27.093380\n",
      "Train Epoche: 1 [302/96 (315%)]\tLoss: 0.308101\n",
      "Train Epoche: 1 [303/96 (316%)]\tLoss: 331.211121\n",
      "Train Epoche: 1 [304/96 (317%)]\tLoss: 18.449673\n",
      "Train Epoche: 1 [305/96 (318%)]\tLoss: 7.291451\n",
      "Train Epoche: 1 [306/96 (319%)]\tLoss: 87.953430\n",
      "Train Epoche: 1 [307/96 (320%)]\tLoss: 27.948765\n",
      "Train Epoche: 1 [308/96 (321%)]\tLoss: 17.879923\n",
      "Train Epoche: 1 [309/96 (322%)]\tLoss: 5.479835\n",
      "Train Epoche: 1 [310/96 (323%)]\tLoss: 13.607803\n",
      "Train Epoche: 1 [311/96 (324%)]\tLoss: 46.321507\n",
      "Train Epoche: 1 [312/96 (325%)]\tLoss: 52.385487\n",
      "Train Epoche: 1 [313/96 (326%)]\tLoss: 104.973831\n",
      "Train Epoche: 1 [314/96 (327%)]\tLoss: 17.855272\n",
      "Train Epoche: 1 [315/96 (328%)]\tLoss: 66.237358\n",
      "Train Epoche: 1 [316/96 (329%)]\tLoss: 174.524902\n",
      "Train Epoche: 1 [317/96 (330%)]\tLoss: 12.279477\n",
      "Train Epoche: 1 [318/96 (331%)]\tLoss: 10.028682\n",
      "Train Epoche: 1 [319/96 (332%)]\tLoss: 5.537817\n",
      "Train Epoche: 1 [320/96 (333%)]\tLoss: 0.128114\n",
      "Train Epoche: 1 [321/96 (334%)]\tLoss: 2.748739\n",
      "Train Epoche: 1 [322/96 (335%)]\tLoss: 4.698868\n",
      "Train Epoche: 1 [323/96 (336%)]\tLoss: 83.237518\n",
      "Train Epoche: 1 [324/96 (338%)]\tLoss: 54.607689\n",
      "Train Epoche: 1 [325/96 (339%)]\tLoss: 159.228104\n",
      "Train Epoche: 1 [326/96 (340%)]\tLoss: 25.683142\n",
      "Train Epoche: 1 [327/96 (341%)]\tLoss: 16.573584\n",
      "Train Epoche: 1 [328/96 (342%)]\tLoss: 1.253576\n",
      "Train Epoche: 1 [329/96 (343%)]\tLoss: 10.134784\n",
      "Train Epoche: 1 [330/96 (344%)]\tLoss: 6.702277\n",
      "Train Epoche: 1 [331/96 (345%)]\tLoss: 30.133221\n",
      "Train Epoche: 1 [332/96 (346%)]\tLoss: 79.114357\n",
      "Train Epoche: 1 [333/96 (347%)]\tLoss: 80.013237\n",
      "Train Epoche: 1 [334/96 (348%)]\tLoss: 48.225491\n",
      "Train Epoche: 1 [335/96 (349%)]\tLoss: 25.658472\n",
      "Train Epoche: 1 [336/96 (350%)]\tLoss: 9.476970\n",
      "Train Epoche: 1 [337/96 (351%)]\tLoss: 4.040655\n",
      "Train Epoche: 1 [338/96 (352%)]\tLoss: 1.842513\n",
      "Train Epoche: 1 [339/96 (353%)]\tLoss: 0.232609\n",
      "Train Epoche: 1 [340/96 (354%)]\tLoss: 10.247821\n",
      "Train Epoche: 1 [341/96 (355%)]\tLoss: 2.307779\n",
      "Train Epoche: 1 [342/96 (356%)]\tLoss: 261.018616\n",
      "Train Epoche: 1 [343/96 (357%)]\tLoss: 3.548724\n",
      "Train Epoche: 1 [344/96 (358%)]\tLoss: 36.389595\n",
      "Train Epoche: 1 [345/96 (359%)]\tLoss: 11.128523\n",
      "Train Epoche: 1 [346/96 (360%)]\tLoss: 18.740538\n",
      "Train Epoche: 1 [347/96 (361%)]\tLoss: 5.630981\n",
      "Train Epoche: 1 [348/96 (362%)]\tLoss: 1.769244\n",
      "Train Epoche: 1 [349/96 (364%)]\tLoss: 61.168369\n",
      "Train Epoche: 1 [350/96 (365%)]\tLoss: 26.335438\n",
      "Train Epoche: 1 [351/96 (366%)]\tLoss: 0.273125\n",
      "Train Epoche: 1 [352/96 (367%)]\tLoss: 181.560287\n",
      "Train Epoche: 1 [353/96 (368%)]\tLoss: 118.117599\n",
      "Train Epoche: 1 [354/96 (369%)]\tLoss: 26.792593\n",
      "Train Epoche: 1 [355/96 (370%)]\tLoss: 0.449411\n",
      "Train Epoche: 1 [356/96 (371%)]\tLoss: 4.197030\n",
      "Train Epoche: 1 [357/96 (372%)]\tLoss: 36.148983\n",
      "Train Epoche: 1 [358/96 (373%)]\tLoss: 20.879963\n",
      "Train Epoche: 1 [359/96 (374%)]\tLoss: 64.529541\n",
      "Train Epoche: 1 [360/96 (375%)]\tLoss: 6.948615\n",
      "Train Epoche: 1 [361/96 (376%)]\tLoss: 5.939639\n",
      "Train Epoche: 1 [362/96 (377%)]\tLoss: 66.633514\n",
      "Train Epoche: 1 [363/96 (378%)]\tLoss: 6.419188\n",
      "Train Epoche: 1 [364/96 (379%)]\tLoss: 42.393761\n",
      "Train Epoche: 1 [365/96 (380%)]\tLoss: 20.832026\n",
      "Train Epoche: 1 [366/96 (381%)]\tLoss: 15.836774\n",
      "Train Epoche: 1 [367/96 (382%)]\tLoss: 0.227724\n",
      "Train Epoche: 1 [368/96 (383%)]\tLoss: 3.762212\n",
      "Train Epoche: 1 [369/96 (384%)]\tLoss: 24.980581\n",
      "Train Epoche: 1 [370/96 (385%)]\tLoss: 12.196184\n",
      "Train Epoche: 1 [371/96 (386%)]\tLoss: 24.468479\n",
      "Train Epoche: 1 [372/96 (388%)]\tLoss: 10.594608\n",
      "Train Epoche: 1 [373/96 (389%)]\tLoss: 53.123821\n",
      "Train Epoche: 1 [374/96 (390%)]\tLoss: 77.210129\n",
      "Train Epoche: 1 [375/96 (391%)]\tLoss: 19.343067\n",
      "Train Epoche: 1 [376/96 (392%)]\tLoss: 3.894592\n",
      "Train Epoche: 1 [377/96 (393%)]\tLoss: 4.023750\n",
      "Train Epoche: 1 [378/96 (394%)]\tLoss: 10.745700\n",
      "Train Epoche: 1 [379/96 (395%)]\tLoss: 30.101814\n",
      "Train Epoche: 1 [380/96 (396%)]\tLoss: 28.846443\n",
      "Train Epoche: 1 [381/96 (397%)]\tLoss: 42.305920\n",
      "Train Epoche: 1 [382/96 (398%)]\tLoss: 62.614437\n",
      "Train Epoche: 1 [383/96 (399%)]\tLoss: 57.689198\n",
      "Train Epoche: 1 [384/96 (400%)]\tLoss: 4.458525\n",
      "Train Epoche: 1 [385/96 (401%)]\tLoss: 12.791842\n",
      "Train Epoche: 1 [386/96 (402%)]\tLoss: 4.917845\n",
      "Train Epoche: 1 [387/96 (403%)]\tLoss: 254.253296\n",
      "Train Epoche: 1 [388/96 (404%)]\tLoss: 10.671467\n",
      "Train Epoche: 1 [389/96 (405%)]\tLoss: 25.731989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [390/96 (406%)]\tLoss: 39.738010\n",
      "Train Epoche: 1 [391/96 (407%)]\tLoss: 10.906814\n",
      "Train Epoche: 1 [392/96 (408%)]\tLoss: 7.234331\n",
      "Train Epoche: 1 [393/96 (409%)]\tLoss: 0.992789\n",
      "Train Epoche: 1 [394/96 (410%)]\tLoss: 183.936554\n",
      "Train Epoche: 1 [395/96 (411%)]\tLoss: 17.678335\n",
      "Train Epoche: 1 [396/96 (412%)]\tLoss: 41.678051\n",
      "Train Epoche: 1 [397/96 (414%)]\tLoss: 143.148697\n",
      "Train Epoche: 1 [398/96 (415%)]\tLoss: 93.661591\n",
      "Train Epoche: 1 [399/96 (416%)]\tLoss: 40.389679\n",
      "Train Epoche: 1 [400/96 (417%)]\tLoss: 21.447989\n",
      "Train Epoche: 1 [401/96 (418%)]\tLoss: 2.849973\n",
      "Train Epoche: 1 [402/96 (419%)]\tLoss: 2.537155\n",
      "Train Epoche: 1 [403/96 (420%)]\tLoss: 23.462650\n",
      "Train Epoche: 1 [404/96 (421%)]\tLoss: 0.755511\n",
      "Train Epoche: 1 [405/96 (422%)]\tLoss: 0.115567\n",
      "Train Epoche: 1 [406/96 (423%)]\tLoss: 5.159516\n",
      "Train Epoche: 1 [407/96 (424%)]\tLoss: 40.537903\n",
      "Train Epoche: 1 [408/96 (425%)]\tLoss: 39.596500\n",
      "Train Epoche: 1 [409/96 (426%)]\tLoss: 275.117096\n",
      "Train Epoche: 1 [410/96 (427%)]\tLoss: 207.200348\n",
      "Train Epoche: 1 [411/96 (428%)]\tLoss: 9.717478\n",
      "Train Epoche: 1 [412/96 (429%)]\tLoss: 86.684074\n",
      "Train Epoche: 1 [413/96 (430%)]\tLoss: 0.834401\n",
      "Train Epoche: 1 [414/96 (431%)]\tLoss: 145.663361\n",
      "Train Epoche: 1 [415/96 (432%)]\tLoss: 182.774979\n",
      "Train Epoche: 1 [416/96 (433%)]\tLoss: 92.244576\n",
      "Train Epoche: 1 [417/96 (434%)]\tLoss: 32.209126\n",
      "Train Epoche: 1 [418/96 (435%)]\tLoss: 1.287473\n",
      "Train Epoche: 1 [419/96 (436%)]\tLoss: 74.850136\n",
      "Train Epoche: 1 [420/96 (438%)]\tLoss: 67.021255\n",
      "Train Epoche: 1 [421/96 (439%)]\tLoss: 0.392835\n",
      "Train Epoche: 1 [422/96 (440%)]\tLoss: 8.978651\n",
      "Train Epoche: 1 [423/96 (441%)]\tLoss: 1.109970\n",
      "Train Epoche: 1 [424/96 (442%)]\tLoss: 20.282772\n",
      "Train Epoche: 1 [425/96 (443%)]\tLoss: 63.916164\n",
      "Train Epoche: 1 [426/96 (444%)]\tLoss: 30.519079\n",
      "Train Epoche: 1 [427/96 (445%)]\tLoss: 3.020324\n",
      "Train Epoche: 1 [428/96 (446%)]\tLoss: 32.424698\n",
      "Train Epoche: 1 [429/96 (447%)]\tLoss: 21.175905\n",
      "Train Epoche: 1 [430/96 (448%)]\tLoss: 0.830493\n",
      "Train Epoche: 1 [431/96 (449%)]\tLoss: 4.680315\n",
      "Train Epoche: 1 [432/96 (450%)]\tLoss: 195.516251\n",
      "Train Epoche: 1 [433/96 (451%)]\tLoss: 1.260074\n",
      "Train Epoche: 1 [434/96 (452%)]\tLoss: 38.788170\n",
      "Train Epoche: 1 [435/96 (453%)]\tLoss: 49.526913\n",
      "Train Epoche: 1 [436/96 (454%)]\tLoss: 27.261133\n",
      "Train Epoche: 1 [437/96 (455%)]\tLoss: 23.354895\n",
      "Train Epoche: 1 [438/96 (456%)]\tLoss: 19.750105\n",
      "Train Epoche: 1 [439/96 (457%)]\tLoss: 71.234886\n",
      "Train Epoche: 1 [440/96 (458%)]\tLoss: 0.354808\n",
      "Train Epoche: 1 [441/96 (459%)]\tLoss: 39.116184\n",
      "Train Epoche: 1 [442/96 (460%)]\tLoss: 49.474957\n",
      "Train Epoche: 1 [443/96 (461%)]\tLoss: 0.939487\n",
      "Train Epoche: 1 [444/96 (462%)]\tLoss: 15.447281\n",
      "Train Epoche: 1 [445/96 (464%)]\tLoss: 4.911352\n",
      "Train Epoche: 1 [446/96 (465%)]\tLoss: 2.629600\n",
      "Train Epoche: 1 [447/96 (466%)]\tLoss: 143.901672\n",
      "Train Epoche: 1 [448/96 (467%)]\tLoss: 0.367446\n",
      "Train Epoche: 1 [449/96 (468%)]\tLoss: 46.989529\n",
      "Train Epoche: 1 [450/96 (469%)]\tLoss: 63.201641\n",
      "Train Epoche: 1 [451/96 (470%)]\tLoss: 12.109079\n",
      "Train Epoche: 1 [452/96 (471%)]\tLoss: 10.487670\n",
      "Train Epoche: 1 [453/96 (472%)]\tLoss: 5.904642\n",
      "Train Epoche: 1 [454/96 (473%)]\tLoss: 37.709602\n",
      "Train Epoche: 1 [455/96 (474%)]\tLoss: 1.837899\n",
      "Train Epoche: 1 [456/96 (475%)]\tLoss: 0.547285\n",
      "Train Epoche: 1 [457/96 (476%)]\tLoss: 36.110004\n",
      "Train Epoche: 1 [458/96 (477%)]\tLoss: 42.313816\n",
      "Train Epoche: 1 [459/96 (478%)]\tLoss: 38.972008\n",
      "Train Epoche: 1 [460/96 (479%)]\tLoss: 80.014061\n",
      "Train Epoche: 1 [461/96 (480%)]\tLoss: 90.772392\n",
      "Train Epoche: 1 [462/96 (481%)]\tLoss: 53.933361\n",
      "Train Epoche: 1 [463/96 (482%)]\tLoss: 0.365145\n",
      "Train Epoche: 1 [464/96 (483%)]\tLoss: 25.479361\n",
      "Train Epoche: 1 [465/96 (484%)]\tLoss: 56.449387\n",
      "Train Epoche: 1 [466/96 (485%)]\tLoss: 44.912632\n",
      "Train Epoche: 1 [467/96 (486%)]\tLoss: 74.688049\n",
      "Train Epoche: 1 [468/96 (488%)]\tLoss: 155.352722\n",
      "Train Epoche: 1 [469/96 (489%)]\tLoss: 25.670101\n",
      "Train Epoche: 1 [470/96 (490%)]\tLoss: 2.532174\n",
      "Train Epoche: 1 [471/96 (491%)]\tLoss: 140.695465\n",
      "Train Epoche: 1 [472/96 (492%)]\tLoss: 0.629053\n",
      "Train Epoche: 1 [473/96 (493%)]\tLoss: 4.540009\n",
      "Train Epoche: 1 [474/96 (494%)]\tLoss: 4.636611\n",
      "Train Epoche: 1 [475/96 (495%)]\tLoss: 14.564972\n",
      "Train Epoche: 1 [476/96 (496%)]\tLoss: 35.734146\n",
      "Train Epoche: 1 [477/96 (497%)]\tLoss: 7.862564\n",
      "Train Epoche: 1 [478/96 (498%)]\tLoss: 53.465618\n",
      "Train Epoche: 1 [479/96 (499%)]\tLoss: 156.851974\n",
      "Train Epoche: 1 [480/96 (500%)]\tLoss: 83.560585\n",
      "Train Epoche: 1 [481/96 (501%)]\tLoss: 104.250870\n",
      "Train Epoche: 1 [482/96 (502%)]\tLoss: 73.782257\n",
      "Train Epoche: 1 [483/96 (503%)]\tLoss: 0.432056\n",
      "Train Epoche: 1 [484/96 (504%)]\tLoss: 2.415503\n",
      "Train Epoche: 1 [485/96 (505%)]\tLoss: 28.667948\n",
      "Train Epoche: 1 [486/96 (506%)]\tLoss: 3.720871\n",
      "Train Epoche: 1 [487/96 (507%)]\tLoss: 11.376498\n",
      "Train Epoche: 1 [488/96 (508%)]\tLoss: 0.340699\n",
      "Train Epoche: 1 [489/96 (509%)]\tLoss: 1.160419\n",
      "Train Epoche: 1 [490/96 (510%)]\tLoss: 24.093582\n",
      "Train Epoche: 1 [491/96 (511%)]\tLoss: 14.103584\n",
      "Train Epoche: 1 [492/96 (512%)]\tLoss: 1.238472\n",
      "Train Epoche: 1 [493/96 (514%)]\tLoss: 156.072968\n",
      "Train Epoche: 1 [494/96 (515%)]\tLoss: 0.083242\n",
      "Train Epoche: 1 [495/96 (516%)]\tLoss: 15.879670\n",
      "Train Epoche: 1 [496/96 (517%)]\tLoss: 2.109090\n",
      "Train Epoche: 1 [497/96 (518%)]\tLoss: 0.854017\n",
      "Train Epoche: 1 [498/96 (519%)]\tLoss: 24.562696\n",
      "Train Epoche: 1 [499/96 (520%)]\tLoss: 220.890884\n",
      "Train Epoche: 1 [500/96 (521%)]\tLoss: 123.461983\n",
      "Train Epoche: 1 [501/96 (522%)]\tLoss: 129.801895\n",
      "Train Epoche: 1 [502/96 (523%)]\tLoss: 81.211769\n",
      "Train Epoche: 1 [503/96 (524%)]\tLoss: 70.305145\n",
      "Train Epoche: 1 [504/96 (525%)]\tLoss: 1.396274\n",
      "Train Epoche: 1 [505/96 (526%)]\tLoss: 1.211254\n",
      "Train Epoche: 1 [506/96 (527%)]\tLoss: 4.651748\n",
      "Train Epoche: 1 [507/96 (528%)]\tLoss: 33.391441\n",
      "Train Epoche: 1 [508/96 (529%)]\tLoss: 8.355627\n",
      "Train Epoche: 1 [509/96 (530%)]\tLoss: 69.323128\n",
      "Train Epoche: 1 [510/96 (531%)]\tLoss: 51.643208\n",
      "Train Epoche: 1 [511/96 (532%)]\tLoss: 242.224777\n",
      "Train Epoche: 1 [512/96 (533%)]\tLoss: 8.722497\n",
      "Train Epoche: 1 [513/96 (534%)]\tLoss: 3.665058\n",
      "Train Epoche: 1 [514/96 (535%)]\tLoss: 2.645011\n",
      "Train Epoche: 1 [515/96 (536%)]\tLoss: 315.238190\n",
      "Train Epoche: 1 [516/96 (538%)]\tLoss: 22.946217\n",
      "Train Epoche: 1 [517/96 (539%)]\tLoss: 16.963022\n",
      "Train Epoche: 1 [518/96 (540%)]\tLoss: 175.629929\n",
      "Train Epoche: 1 [519/96 (541%)]\tLoss: 0.538691\n",
      "Train Epoche: 1 [520/96 (542%)]\tLoss: 148.338989\n",
      "Train Epoche: 1 [521/96 (543%)]\tLoss: 189.504395\n",
      "Train Epoche: 1 [522/96 (544%)]\tLoss: 1.271866\n",
      "Train Epoche: 1 [523/96 (545%)]\tLoss: 63.328968\n",
      "Train Epoche: 1 [524/96 (546%)]\tLoss: 14.561657\n",
      "Train Epoche: 1 [525/96 (547%)]\tLoss: 12.559019\n",
      "Train Epoche: 1 [526/96 (548%)]\tLoss: 30.949497\n",
      "Train Epoche: 1 [527/96 (549%)]\tLoss: 34.576160\n",
      "Train Epoche: 1 [528/96 (550%)]\tLoss: 2.380098\n",
      "Train Epoche: 1 [529/96 (551%)]\tLoss: 1.453879\n",
      "Train Epoche: 1 [530/96 (552%)]\tLoss: 180.103806\n",
      "Train Epoche: 1 [531/96 (553%)]\tLoss: 19.471251\n",
      "Train Epoche: 1 [532/96 (554%)]\tLoss: 0.542020\n",
      "Train Epoche: 1 [533/96 (555%)]\tLoss: 11.356421\n",
      "Train Epoche: 1 [534/96 (556%)]\tLoss: 8.204367\n",
      "Train Epoche: 1 [535/96 (557%)]\tLoss: 3.014565\n",
      "Train Epoche: 1 [536/96 (558%)]\tLoss: 23.605499\n",
      "Train Epoche: 1 [537/96 (559%)]\tLoss: 23.287443\n",
      "Train Epoche: 1 [538/96 (560%)]\tLoss: 0.886155\n",
      "Train Epoche: 1 [539/96 (561%)]\tLoss: 6.619427\n",
      "Train Epoche: 1 [540/96 (562%)]\tLoss: 22.069838\n",
      "Train Epoche: 1 [541/96 (564%)]\tLoss: 48.157986\n",
      "Train Epoche: 1 [542/96 (565%)]\tLoss: 7.157869\n",
      "Train Epoche: 1 [543/96 (566%)]\tLoss: 384.668793\n",
      "Train Epoche: 1 [544/96 (567%)]\tLoss: 56.126995\n",
      "Train Epoche: 1 [545/96 (568%)]\tLoss: 11.634396\n",
      "Train Epoche: 1 [546/96 (569%)]\tLoss: 1.871672\n",
      "Train Epoche: 1 [547/96 (570%)]\tLoss: 325.215942\n",
      "Train Epoche: 1 [548/96 (571%)]\tLoss: 14.221364\n",
      "Train Epoche: 1 [549/96 (572%)]\tLoss: 4.407672\n",
      "Train Epoche: 1 [550/96 (573%)]\tLoss: 8.860484\n",
      "Train Epoche: 1 [551/96 (574%)]\tLoss: 58.672806\n",
      "Train Epoche: 1 [552/96 (575%)]\tLoss: 30.827791\n",
      "Train Epoche: 1 [553/96 (576%)]\tLoss: 57.889061\n",
      "Train Epoche: 1 [554/96 (577%)]\tLoss: 49.131226\n",
      "Train Epoche: 1 [555/96 (578%)]\tLoss: 0.852766\n",
      "Train Epoche: 1 [556/96 (579%)]\tLoss: 33.239716\n",
      "Train Epoche: 1 [557/96 (580%)]\tLoss: 39.242126\n",
      "Train Epoche: 1 [558/96 (581%)]\tLoss: 45.176216\n",
      "Train Epoche: 1 [559/96 (582%)]\tLoss: 41.739311\n",
      "Train Epoche: 1 [560/96 (583%)]\tLoss: 1.269018\n",
      "Train Epoche: 1 [561/96 (584%)]\tLoss: 198.223633\n",
      "Train Epoche: 1 [562/96 (585%)]\tLoss: 37.751312\n",
      "Train Epoche: 1 [563/96 (586%)]\tLoss: 6.782184\n",
      "Train Epoche: 1 [564/96 (588%)]\tLoss: 57.214203\n",
      "Train Epoche: 1 [565/96 (589%)]\tLoss: 56.519630\n",
      "Train Epoche: 1 [566/96 (590%)]\tLoss: 42.178020\n",
      "Train Epoche: 1 [567/96 (591%)]\tLoss: 0.306220\n",
      "Train Epoche: 1 [568/96 (592%)]\tLoss: 0.350467\n",
      "Train Epoche: 1 [569/96 (593%)]\tLoss: 73.146400\n",
      "Train Epoche: 1 [570/96 (594%)]\tLoss: 42.410698\n",
      "Train Epoche: 1 [571/96 (595%)]\tLoss: 15.378475\n",
      "Train Epoche: 1 [572/96 (596%)]\tLoss: 59.017952\n",
      "Train Epoche: 1 [573/96 (597%)]\tLoss: 27.753294\n",
      "Train Epoche: 1 [574/96 (598%)]\tLoss: 15.859870\n",
      "Train Epoche: 1 [575/96 (599%)]\tLoss: 156.563889\n",
      "Train Epoche: 1 [576/96 (600%)]\tLoss: 99.545456\n",
      "Train Epoche: 1 [577/96 (601%)]\tLoss: 0.067568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [578/96 (602%)]\tLoss: 0.244583\n",
      "Train Epoche: 1 [579/96 (603%)]\tLoss: 4.498936\n",
      "Train Epoche: 1 [580/96 (604%)]\tLoss: 1.477750\n",
      "Train Epoche: 1 [581/96 (605%)]\tLoss: 4.754729\n",
      "Train Epoche: 1 [582/96 (606%)]\tLoss: 75.632660\n",
      "Train Epoche: 1 [583/96 (607%)]\tLoss: 0.774184\n",
      "Train Epoche: 1 [584/96 (608%)]\tLoss: 20.962582\n",
      "Train Epoche: 1 [585/96 (609%)]\tLoss: 1.482574\n",
      "Train Epoche: 1 [586/96 (610%)]\tLoss: 38.272060\n",
      "Train Epoche: 1 [587/96 (611%)]\tLoss: 2.385363\n",
      "Train Epoche: 1 [588/96 (612%)]\tLoss: 6.680233\n",
      "Train Epoche: 1 [589/96 (614%)]\tLoss: 46.327820\n",
      "Train Epoche: 1 [590/96 (615%)]\tLoss: 41.965134\n",
      "Train Epoche: 1 [591/96 (616%)]\tLoss: 21.344353\n",
      "Train Epoche: 1 [592/96 (617%)]\tLoss: 69.510475\n",
      "Train Epoche: 1 [593/96 (618%)]\tLoss: 16.561840\n",
      "Train Epoche: 1 [594/96 (619%)]\tLoss: 46.690796\n",
      "Train Epoche: 1 [595/96 (620%)]\tLoss: 18.744669\n",
      "Train Epoche: 1 [596/96 (621%)]\tLoss: 121.747429\n",
      "Train Epoche: 1 [597/96 (622%)]\tLoss: 44.413128\n",
      "Train Epoche: 1 [598/96 (623%)]\tLoss: 0.936171\n",
      "Train Epoche: 1 [599/96 (624%)]\tLoss: 25.961674\n",
      "Train Epoche: 1 [600/96 (625%)]\tLoss: 49.225979\n",
      "Train Epoche: 1 [601/96 (626%)]\tLoss: 16.251526\n",
      "Train Epoche: 1 [602/96 (627%)]\tLoss: 0.540934\n",
      "Train Epoche: 1 [603/96 (628%)]\tLoss: 0.127975\n",
      "Train Epoche: 1 [604/96 (629%)]\tLoss: 20.542702\n",
      "Train Epoche: 1 [605/96 (630%)]\tLoss: 8.880001\n",
      "Train Epoche: 1 [606/96 (631%)]\tLoss: 0.058330\n",
      "Train Epoche: 1 [607/96 (632%)]\tLoss: 6.321332\n",
      "Train Epoche: 1 [608/96 (633%)]\tLoss: 312.111053\n",
      "Train Epoche: 1 [609/96 (634%)]\tLoss: 198.662521\n",
      "Train Epoche: 1 [610/96 (635%)]\tLoss: 0.464747\n",
      "Train Epoche: 1 [611/96 (636%)]\tLoss: 0.794733\n",
      "Train Epoche: 1 [612/96 (638%)]\tLoss: 182.975632\n",
      "Train Epoche: 1 [613/96 (639%)]\tLoss: 59.814285\n",
      "Train Epoche: 1 [614/96 (640%)]\tLoss: 71.625076\n",
      "Train Epoche: 1 [615/96 (641%)]\tLoss: 112.545486\n",
      "Train Epoche: 1 [616/96 (642%)]\tLoss: 66.142395\n",
      "Train Epoche: 1 [617/96 (643%)]\tLoss: 0.555817\n",
      "Train Epoche: 1 [618/96 (644%)]\tLoss: 94.080330\n",
      "Train Epoche: 1 [619/96 (645%)]\tLoss: 79.259743\n",
      "Train Epoche: 1 [620/96 (646%)]\tLoss: 15.169274\n",
      "Train Epoche: 1 [621/96 (647%)]\tLoss: 2.143758\n",
      "Train Epoche: 1 [622/96 (648%)]\tLoss: 28.598841\n",
      "Train Epoche: 1 [623/96 (649%)]\tLoss: 4.498693\n",
      "Train Epoche: 1 [624/96 (650%)]\tLoss: 3.920704\n",
      "Train Epoche: 1 [625/96 (651%)]\tLoss: 12.538115\n",
      "Train Epoche: 1 [626/96 (652%)]\tLoss: 367.002472\n",
      "Train Epoche: 1 [627/96 (653%)]\tLoss: 0.786298\n",
      "Train Epoche: 1 [628/96 (654%)]\tLoss: 11.402912\n",
      "Train Epoche: 1 [629/96 (655%)]\tLoss: 13.947953\n",
      "Train Epoche: 1 [630/96 (656%)]\tLoss: 0.186871\n",
      "Train Epoche: 1 [631/96 (657%)]\tLoss: 49.602894\n",
      "Train Epoche: 1 [632/96 (658%)]\tLoss: 30.440470\n",
      "Train Epoche: 1 [633/96 (659%)]\tLoss: 0.087378\n",
      "Train Epoche: 1 [634/96 (660%)]\tLoss: 12.092578\n",
      "Train Epoche: 1 [635/96 (661%)]\tLoss: 49.510120\n",
      "Train Epoche: 1 [636/96 (662%)]\tLoss: 33.261700\n",
      "Train Epoche: 1 [637/96 (664%)]\tLoss: 0.437451\n",
      "Train Epoche: 1 [638/96 (665%)]\tLoss: 13.573231\n",
      "Train Epoche: 1 [639/96 (666%)]\tLoss: 2.003630\n",
      "Train Epoche: 1 [640/96 (667%)]\tLoss: 3.294629\n",
      "Train Epoche: 1 [641/96 (668%)]\tLoss: 9.114635\n",
      "Train Epoche: 1 [642/96 (669%)]\tLoss: 39.887539\n",
      "Train Epoche: 1 [643/96 (670%)]\tLoss: 47.047794\n",
      "Train Epoche: 1 [644/96 (671%)]\tLoss: 0.390788\n",
      "Train Epoche: 1 [645/96 (672%)]\tLoss: 58.599876\n",
      "Train Epoche: 1 [646/96 (673%)]\tLoss: 2.473139\n",
      "Train Epoche: 1 [647/96 (674%)]\tLoss: 11.040300\n",
      "Train Epoche: 1 [648/96 (675%)]\tLoss: 11.880112\n",
      "Train Epoche: 1 [649/96 (676%)]\tLoss: 424.546143\n",
      "Train Epoche: 1 [650/96 (677%)]\tLoss: 69.724709\n",
      "Train Epoche: 1 [651/96 (678%)]\tLoss: 91.402611\n",
      "Train Epoche: 1 [652/96 (679%)]\tLoss: 113.482285\n",
      "Train Epoche: 1 [653/96 (680%)]\tLoss: 113.379364\n",
      "Train Epoche: 1 [654/96 (681%)]\tLoss: 53.742153\n",
      "Train Epoche: 1 [655/96 (682%)]\tLoss: 5.273561\n",
      "Train Epoche: 1 [656/96 (683%)]\tLoss: 39.835541\n",
      "Train Epoche: 1 [657/96 (684%)]\tLoss: 86.292816\n",
      "Train Epoche: 1 [658/96 (685%)]\tLoss: 62.503010\n",
      "Train Epoche: 1 [659/96 (686%)]\tLoss: 52.440041\n",
      "Train Epoche: 1 [660/96 (688%)]\tLoss: 66.726128\n",
      "Train Epoche: 1 [661/96 (689%)]\tLoss: 54.644478\n",
      "Train Epoche: 1 [662/96 (690%)]\tLoss: 84.847496\n",
      "Train Epoche: 1 [663/96 (691%)]\tLoss: 32.432072\n",
      "Train Epoche: 1 [664/96 (692%)]\tLoss: 17.881256\n",
      "Train Epoche: 1 [665/96 (693%)]\tLoss: 54.403469\n",
      "Train Epoche: 1 [666/96 (694%)]\tLoss: 16.542728\n",
      "Train Epoche: 1 [667/96 (695%)]\tLoss: 7.365330\n",
      "Train Epoche: 1 [668/96 (696%)]\tLoss: 4.576736\n",
      "Train Epoche: 1 [669/96 (697%)]\tLoss: 24.110260\n",
      "Train Epoche: 1 [670/96 (698%)]\tLoss: 5.300917\n",
      "Train Epoche: 1 [671/96 (699%)]\tLoss: 6.775187\n",
      "Train Epoche: 1 [672/96 (700%)]\tLoss: 20.010979\n",
      "Train Epoche: 1 [673/96 (701%)]\tLoss: 1.808645\n",
      "Train Epoche: 1 [674/96 (702%)]\tLoss: 3.585095\n",
      "Train Epoche: 1 [675/96 (703%)]\tLoss: 5.325782\n",
      "Train Epoche: 1 [676/96 (704%)]\tLoss: 13.982331\n",
      "Train Epoche: 1 [677/96 (705%)]\tLoss: 4.143784\n",
      "Train Epoche: 1 [678/96 (706%)]\tLoss: 24.724878\n",
      "Train Epoche: 1 [679/96 (707%)]\tLoss: 26.784040\n",
      "Train Epoche: 1 [680/96 (708%)]\tLoss: 76.109718\n",
      "Train Epoche: 1 [681/96 (709%)]\tLoss: 89.335487\n",
      "Train Epoche: 1 [682/96 (710%)]\tLoss: 74.482651\n",
      "Train Epoche: 1 [683/96 (711%)]\tLoss: 14.999468\n",
      "Train Epoche: 1 [684/96 (712%)]\tLoss: 15.250555\n",
      "Train Epoche: 1 [685/96 (714%)]\tLoss: 30.788414\n",
      "Train Epoche: 1 [686/96 (715%)]\tLoss: 30.984930\n",
      "Train Epoche: 1 [687/96 (716%)]\tLoss: 95.919861\n",
      "Train Epoche: 1 [688/96 (717%)]\tLoss: 0.304202\n",
      "Train Epoche: 1 [689/96 (718%)]\tLoss: 72.733849\n",
      "Train Epoche: 1 [690/96 (719%)]\tLoss: 30.487572\n",
      "Train Epoche: 1 [691/96 (720%)]\tLoss: 41.201763\n",
      "Train Epoche: 1 [692/96 (721%)]\tLoss: 125.592628\n",
      "Train Epoche: 1 [693/96 (722%)]\tLoss: 26.021599\n",
      "Train Epoche: 1 [694/96 (723%)]\tLoss: 45.387169\n",
      "Train Epoche: 1 [695/96 (724%)]\tLoss: 26.171915\n",
      "Train Epoche: 1 [696/96 (725%)]\tLoss: 40.862694\n",
      "Train Epoche: 1 [697/96 (726%)]\tLoss: 0.075040\n",
      "Train Epoche: 1 [698/96 (727%)]\tLoss: 11.786102\n",
      "Train Epoche: 1 [699/96 (728%)]\tLoss: 35.984909\n",
      "Train Epoche: 1 [700/96 (729%)]\tLoss: 97.150955\n",
      "Train Epoche: 1 [701/96 (730%)]\tLoss: 57.758720\n",
      "Train Epoche: 1 [702/96 (731%)]\tLoss: 5.121910\n",
      "Train Epoche: 1 [703/96 (732%)]\tLoss: 1.796303\n",
      "Train Epoche: 1 [704/96 (733%)]\tLoss: 130.056137\n",
      "Train Epoche: 1 [705/96 (734%)]\tLoss: 5.775567\n",
      "Train Epoche: 1 [706/96 (735%)]\tLoss: 32.168953\n",
      "Train Epoche: 1 [707/96 (736%)]\tLoss: 66.282120\n",
      "Train Epoche: 1 [708/96 (738%)]\tLoss: 6.288291\n",
      "Train Epoche: 1 [709/96 (739%)]\tLoss: 0.793582\n",
      "Train Epoche: 1 [710/96 (740%)]\tLoss: 22.931292\n",
      "Train Epoche: 1 [711/96 (741%)]\tLoss: 4.309388\n",
      "Train Epoche: 1 [712/96 (742%)]\tLoss: 10.452981\n",
      "Train Epoche: 1 [713/96 (743%)]\tLoss: 32.119297\n",
      "Train Epoche: 1 [714/96 (744%)]\tLoss: 0.045803\n",
      "Train Epoche: 1 [715/96 (745%)]\tLoss: 0.089793\n",
      "Train Epoche: 1 [716/96 (746%)]\tLoss: 20.508543\n",
      "Train Epoche: 1 [717/96 (747%)]\tLoss: 22.186234\n",
      "Train Epoche: 1 [718/96 (748%)]\tLoss: 45.889256\n",
      "Train Epoche: 1 [719/96 (749%)]\tLoss: 0.147566\n",
      "Train Epoche: 1 [720/96 (750%)]\tLoss: 64.308411\n",
      "Train Epoche: 1 [721/96 (751%)]\tLoss: 63.279083\n",
      "Train Epoche: 1 [722/96 (752%)]\tLoss: 54.063934\n",
      "Train Epoche: 1 [723/96 (753%)]\tLoss: 1.408846\n",
      "Train Epoche: 1 [724/96 (754%)]\tLoss: 2.581244\n",
      "Train Epoche: 1 [725/96 (755%)]\tLoss: 21.389505\n",
      "Train Epoche: 1 [726/96 (756%)]\tLoss: 14.228602\n",
      "Train Epoche: 1 [727/96 (757%)]\tLoss: 0.173209\n",
      "Train Epoche: 1 [728/96 (758%)]\tLoss: 210.275543\n",
      "Train Epoche: 1 [729/96 (759%)]\tLoss: 242.273193\n",
      "Train Epoche: 1 [730/96 (760%)]\tLoss: 1.056004\n",
      "Train Epoche: 1 [731/96 (761%)]\tLoss: 0.442300\n",
      "Train Epoche: 1 [732/96 (762%)]\tLoss: 34.395729\n",
      "Train Epoche: 1 [733/96 (764%)]\tLoss: 5.316687\n",
      "Train Epoche: 1 [734/96 (765%)]\tLoss: 32.490627\n",
      "Train Epoche: 1 [735/96 (766%)]\tLoss: 1.660013\n",
      "Train Epoche: 1 [736/96 (767%)]\tLoss: 1.425849\n",
      "Train Epoche: 1 [737/96 (768%)]\tLoss: 15.510533\n",
      "Train Epoche: 1 [738/96 (769%)]\tLoss: 93.397926\n",
      "Train Epoche: 1 [739/96 (770%)]\tLoss: 18.144730\n",
      "Train Epoche: 1 [740/96 (771%)]\tLoss: 29.602531\n",
      "Train Epoche: 1 [741/96 (772%)]\tLoss: 1.633665\n",
      "Train Epoche: 1 [742/96 (773%)]\tLoss: 252.316650\n",
      "Train Epoche: 1 [743/96 (774%)]\tLoss: 5.583091\n",
      "Train Epoche: 1 [744/96 (775%)]\tLoss: 3.857521\n",
      "Train Epoche: 1 [745/96 (776%)]\tLoss: 37.162727\n",
      "Train Epoche: 1 [746/96 (777%)]\tLoss: 44.907127\n",
      "Train Epoche: 1 [747/96 (778%)]\tLoss: 30.857830\n",
      "Train Epoche: 1 [748/96 (779%)]\tLoss: 1.631221\n",
      "Train Epoche: 1 [749/96 (780%)]\tLoss: 2.194299\n",
      "Train Epoche: 1 [750/96 (781%)]\tLoss: 7.448163\n",
      "Train Epoche: 1 [751/96 (782%)]\tLoss: 3.893446\n",
      "Train Epoche: 1 [752/96 (783%)]\tLoss: 202.212280\n",
      "Train Epoche: 1 [753/96 (784%)]\tLoss: 127.944443\n",
      "Train Epoche: 1 [754/96 (785%)]\tLoss: 8.274612\n",
      "Train Epoche: 1 [755/96 (786%)]\tLoss: 29.853174\n",
      "Train Epoche: 1 [756/96 (788%)]\tLoss: 16.106403\n",
      "Train Epoche: 1 [757/96 (789%)]\tLoss: 3.590465\n",
      "Train Epoche: 1 [758/96 (790%)]\tLoss: 111.264236\n",
      "Train Epoche: 1 [759/96 (791%)]\tLoss: 35.589054\n",
      "Train Epoche: 1 [760/96 (792%)]\tLoss: 2.078325\n",
      "Train Epoche: 1 [761/96 (793%)]\tLoss: 70.868988\n",
      "Train Epoche: 1 [762/96 (794%)]\tLoss: 72.208900\n",
      "Train Epoche: 1 [763/96 (795%)]\tLoss: 31.828850\n",
      "Train Epoche: 1 [764/96 (796%)]\tLoss: 38.296417\n",
      "Train Epoche: 1 [765/96 (797%)]\tLoss: 75.096329\n",
      "Train Epoche: 1 [766/96 (798%)]\tLoss: 5.393950\n",
      "Train Epoche: 1 [767/96 (799%)]\tLoss: 37.088120\n",
      "Train Epoche: 1 [768/96 (800%)]\tLoss: 2.694406\n",
      "Train Epoche: 1 [769/96 (801%)]\tLoss: 5.715407\n",
      "Train Epoche: 1 [770/96 (802%)]\tLoss: 19.477732\n",
      "Train Epoche: 1 [771/96 (803%)]\tLoss: 16.558590\n",
      "Train Epoche: 1 [772/96 (804%)]\tLoss: 4.313738\n",
      "Train Epoche: 1 [773/96 (805%)]\tLoss: 3.053652\n",
      "Train Epoche: 1 [774/96 (806%)]\tLoss: 0.053605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [775/96 (807%)]\tLoss: 0.350736\n",
      "Train Epoche: 1 [776/96 (808%)]\tLoss: 34.478626\n",
      "Train Epoche: 1 [777/96 (809%)]\tLoss: 0.241816\n",
      "Train Epoche: 1 [778/96 (810%)]\tLoss: 23.807800\n",
      "Train Epoche: 1 [779/96 (811%)]\tLoss: 7.045190\n",
      "Train Epoche: 1 [780/96 (812%)]\tLoss: 144.363968\n",
      "Train Epoche: 1 [781/96 (814%)]\tLoss: 100.834198\n",
      "Train Epoche: 1 [782/96 (815%)]\tLoss: 111.850723\n",
      "Train Epoche: 1 [783/96 (816%)]\tLoss: 46.577843\n",
      "Train Epoche: 1 [784/96 (817%)]\tLoss: 31.820610\n",
      "Train Epoche: 1 [785/96 (818%)]\tLoss: 60.653587\n",
      "Train Epoche: 1 [786/96 (819%)]\tLoss: 238.372879\n",
      "Train Epoche: 1 [787/96 (820%)]\tLoss: 8.744222\n",
      "Train Epoche: 1 [788/96 (821%)]\tLoss: 16.192875\n",
      "Train Epoche: 1 [789/96 (822%)]\tLoss: 0.870633\n",
      "Train Epoche: 1 [790/96 (823%)]\tLoss: 44.508373\n",
      "Train Epoche: 1 [791/96 (824%)]\tLoss: 102.081039\n",
      "Train Epoche: 1 [792/96 (825%)]\tLoss: 2.461961\n",
      "Train Epoche: 1 [793/96 (826%)]\tLoss: 3.156765\n",
      "Train Epoche: 1 [794/96 (827%)]\tLoss: 5.152709\n",
      "Train Epoche: 1 [795/96 (828%)]\tLoss: 173.557816\n",
      "Train Epoche: 1 [796/96 (829%)]\tLoss: 51.691093\n",
      "Train Epoche: 1 [797/96 (830%)]\tLoss: 54.284809\n",
      "Train Epoche: 1 [798/96 (831%)]\tLoss: 8.187134\n",
      "Train Epoche: 1 [799/96 (832%)]\tLoss: 0.156488\n",
      "Train Epoche: 1 [800/96 (833%)]\tLoss: 23.675871\n",
      "Train Epoche: 1 [801/96 (834%)]\tLoss: 259.840790\n",
      "Train Epoche: 1 [802/96 (835%)]\tLoss: 93.455292\n",
      "Train Epoche: 1 [803/96 (836%)]\tLoss: 0.208354\n",
      "Train Epoche: 1 [804/96 (838%)]\tLoss: 59.546768\n",
      "Train Epoche: 1 [805/96 (839%)]\tLoss: 37.983639\n",
      "Train Epoche: 1 [806/96 (840%)]\tLoss: 111.394821\n",
      "Train Epoche: 1 [807/96 (841%)]\tLoss: 65.421677\n",
      "Train Epoche: 1 [808/96 (842%)]\tLoss: 17.263002\n",
      "Train Epoche: 1 [809/96 (843%)]\tLoss: 0.403407\n",
      "Train Epoche: 1 [810/96 (844%)]\tLoss: 43.483658\n",
      "Train Epoche: 1 [811/96 (845%)]\tLoss: 26.569016\n",
      "Train Epoche: 1 [812/96 (846%)]\tLoss: 0.897373\n",
      "Train Epoche: 1 [813/96 (847%)]\tLoss: 244.076279\n",
      "Train Epoche: 1 [814/96 (848%)]\tLoss: 0.470236\n",
      "Train Epoche: 1 [815/96 (849%)]\tLoss: 4.265251\n",
      "Train Epoche: 1 [816/96 (850%)]\tLoss: 19.251797\n",
      "Train Epoche: 1 [817/96 (851%)]\tLoss: 1.674731\n",
      "Train Epoche: 1 [818/96 (852%)]\tLoss: 2.652692\n",
      "Train Epoche: 1 [819/96 (853%)]\tLoss: 15.869254\n",
      "Train Epoche: 1 [820/96 (854%)]\tLoss: 8.884919\n",
      "Train Epoche: 1 [821/96 (855%)]\tLoss: 1.869547\n",
      "Train Epoche: 1 [822/96 (856%)]\tLoss: 71.518105\n",
      "Train Epoche: 1 [823/96 (857%)]\tLoss: 19.209183\n",
      "Train Epoche: 1 [824/96 (858%)]\tLoss: 14.965190\n",
      "Train Epoche: 1 [825/96 (859%)]\tLoss: 9.267270\n",
      "Train Epoche: 1 [826/96 (860%)]\tLoss: 0.853208\n",
      "Train Epoche: 1 [827/96 (861%)]\tLoss: 66.900864\n",
      "Train Epoche: 1 [828/96 (862%)]\tLoss: 10.306151\n",
      "Train Epoche: 1 [829/96 (864%)]\tLoss: 2.073750\n",
      "Train Epoche: 1 [830/96 (865%)]\tLoss: 15.633369\n",
      "Train Epoche: 1 [831/96 (866%)]\tLoss: 44.805511\n",
      "Train Epoche: 1 [832/96 (867%)]\tLoss: 59.599831\n",
      "Train Epoche: 1 [833/96 (868%)]\tLoss: 89.589218\n",
      "Train Epoche: 1 [834/96 (869%)]\tLoss: 7.094449\n",
      "Train Epoche: 1 [835/96 (870%)]\tLoss: 7.663237\n",
      "Train Epoche: 1 [836/96 (871%)]\tLoss: 5.518582\n",
      "Train Epoche: 1 [837/96 (872%)]\tLoss: 14.810860\n",
      "Train Epoche: 1 [838/96 (873%)]\tLoss: 10.621427\n",
      "Train Epoche: 1 [839/96 (874%)]\tLoss: 3.952206\n",
      "Train Epoche: 1 [840/96 (875%)]\tLoss: 0.155157\n",
      "Train Epoche: 1 [841/96 (876%)]\tLoss: 76.456329\n",
      "Train Epoche: 1 [842/96 (877%)]\tLoss: 37.356155\n",
      "Train Epoche: 1 [843/96 (878%)]\tLoss: 66.857765\n",
      "Train Epoche: 1 [844/96 (879%)]\tLoss: 1.307257\n",
      "Train Epoche: 1 [845/96 (880%)]\tLoss: 2.222836\n",
      "Train Epoche: 1 [846/96 (881%)]\tLoss: 4.508588\n",
      "Train Epoche: 1 [847/96 (882%)]\tLoss: 0.075418\n",
      "Train Epoche: 1 [848/96 (883%)]\tLoss: 5.617369\n",
      "Train Epoche: 1 [849/96 (884%)]\tLoss: 178.119995\n",
      "Train Epoche: 1 [850/96 (885%)]\tLoss: 11.073164\n",
      "Train Epoche: 1 [851/96 (886%)]\tLoss: 199.015244\n",
      "Train Epoche: 1 [852/96 (888%)]\tLoss: 2.728380\n",
      "Train Epoche: 1 [853/96 (889%)]\tLoss: 64.006851\n",
      "Train Epoche: 1 [854/96 (890%)]\tLoss: 23.003727\n",
      "Train Epoche: 1 [855/96 (891%)]\tLoss: 0.032389\n",
      "Train Epoche: 1 [856/96 (892%)]\tLoss: 0.743575\n",
      "Train Epoche: 1 [857/96 (893%)]\tLoss: 9.441952\n",
      "Train Epoche: 1 [858/96 (894%)]\tLoss: 28.726311\n",
      "Train Epoche: 1 [859/96 (895%)]\tLoss: 6.959101\n",
      "Train Epoche: 1 [860/96 (896%)]\tLoss: 8.680281\n",
      "Train Epoche: 1 [861/96 (897%)]\tLoss: 175.909210\n",
      "Train Epoche: 1 [862/96 (898%)]\tLoss: 6.432158\n",
      "Train Epoche: 1 [863/96 (899%)]\tLoss: 42.032269\n",
      "Train Epoche: 1 [864/96 (900%)]\tLoss: 93.299637\n",
      "Train Epoche: 1 [865/96 (901%)]\tLoss: 58.902851\n",
      "Train Epoche: 1 [866/96 (902%)]\tLoss: 29.562828\n",
      "Train Epoche: 1 [867/96 (903%)]\tLoss: 9.823807\n",
      "Train Epoche: 1 [868/96 (904%)]\tLoss: 36.440109\n",
      "Train Epoche: 1 [869/96 (905%)]\tLoss: 54.915134\n",
      "Train Epoche: 1 [870/96 (906%)]\tLoss: 23.682276\n",
      "Train Epoche: 1 [871/96 (907%)]\tLoss: 170.527512\n",
      "Train Epoche: 1 [872/96 (908%)]\tLoss: 10.101385\n",
      "Train Epoche: 1 [873/96 (909%)]\tLoss: 1.087631\n",
      "Train Epoche: 1 [874/96 (910%)]\tLoss: 218.924530\n",
      "Train Epoche: 1 [875/96 (911%)]\tLoss: 6.994482\n",
      "Train Epoche: 1 [876/96 (912%)]\tLoss: 48.416626\n",
      "Train Epoche: 1 [877/96 (914%)]\tLoss: 180.376617\n",
      "Train Epoche: 1 [878/96 (915%)]\tLoss: 18.890406\n",
      "Train Epoche: 1 [879/96 (916%)]\tLoss: 0.759084\n",
      "Train Epoche: 1 [880/96 (917%)]\tLoss: 8.643577\n",
      "Train Epoche: 1 [881/96 (918%)]\tLoss: 71.733429\n",
      "Train Epoche: 1 [882/96 (919%)]\tLoss: 0.663898\n",
      "Train Epoche: 1 [883/96 (920%)]\tLoss: 13.596803\n",
      "Train Epoche: 1 [884/96 (921%)]\tLoss: 61.835510\n",
      "Train Epoche: 1 [885/96 (922%)]\tLoss: 14.036576\n",
      "Train Epoche: 1 [886/96 (923%)]\tLoss: 11.335066\n",
      "Train Epoche: 1 [887/96 (924%)]\tLoss: 71.078506\n",
      "Train Epoche: 1 [888/96 (925%)]\tLoss: 0.493870\n",
      "Train Epoche: 1 [889/96 (926%)]\tLoss: 4.987723\n",
      "Train Epoche: 1 [890/96 (927%)]\tLoss: 9.205811\n",
      "Train Epoche: 1 [891/96 (928%)]\tLoss: 67.914375\n",
      "Train Epoche: 1 [892/96 (929%)]\tLoss: 0.177104\n",
      "Train Epoche: 1 [893/96 (930%)]\tLoss: 5.326063\n",
      "Train Epoche: 1 [894/96 (931%)]\tLoss: 75.066376\n",
      "Train Epoche: 1 [895/96 (932%)]\tLoss: 2.209893\n",
      "Train Epoche: 1 [896/96 (933%)]\tLoss: 129.506607\n",
      "Train Epoche: 1 [897/96 (934%)]\tLoss: 0.168260\n",
      "Train Epoche: 1 [898/96 (935%)]\tLoss: 0.532391\n",
      "Train Epoche: 1 [899/96 (936%)]\tLoss: 24.377150\n",
      "Train Epoche: 1 [900/96 (938%)]\tLoss: 19.435905\n",
      "Train Epoche: 1 [901/96 (939%)]\tLoss: 0.362820\n",
      "Train Epoche: 1 [902/96 (940%)]\tLoss: 7.550278\n",
      "Train Epoche: 1 [903/96 (941%)]\tLoss: 83.875816\n",
      "Train Epoche: 1 [904/96 (942%)]\tLoss: 4.443060\n",
      "Train Epoche: 1 [905/96 (943%)]\tLoss: 122.007561\n",
      "Train Epoche: 1 [906/96 (944%)]\tLoss: 0.536556\n",
      "Train Epoche: 1 [907/96 (945%)]\tLoss: 52.351440\n",
      "Train Epoche: 1 [908/96 (946%)]\tLoss: 26.952358\n",
      "Train Epoche: 1 [909/96 (947%)]\tLoss: 48.523045\n",
      "Train Epoche: 1 [910/96 (948%)]\tLoss: 38.955315\n",
      "Train Epoche: 1 [911/96 (949%)]\tLoss: 37.090485\n",
      "Train Epoche: 1 [912/96 (950%)]\tLoss: 1.635307\n",
      "Train Epoche: 1 [913/96 (951%)]\tLoss: 15.072918\n",
      "Train Epoche: 1 [914/96 (952%)]\tLoss: 0.074360\n",
      "Train Epoche: 1 [915/96 (953%)]\tLoss: 8.505583\n",
      "Train Epoche: 1 [916/96 (954%)]\tLoss: 0.425641\n",
      "Train Epoche: 1 [917/96 (955%)]\tLoss: 56.799168\n",
      "Train Epoche: 1 [918/96 (956%)]\tLoss: 17.785234\n",
      "Train Epoche: 1 [919/96 (957%)]\tLoss: 236.272736\n",
      "Train Epoche: 1 [920/96 (958%)]\tLoss: 37.005489\n",
      "Train Epoche: 1 [921/96 (959%)]\tLoss: 163.967255\n",
      "Train Epoche: 1 [922/96 (960%)]\tLoss: 2.930284\n",
      "Train Epoche: 1 [923/96 (961%)]\tLoss: 5.195548\n",
      "Train Epoche: 1 [924/96 (962%)]\tLoss: 229.522263\n",
      "Train Epoche: 1 [925/96 (964%)]\tLoss: 39.940208\n",
      "Train Epoche: 1 [926/96 (965%)]\tLoss: 79.768852\n",
      "Train Epoche: 1 [927/96 (966%)]\tLoss: 7.225753\n",
      "Train Epoche: 1 [928/96 (967%)]\tLoss: 0.375721\n",
      "Train Epoche: 1 [929/96 (968%)]\tLoss: 151.336838\n",
      "Train Epoche: 1 [930/96 (969%)]\tLoss: 56.828579\n",
      "Train Epoche: 1 [931/96 (970%)]\tLoss: 4.767810\n",
      "Train Epoche: 1 [932/96 (971%)]\tLoss: 3.698853\n",
      "Train Epoche: 1 [933/96 (972%)]\tLoss: 25.784824\n",
      "Train Epoche: 1 [934/96 (973%)]\tLoss: 2.312237\n",
      "Train Epoche: 1 [935/96 (974%)]\tLoss: 1.880260\n",
      "Train Epoche: 1 [936/96 (975%)]\tLoss: 27.551392\n",
      "Train Epoche: 1 [937/96 (976%)]\tLoss: 160.040771\n",
      "Train Epoche: 1 [938/96 (977%)]\tLoss: 57.518990\n",
      "Train Epoche: 1 [939/96 (978%)]\tLoss: 0.343844\n",
      "Train Epoche: 1 [940/96 (979%)]\tLoss: 93.030426\n",
      "Train Epoche: 1 [941/96 (980%)]\tLoss: 29.618408\n",
      "Train Epoche: 1 [942/96 (981%)]\tLoss: 18.023533\n",
      "Train Epoche: 1 [943/96 (982%)]\tLoss: 5.525226\n",
      "Train Epoche: 1 [944/96 (983%)]\tLoss: 15.374998\n",
      "Train Epoche: 1 [945/96 (984%)]\tLoss: 141.351501\n",
      "Train Epoche: 1 [946/96 (985%)]\tLoss: 28.986021\n",
      "Train Epoche: 1 [947/96 (986%)]\tLoss: 44.949760\n",
      "Train Epoche: 1 [948/96 (988%)]\tLoss: 8.181323\n",
      "Train Epoche: 1 [949/96 (989%)]\tLoss: 0.335556\n",
      "Train Epoche: 1 [950/96 (990%)]\tLoss: 60.276741\n",
      "Train Epoche: 1 [951/96 (991%)]\tLoss: 0.356707\n",
      "Train Epoche: 1 [952/96 (992%)]\tLoss: 98.167511\n",
      "Train Epoche: 1 [953/96 (993%)]\tLoss: 45.382130\n",
      "Train Epoche: 1 [954/96 (994%)]\tLoss: 5.212240\n",
      "Train Epoche: 1 [955/96 (995%)]\tLoss: 1.599404\n",
      "Train Epoche: 1 [956/96 (996%)]\tLoss: 40.410461\n",
      "Train Epoche: 1 [957/96 (997%)]\tLoss: 0.770927\n",
      "Train Epoche: 1 [958/96 (998%)]\tLoss: 4.197537\n",
      "Train Epoche: 1 [959/96 (999%)]\tLoss: 20.387712\n",
      "Train Epoche: 1 [960/96 (1000%)]\tLoss: 56.992851\n",
      "Train Epoche: 1 [961/96 (1001%)]\tLoss: 10.944105\n",
      "Train Epoche: 1 [962/96 (1002%)]\tLoss: 3.531617\n",
      "Train Epoche: 1 [963/96 (1003%)]\tLoss: 18.072178\n",
      "Train Epoche: 1 [964/96 (1004%)]\tLoss: 19.013784\n",
      "Train Epoche: 1 [965/96 (1005%)]\tLoss: 1.891826\n",
      "Train Epoche: 1 [966/96 (1006%)]\tLoss: 0.124576\n",
      "Train Epoche: 1 [967/96 (1007%)]\tLoss: 58.055244\n",
      "Train Epoche: 1 [968/96 (1008%)]\tLoss: 44.929317\n",
      "Train Epoche: 1 [969/96 (1009%)]\tLoss: 67.147705\n",
      "Train Epoche: 1 [970/96 (1010%)]\tLoss: 44.744663\n",
      "Train Epoche: 1 [971/96 (1011%)]\tLoss: 24.780710\n",
      "Train Epoche: 1 [972/96 (1012%)]\tLoss: 12.590027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [973/96 (1014%)]\tLoss: 1.464396\n",
      "Train Epoche: 1 [974/96 (1015%)]\tLoss: 238.289673\n",
      "Train Epoche: 1 [975/96 (1016%)]\tLoss: 198.677612\n",
      "Train Epoche: 1 [976/96 (1017%)]\tLoss: 58.471233\n",
      "Train Epoche: 1 [977/96 (1018%)]\tLoss: 2.373745\n",
      "Train Epoche: 1 [978/96 (1019%)]\tLoss: 122.759781\n",
      "Train Epoche: 1 [979/96 (1020%)]\tLoss: 5.868766\n",
      "Train Epoche: 1 [980/96 (1021%)]\tLoss: 18.766680\n",
      "Train Epoche: 1 [981/96 (1022%)]\tLoss: 12.435719\n",
      "Train Epoche: 1 [982/96 (1023%)]\tLoss: 54.365021\n",
      "Train Epoche: 1 [983/96 (1024%)]\tLoss: 4.338570\n",
      "Train Epoche: 1 [984/96 (1025%)]\tLoss: 0.571979\n",
      "Train Epoche: 1 [985/96 (1026%)]\tLoss: 0.115165\n",
      "Train Epoche: 1 [986/96 (1027%)]\tLoss: 3.914530\n",
      "Train Epoche: 1 [987/96 (1028%)]\tLoss: 0.424001\n",
      "Train Epoche: 1 [988/96 (1029%)]\tLoss: 21.517200\n",
      "Train Epoche: 1 [989/96 (1030%)]\tLoss: 34.805454\n",
      "Train Epoche: 1 [990/96 (1031%)]\tLoss: 16.333323\n",
      "Train Epoche: 1 [991/96 (1032%)]\tLoss: 11.785390\n",
      "Train Epoche: 1 [992/96 (1033%)]\tLoss: 27.216831\n",
      "Train Epoche: 1 [993/96 (1034%)]\tLoss: 1.964270\n",
      "Train Epoche: 1 [994/96 (1035%)]\tLoss: 4.570165\n",
      "Train Epoche: 1 [995/96 (1036%)]\tLoss: 47.699917\n",
      "Train Epoche: 1 [996/96 (1038%)]\tLoss: 28.149160\n",
      "Train Epoche: 1 [997/96 (1039%)]\tLoss: 9.808230\n",
      "Train Epoche: 1 [998/96 (1040%)]\tLoss: 0.792137\n",
      "Train Epoche: 1 [999/96 (1041%)]\tLoss: 17.752863\n",
      "Train Epoche: 1 [1000/96 (1042%)]\tLoss: 2.909537\n",
      "Train Epoche: 1 [1001/96 (1043%)]\tLoss: 16.432665\n",
      "Train Epoche: 1 [1002/96 (1044%)]\tLoss: 19.791780\n",
      "Train Epoche: 1 [1003/96 (1045%)]\tLoss: 3.384363\n",
      "Train Epoche: 1 [1004/96 (1046%)]\tLoss: 193.968704\n",
      "Train Epoche: 1 [1005/96 (1047%)]\tLoss: 10.604726\n",
      "Train Epoche: 1 [1006/96 (1048%)]\tLoss: 0.759959\n",
      "Train Epoche: 1 [1007/96 (1049%)]\tLoss: 29.019571\n",
      "Train Epoche: 1 [1008/96 (1050%)]\tLoss: 42.299061\n",
      "Train Epoche: 1 [1009/96 (1051%)]\tLoss: 2.425433\n",
      "Train Epoche: 1 [1010/96 (1052%)]\tLoss: 39.634357\n",
      "Train Epoche: 1 [1011/96 (1053%)]\tLoss: 3.472296\n",
      "Train Epoche: 1 [1012/96 (1054%)]\tLoss: 9.874587\n",
      "Train Epoche: 1 [1013/96 (1055%)]\tLoss: 111.955475\n",
      "Train Epoche: 1 [1014/96 (1056%)]\tLoss: 8.546003\n",
      "Train Epoche: 1 [1015/96 (1057%)]\tLoss: 54.099667\n",
      "Train Epoche: 1 [1016/96 (1058%)]\tLoss: 55.000896\n",
      "Train Epoche: 1 [1017/96 (1059%)]\tLoss: 180.546204\n",
      "Train Epoche: 1 [1018/96 (1060%)]\tLoss: 14.982862\n",
      "Train Epoche: 1 [1019/96 (1061%)]\tLoss: 3.230274\n",
      "Train Epoche: 1 [1020/96 (1062%)]\tLoss: 66.914726\n",
      "Train Epoche: 1 [1021/96 (1064%)]\tLoss: 11.091190\n",
      "Train Epoche: 1 [1022/96 (1065%)]\tLoss: 97.013771\n",
      "Train Epoche: 1 [1023/96 (1066%)]\tLoss: 4.284232\n",
      "Train Epoche: 1 [1024/96 (1067%)]\tLoss: 0.441040\n",
      "Train Epoche: 1 [1025/96 (1068%)]\tLoss: 11.540125\n",
      "Train Epoche: 1 [1026/96 (1069%)]\tLoss: 6.451868\n",
      "Train Epoche: 1 [1027/96 (1070%)]\tLoss: 30.730461\n",
      "Train Epoche: 1 [1028/96 (1071%)]\tLoss: 62.129414\n",
      "Train Epoche: 1 [1029/96 (1072%)]\tLoss: 7.670735\n",
      "Train Epoche: 1 [1030/96 (1073%)]\tLoss: 86.762947\n",
      "Train Epoche: 1 [1031/96 (1074%)]\tLoss: 50.943584\n",
      "Train Epoche: 1 [1032/96 (1075%)]\tLoss: 2.127901\n",
      "Train Epoche: 1 [1033/96 (1076%)]\tLoss: 1.117646\n",
      "Train Epoche: 1 [1034/96 (1077%)]\tLoss: 35.925537\n",
      "Train Epoche: 1 [1035/96 (1078%)]\tLoss: 139.883423\n",
      "Train Epoche: 1 [1036/96 (1079%)]\tLoss: 14.861128\n",
      "Train Epoche: 1 [1037/96 (1080%)]\tLoss: 2.820868\n",
      "Train Epoche: 1 [1038/96 (1081%)]\tLoss: 29.162457\n",
      "Train Epoche: 1 [1039/96 (1082%)]\tLoss: 67.131248\n",
      "Train Epoche: 1 [1040/96 (1083%)]\tLoss: 82.828209\n",
      "Train Epoche: 1 [1041/96 (1084%)]\tLoss: 18.004063\n",
      "Train Epoche: 1 [1042/96 (1085%)]\tLoss: 19.587383\n",
      "Train Epoche: 1 [1043/96 (1086%)]\tLoss: 4.843644\n",
      "Train Epoche: 1 [1044/96 (1088%)]\tLoss: 15.249698\n",
      "Train Epoche: 1 [1045/96 (1089%)]\tLoss: 23.316290\n",
      "Train Epoche: 1 [1046/96 (1090%)]\tLoss: 97.159088\n",
      "Train Epoche: 1 [1047/96 (1091%)]\tLoss: 11.053590\n",
      "Train Epoche: 1 [1048/96 (1092%)]\tLoss: 4.499510\n",
      "Train Epoche: 1 [1049/96 (1093%)]\tLoss: 0.325007\n",
      "Train Epoche: 1 [1050/96 (1094%)]\tLoss: 4.225727\n",
      "Train Epoche: 1 [1051/96 (1095%)]\tLoss: 244.903702\n",
      "Train Epoche: 1 [1052/96 (1096%)]\tLoss: 0.083580\n",
      "Train Epoche: 1 [1053/96 (1097%)]\tLoss: 41.667767\n",
      "Train Epoche: 1 [1054/96 (1098%)]\tLoss: 26.791698\n",
      "Train Epoche: 1 [1055/96 (1099%)]\tLoss: 3.267565\n",
      "Train Epoche: 1 [1056/96 (1100%)]\tLoss: 64.379585\n",
      "Train Epoche: 1 [1057/96 (1101%)]\tLoss: 3.541440\n",
      "Train Epoche: 1 [1058/96 (1102%)]\tLoss: 38.932247\n",
      "Train Epoche: 1 [1059/96 (1103%)]\tLoss: 91.250351\n",
      "Train Epoche: 1 [1060/96 (1104%)]\tLoss: 3.411653\n",
      "Train Epoche: 1 [1061/96 (1105%)]\tLoss: 20.256212\n",
      "Train Epoche: 1 [1062/96 (1106%)]\tLoss: 16.056404\n",
      "Train Epoche: 1 [1063/96 (1107%)]\tLoss: 8.279446\n",
      "Train Epoche: 1 [1064/96 (1108%)]\tLoss: 8.265177\n",
      "Train Epoche: 1 [1065/96 (1109%)]\tLoss: 21.225212\n",
      "Train Epoche: 1 [1066/96 (1110%)]\tLoss: 14.955218\n",
      "Train Epoche: 1 [1067/96 (1111%)]\tLoss: 0.584293\n",
      "Train Epoche: 1 [1068/96 (1112%)]\tLoss: 17.105408\n",
      "Train Epoche: 1 [1069/96 (1114%)]\tLoss: 0.371040\n",
      "Train Epoche: 1 [1070/96 (1115%)]\tLoss: 9.111351\n",
      "Train Epoche: 1 [1071/96 (1116%)]\tLoss: 27.424942\n",
      "Train Epoche: 1 [1072/96 (1117%)]\tLoss: 34.032970\n",
      "Train Epoche: 1 [1073/96 (1118%)]\tLoss: 2.141263\n",
      "Train Epoche: 1 [1074/96 (1119%)]\tLoss: 0.398108\n",
      "Train Epoche: 1 [1075/96 (1120%)]\tLoss: 15.512709\n",
      "Train Epoche: 1 [1076/96 (1121%)]\tLoss: 14.039922\n",
      "Train Epoche: 1 [1077/96 (1122%)]\tLoss: 54.056316\n",
      "Train Epoche: 1 [1078/96 (1123%)]\tLoss: 0.345715\n",
      "Train Epoche: 1 [1079/96 (1124%)]\tLoss: 33.073074\n",
      "Train Epoche: 1 [1080/96 (1125%)]\tLoss: 64.246994\n",
      "Train Epoche: 1 [1081/96 (1126%)]\tLoss: 3.048137\n",
      "Train Epoche: 1 [1082/96 (1127%)]\tLoss: 3.504850\n",
      "Train Epoche: 1 [1083/96 (1128%)]\tLoss: 0.542573\n",
      "Train Epoche: 1 [1084/96 (1129%)]\tLoss: 0.059885\n",
      "Train Epoche: 1 [1085/96 (1130%)]\tLoss: 9.326107\n",
      "Train Epoche: 1 [1086/96 (1131%)]\tLoss: 10.996845\n",
      "Train Epoche: 1 [1087/96 (1132%)]\tLoss: 0.748442\n",
      "Train Epoche: 1 [1088/96 (1133%)]\tLoss: 7.564530\n",
      "Train Epoche: 1 [1089/96 (1134%)]\tLoss: 0.056485\n",
      "Train Epoche: 1 [1090/96 (1135%)]\tLoss: 4.492294\n",
      "Train Epoche: 1 [1091/96 (1136%)]\tLoss: 36.484444\n",
      "Train Epoche: 1 [1092/96 (1138%)]\tLoss: 40.043808\n",
      "Train Epoche: 1 [1093/96 (1139%)]\tLoss: 63.424938\n",
      "Train Epoche: 1 [1094/96 (1140%)]\tLoss: 128.464188\n",
      "Train Epoche: 1 [1095/96 (1141%)]\tLoss: 16.600050\n",
      "Train Epoche: 1 [1096/96 (1142%)]\tLoss: 5.127825\n",
      "Train Epoche: 1 [1097/96 (1143%)]\tLoss: 4.405116\n",
      "Train Epoche: 1 [1098/96 (1144%)]\tLoss: 267.579926\n",
      "Train Epoche: 1 [1099/96 (1145%)]\tLoss: 40.929787\n",
      "Train Epoche: 1 [1100/96 (1146%)]\tLoss: 4.891116\n",
      "Train Epoche: 1 [1101/96 (1147%)]\tLoss: 174.201355\n",
      "Train Epoche: 1 [1102/96 (1148%)]\tLoss: 9.649837\n",
      "Train Epoche: 1 [1103/96 (1149%)]\tLoss: 118.216660\n",
      "Train Epoche: 1 [1104/96 (1150%)]\tLoss: 64.698837\n",
      "Train Epoche: 1 [1105/96 (1151%)]\tLoss: 10.455678\n",
      "Train Epoche: 1 [1106/96 (1152%)]\tLoss: 15.280335\n",
      "Train Epoche: 1 [1107/96 (1153%)]\tLoss: 43.415874\n",
      "Train Epoche: 1 [1108/96 (1154%)]\tLoss: 7.354216\n",
      "Train Epoche: 1 [1109/96 (1155%)]\tLoss: 82.399956\n",
      "Train Epoche: 1 [1110/96 (1156%)]\tLoss: 0.303966\n",
      "Train Epoche: 1 [1111/96 (1157%)]\tLoss: 8.170698\n",
      "Train Epoche: 1 [1112/96 (1158%)]\tLoss: 32.317894\n",
      "Train Epoche: 1 [1113/96 (1159%)]\tLoss: 165.603378\n",
      "Train Epoche: 1 [1114/96 (1160%)]\tLoss: 18.316038\n",
      "Train Epoche: 1 [1115/96 (1161%)]\tLoss: 95.684601\n",
      "Train Epoche: 1 [1116/96 (1162%)]\tLoss: 2.372934\n",
      "Train Epoche: 1 [1117/96 (1164%)]\tLoss: 2.741056\n",
      "Train Epoche: 1 [1118/96 (1165%)]\tLoss: 25.710810\n",
      "Train Epoche: 1 [1119/96 (1166%)]\tLoss: 30.283821\n",
      "Train Epoche: 1 [1120/96 (1167%)]\tLoss: 0.295902\n",
      "Train Epoche: 1 [1121/96 (1168%)]\tLoss: 14.230218\n",
      "Train Epoche: 1 [1122/96 (1169%)]\tLoss: 37.260513\n",
      "Train Epoche: 1 [1123/96 (1170%)]\tLoss: 34.578701\n",
      "Train Epoche: 1 [1124/96 (1171%)]\tLoss: 32.581364\n",
      "Train Epoche: 1 [1125/96 (1172%)]\tLoss: 77.169472\n",
      "Train Epoche: 1 [1126/96 (1173%)]\tLoss: 25.249222\n",
      "Train Epoche: 1 [1127/96 (1174%)]\tLoss: 28.140274\n",
      "Train Epoche: 1 [1128/96 (1175%)]\tLoss: 34.170074\n",
      "Train Epoche: 1 [1129/96 (1176%)]\tLoss: 176.472961\n",
      "Train Epoche: 1 [1130/96 (1177%)]\tLoss: 88.022491\n",
      "Train Epoche: 1 [1131/96 (1178%)]\tLoss: 45.183315\n",
      "Train Epoche: 1 [1132/96 (1179%)]\tLoss: 62.876671\n",
      "Train Epoche: 1 [1133/96 (1180%)]\tLoss: 18.514069\n",
      "Train Epoche: 1 [1134/96 (1181%)]\tLoss: 5.270045\n",
      "Train Epoche: 1 [1135/96 (1182%)]\tLoss: 2.074763\n",
      "Train Epoche: 1 [1136/96 (1183%)]\tLoss: 63.333702\n",
      "Train Epoche: 1 [1137/96 (1184%)]\tLoss: 66.165840\n",
      "Train Epoche: 1 [1138/96 (1185%)]\tLoss: 2.228887\n",
      "Train Epoche: 1 [1139/96 (1186%)]\tLoss: 4.185249\n",
      "Train Epoche: 1 [1140/96 (1188%)]\tLoss: 0.415185\n",
      "Train Epoche: 1 [1141/96 (1189%)]\tLoss: 18.391356\n",
      "Train Epoche: 1 [1142/96 (1190%)]\tLoss: 33.611225\n",
      "Train Epoche: 1 [1143/96 (1191%)]\tLoss: 3.276887\n",
      "Train Epoche: 1 [1144/96 (1192%)]\tLoss: 118.014618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1145/96 (1193%)]\tLoss: 1.024297\n",
      "Train Epoche: 1 [1146/96 (1194%)]\tLoss: 36.322571\n",
      "Train Epoche: 1 [1147/96 (1195%)]\tLoss: 0.410565\n",
      "Train Epoche: 1 [1148/96 (1196%)]\tLoss: 9.524872\n",
      "Train Epoche: 1 [1149/96 (1197%)]\tLoss: 11.379649\n",
      "Train Epoche: 1 [1150/96 (1198%)]\tLoss: 32.535675\n",
      "Train Epoche: 1 [1151/96 (1199%)]\tLoss: 39.703999\n",
      "Train Epoche: 1 [1152/96 (1200%)]\tLoss: 0.032461\n",
      "Train Epoche: 1 [1153/96 (1201%)]\tLoss: 4.004216\n",
      "Train Epoche: 1 [1154/96 (1202%)]\tLoss: 23.969515\n",
      "Train Epoche: 1 [1155/96 (1203%)]\tLoss: 45.560780\n",
      "Train Epoche: 1 [1156/96 (1204%)]\tLoss: 1.179672\n",
      "Train Epoche: 1 [1157/96 (1205%)]\tLoss: 15.310838\n",
      "Train Epoche: 1 [1158/96 (1206%)]\tLoss: 36.338421\n",
      "Train Epoche: 1 [1159/96 (1207%)]\tLoss: 33.488377\n",
      "Train Epoche: 1 [1160/96 (1208%)]\tLoss: 4.967696\n",
      "Train Epoche: 1 [1161/96 (1209%)]\tLoss: 64.890144\n",
      "Train Epoche: 1 [1162/96 (1210%)]\tLoss: 17.401361\n",
      "Train Epoche: 1 [1163/96 (1211%)]\tLoss: 6.960855\n",
      "Train Epoche: 1 [1164/96 (1212%)]\tLoss: 14.731134\n",
      "Train Epoche: 1 [1165/96 (1214%)]\tLoss: 2.288204\n",
      "Train Epoche: 1 [1166/96 (1215%)]\tLoss: 159.809128\n",
      "Train Epoche: 1 [1167/96 (1216%)]\tLoss: 1.773613\n",
      "Train Epoche: 1 [1168/96 (1217%)]\tLoss: 10.066086\n",
      "Train Epoche: 1 [1169/96 (1218%)]\tLoss: 18.994350\n",
      "Train Epoche: 1 [1170/96 (1219%)]\tLoss: 8.178717\n",
      "Train Epoche: 1 [1171/96 (1220%)]\tLoss: 17.115988\n",
      "Train Epoche: 1 [1172/96 (1221%)]\tLoss: 261.245392\n",
      "Train Epoche: 1 [1173/96 (1222%)]\tLoss: 1.219345\n",
      "Train Epoche: 1 [1174/96 (1223%)]\tLoss: 0.451043\n",
      "Train Epoche: 1 [1175/96 (1224%)]\tLoss: 22.083235\n",
      "Train Epoche: 1 [1176/96 (1225%)]\tLoss: 10.868445\n",
      "Train Epoche: 1 [1177/96 (1226%)]\tLoss: 9.095320\n",
      "Train Epoche: 1 [1178/96 (1227%)]\tLoss: 0.836057\n",
      "Train Epoche: 1 [1179/96 (1228%)]\tLoss: 31.116421\n",
      "Train Epoche: 1 [1180/96 (1229%)]\tLoss: 8.877250\n",
      "Train Epoche: 1 [1181/96 (1230%)]\tLoss: 15.419355\n",
      "Train Epoche: 1 [1182/96 (1231%)]\tLoss: 36.390442\n",
      "Train Epoche: 1 [1183/96 (1232%)]\tLoss: 3.370072\n",
      "Train Epoche: 1 [1184/96 (1233%)]\tLoss: 4.273186\n",
      "Train Epoche: 1 [1185/96 (1234%)]\tLoss: 9.453935\n",
      "Train Epoche: 1 [1186/96 (1235%)]\tLoss: 1.422979\n",
      "Train Epoche: 1 [1187/96 (1236%)]\tLoss: 0.211483\n",
      "Train Epoche: 1 [1188/96 (1238%)]\tLoss: 7.564042\n",
      "Train Epoche: 1 [1189/96 (1239%)]\tLoss: 0.493609\n",
      "Train Epoche: 1 [1190/96 (1240%)]\tLoss: 5.622591\n",
      "Train Epoche: 1 [1191/96 (1241%)]\tLoss: 60.562252\n",
      "Train Epoche: 1 [1192/96 (1242%)]\tLoss: 0.593300\n",
      "Train Epoche: 1 [1193/96 (1243%)]\tLoss: 1.144907\n",
      "Train Epoche: 1 [1194/96 (1244%)]\tLoss: 181.522293\n",
      "Train Epoche: 1 [1195/96 (1245%)]\tLoss: 139.403687\n",
      "Train Epoche: 1 [1196/96 (1246%)]\tLoss: 119.470299\n",
      "Train Epoche: 1 [1197/96 (1247%)]\tLoss: 23.277632\n",
      "Train Epoche: 1 [1198/96 (1248%)]\tLoss: 47.851997\n",
      "Train Epoche: 1 [1199/96 (1249%)]\tLoss: 96.895889\n",
      "Train Epoche: 1 [1200/96 (1250%)]\tLoss: 56.570297\n",
      "Train Epoche: 1 [1201/96 (1251%)]\tLoss: 19.396597\n",
      "Train Epoche: 1 [1202/96 (1252%)]\tLoss: 53.678001\n",
      "Train Epoche: 1 [1203/96 (1253%)]\tLoss: 0.860244\n",
      "Train Epoche: 1 [1204/96 (1254%)]\tLoss: 1.352982\n",
      "Train Epoche: 1 [1205/96 (1255%)]\tLoss: 0.062272\n",
      "Train Epoche: 1 [1206/96 (1256%)]\tLoss: 0.341642\n",
      "Train Epoche: 1 [1207/96 (1257%)]\tLoss: 22.464796\n",
      "Train Epoche: 1 [1208/96 (1258%)]\tLoss: 31.445704\n",
      "Train Epoche: 1 [1209/96 (1259%)]\tLoss: 10.660939\n",
      "Train Epoche: 1 [1210/96 (1260%)]\tLoss: 0.849315\n",
      "Train Epoche: 1 [1211/96 (1261%)]\tLoss: 2.923380\n",
      "Train Epoche: 1 [1212/96 (1262%)]\tLoss: 28.703684\n",
      "Train Epoche: 1 [1213/96 (1264%)]\tLoss: 20.780838\n",
      "Train Epoche: 1 [1214/96 (1265%)]\tLoss: 42.182278\n",
      "Train Epoche: 1 [1215/96 (1266%)]\tLoss: 43.708260\n",
      "Train Epoche: 1 [1216/96 (1267%)]\tLoss: 4.334427\n",
      "Train Epoche: 1 [1217/96 (1268%)]\tLoss: 18.672110\n",
      "Train Epoche: 1 [1218/96 (1269%)]\tLoss: 99.730919\n",
      "Train Epoche: 1 [1219/96 (1270%)]\tLoss: 0.346747\n",
      "Train Epoche: 1 [1220/96 (1271%)]\tLoss: 1.757372\n",
      "Train Epoche: 1 [1221/96 (1272%)]\tLoss: 89.675583\n",
      "Train Epoche: 1 [1222/96 (1273%)]\tLoss: 9.770336\n",
      "Train Epoche: 1 [1223/96 (1274%)]\tLoss: 10.614614\n",
      "Train Epoche: 1 [1224/96 (1275%)]\tLoss: 39.065002\n",
      "Train Epoche: 1 [1225/96 (1276%)]\tLoss: 61.156090\n",
      "Train Epoche: 1 [1226/96 (1277%)]\tLoss: 21.035456\n",
      "Train Epoche: 1 [1227/96 (1278%)]\tLoss: 52.773033\n",
      "Train Epoche: 1 [1228/96 (1279%)]\tLoss: 26.006067\n",
      "Train Epoche: 1 [1229/96 (1280%)]\tLoss: 163.650131\n",
      "Train Epoche: 1 [1230/96 (1281%)]\tLoss: 7.819287\n",
      "Train Epoche: 1 [1231/96 (1282%)]\tLoss: 38.949150\n",
      "Train Epoche: 1 [1232/96 (1283%)]\tLoss: 46.406845\n",
      "Train Epoche: 1 [1233/96 (1284%)]\tLoss: 215.255768\n",
      "Train Epoche: 1 [1234/96 (1285%)]\tLoss: 38.185265\n",
      "Train Epoche: 1 [1235/96 (1286%)]\tLoss: 3.299770\n",
      "Train Epoche: 1 [1236/96 (1288%)]\tLoss: 46.323856\n",
      "Train Epoche: 1 [1237/96 (1289%)]\tLoss: 67.017441\n",
      "Train Epoche: 1 [1238/96 (1290%)]\tLoss: 28.399033\n",
      "Train Epoche: 1 [1239/96 (1291%)]\tLoss: 7.467959\n",
      "Train Epoche: 1 [1240/96 (1292%)]\tLoss: 285.408936\n",
      "Train Epoche: 1 [1241/96 (1293%)]\tLoss: 36.093647\n",
      "Train Epoche: 1 [1242/96 (1294%)]\tLoss: 0.635077\n",
      "Train Epoche: 1 [1243/96 (1295%)]\tLoss: 21.296383\n",
      "Train Epoche: 1 [1244/96 (1296%)]\tLoss: 0.394819\n",
      "Train Epoche: 1 [1245/96 (1297%)]\tLoss: 1.042965\n",
      "Train Epoche: 1 [1246/96 (1298%)]\tLoss: 3.839577\n",
      "Train Epoche: 1 [1247/96 (1299%)]\tLoss: 40.696465\n",
      "Train Epoche: 1 [1248/96 (1300%)]\tLoss: 4.725252\n",
      "Train Epoche: 1 [1249/96 (1301%)]\tLoss: 350.670258\n",
      "Train Epoche: 1 [1250/96 (1302%)]\tLoss: 7.014246\n",
      "Train Epoche: 1 [1251/96 (1303%)]\tLoss: 25.222866\n",
      "Train Epoche: 1 [1252/96 (1304%)]\tLoss: 20.053522\n",
      "Train Epoche: 1 [1253/96 (1305%)]\tLoss: 7.545681\n",
      "Train Epoche: 1 [1254/96 (1306%)]\tLoss: 0.103626\n",
      "Train Epoche: 1 [1255/96 (1307%)]\tLoss: 4.778622\n",
      "Train Epoche: 1 [1256/96 (1308%)]\tLoss: 29.891973\n",
      "Train Epoche: 1 [1257/96 (1309%)]\tLoss: 17.237026\n",
      "Train Epoche: 1 [1258/96 (1310%)]\tLoss: 1.256866\n",
      "Train Epoche: 1 [1259/96 (1311%)]\tLoss: 35.037155\n",
      "Train Epoche: 1 [1260/96 (1312%)]\tLoss: 2.038950\n",
      "Train Epoche: 1 [1261/96 (1314%)]\tLoss: 92.211327\n",
      "Train Epoche: 1 [1262/96 (1315%)]\tLoss: 0.735529\n",
      "Train Epoche: 1 [1263/96 (1316%)]\tLoss: 7.150311\n",
      "Train Epoche: 1 [1264/96 (1317%)]\tLoss: 61.439835\n",
      "Train Epoche: 1 [1265/96 (1318%)]\tLoss: 5.165081\n",
      "Train Epoche: 1 [1266/96 (1319%)]\tLoss: 1.314584\n",
      "Train Epoche: 1 [1267/96 (1320%)]\tLoss: 31.789627\n",
      "Train Epoche: 1 [1268/96 (1321%)]\tLoss: 0.405865\n",
      "Train Epoche: 1 [1269/96 (1322%)]\tLoss: 2.535896\n",
      "Train Epoche: 1 [1270/96 (1323%)]\tLoss: 96.905548\n",
      "Train Epoche: 1 [1271/96 (1324%)]\tLoss: 4.831070\n",
      "Train Epoche: 1 [1272/96 (1325%)]\tLoss: 2.554842\n",
      "Train Epoche: 1 [1273/96 (1326%)]\tLoss: 21.833084\n",
      "Train Epoche: 1 [1274/96 (1327%)]\tLoss: 1.320535\n",
      "Train Epoche: 1 [1275/96 (1328%)]\tLoss: 184.357422\n",
      "Train Epoche: 1 [1276/96 (1329%)]\tLoss: 29.520615\n",
      "Train Epoche: 1 [1277/96 (1330%)]\tLoss: 19.231480\n",
      "Train Epoche: 1 [1278/96 (1331%)]\tLoss: 17.102945\n",
      "Train Epoche: 1 [1279/96 (1332%)]\tLoss: 5.071608\n",
      "Train Epoche: 1 [1280/96 (1333%)]\tLoss: 97.050301\n",
      "Train Epoche: 1 [1281/96 (1334%)]\tLoss: 41.096210\n",
      "Train Epoche: 1 [1282/96 (1335%)]\tLoss: 102.980179\n",
      "Train Epoche: 1 [1283/96 (1336%)]\tLoss: 1.236560\n",
      "Train Epoche: 1 [1284/96 (1338%)]\tLoss: 2.024973\n",
      "Train Epoche: 1 [1285/96 (1339%)]\tLoss: 162.513077\n",
      "Train Epoche: 1 [1286/96 (1340%)]\tLoss: 102.298454\n",
      "Train Epoche: 1 [1287/96 (1341%)]\tLoss: 0.192926\n",
      "Train Epoche: 1 [1288/96 (1342%)]\tLoss: 1.417072\n",
      "Train Epoche: 1 [1289/96 (1343%)]\tLoss: 20.438046\n",
      "Train Epoche: 1 [1290/96 (1344%)]\tLoss: 5.326744\n",
      "Train Epoche: 1 [1291/96 (1345%)]\tLoss: 91.223824\n",
      "Train Epoche: 1 [1292/96 (1346%)]\tLoss: 0.371196\n",
      "Train Epoche: 1 [1293/96 (1347%)]\tLoss: 5.485053\n",
      "Train Epoche: 1 [1294/96 (1348%)]\tLoss: 0.374324\n",
      "Train Epoche: 1 [1295/96 (1349%)]\tLoss: 77.697296\n",
      "Train Epoche: 1 [1296/96 (1350%)]\tLoss: 14.460616\n",
      "Train Epoche: 1 [1297/96 (1351%)]\tLoss: 10.871089\n",
      "Train Epoche: 1 [1298/96 (1352%)]\tLoss: 145.587158\n",
      "Train Epoche: 1 [1299/96 (1353%)]\tLoss: 61.367878\n",
      "Train Epoche: 1 [1300/96 (1354%)]\tLoss: 26.213093\n",
      "Train Epoche: 1 [1301/96 (1355%)]\tLoss: 3.156449\n",
      "Train Epoche: 1 [1302/96 (1356%)]\tLoss: 90.197144\n",
      "Train Epoche: 1 [1303/96 (1357%)]\tLoss: 5.147064\n",
      "Train Epoche: 1 [1304/96 (1358%)]\tLoss: 24.889023\n",
      "Train Epoche: 1 [1305/96 (1359%)]\tLoss: 121.588013\n",
      "Train Epoche: 1 [1306/96 (1360%)]\tLoss: 67.296577\n",
      "Train Epoche: 1 [1307/96 (1361%)]\tLoss: 3.455441\n",
      "Train Epoche: 1 [1308/96 (1362%)]\tLoss: 10.186410\n",
      "Train Epoche: 1 [1309/96 (1364%)]\tLoss: 45.866470\n",
      "Train Epoche: 1 [1310/96 (1365%)]\tLoss: 60.769939\n",
      "Train Epoche: 1 [1311/96 (1366%)]\tLoss: 0.476997\n",
      "Train Epoche: 1 [1312/96 (1367%)]\tLoss: 44.173504\n",
      "Train Epoche: 1 [1313/96 (1368%)]\tLoss: 6.231293\n",
      "Train Epoche: 1 [1314/96 (1369%)]\tLoss: 6.843208\n",
      "Train Epoche: 1 [1315/96 (1370%)]\tLoss: 19.078989\n",
      "Train Epoche: 1 [1316/96 (1371%)]\tLoss: 32.266994\n",
      "Train Epoche: 1 [1317/96 (1372%)]\tLoss: 20.753786\n",
      "Train Epoche: 1 [1318/96 (1373%)]\tLoss: 4.776617\n",
      "Train Epoche: 1 [1319/96 (1374%)]\tLoss: 7.810893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1320/96 (1375%)]\tLoss: 15.324812\n",
      "Train Epoche: 1 [1321/96 (1376%)]\tLoss: 17.992195\n",
      "Train Epoche: 1 [1322/96 (1377%)]\tLoss: 14.362525\n",
      "Train Epoche: 1 [1323/96 (1378%)]\tLoss: 212.326111\n",
      "Train Epoche: 1 [1324/96 (1379%)]\tLoss: 4.001437\n",
      "Train Epoche: 1 [1325/96 (1380%)]\tLoss: 3.203600\n",
      "Train Epoche: 1 [1326/96 (1381%)]\tLoss: 18.850498\n",
      "Train Epoche: 1 [1327/96 (1382%)]\tLoss: 186.005646\n",
      "Train Epoche: 1 [1328/96 (1383%)]\tLoss: 7.299314\n",
      "Train Epoche: 1 [1329/96 (1384%)]\tLoss: 20.561178\n",
      "Train Epoche: 1 [1330/96 (1385%)]\tLoss: 117.645264\n",
      "Train Epoche: 1 [1331/96 (1386%)]\tLoss: 0.392610\n",
      "Train Epoche: 1 [1332/96 (1388%)]\tLoss: 0.371951\n",
      "Train Epoche: 1 [1333/96 (1389%)]\tLoss: 2.502972\n",
      "Train Epoche: 1 [1334/96 (1390%)]\tLoss: 104.215324\n",
      "Train Epoche: 1 [1335/96 (1391%)]\tLoss: 28.962883\n",
      "Train Epoche: 1 [1336/96 (1392%)]\tLoss: 214.787018\n",
      "Train Epoche: 1 [1337/96 (1393%)]\tLoss: 20.584948\n",
      "Train Epoche: 1 [1338/96 (1394%)]\tLoss: 11.016150\n",
      "Train Epoche: 1 [1339/96 (1395%)]\tLoss: 13.286571\n",
      "Train Epoche: 1 [1340/96 (1396%)]\tLoss: 44.619987\n",
      "Train Epoche: 1 [1341/96 (1397%)]\tLoss: 33.326256\n",
      "Train Epoche: 1 [1342/96 (1398%)]\tLoss: 0.681169\n",
      "Train Epoche: 1 [1343/96 (1399%)]\tLoss: 46.538403\n",
      "Train Epoche: 1 [1344/96 (1400%)]\tLoss: 18.764797\n",
      "Train Epoche: 1 [1345/96 (1401%)]\tLoss: 3.007253\n",
      "Train Epoche: 1 [1346/96 (1402%)]\tLoss: 111.082664\n",
      "Train Epoche: 1 [1347/96 (1403%)]\tLoss: 3.197078\n",
      "Train Epoche: 1 [1348/96 (1404%)]\tLoss: 0.403866\n",
      "Train Epoche: 1 [1349/96 (1405%)]\tLoss: 6.870846\n",
      "Train Epoche: 1 [1350/96 (1406%)]\tLoss: 15.472464\n",
      "Train Epoche: 1 [1351/96 (1407%)]\tLoss: 141.892319\n",
      "Train Epoche: 1 [1352/96 (1408%)]\tLoss: 35.234577\n",
      "Train Epoche: 1 [1353/96 (1409%)]\tLoss: 7.097068\n",
      "Train Epoche: 1 [1354/96 (1410%)]\tLoss: 1.368671\n",
      "Train Epoche: 1 [1355/96 (1411%)]\tLoss: 0.310972\n",
      "Train Epoche: 1 [1356/96 (1412%)]\tLoss: 0.058831\n",
      "Train Epoche: 1 [1357/96 (1414%)]\tLoss: 16.567997\n",
      "Train Epoche: 1 [1358/96 (1415%)]\tLoss: 13.604906\n",
      "Train Epoche: 1 [1359/96 (1416%)]\tLoss: 4.112370\n",
      "Train Epoche: 1 [1360/96 (1417%)]\tLoss: 19.326202\n",
      "Train Epoche: 1 [1361/96 (1418%)]\tLoss: 44.650120\n",
      "Train Epoche: 1 [1362/96 (1419%)]\tLoss: 20.043247\n",
      "Train Epoche: 1 [1363/96 (1420%)]\tLoss: 158.569504\n",
      "Train Epoche: 1 [1364/96 (1421%)]\tLoss: 84.633698\n",
      "Train Epoche: 1 [1365/96 (1422%)]\tLoss: 4.222792\n",
      "Train Epoche: 1 [1366/96 (1423%)]\tLoss: 2.278351\n",
      "Train Epoche: 1 [1367/96 (1424%)]\tLoss: 34.416115\n",
      "Train Epoche: 1 [1368/96 (1425%)]\tLoss: 31.204102\n",
      "Train Epoche: 1 [1369/96 (1426%)]\tLoss: 21.662428\n",
      "Train Epoche: 1 [1370/96 (1427%)]\tLoss: 27.896320\n",
      "Train Epoche: 1 [1371/96 (1428%)]\tLoss: 7.906018\n",
      "Train Epoche: 1 [1372/96 (1429%)]\tLoss: 7.836800\n",
      "Train Epoche: 1 [1373/96 (1430%)]\tLoss: 9.381750\n",
      "Train Epoche: 1 [1374/96 (1431%)]\tLoss: 75.120476\n",
      "Train Epoche: 1 [1375/96 (1432%)]\tLoss: 142.572845\n",
      "Train Epoche: 1 [1376/96 (1433%)]\tLoss: 45.165668\n",
      "Train Epoche: 1 [1377/96 (1434%)]\tLoss: 85.820259\n",
      "Train Epoche: 1 [1378/96 (1435%)]\tLoss: 58.062721\n",
      "Train Epoche: 1 [1379/96 (1436%)]\tLoss: 44.545532\n",
      "Train Epoche: 1 [1380/96 (1438%)]\tLoss: 123.854630\n",
      "Train Epoche: 1 [1381/96 (1439%)]\tLoss: 21.628508\n",
      "Train Epoche: 1 [1382/96 (1440%)]\tLoss: 0.585922\n",
      "Train Epoche: 1 [1383/96 (1441%)]\tLoss: 6.633855\n",
      "Train Epoche: 1 [1384/96 (1442%)]\tLoss: 0.836139\n",
      "Train Epoche: 1 [1385/96 (1443%)]\tLoss: 41.206882\n",
      "Train Epoche: 1 [1386/96 (1444%)]\tLoss: 1.398523\n",
      "Train Epoche: 1 [1387/96 (1445%)]\tLoss: 3.635999\n",
      "Train Epoche: 1 [1388/96 (1446%)]\tLoss: 1.064838\n",
      "Train Epoche: 1 [1389/96 (1447%)]\tLoss: 3.344642\n",
      "Train Epoche: 1 [1390/96 (1448%)]\tLoss: 3.993695\n",
      "Train Epoche: 1 [1391/96 (1449%)]\tLoss: 11.090133\n",
      "Train Epoche: 1 [1392/96 (1450%)]\tLoss: 0.123870\n",
      "Train Epoche: 1 [1393/96 (1451%)]\tLoss: 20.991838\n",
      "Train Epoche: 1 [1394/96 (1452%)]\tLoss: 0.104457\n",
      "Train Epoche: 1 [1395/96 (1453%)]\tLoss: 34.842216\n",
      "Train Epoche: 1 [1396/96 (1454%)]\tLoss: 0.978739\n",
      "Train Epoche: 1 [1397/96 (1455%)]\tLoss: 20.476597\n",
      "Train Epoche: 1 [1398/96 (1456%)]\tLoss: 9.942685\n",
      "Train Epoche: 1 [1399/96 (1457%)]\tLoss: 2.584554\n",
      "Train Epoche: 1 [1400/96 (1458%)]\tLoss: 87.291603\n",
      "Train Epoche: 1 [1401/96 (1459%)]\tLoss: 17.916697\n",
      "Train Epoche: 1 [1402/96 (1460%)]\tLoss: 33.099270\n",
      "Train Epoche: 1 [1403/96 (1461%)]\tLoss: 10.238167\n",
      "Train Epoche: 1 [1404/96 (1462%)]\tLoss: 0.302905\n",
      "Train Epoche: 1 [1405/96 (1464%)]\tLoss: 4.112096\n",
      "Train Epoche: 1 [1406/96 (1465%)]\tLoss: 5.428890\n",
      "Train Epoche: 1 [1407/96 (1466%)]\tLoss: 29.206251\n",
      "Train Epoche: 1 [1408/96 (1467%)]\tLoss: 0.723158\n",
      "Train Epoche: 1 [1409/96 (1468%)]\tLoss: 26.429090\n",
      "Train Epoche: 1 [1410/96 (1469%)]\tLoss: 73.122299\n",
      "Train Epoche: 1 [1411/96 (1470%)]\tLoss: 45.244301\n",
      "Train Epoche: 1 [1412/96 (1471%)]\tLoss: 4.237789\n",
      "Train Epoche: 1 [1413/96 (1472%)]\tLoss: 51.101559\n",
      "Train Epoche: 1 [1414/96 (1473%)]\tLoss: 81.680344\n",
      "Train Epoche: 1 [1415/96 (1474%)]\tLoss: 28.787807\n",
      "Train Epoche: 1 [1416/96 (1475%)]\tLoss: 9.690682\n",
      "Train Epoche: 1 [1417/96 (1476%)]\tLoss: 4.290383\n",
      "Train Epoche: 1 [1418/96 (1477%)]\tLoss: 32.096558\n",
      "Train Epoche: 1 [1419/96 (1478%)]\tLoss: 51.441090\n",
      "Train Epoche: 1 [1420/96 (1479%)]\tLoss: 41.988438\n",
      "Train Epoche: 1 [1421/96 (1480%)]\tLoss: 8.129290\n",
      "Train Epoche: 1 [1422/96 (1481%)]\tLoss: 30.367889\n",
      "Train Epoche: 1 [1423/96 (1482%)]\tLoss: 25.469160\n",
      "Train Epoche: 1 [1424/96 (1483%)]\tLoss: 51.126919\n",
      "Train Epoche: 1 [1425/96 (1484%)]\tLoss: 93.140030\n",
      "Train Epoche: 1 [1426/96 (1485%)]\tLoss: 97.920334\n",
      "Train Epoche: 1 [1427/96 (1486%)]\tLoss: 48.138901\n",
      "Train Epoche: 1 [1428/96 (1488%)]\tLoss: 13.568949\n",
      "Train Epoche: 1 [1429/96 (1489%)]\tLoss: 4.613573\n",
      "Train Epoche: 1 [1430/96 (1490%)]\tLoss: 88.142731\n",
      "Train Epoche: 1 [1431/96 (1491%)]\tLoss: 0.181685\n",
      "Train Epoche: 1 [1432/96 (1492%)]\tLoss: 6.116799\n",
      "Train Epoche: 1 [1433/96 (1493%)]\tLoss: 12.225252\n",
      "Train Epoche: 1 [1434/96 (1494%)]\tLoss: 9.718709\n",
      "Train Epoche: 1 [1435/96 (1495%)]\tLoss: 1.476400\n",
      "Train Epoche: 1 [1436/96 (1496%)]\tLoss: 119.121468\n",
      "Train Epoche: 1 [1437/96 (1497%)]\tLoss: 189.800247\n",
      "Train Epoche: 1 [1438/96 (1498%)]\tLoss: 1.754434\n",
      "Train Epoche: 1 [1439/96 (1499%)]\tLoss: 0.159719\n",
      "Train Epoche: 1 [1440/96 (1500%)]\tLoss: 3.255971\n",
      "Train Epoche: 1 [1441/96 (1501%)]\tLoss: 5.715023\n",
      "Train Epoche: 1 [1442/96 (1502%)]\tLoss: 10.624586\n",
      "Train Epoche: 1 [1443/96 (1503%)]\tLoss: 72.939812\n",
      "Train Epoche: 1 [1444/96 (1504%)]\tLoss: 19.351452\n",
      "Train Epoche: 1 [1445/96 (1505%)]\tLoss: 50.263081\n",
      "Train Epoche: 1 [1446/96 (1506%)]\tLoss: 9.947764\n",
      "Train Epoche: 1 [1447/96 (1507%)]\tLoss: 23.885786\n",
      "Train Epoche: 1 [1448/96 (1508%)]\tLoss: 7.325442\n",
      "Train Epoche: 1 [1449/96 (1509%)]\tLoss: 0.512385\n",
      "Train Epoche: 1 [1450/96 (1510%)]\tLoss: 8.863491\n",
      "Train Epoche: 1 [1451/96 (1511%)]\tLoss: 13.157681\n",
      "Train Epoche: 1 [1452/96 (1512%)]\tLoss: 9.739012\n",
      "Train Epoche: 1 [1453/96 (1514%)]\tLoss: 194.530762\n",
      "Train Epoche: 1 [1454/96 (1515%)]\tLoss: 0.071106\n",
      "Train Epoche: 1 [1455/96 (1516%)]\tLoss: 23.872734\n",
      "Train Epoche: 1 [1456/96 (1517%)]\tLoss: 21.151525\n",
      "Train Epoche: 1 [1457/96 (1518%)]\tLoss: 19.496574\n",
      "Train Epoche: 1 [1458/96 (1519%)]\tLoss: 15.213128\n",
      "Train Epoche: 1 [1459/96 (1520%)]\tLoss: 60.952053\n",
      "Train Epoche: 1 [1460/96 (1521%)]\tLoss: 18.637705\n",
      "Train Epoche: 1 [1461/96 (1522%)]\tLoss: 251.641342\n",
      "Train Epoche: 1 [1462/96 (1523%)]\tLoss: 5.399059\n",
      "Train Epoche: 1 [1463/96 (1524%)]\tLoss: 28.277534\n",
      "Train Epoche: 1 [1464/96 (1525%)]\tLoss: 242.542892\n",
      "Train Epoche: 1 [1465/96 (1526%)]\tLoss: 11.036344\n",
      "Train Epoche: 1 [1466/96 (1527%)]\tLoss: 0.313920\n",
      "Train Epoche: 1 [1467/96 (1528%)]\tLoss: 160.258606\n",
      "Train Epoche: 1 [1468/96 (1529%)]\tLoss: 8.630207\n",
      "Train Epoche: 1 [1469/96 (1530%)]\tLoss: 6.284429\n",
      "Train Epoche: 1 [1470/96 (1531%)]\tLoss: 107.336021\n",
      "Train Epoche: 1 [1471/96 (1532%)]\tLoss: 35.522530\n",
      "Train Epoche: 1 [1472/96 (1533%)]\tLoss: 37.739422\n",
      "Train Epoche: 1 [1473/96 (1534%)]\tLoss: 8.921973\n",
      "Train Epoche: 1 [1474/96 (1535%)]\tLoss: 2.211054\n",
      "Train Epoche: 1 [1475/96 (1536%)]\tLoss: 10.600824\n",
      "Train Epoche: 1 [1476/96 (1538%)]\tLoss: 38.387623\n",
      "Train Epoche: 1 [1477/96 (1539%)]\tLoss: 25.227913\n",
      "Train Epoche: 1 [1478/96 (1540%)]\tLoss: 0.227895\n",
      "Train Epoche: 1 [1479/96 (1541%)]\tLoss: 4.323816\n",
      "Train Epoche: 1 [1480/96 (1542%)]\tLoss: 5.979877\n",
      "Train Epoche: 1 [1481/96 (1543%)]\tLoss: 13.230579\n",
      "Train Epoche: 1 [1482/96 (1544%)]\tLoss: 5.459728\n",
      "Train Epoche: 1 [1483/96 (1545%)]\tLoss: 0.055272\n",
      "Train Epoche: 1 [1484/96 (1546%)]\tLoss: 18.693541\n",
      "Train Epoche: 1 [1485/96 (1547%)]\tLoss: 99.866508\n",
      "Train Epoche: 1 [1486/96 (1548%)]\tLoss: 258.318909\n",
      "Train Epoche: 1 [1487/96 (1549%)]\tLoss: 21.642078\n",
      "Train Epoche: 1 [1488/96 (1550%)]\tLoss: 1.021788\n",
      "Train Epoche: 1 [1489/96 (1551%)]\tLoss: 8.249776\n",
      "Train Epoche: 1 [1490/96 (1552%)]\tLoss: 3.689351\n",
      "Train Epoche: 1 [1491/96 (1553%)]\tLoss: 13.594439\n",
      "Train Epoche: 1 [1492/96 (1554%)]\tLoss: 19.956251\n",
      "Train Epoche: 1 [1493/96 (1555%)]\tLoss: 8.735755\n",
      "Train Epoche: 1 [1494/96 (1556%)]\tLoss: 1.418388\n",
      "Train Epoche: 1 [1495/96 (1557%)]\tLoss: 23.946852\n",
      "Train Epoche: 1 [1496/96 (1558%)]\tLoss: 97.867859\n",
      "Train Epoche: 1 [1497/96 (1559%)]\tLoss: 7.982453\n",
      "Train Epoche: 1 [1498/96 (1560%)]\tLoss: 2.303163\n",
      "Train Epoche: 1 [1499/96 (1561%)]\tLoss: 2.512991\n",
      "Train Epoche: 1 [1500/96 (1562%)]\tLoss: 56.438457\n",
      "Train Epoche: 1 [1501/96 (1564%)]\tLoss: 20.435112\n",
      "Train Epoche: 1 [1502/96 (1565%)]\tLoss: 60.209919\n",
      "Train Epoche: 1 [1503/96 (1566%)]\tLoss: 0.281400\n",
      "Train Epoche: 1 [1504/96 (1567%)]\tLoss: 0.353826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1505/96 (1568%)]\tLoss: 56.683247\n",
      "Train Epoche: 1 [1506/96 (1569%)]\tLoss: 161.624786\n",
      "Train Epoche: 1 [1507/96 (1570%)]\tLoss: 19.741293\n",
      "Train Epoche: 1 [1508/96 (1571%)]\tLoss: 0.227330\n",
      "Train Epoche: 1 [1509/96 (1572%)]\tLoss: 47.442284\n",
      "Train Epoche: 1 [1510/96 (1573%)]\tLoss: 70.571579\n",
      "Train Epoche: 1 [1511/96 (1574%)]\tLoss: 38.556194\n",
      "Train Epoche: 1 [1512/96 (1575%)]\tLoss: 17.014875\n",
      "Train Epoche: 1 [1513/96 (1576%)]\tLoss: 46.534645\n",
      "Train Epoche: 1 [1514/96 (1577%)]\tLoss: 10.656707\n",
      "Train Epoche: 1 [1515/96 (1578%)]\tLoss: 2.453399\n",
      "Train Epoche: 1 [1516/96 (1579%)]\tLoss: 15.673967\n",
      "Train Epoche: 1 [1517/96 (1580%)]\tLoss: 197.034500\n",
      "Train Epoche: 1 [1518/96 (1581%)]\tLoss: 2.902243\n",
      "Train Epoche: 1 [1519/96 (1582%)]\tLoss: 230.786514\n",
      "Train Epoche: 1 [1520/96 (1583%)]\tLoss: 0.286437\n",
      "Train Epoche: 1 [1521/96 (1584%)]\tLoss: 25.733242\n",
      "Train Epoche: 1 [1522/96 (1585%)]\tLoss: 77.682564\n",
      "Train Epoche: 1 [1523/96 (1586%)]\tLoss: 199.939331\n",
      "Train Epoche: 1 [1524/96 (1588%)]\tLoss: 5.089627\n",
      "Train Epoche: 1 [1525/96 (1589%)]\tLoss: 23.197742\n",
      "Train Epoche: 1 [1526/96 (1590%)]\tLoss: 78.354820\n",
      "Train Epoche: 1 [1527/96 (1591%)]\tLoss: 77.355431\n",
      "Train Epoche: 1 [1528/96 (1592%)]\tLoss: 3.446892\n",
      "Train Epoche: 1 [1529/96 (1593%)]\tLoss: 99.377808\n",
      "Train Epoche: 1 [1530/96 (1594%)]\tLoss: 71.462646\n",
      "Train Epoche: 1 [1531/96 (1595%)]\tLoss: 1.639200\n",
      "Train Epoche: 1 [1532/96 (1596%)]\tLoss: 0.257536\n",
      "Train Epoche: 1 [1533/96 (1597%)]\tLoss: 82.438766\n",
      "Train Epoche: 1 [1534/96 (1598%)]\tLoss: 118.654793\n",
      "Train Epoche: 1 [1535/96 (1599%)]\tLoss: 15.716573\n",
      "Train Epoche: 1 [1536/96 (1600%)]\tLoss: 8.845232\n",
      "Train Epoche: 1 [1537/96 (1601%)]\tLoss: 18.672428\n",
      "Train Epoche: 1 [1538/96 (1602%)]\tLoss: 200.965073\n",
      "Train Epoche: 1 [1539/96 (1603%)]\tLoss: 38.767097\n",
      "Train Epoche: 1 [1540/96 (1604%)]\tLoss: 19.174057\n",
      "Train Epoche: 1 [1541/96 (1605%)]\tLoss: 396.550171\n",
      "Train Epoche: 1 [1542/96 (1606%)]\tLoss: 17.418171\n",
      "Train Epoche: 1 [1543/96 (1607%)]\tLoss: 303.339722\n",
      "Train Epoche: 1 [1544/96 (1608%)]\tLoss: 45.219296\n",
      "Train Epoche: 1 [1545/96 (1609%)]\tLoss: 11.526030\n",
      "Train Epoche: 1 [1546/96 (1610%)]\tLoss: 2.657568\n",
      "Train Epoche: 1 [1547/96 (1611%)]\tLoss: 5.439528\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6a9efb9a51df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_T\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdev_T\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_combinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_information\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mopt_combination\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt_combination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-cfa4529a649e>\u001b[0m in \u001b[0;36mvalidate_combinations\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;31m#trainieren des modells\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-cfa4529a649e>\u001b[0m in \u001b[0;36m__train\u001b[1;34m(self, epoch, optimizer)\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                     print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n\u001b[0;32m    106\u001b[0m                         \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l = HP_Layer_Optimizer(layer_range=(8,12),\n",
    "                       max_epochs = 6,\n",
    "                       input_start = 10,\n",
    "                       activations = ['relu'], \n",
    "                       random_activation=True,\n",
    "                       create_combinations=True, \n",
    "                       create_variations = True, \n",
    "                       cuda = False)\n",
    "#l.model_specs_combinations = netze\n",
    "l.train_data = train_T\n",
    "l.test_data = dev_T\n",
    "l.validate_combinations()\n",
    "l.get_all_information()\n",
    "opt_combination = l.opt_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt_layer = HP_Layer_Optimizer(layer_range=(6,10),\n",
    "#                               in_out_range = (30,200),\n",
    "#                               random_activation=True,#randompick von activation function, momentan geht False nicht\n",
    "#                               cuda = True,\n",
    "#                               dropout_num = 1,#anzahl zu erzeugender dropoutlayer\n",
    "#                               lr = 0.0001,#lernrate für adam optimizer\n",
    "#                               max_epochs = 5,#anzahl der Trainingsepochen\n",
    "#                               activations = ['relu'],#normalerweise relu, tanh, sigmoid --> auskommentieren wenn wieder default\n",
    "#                              input_start = 10\n",
    "#                              )\n",
    "#opt_layer.train_data = train_T\n",
    "#opt_layer.test_data = dev_T\n",
    "##opt_layer.validate_combinations()\n",
    "##opt_layer.get_all_information()\n",
    "##opt_combination = opt_layer.opt_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Netz(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Netz,self).__init__()\n",
    "        #self.fc1 = nn.Linear(53, 150)\n",
    "        self.fc1 = nn.Linear(10, 150)\n",
    "        self.fc2 = nn.Linear(150, 180)\n",
    "        self.fc3 = nn.Linear(180, 190)\n",
    "        self.fc4 = nn.Linear(190, 120)\n",
    "        self.fc5 = nn.Linear(120, 100)\n",
    "        self.fc6 = nn.Linear(100, 70)\n",
    "        self.fc7 = nn.Linear(70, 30)\n",
    "        self.fc8 = nn.Linear(30, 1)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc2(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc4(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc5(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc6(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc7(x.float())\n",
    "        x = F.relu(x.float())\n",
    "        x = self.fc8(x.float())\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "julian = {'first':['linear', 10,150],\n",
    "         'relu0':['linear',150,180],\n",
    "         'relu1':['dropout',0,0],\n",
    "         'no_activation0':['linear',180,190],\n",
    "         'relu2':['linear',190,120],\n",
    "         'relu3':['linear',120,100],\n",
    "         'relu4':['linear',100,70],\n",
    "         'relu5':['linear',70,30],\n",
    "         'relu6':['linear',30,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HP_Optimizer(object):\n",
    "    \n",
    "    def __init__(self, lr_range = (0.0001,0.0001), step_size = 0.0001, max_epochs = (2,2), opt = 'Adam', cuda = True, dynamic = False, dyn_combination = {}):\n",
    "        \n",
    "        self.__model = Netz()\n",
    "        self.__lr = lr_range\n",
    "        self.__epochs = max_epochs\n",
    "        self.__optimizer = opt\n",
    "        self.__steps = step_size\n",
    "        self.__combination_results = {}\n",
    "        self.__combination_overview = {}\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.cuda = cuda\n",
    "        self.opt_combination = {}\n",
    "        self.__dynamic = dynamic\n",
    "        self.__dyn_combination = dyn_combination\n",
    "        \n",
    "        \n",
    "    def validate_combinations(self):\n",
    "        \n",
    "        specifics = {}\n",
    "        if self.__optimizer == 'Adam':\n",
    "            \n",
    "            #definieren der range für die lernratenoptimierung\n",
    "            '''\n",
    "            \n",
    "            IST HIER WAS KAPUTT?\n",
    "            \n",
    "            '''\n",
    "            if isinstance(self.__lr, tuple):\n",
    "                lr_s = self.__lr[0]\n",
    "                lr_e = self.__lr[1]\n",
    "                #wurde eine range für die anzahl der epochen übergeben?\n",
    "                if self.__epochs[0] == self.__epochs[1]:\n",
    "                    #hyperparamter epochenanzahl wird nicht optimiert\n",
    "                    max_epoch = self.__epochs[0]\n",
    "                    if lr_s == lr_e:\n",
    "                        #es ist keine range für die lernrate gegeben, in der diese optimiert werden soll\n",
    "                        print('Parameter Epochen und Lernrate können nicht optimiert werden, da kein Intervall übergeben wurde')\n",
    "                        #setzen des optimizers als Adam und erzeugen des Modells\n",
    "                        if self.__dynamic:\n",
    "                            self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                        else:\n",
    "                            self.__model = Netz()\n",
    "                        optimizer = optim.Adam(self.__model.parameters(), lr = lr_s)\n",
    "                        for epoch in range(1,max_epoch):\n",
    "                            self.__train(epoch, optimizer)     \n",
    "                        result = self.__test()\n",
    "                        A = result.prediction.tolist()\n",
    "                        y = result.target.tolist()\n",
    "                        mae = MAE(A,y)\n",
    "                        specifics = {}\n",
    "                        specifics['lr'] = lr_s\n",
    "                        specifics['epochen'] = max_epoch\n",
    "                        key = random.randint(0,10000)\n",
    "                        while key in list(self.__combination_results.keys()):\n",
    "                             key = random.randint(0,10000)\n",
    "                        self.__combination_results[key] = mae\n",
    "                        self.__combination_overview[key] = specifics \n",
    "                    else:\n",
    "                        for l in np.arange(lr_s,lr_e, self.__steps):\n",
    "                            #setzen des optimizers als Adam und erzeugen des Modells\n",
    "                            if self.__dynamic:\n",
    "                                self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                            else:\n",
    "                                self.__model = Netz()\n",
    "                            optimizer = optim.Adam(self.__model.parameters(), lr = l)\n",
    "                            #trainieren des modells\n",
    "                            for epoch in range(1,max_epoch):\n",
    "                                self.__train(epoch, optimizer)  \n",
    "                            result = self.__test()\n",
    "                            A = result.prediction.tolist()\n",
    "                            y = result.target.tolist()\n",
    "                            mae = MAE(A,y)\n",
    "                            specifics = {}\n",
    "                            specifics['lr'] = l\n",
    "                            specifics['epochen'] = max_epoch\n",
    "                            #abspeichern der gewonnenen informationen (MAE nach lr und anzahl durchgeführter epochen)\n",
    "                            key = random.randint(0,10000)\n",
    "                            while key in list(self.__combination_results.keys()):\n",
    "                                 key = random.randint(0,10000)\n",
    "                            self.__combination_results[key] = mae\n",
    "                            self.__combination_overview[key] = specifics\n",
    "                else:\n",
    "                    for max_epoch in range(self.__epochs[0], self.__epochs[1]):\n",
    "                        #definieren der range für die lernratenoptimierung\n",
    "                        lr_s = self.__lr[0]\n",
    "                        lr_e = self.__lr[1]\n",
    "                        #hyperparamter epochenanzahl wird nicht optimiert\n",
    "                        if lr_s == lr_e:\n",
    "                            #es ist keine range für die lernrate gegeben, in der diese optimiert werden soll\n",
    "                            #print('Parameter Epochen und Lernrate können nicht optimiert werden, da kein Intervall übergeben wurde')\n",
    "                            if self.__dynamic:\n",
    "                                self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                            else:\n",
    "                                self.__model = Netz()\n",
    "                            optimizer = optim.Adam(self.__model.parameters(), lr = lr_s)\n",
    "                            for epoch in range(1,max_epoch):\n",
    "                                self.__train(epoch, optimizer)       \n",
    "                            result = self.__test()\n",
    "                            A = result.prediction.tolist()\n",
    "                            y = result.target.tolist()\n",
    "                            mae = MAE(A,y)\n",
    "                            specifics = {}\n",
    "                            specifics['lr'] = lr_s\n",
    "                            specifics['epochen'] = max_epoch\n",
    "                            key = random.randint(0,10000)\n",
    "                            while key in list(self.__combination_results.keys()):\n",
    "                                 key = random.randint(0,10000)  \n",
    "                            self.__combination_results[key] = mae\n",
    "                            self.__combination_overview[key] = specifics          \n",
    "                        else:\n",
    "                            for l in np.arange(lr_s,lr_e, self.__steps):\n",
    "                                if self.__dynamic:\n",
    "                                    self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                                else:\n",
    "                                    self.__model = Netz()\n",
    "                                optimizer = optim.Adam(self.__model.parameters(), lr = l)\n",
    "                                #trainieren des modells\n",
    "                                for epoch in range(1,max_epoch):\n",
    "                                    self.__train(epoch, optimizer)  \n",
    "                                result = self.__test()\n",
    "                                A = result.prediction.tolist()\n",
    "                                y = result.target.tolist()\n",
    "                                mae = MAE(A,y)\n",
    "                                specifics = {}\n",
    "                                specifics['lr'] = l\n",
    "                                specifics['epochen'] = max_epoch\n",
    "                                #abspeichern der gewonnenen informationen (MAE nach lr und anzahl durchgeführter epochen)\n",
    "                                key = random.randint(0,10000)\n",
    "                                while key in list(self.__combination_results.keys()):\n",
    "                                     key = random.randint(0,10000)\n",
    "                                self.__combination_results[key] = mae\n",
    "                                self.__combination_overview[key] = specifics\n",
    "                #finden der besten kombination nach minimalstem Error (MAE)\n",
    "                key_min = min(self.__combination_results.keys(), key=(lambda k: self.__combination_results[k]))\n",
    "                best_combination = self.__combination_overview[key_min]\n",
    "                best_combination['mae'] = self.__combination_results[key_min]\n",
    "                self.opt_combination = best_combination\n",
    "            elif isinstance(self.__lr, list):\n",
    "                #wurde eine range für die anzahl der epochen übergeben?\n",
    "                if self.__epochs[0] == self.__epochs[1]:\n",
    "                    #hyperparamter epochenanzahl wird nicht optimiert\n",
    "                    max_epoch = self.__epochs[0]\n",
    "                    if len(self.__lr) ==1:\n",
    "                        #es ist keine range für die lernrate gegeben, in der diese optimiert werden soll\n",
    "                        print('Parameter Epochen und Lernrate können nicht optimiert werden, da kein Intervall übergeben wurde')\n",
    "                        #setzen des optimizers als Adam und erzeugen des Modells\n",
    "                        if self.__dynamic:\n",
    "                            self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                        else:\n",
    "                            self.__model = Netz()\n",
    "                        optimizer = optim.Adam(self.__model.parameters(), lr = self.__lr[0])\n",
    "                        for epoch in range(1,max_epoch):\n",
    "                            self.__train(epoch, optimizer)     \n",
    "                        result = self.__test()\n",
    "                        A = result.prediction.tolist()\n",
    "                        y = result.target.tolist()\n",
    "                        mae = MAE(A,y)\n",
    "                        specifics = {}\n",
    "                        specifics['lr'] = self.__lr[0]\n",
    "                        specifics['epochen'] = max_epoch\n",
    "                        key = random.randint(0,10000)\n",
    "                        while key in list(self.__combination_results.keys()):\n",
    "                             key = random.randint(0,10000)\n",
    "                        self.__combination_results[key] = mae\n",
    "                        self.__combination_overview[key] = specifics \n",
    "                    else:\n",
    "                        for l in self.__lr:\n",
    "                            #setzen des optimizers als Adam und erzeugen des Modells\n",
    "                            if self.__dynamic:\n",
    "                                self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                            else:\n",
    "                                self.__model = Netz()\n",
    "                            optimizer = optim.Adam(self.__model.parameters(), lr = l)\n",
    "                            #trainieren des modells\n",
    "                            for epoch in range(1,max_epoch):\n",
    "                                self.__train(epoch, optimizer)  \n",
    "                            result = self.__test()\n",
    "                            A = result.prediction.tolist()\n",
    "                            y = result.target.tolist()\n",
    "                            mae = MAE(A,y)\n",
    "                            specifics = {}\n",
    "                            specifics['lr'] = l\n",
    "                            specifics['epochen'] = max_epoch\n",
    "                            #abspeichern der gewonnenen informationen (MAE nach lr und anzahl durchgeführter epochen)\n",
    "                            key = random.randint(0,10000)\n",
    "                            while key in list(self.__combination_results.keys()):\n",
    "                                 key = random.randint(0,10000)\n",
    "                            self.__combination_results[key] = mae\n",
    "                            self.__combination_overview[key] = specifics\n",
    "                else:\n",
    "                    for max_epoch in range(self.__epochs[0], self.__epochs[1]):\n",
    "                        #definieren der range für die lernratenoptimierung\n",
    "                        lr_s = self.__lr[0]\n",
    "                        lr_e = self.__lr[1]\n",
    "                        #hyperparamter epochenanzahl wird nicht optimiert\n",
    "                        if len(self.__lr) == 1:\n",
    "                            #es ist keine range für die lernrate gegeben, in der diese optimiert werden soll\n",
    "                            #print('Parameter Epochen und Lernrate können nicht optimiert werden, da kein Intervall übergeben wurde')\n",
    "                            if self.__dynamic:\n",
    "                                self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                            else:\n",
    "                                self.__model = Netz()\n",
    "                            optimizer = optim.Adam(self.__model.parameters(), lr = self.__lr[0])\n",
    "                            for epoch in range(1,max_epoch):\n",
    "                                self.__train(epoch, optimizer)       \n",
    "                            result = self.__test()\n",
    "                            A = result.prediction.tolist()\n",
    "                            y = result.target.tolist()\n",
    "                            mae = MAE(A,y)\n",
    "                            specifics = {}\n",
    "                            specifics['lr'] = self.__lr[0]\n",
    "                            specifics['epochen'] = max_epoch\n",
    "                            key = random.randint(0,10000)\n",
    "                            while key in list(self.__combination_results.keys()):\n",
    "                                 key = random.randint(0,10000)  \n",
    "                            self.__combination_results[key] = mae\n",
    "                            self.__combination_overview[key] = specifics          \n",
    "                        else:\n",
    "                            for l in self.__lr:\n",
    "                                if self.__dynamic:\n",
    "                                    self.__model = NetzDynamic(self.__dyn_combination)\n",
    "                                else:\n",
    "                                    self.__model = Netz()\n",
    "                                optimizer = optim.Adam(self.__model.parameters(), lr = l)\n",
    "                                #trainieren des modells\n",
    "                                for epoch in range(1,max_epoch):\n",
    "                                    self.__train(epoch, optimizer)  \n",
    "                                result = self.__test()\n",
    "                                A = result.prediction.tolist()\n",
    "                                y = result.target.tolist()\n",
    "                                mae = MAE(A,y)\n",
    "                                specifics = {}\n",
    "                                specifics['lr'] = l\n",
    "                                specifics['epochen'] = max_epoch\n",
    "                                #abspeichern der gewonnenen informationen (MAE nach lr und anzahl durchgeführter epochen)\n",
    "                                key = random.randint(0,10000)\n",
    "                                while key in list(self.__combination_results.keys()):\n",
    "                                     key = random.randint(0,10000)\n",
    "                                self.__combination_results[key] = mae\n",
    "                                self.__combination_overview[key] = specifics\n",
    "                #finden der besten kombination nach minimalstem Error (MAE)\n",
    "                key_min = min(self.__combination_results.keys(), key=(lambda k: self.__combination_results[k]))\n",
    "                best_combination = self.__combination_overview[key_min]\n",
    "                best_combination['mae'] = self.__combination_results[key_min]\n",
    "                self.opt_combination = best_combination\n",
    "                \n",
    "        else:\n",
    "            raise ('No valid optimizer given! Try Adam for example!')\n",
    "            \n",
    "    def __train(self, epoch, optimizer):\n",
    "        if self.cuda:\n",
    "            self.__model.cuda()\n",
    "            self.__model.train()\n",
    "            batch_id = 0\n",
    "            for key in self.train_data.keys():\n",
    "                for data, target in self.train_data[key]:\n",
    "                    data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1).cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    out = self.__model(data)\n",
    "                    #print(\"Out: \", out, out.size())\n",
    "                    #print(\"Target: \", target, target.size())\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss = criterion(out, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch, batch_id *len(data), len(self.train_data),\n",
    "                    100. * batch_id / len(self.train_data), loss.item()))\n",
    "                    batch_id +=1\n",
    "        else:\n",
    "            self.__model.train()\n",
    "            batch_id = 0\n",
    "            for key in self.train_data.keys():\n",
    "                for data, target in self.train_data[key]:\n",
    "                    #data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)#.cuda()\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1)#.cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    out = self.__model(data)\n",
    "                    #print(\"Out: \", out, out.size())\n",
    "                    #print(\"Target: \", target, target.size())\n",
    "                    criterion = nn.MSELoss()\n",
    "                    loss = criterion(out, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    #    epoch, batch_id *len(data), len(train_data),\n",
    "                    #100. * batch_id / len(train_data), loss.item()))\n",
    "                    batch_id +=1\n",
    "                    \n",
    "            \n",
    "    def __test(self):\n",
    "        total = 0\n",
    "        count = 0\n",
    "        result_dict = {}\n",
    "        result = pd.DataFrame(columns = ['target','prediction'])\n",
    "        help_dict = {}\n",
    "        if self.cuda:\n",
    "            self.__model.cuda()\n",
    "            for key in self.test_data.keys():\n",
    "                #print(key)\n",
    "                help_dict = {}\n",
    "                for data, target in self.test_data[key]:\n",
    "                    self.__model.eval()\n",
    "                    #files.listdir(path)\n",
    "                    data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1).cuda()\n",
    "                    out = self.__model(data).cpu()\n",
    "                    #print(out)\n",
    "                    out = out.detach().numpy()\n",
    "                    #out = np.round(out)\n",
    "                    target = target.cpu()\n",
    "                    target = target.detach().numpy()\n",
    "                    #print(data)\n",
    "                    #print(data[\"driverId\"])\n",
    "                    total += abs(out - target[0][0])\n",
    "                    #print(\"current_position: \", data[0][0].item())\n",
    "                    #print(\"Output: \", out)\n",
    "                    #print(\"Target: \", target)\n",
    "                    help_dict[target[0][0]] = out\n",
    "                    #print(\"Difference: \", out - target)\n",
    "                    count+=1\n",
    "                #Auslesen der predicteten Werte A und der zugehörigen targets y\n",
    "                A = [x[0][0] for x in list((help_dict.values()))]\n",
    "                y = list(help_dict.keys())\n",
    "                \n",
    "                #Anfügen der Werte an Result\n",
    "                t = pd.DataFrame()\n",
    "                t['target'] = y\n",
    "                t['prediction_value'] = A\n",
    "                t = sqldf.sqldf('''select * from t order by prediction_value ASC''')\n",
    "                t.reset_index(inplace = True)\n",
    "                t.rename(columns = {'index':'prediction'}, inplace = True)\n",
    "                t['prediction'] = t['prediction']+1\n",
    "                \n",
    "                \n",
    "                result = result.append(t, sort = True)\n",
    "                \n",
    "        else:\n",
    "            for raceId in self.test_data.keys():\n",
    "                #print(key)\n",
    "                help_dict = {}\n",
    "                for data, target in self.test_data[raceId]:\n",
    "                    self.__model.eval()\n",
    "                    #files.listdir(path)\n",
    "                    #data = data.cuda()\n",
    "                    target = torch.Tensor(target).unsqueeze(0)\n",
    "                    shape = target.size()[1]\n",
    "                    target = target.resize(shape,1)#.cuda()\n",
    "                    out = self.__model(data)#.cpu()\n",
    "                    #print(out)\n",
    "                    out = out.detach().numpy()\n",
    "                    #out = np.round(out)\n",
    "                    #target = target.cpu()\n",
    "                    target = target.detach().numpy()\n",
    "                    #print(data)\n",
    "                    #print(data[\"driverId\"])\n",
    "                    total += abs(out - target[0][0])\n",
    "                    #print(\"current_position: \", data[0][0].item())\n",
    "                    #print(\"Output: \", out)\n",
    "                    #print(\"Target: \", target)\n",
    "                    help_dict[target[0][0]] = out\n",
    "                    #print(\"Difference: \", out - target)\n",
    "                    count+=1\n",
    "                #Auslesen der predicteten Werte A und der zugehörigen targets y\n",
    "                A = [x[0][0] for x in list((help_dict.values()))]\n",
    "                y = list(help_dict.keys())\n",
    "                \n",
    "                #Anfügen der Werte an Result\n",
    "                t = pd.DataFrame()\n",
    "                t['target'] = y\n",
    "                t['prediction_value'] = A\n",
    "                t = sqldf.sqldf('''select * from t order by prediction_value ASC''')\n",
    "                t.reset_index(inplace = True)\n",
    "                t.rename(columns = {'index':'prediction'}, inplace = True)\n",
    "                t['prediction'] = t['prediction']+1\n",
    "                \n",
    "                result = result.append(t, sort = True)\n",
    "        return result\n",
    "         \n",
    "            \n",
    "    def get_all_information(self):\n",
    "        \n",
    "        print('Chosen Model:',self.__model)\n",
    "        print('Learningrate Range:',self.__lr)\n",
    "        print('Maximum Epochs:', self.__epochs)\n",
    "        print('Chosen Optimizer:', self.__optimizer)\n",
    "        print('Result Encoding:', self.__combination_overview)\n",
    "        print('Results:', self.__combination_results)\n",
    "        print('Optimale Kombination:', self.opt_combination)\n",
    "        \n",
    "    def help(self):\n",
    "        print('Parameters with defaults:\\nlr_range --> (0.0001,0.0001),\\nstep_size--> 0.0001,\\nmax_epochs-->(2,2),\\nopt-->\"Adam\",\\ncuda=True')\n",
    "        print('lr_range: Tupel with learnrate range')\n",
    "        print('step_size: float/int for step_size of learnrate')\n",
    "        print('max_epochs: Tupel with number of epochs range')\n",
    "        print('opt: Optimizer (by default Adam)')\n",
    "        print('cuda: True/False if cuda should be used, default = True\\n')\n",
    "        print('Attributes:')\n",
    "        print('set self.train_data as dictionary with races (form: {raceId: race(dataframe)})')\n",
    "        print('set self.test_data as dictionary with races (form: {raceId: race(dataframe)})')\n",
    "        print('self.opt_combination: Dictionary which contains the best combination of the given parameters\\n')\n",
    "        print('Methods:')\n",
    "        print('call self.validate_combination() to compare all combinations')\n",
    "        print('get all information/results with self.get_all_information()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paralleles Aufrufen des Optimierers, übergeben der zuvor erzeugten Test- und Trainingsdatensätze, angeben der Intervalle für die die Hyperparameter getestet werden sollen (Lernrate und Epochenanzahl). Für das dynamische Modell wird das Dictionary übergeben, welches von dem Layer Optimizer am besten bewertet wurde. Der \"normale\" HP_Optimizer basiert auf der Klasse Netz()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters with defaults:\n",
      "lr_range --> (0.0001,0.0001),\n",
      "step_size--> 0.0001,\n",
      "max_epochs-->(2,2),\n",
      "opt-->\"Adam\",\n",
      "cuda=True\n",
      "lr_range: Tupel with learnrate range\n",
      "step_size: float/int for step_size of learnrate\n",
      "max_epochs: Tupel with number of epochs range\n",
      "opt: Optimizer (by default Adam)\n",
      "cuda: True/False if cuda should be used, default = True\n",
      "\n",
      "Attributes:\n",
      "set self.train_data as dictionary with races (form: {raceId: race(dataframe)})\n",
      "set self.test_data as dictionary with races (form: {raceId: race(dataframe)})\n",
      "self.opt_combination: Dictionary which contains the best combination of the given parameters\n",
      "\n",
      "Methods:\n",
      "call self.validate_combination() to compare all combinations\n",
      "get all information/results with self.get_all_information()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n",
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Model: Netz(\n",
      "  (fc1): Linear(in_features=10, out_features=150, bias=True)\n",
      "  (fc2): Linear(in_features=150, out_features=180, bias=True)\n",
      "  (fc3): Linear(in_features=180, out_features=190, bias=True)\n",
      "  (fc4): Linear(in_features=190, out_features=120, bias=True)\n",
      "  (fc5): Linear(in_features=120, out_features=100, bias=True)\n",
      "  (fc6): Linear(in_features=100, out_features=70, bias=True)\n",
      "  (fc7): Linear(in_features=70, out_features=30, bias=True)\n",
      "  (fc8): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Learningrate Range: [0.0002, 0.00045]\n",
      "Maximum Epochs: (3, 5)\n",
      "Chosen Optimizer: Adam\n",
      "Result Encoding: {4645: {'lr': 0.0002, 'epochen': 3}, 6904: {'lr': 0.00045, 'epochen': 3}, 5686: {'lr': 0.0002, 'epochen': 4}, 3013: {'lr': 0.00045, 'epochen': 4, 'mae': 2.2250489236790605}}\n",
      "Results: {4645: 2.5538160469667317, 6904: 2.3424657534246576, 5686: 2.2915851272015657, 3013: 2.2250489236790605}\n",
      "Optimale Kombination: {'lr': 0.00045, 'epochen': 4, 'mae': 2.2250489236790605}\n",
      "Chosen Model: NetzDynamic(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=150, bias=True)\n",
      "    (1): Linear(in_features=150, out_features=180, bias=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=180, out_features=190, bias=True)\n",
      "    (4): Linear(in_features=190, out_features=120, bias=True)\n",
      "    (5): Linear(in_features=120, out_features=100, bias=True)\n",
      "    (6): Linear(in_features=100, out_features=70, bias=True)\n",
      "    (7): Linear(in_features=70, out_features=30, bias=True)\n",
      "    (8): Linear(in_features=30, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Learningrate Range: [0.0002, 0.00045]\n",
      "Maximum Epochs: (3, 5)\n",
      "Chosen Optimizer: Adam\n",
      "Result Encoding: {1520: {'lr': 0.0002, 'epochen': 3}, 5791: {'lr': 0.00045, 'epochen': 3, 'mae': 2.2015655577299413}, 5933: {'lr': 0.0002, 'epochen': 4}, 547: {'lr': 0.00045, 'epochen': 4}}\n",
      "Results: {1520: 2.350293542074364, 5791: 2.2015655577299413, 5933: 2.3228962818003915, 547: 2.299412915851272}\n",
      "Optimale Kombination: {'lr': 0.00045, 'epochen': 3, 'mae': 2.2015655577299413}\n"
     ]
    }
   ],
   "source": [
    "h = HP_Optimizer(lr_range = [0.0002,0.00045],step_size = 0.0001, max_epochs=(3,5),cuda = False)\n",
    "opt_={}\n",
    "#for key in opt_combination.keys():\n",
    "#    if key != 'mae':#mae wird noch in dictionary von HP_Optimizer hinzugefügt und muss entfernt werden\n",
    "#        opt_[key] = opt_combination[key]\n",
    "#h_dynamic = HP_Optimizer(lr_range = [0.0002,0.00045],step_size = 0.0001, max_epochs=(3,5),cuda = False, dynamic = True, dyn_combination = opt_)\n",
    "h_dynamic = HP_Optimizer(lr_range = [0.0002,0.00045],step_size = 0.0001, max_epochs=(3,5),cuda = False, dynamic = True, dyn_combination = julian)\n",
    "h.help()\n",
    "h.train_data = train_T\n",
    "h_dynamic.train_data = train_T\n",
    "\n",
    "h.test_data = dev_T\n",
    "h_dynamic.test_data = dev_T\n",
    "\n",
    "h.validate_combinations()\n",
    "h_dynamic.validate_combinations()\n",
    "\n",
    "h.get_all_information()\n",
    "h_dynamic.get_all_information()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden die als optimal in den gegebenen Intervallen genommenen Werte aus dem Optimierer genommen und ein neues Modell mit diesen Parametern trainiert. Zum Schluss werden die Ergebnisse des Testlaufes auf den Testdaten ausgegeben.\n",
    "Dies geschieht genauso mit dem zuvor dynamisch erzeugten Modell. Diese können so direkt miteinander verglichen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hwebe\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [0/96 (0%)]\tLoss: 395.346191\n",
      "Train Epoche: 1 [1/96 (1%)]\tLoss: 140.219345\n",
      "Train Epoche: 1 [2/96 (2%)]\tLoss: 78.740875\n",
      "Train Epoche: 1 [3/96 (3%)]\tLoss: 116.809723\n",
      "Train Epoche: 1 [4/96 (4%)]\tLoss: 391.713287\n",
      "Train Epoche: 1 [5/96 (5%)]\tLoss: 45.097420\n",
      "Train Epoche: 1 [6/96 (6%)]\tLoss: 95.557068\n",
      "Train Epoche: 1 [7/96 (7%)]\tLoss: 162.535187\n",
      "Train Epoche: 1 [8/96 (8%)]\tLoss: 7.424778\n",
      "Train Epoche: 1 [9/96 (9%)]\tLoss: 2.785810\n",
      "Train Epoche: 1 [10/96 (10%)]\tLoss: 58.264370\n",
      "Train Epoche: 1 [11/96 (11%)]\tLoss: 30.966530\n",
      "Train Epoche: 1 [12/96 (12%)]\tLoss: 384.086670\n",
      "Train Epoche: 1 [13/96 (14%)]\tLoss: 12.478821\n",
      "Train Epoche: 1 [14/96 (15%)]\tLoss: 208.961334\n",
      "Train Epoche: 1 [15/96 (16%)]\tLoss: 375.262543\n",
      "Train Epoche: 1 [16/96 (17%)]\tLoss: 0.015841\n",
      "Train Epoche: 1 [17/96 (18%)]\tLoss: 15.986426\n",
      "Train Epoche: 1 [18/96 (19%)]\tLoss: 155.796310\n",
      "Train Epoche: 1 [19/96 (20%)]\tLoss: 211.171753\n",
      "Train Epoche: 1 [20/96 (21%)]\tLoss: 52.303596\n",
      "Train Epoche: 1 [21/96 (22%)]\tLoss: 296.412506\n",
      "Train Epoche: 1 [22/96 (23%)]\tLoss: 446.008667\n",
      "Train Epoche: 1 [23/96 (24%)]\tLoss: 530.318665\n",
      "Train Epoche: 1 [24/96 (25%)]\tLoss: 196.018906\n",
      "Train Epoche: 1 [25/96 (26%)]\tLoss: 161.469131\n",
      "Train Epoche: 1 [26/96 (27%)]\tLoss: 30.994827\n",
      "Train Epoche: 1 [27/96 (28%)]\tLoss: 54.654716\n",
      "Train Epoche: 1 [28/96 (29%)]\tLoss: 94.381317\n",
      "Train Epoche: 1 [29/96 (30%)]\tLoss: 13.923079\n",
      "Train Epoche: 1 [30/96 (31%)]\tLoss: 1.253779\n",
      "Train Epoche: 1 [31/96 (32%)]\tLoss: 4.075930\n",
      "Train Epoche: 1 [32/96 (33%)]\tLoss: 95.399208\n",
      "Train Epoche: 1 [33/96 (34%)]\tLoss: 61.334530\n",
      "Train Epoche: 1 [34/96 (35%)]\tLoss: 9.059769\n",
      "Train Epoche: 1 [35/96 (36%)]\tLoss: 0.628821\n",
      "Train Epoche: 1 [36/96 (38%)]\tLoss: 41.405952\n",
      "Train Epoche: 1 [37/96 (39%)]\tLoss: 60.587234\n",
      "Train Epoche: 1 [38/96 (40%)]\tLoss: 30.238882\n",
      "Train Epoche: 1 [39/96 (41%)]\tLoss: 1.840073\n",
      "Train Epoche: 1 [40/96 (42%)]\tLoss: 661.023499\n",
      "Train Epoche: 1 [41/96 (43%)]\tLoss: 326.148834\n",
      "Train Epoche: 1 [42/96 (44%)]\tLoss: 1.389184\n",
      "Train Epoche: 1 [43/96 (45%)]\tLoss: 22.896881\n",
      "Train Epoche: 1 [44/96 (46%)]\tLoss: 196.041306\n",
      "Train Epoche: 1 [45/96 (47%)]\tLoss: 73.334717\n",
      "Train Epoche: 1 [46/96 (48%)]\tLoss: 9.047869\n",
      "Train Epoche: 1 [47/96 (49%)]\tLoss: 44.154922\n",
      "Train Epoche: 1 [48/96 (50%)]\tLoss: 1.502149\n",
      "Train Epoche: 1 [49/96 (51%)]\tLoss: 76.551384\n",
      "Train Epoche: 1 [50/96 (52%)]\tLoss: 46.202930\n",
      "Train Epoche: 1 [51/96 (53%)]\tLoss: 22.850035\n",
      "Train Epoche: 1 [52/96 (54%)]\tLoss: 1.031679\n",
      "Train Epoche: 1 [53/96 (55%)]\tLoss: 5.009647\n",
      "Train Epoche: 1 [54/96 (56%)]\tLoss: 166.046768\n",
      "Train Epoche: 1 [55/96 (57%)]\tLoss: 5.595058\n",
      "Train Epoche: 1 [56/96 (58%)]\tLoss: 4.921844\n",
      "Train Epoche: 1 [57/96 (59%)]\tLoss: 315.023041\n",
      "Train Epoche: 1 [58/96 (60%)]\tLoss: 76.936111\n",
      "Train Epoche: 1 [59/96 (61%)]\tLoss: 281.447510\n",
      "Train Epoche: 1 [60/96 (62%)]\tLoss: 28.325953\n",
      "Train Epoche: 1 [61/96 (64%)]\tLoss: 6.844527\n",
      "Train Epoche: 1 [62/96 (65%)]\tLoss: 40.089138\n",
      "Train Epoche: 1 [63/96 (66%)]\tLoss: 6.308597\n",
      "Train Epoche: 1 [64/96 (67%)]\tLoss: 21.675291\n",
      "Train Epoche: 1 [65/96 (68%)]\tLoss: 12.189886\n",
      "Train Epoche: 1 [66/96 (69%)]\tLoss: 0.805635\n",
      "Train Epoche: 1 [67/96 (70%)]\tLoss: 18.275589\n",
      "Train Epoche: 1 [68/96 (71%)]\tLoss: 11.035843\n",
      "Train Epoche: 1 [69/96 (72%)]\tLoss: 11.399883\n",
      "Train Epoche: 1 [70/96 (73%)]\tLoss: 10.871724\n",
      "Train Epoche: 1 [71/96 (74%)]\tLoss: 1.163859\n",
      "Train Epoche: 1 [72/96 (75%)]\tLoss: 44.183830\n",
      "Train Epoche: 1 [73/96 (76%)]\tLoss: 12.485203\n",
      "Train Epoche: 1 [74/96 (77%)]\tLoss: 182.095718\n",
      "Train Epoche: 1 [75/96 (78%)]\tLoss: 37.586769\n",
      "Train Epoche: 1 [76/96 (79%)]\tLoss: 15.882730\n",
      "Train Epoche: 1 [77/96 (80%)]\tLoss: 105.212502\n",
      "Train Epoche: 1 [78/96 (81%)]\tLoss: 50.378456\n",
      "Train Epoche: 1 [79/96 (82%)]\tLoss: 5.732223\n",
      "Train Epoche: 1 [80/96 (83%)]\tLoss: 26.616230\n",
      "Train Epoche: 1 [81/96 (84%)]\tLoss: 15.607833\n",
      "Train Epoche: 1 [82/96 (85%)]\tLoss: 55.288448\n",
      "Train Epoche: 1 [83/96 (86%)]\tLoss: 63.207729\n",
      "Train Epoche: 1 [84/96 (88%)]\tLoss: 84.790833\n",
      "Train Epoche: 1 [85/96 (89%)]\tLoss: 84.644791\n",
      "Train Epoche: 1 [86/96 (90%)]\tLoss: 176.651596\n",
      "Train Epoche: 1 [87/96 (91%)]\tLoss: 48.329147\n",
      "Train Epoche: 1 [88/96 (92%)]\tLoss: 1.045855\n",
      "Train Epoche: 1 [89/96 (93%)]\tLoss: 0.836916\n",
      "Train Epoche: 1 [90/96 (94%)]\tLoss: 18.451818\n",
      "Train Epoche: 1 [91/96 (95%)]\tLoss: 143.172363\n",
      "Train Epoche: 1 [92/96 (96%)]\tLoss: 0.940558\n",
      "Train Epoche: 1 [93/96 (97%)]\tLoss: 9.631673\n",
      "Train Epoche: 1 [94/96 (98%)]\tLoss: 66.844666\n",
      "Train Epoche: 1 [95/96 (99%)]\tLoss: 64.133522\n",
      "Train Epoche: 1 [96/96 (100%)]\tLoss: 30.681250\n",
      "Train Epoche: 1 [97/96 (101%)]\tLoss: 56.349236\n",
      "Train Epoche: 1 [98/96 (102%)]\tLoss: 39.523945\n",
      "Train Epoche: 1 [99/96 (103%)]\tLoss: 1.133395\n",
      "Train Epoche: 1 [100/96 (104%)]\tLoss: 6.767006\n",
      "Train Epoche: 1 [101/96 (105%)]\tLoss: 46.950169\n",
      "Train Epoche: 1 [102/96 (106%)]\tLoss: 28.934694\n",
      "Train Epoche: 1 [103/96 (107%)]\tLoss: 8.427785\n",
      "Train Epoche: 1 [104/96 (108%)]\tLoss: 6.543509\n",
      "Train Epoche: 1 [105/96 (109%)]\tLoss: 56.674026\n",
      "Train Epoche: 1 [106/96 (110%)]\tLoss: 20.294296\n",
      "Train Epoche: 1 [107/96 (111%)]\tLoss: 11.851411\n",
      "Train Epoche: 1 [108/96 (112%)]\tLoss: 10.651940\n",
      "Train Epoche: 1 [109/96 (114%)]\tLoss: 55.204296\n",
      "Train Epoche: 1 [110/96 (115%)]\tLoss: 4.749437\n",
      "Train Epoche: 1 [111/96 (116%)]\tLoss: 0.515478\n",
      "Train Epoche: 1 [112/96 (117%)]\tLoss: 20.456173\n",
      "Train Epoche: 1 [113/96 (118%)]\tLoss: 41.840519\n",
      "Train Epoche: 1 [114/96 (119%)]\tLoss: 52.266769\n",
      "Train Epoche: 1 [115/96 (120%)]\tLoss: 136.939651\n",
      "Train Epoche: 1 [116/96 (121%)]\tLoss: 64.238518\n",
      "Train Epoche: 1 [117/96 (122%)]\tLoss: 71.795357\n",
      "Train Epoche: 1 [118/96 (123%)]\tLoss: 48.049362\n",
      "Train Epoche: 1 [119/96 (124%)]\tLoss: 0.030597\n",
      "Train Epoche: 1 [120/96 (125%)]\tLoss: 39.938393\n",
      "Train Epoche: 1 [121/96 (126%)]\tLoss: 17.735023\n",
      "Train Epoche: 1 [122/96 (127%)]\tLoss: 16.077623\n",
      "Train Epoche: 1 [123/96 (128%)]\tLoss: 0.476143\n",
      "Train Epoche: 1 [124/96 (129%)]\tLoss: 0.910961\n",
      "Train Epoche: 1 [125/96 (130%)]\tLoss: 15.216209\n",
      "Train Epoche: 1 [126/96 (131%)]\tLoss: 12.726819\n",
      "Train Epoche: 1 [127/96 (132%)]\tLoss: 3.024567\n",
      "Train Epoche: 1 [128/96 (133%)]\tLoss: 24.233971\n",
      "Train Epoche: 1 [129/96 (134%)]\tLoss: 30.579145\n",
      "Train Epoche: 1 [130/96 (135%)]\tLoss: 101.101456\n",
      "Train Epoche: 1 [131/96 (136%)]\tLoss: 5.455829\n",
      "Train Epoche: 1 [132/96 (138%)]\tLoss: 5.214275\n",
      "Train Epoche: 1 [133/96 (139%)]\tLoss: 79.425560\n",
      "Train Epoche: 1 [134/96 (140%)]\tLoss: 20.522863\n",
      "Train Epoche: 1 [135/96 (141%)]\tLoss: 20.061913\n",
      "Train Epoche: 1 [136/96 (142%)]\tLoss: 17.397818\n",
      "Train Epoche: 1 [137/96 (143%)]\tLoss: 148.459991\n",
      "Train Epoche: 1 [138/96 (144%)]\tLoss: 6.701219\n",
      "Train Epoche: 1 [139/96 (145%)]\tLoss: 11.057323\n",
      "Train Epoche: 1 [140/96 (146%)]\tLoss: 5.273090\n",
      "Train Epoche: 1 [141/96 (147%)]\tLoss: 66.941368\n",
      "Train Epoche: 1 [142/96 (148%)]\tLoss: 85.635788\n",
      "Train Epoche: 1 [143/96 (149%)]\tLoss: 5.222135\n",
      "Train Epoche: 1 [144/96 (150%)]\tLoss: 0.181111\n",
      "Train Epoche: 1 [145/96 (151%)]\tLoss: 28.130524\n",
      "Train Epoche: 1 [146/96 (152%)]\tLoss: 16.888639\n",
      "Train Epoche: 1 [147/96 (153%)]\tLoss: 45.288261\n",
      "Train Epoche: 1 [148/96 (154%)]\tLoss: 107.331314\n",
      "Train Epoche: 1 [149/96 (155%)]\tLoss: 2.086566\n",
      "Train Epoche: 1 [150/96 (156%)]\tLoss: 47.914268\n",
      "Train Epoche: 1 [151/96 (157%)]\tLoss: 31.557049\n",
      "Train Epoche: 1 [152/96 (158%)]\tLoss: 0.120738\n",
      "Train Epoche: 1 [153/96 (159%)]\tLoss: 18.511200\n",
      "Train Epoche: 1 [154/96 (160%)]\tLoss: 1.345816\n",
      "Train Epoche: 1 [155/96 (161%)]\tLoss: 55.937626\n",
      "Train Epoche: 1 [156/96 (162%)]\tLoss: 46.428596\n",
      "Train Epoche: 1 [157/96 (164%)]\tLoss: 2.670043\n",
      "Train Epoche: 1 [158/96 (165%)]\tLoss: 2.894839\n",
      "Train Epoche: 1 [159/96 (166%)]\tLoss: 7.185812\n",
      "Train Epoche: 1 [160/96 (167%)]\tLoss: 2.291177\n",
      "Train Epoche: 1 [161/96 (168%)]\tLoss: 171.444534\n",
      "Train Epoche: 1 [162/96 (169%)]\tLoss: 78.619774\n",
      "Train Epoche: 1 [163/96 (170%)]\tLoss: 20.814919\n",
      "Train Epoche: 1 [164/96 (171%)]\tLoss: 205.707794\n",
      "Train Epoche: 1 [165/96 (172%)]\tLoss: 36.196465\n",
      "Train Epoche: 1 [166/96 (173%)]\tLoss: 13.954867\n",
      "Train Epoche: 1 [167/96 (174%)]\tLoss: 18.359835\n",
      "Train Epoche: 1 [168/96 (175%)]\tLoss: 95.668503\n",
      "Train Epoche: 1 [169/96 (176%)]\tLoss: 6.972336\n",
      "Train Epoche: 1 [170/96 (177%)]\tLoss: 10.845822\n",
      "Train Epoche: 1 [171/96 (178%)]\tLoss: 16.793882\n",
      "Train Epoche: 1 [172/96 (179%)]\tLoss: 13.276144\n",
      "Train Epoche: 1 [173/96 (180%)]\tLoss: 10.317238\n",
      "Train Epoche: 1 [174/96 (181%)]\tLoss: 26.046076\n",
      "Train Epoche: 1 [175/96 (182%)]\tLoss: 57.911034\n",
      "Train Epoche: 1 [176/96 (183%)]\tLoss: 23.629906\n",
      "Train Epoche: 1 [177/96 (184%)]\tLoss: 9.190049\n",
      "Train Epoche: 1 [178/96 (185%)]\tLoss: 36.889229\n",
      "Train Epoche: 1 [179/96 (186%)]\tLoss: 34.629299\n",
      "Train Epoche: 1 [180/96 (188%)]\tLoss: 79.708511\n",
      "Train Epoche: 1 [181/96 (189%)]\tLoss: 3.078690\n",
      "Train Epoche: 1 [182/96 (190%)]\tLoss: 0.416034\n",
      "Train Epoche: 1 [183/96 (191%)]\tLoss: 109.750809\n",
      "Train Epoche: 1 [184/96 (192%)]\tLoss: 129.492859\n",
      "Train Epoche: 1 [185/96 (193%)]\tLoss: 28.117941\n",
      "Train Epoche: 1 [186/96 (194%)]\tLoss: 100.944160\n",
      "Train Epoche: 1 [187/96 (195%)]\tLoss: 103.971603\n",
      "Train Epoche: 1 [188/96 (196%)]\tLoss: 22.617609\n",
      "Train Epoche: 1 [189/96 (197%)]\tLoss: 42.855053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [190/96 (198%)]\tLoss: 0.051629\n",
      "Train Epoche: 1 [191/96 (199%)]\tLoss: 10.067822\n",
      "Train Epoche: 1 [192/96 (200%)]\tLoss: 0.315764\n",
      "Train Epoche: 1 [193/96 (201%)]\tLoss: 30.583750\n",
      "Train Epoche: 1 [194/96 (202%)]\tLoss: 0.034433\n",
      "Train Epoche: 1 [195/96 (203%)]\tLoss: 0.443062\n",
      "Train Epoche: 1 [196/96 (204%)]\tLoss: 161.874374\n",
      "Train Epoche: 1 [197/96 (205%)]\tLoss: 1.814044\n",
      "Train Epoche: 1 [198/96 (206%)]\tLoss: 2.101446\n",
      "Train Epoche: 1 [199/96 (207%)]\tLoss: 10.311118\n",
      "Train Epoche: 1 [200/96 (208%)]\tLoss: 3.314520\n",
      "Train Epoche: 1 [201/96 (209%)]\tLoss: 15.604074\n",
      "Train Epoche: 1 [202/96 (210%)]\tLoss: 32.997906\n",
      "Train Epoche: 1 [203/96 (211%)]\tLoss: 0.422692\n",
      "Train Epoche: 1 [204/96 (212%)]\tLoss: 0.033852\n",
      "Train Epoche: 1 [205/96 (214%)]\tLoss: 49.407192\n",
      "Train Epoche: 1 [206/96 (215%)]\tLoss: 126.914436\n",
      "Train Epoche: 1 [207/96 (216%)]\tLoss: 0.612019\n",
      "Train Epoche: 1 [208/96 (217%)]\tLoss: 1.827232\n",
      "Train Epoche: 1 [209/96 (218%)]\tLoss: 10.872366\n",
      "Train Epoche: 1 [210/96 (219%)]\tLoss: 69.011467\n",
      "Train Epoche: 1 [211/96 (220%)]\tLoss: 30.802259\n",
      "Train Epoche: 1 [212/96 (221%)]\tLoss: 67.248428\n",
      "Train Epoche: 1 [213/96 (222%)]\tLoss: 6.558539\n",
      "Train Epoche: 1 [214/96 (223%)]\tLoss: 115.138580\n",
      "Train Epoche: 1 [215/96 (224%)]\tLoss: 27.181786\n",
      "Train Epoche: 1 [216/96 (225%)]\tLoss: 74.522720\n",
      "Train Epoche: 1 [217/96 (226%)]\tLoss: 127.927307\n",
      "Train Epoche: 1 [218/96 (227%)]\tLoss: 37.537964\n",
      "Train Epoche: 1 [219/96 (228%)]\tLoss: 2.182592\n",
      "Train Epoche: 1 [220/96 (229%)]\tLoss: 38.577034\n",
      "Train Epoche: 1 [221/96 (230%)]\tLoss: 29.394402\n",
      "Train Epoche: 1 [222/96 (231%)]\tLoss: 30.087429\n",
      "Train Epoche: 1 [223/96 (232%)]\tLoss: 95.019714\n",
      "Train Epoche: 1 [224/96 (233%)]\tLoss: 126.215233\n",
      "Train Epoche: 1 [225/96 (234%)]\tLoss: 13.370270\n",
      "Train Epoche: 1 [226/96 (235%)]\tLoss: 3.141824\n",
      "Train Epoche: 1 [227/96 (236%)]\tLoss: 107.209007\n",
      "Train Epoche: 1 [228/96 (238%)]\tLoss: 74.658241\n",
      "Train Epoche: 1 [229/96 (239%)]\tLoss: 73.447510\n",
      "Train Epoche: 1 [230/96 (240%)]\tLoss: 93.434067\n",
      "Train Epoche: 1 [231/96 (241%)]\tLoss: 1.569548\n",
      "Train Epoche: 1 [232/96 (242%)]\tLoss: 2.106390\n",
      "Train Epoche: 1 [233/96 (243%)]\tLoss: 137.503342\n",
      "Train Epoche: 1 [234/96 (244%)]\tLoss: 84.778343\n",
      "Train Epoche: 1 [235/96 (245%)]\tLoss: 28.573835\n",
      "Train Epoche: 1 [236/96 (246%)]\tLoss: 133.674515\n",
      "Train Epoche: 1 [237/96 (247%)]\tLoss: 450.290314\n",
      "Train Epoche: 1 [238/96 (248%)]\tLoss: 112.062500\n",
      "Train Epoche: 1 [239/96 (249%)]\tLoss: 0.203044\n",
      "Train Epoche: 1 [240/96 (250%)]\tLoss: 20.178600\n",
      "Train Epoche: 1 [241/96 (251%)]\tLoss: 53.512413\n",
      "Train Epoche: 1 [242/96 (252%)]\tLoss: 62.859707\n",
      "Train Epoche: 1 [243/96 (253%)]\tLoss: 0.309827\n",
      "Train Epoche: 1 [244/96 (254%)]\tLoss: 0.686652\n",
      "Train Epoche: 1 [245/96 (255%)]\tLoss: 77.511505\n",
      "Train Epoche: 1 [246/96 (256%)]\tLoss: 17.585478\n",
      "Train Epoche: 1 [247/96 (257%)]\tLoss: 27.195690\n",
      "Train Epoche: 1 [248/96 (258%)]\tLoss: 2.852446\n",
      "Train Epoche: 1 [249/96 (259%)]\tLoss: 124.518860\n",
      "Train Epoche: 1 [250/96 (260%)]\tLoss: 166.042542\n",
      "Train Epoche: 1 [251/96 (261%)]\tLoss: 156.323090\n",
      "Train Epoche: 1 [252/96 (262%)]\tLoss: 148.544662\n",
      "Train Epoche: 1 [253/96 (264%)]\tLoss: 191.270828\n",
      "Train Epoche: 1 [254/96 (265%)]\tLoss: 87.005501\n",
      "Train Epoche: 1 [255/96 (266%)]\tLoss: 9.960578\n",
      "Train Epoche: 1 [256/96 (267%)]\tLoss: 40.424671\n",
      "Train Epoche: 1 [257/96 (268%)]\tLoss: 19.043350\n",
      "Train Epoche: 1 [258/96 (269%)]\tLoss: 121.189934\n",
      "Train Epoche: 1 [259/96 (270%)]\tLoss: 163.501343\n",
      "Train Epoche: 1 [260/96 (271%)]\tLoss: 7.899050\n",
      "Train Epoche: 1 [261/96 (272%)]\tLoss: 2.424900\n",
      "Train Epoche: 1 [262/96 (273%)]\tLoss: 43.115124\n",
      "Train Epoche: 1 [263/96 (274%)]\tLoss: 175.346893\n",
      "Train Epoche: 1 [264/96 (275%)]\tLoss: 47.174454\n",
      "Train Epoche: 1 [265/96 (276%)]\tLoss: 11.580746\n",
      "Train Epoche: 1 [266/96 (277%)]\tLoss: 50.318893\n",
      "Train Epoche: 1 [267/96 (278%)]\tLoss: 72.042580\n",
      "Train Epoche: 1 [268/96 (279%)]\tLoss: 15.959224\n",
      "Train Epoche: 1 [269/96 (280%)]\tLoss: 16.567598\n",
      "Train Epoche: 1 [270/96 (281%)]\tLoss: 336.399536\n",
      "Train Epoche: 1 [271/96 (282%)]\tLoss: 493.340485\n",
      "Train Epoche: 1 [272/96 (283%)]\tLoss: 241.793045\n",
      "Train Epoche: 1 [273/96 (284%)]\tLoss: 286.804871\n",
      "Train Epoche: 1 [274/96 (285%)]\tLoss: 4.700744\n",
      "Train Epoche: 1 [275/96 (286%)]\tLoss: 19.255713\n",
      "Train Epoche: 1 [276/96 (288%)]\tLoss: 45.428810\n",
      "Train Epoche: 1 [277/96 (289%)]\tLoss: 46.911961\n",
      "Train Epoche: 1 [278/96 (290%)]\tLoss: 305.495575\n",
      "Train Epoche: 1 [279/96 (291%)]\tLoss: 23.446308\n",
      "Train Epoche: 1 [280/96 (292%)]\tLoss: 9.482981\n",
      "Train Epoche: 1 [281/96 (293%)]\tLoss: 15.663919\n",
      "Train Epoche: 1 [282/96 (294%)]\tLoss: 0.369192\n",
      "Train Epoche: 1 [283/96 (295%)]\tLoss: 5.482645\n",
      "Train Epoche: 1 [284/96 (296%)]\tLoss: 98.797127\n",
      "Train Epoche: 1 [285/96 (297%)]\tLoss: 7.694776\n",
      "Train Epoche: 1 [286/96 (298%)]\tLoss: 3.685721\n",
      "Train Epoche: 1 [287/96 (299%)]\tLoss: 0.080574\n",
      "Train Epoche: 1 [288/96 (300%)]\tLoss: 39.715199\n",
      "Train Epoche: 1 [289/96 (301%)]\tLoss: 120.478600\n",
      "Train Epoche: 1 [290/96 (302%)]\tLoss: 21.570793\n",
      "Train Epoche: 1 [291/96 (303%)]\tLoss: 479.223755\n",
      "Train Epoche: 1 [292/96 (304%)]\tLoss: 348.659210\n",
      "Train Epoche: 1 [293/96 (305%)]\tLoss: 282.032562\n",
      "Train Epoche: 1 [294/96 (306%)]\tLoss: 97.490616\n",
      "Train Epoche: 1 [295/96 (307%)]\tLoss: 95.192253\n",
      "Train Epoche: 1 [296/96 (308%)]\tLoss: 151.718582\n",
      "Train Epoche: 1 [297/96 (309%)]\tLoss: 78.170090\n",
      "Train Epoche: 1 [298/96 (310%)]\tLoss: 40.848595\n",
      "Train Epoche: 1 [299/96 (311%)]\tLoss: 33.335537\n",
      "Train Epoche: 1 [300/96 (312%)]\tLoss: 59.300030\n",
      "Train Epoche: 1 [301/96 (314%)]\tLoss: 36.799377\n",
      "Train Epoche: 1 [302/96 (315%)]\tLoss: 1.118556\n",
      "Train Epoche: 1 [303/96 (316%)]\tLoss: 267.418518\n",
      "Train Epoche: 1 [304/96 (317%)]\tLoss: 18.722399\n",
      "Train Epoche: 1 [305/96 (318%)]\tLoss: 8.097383\n",
      "Train Epoche: 1 [306/96 (319%)]\tLoss: 21.708950\n",
      "Train Epoche: 1 [307/96 (320%)]\tLoss: 19.886297\n",
      "Train Epoche: 1 [308/96 (321%)]\tLoss: 45.664513\n",
      "Train Epoche: 1 [309/96 (322%)]\tLoss: 13.578399\n",
      "Train Epoche: 1 [310/96 (323%)]\tLoss: 35.209358\n",
      "Train Epoche: 1 [311/96 (324%)]\tLoss: 24.781506\n",
      "Train Epoche: 1 [312/96 (325%)]\tLoss: 69.924232\n",
      "Train Epoche: 1 [313/96 (326%)]\tLoss: 18.116430\n",
      "Train Epoche: 1 [314/96 (327%)]\tLoss: 1.773063\n",
      "Train Epoche: 1 [315/96 (328%)]\tLoss: 71.991966\n",
      "Train Epoche: 1 [316/96 (329%)]\tLoss: 80.924210\n",
      "Train Epoche: 1 [317/96 (330%)]\tLoss: 36.758450\n",
      "Train Epoche: 1 [318/96 (331%)]\tLoss: 0.141994\n",
      "Train Epoche: 1 [319/96 (332%)]\tLoss: 0.486508\n",
      "Train Epoche: 1 [320/96 (333%)]\tLoss: 1.087875\n",
      "Train Epoche: 1 [321/96 (334%)]\tLoss: 0.026286\n",
      "Train Epoche: 1 [322/96 (335%)]\tLoss: 0.969958\n",
      "Train Epoche: 1 [323/96 (336%)]\tLoss: 78.204071\n",
      "Train Epoche: 1 [324/96 (338%)]\tLoss: 80.820953\n",
      "Train Epoche: 1 [325/96 (339%)]\tLoss: 163.056595\n",
      "Train Epoche: 1 [326/96 (340%)]\tLoss: 11.361689\n",
      "Train Epoche: 1 [327/96 (341%)]\tLoss: 24.060295\n",
      "Train Epoche: 1 [328/96 (342%)]\tLoss: 3.784413\n",
      "Train Epoche: 1 [329/96 (343%)]\tLoss: 5.163450\n",
      "Train Epoche: 1 [330/96 (344%)]\tLoss: 10.724399\n",
      "Train Epoche: 1 [331/96 (345%)]\tLoss: 25.320713\n",
      "Train Epoche: 1 [332/96 (346%)]\tLoss: 38.294552\n",
      "Train Epoche: 1 [333/96 (347%)]\tLoss: 43.586937\n",
      "Train Epoche: 1 [334/96 (348%)]\tLoss: 21.030306\n",
      "Train Epoche: 1 [335/96 (349%)]\tLoss: 3.234361\n",
      "Train Epoche: 1 [336/96 (350%)]\tLoss: 0.334402\n",
      "Train Epoche: 1 [337/96 (351%)]\tLoss: 6.268553\n",
      "Train Epoche: 1 [338/96 (352%)]\tLoss: 12.322050\n",
      "Train Epoche: 1 [339/96 (353%)]\tLoss: 3.431498\n",
      "Train Epoche: 1 [340/96 (354%)]\tLoss: 0.844154\n",
      "Train Epoche: 1 [341/96 (355%)]\tLoss: 22.100479\n",
      "Train Epoche: 1 [342/96 (356%)]\tLoss: 147.474396\n",
      "Train Epoche: 1 [343/96 (357%)]\tLoss: 2.517742\n",
      "Train Epoche: 1 [344/96 (358%)]\tLoss: 1.634761\n",
      "Train Epoche: 1 [345/96 (359%)]\tLoss: 16.499973\n",
      "Train Epoche: 1 [346/96 (360%)]\tLoss: 2.692286\n",
      "Train Epoche: 1 [347/96 (361%)]\tLoss: 0.013061\n",
      "Train Epoche: 1 [348/96 (362%)]\tLoss: 20.036291\n",
      "Train Epoche: 1 [349/96 (364%)]\tLoss: 51.017170\n",
      "Train Epoche: 1 [350/96 (365%)]\tLoss: 31.889059\n",
      "Train Epoche: 1 [351/96 (366%)]\tLoss: 2.068905\n",
      "Train Epoche: 1 [352/96 (367%)]\tLoss: 153.492188\n",
      "Train Epoche: 1 [353/96 (368%)]\tLoss: 310.701141\n",
      "Train Epoche: 1 [354/96 (369%)]\tLoss: 35.354553\n",
      "Train Epoche: 1 [355/96 (370%)]\tLoss: 11.748749\n",
      "Train Epoche: 1 [356/96 (371%)]\tLoss: 13.533932\n",
      "Train Epoche: 1 [357/96 (372%)]\tLoss: 95.370186\n",
      "Train Epoche: 1 [358/96 (373%)]\tLoss: 88.972069\n",
      "Train Epoche: 1 [359/96 (374%)]\tLoss: 100.338669\n",
      "Train Epoche: 1 [360/96 (375%)]\tLoss: 92.122749\n",
      "Train Epoche: 1 [361/96 (376%)]\tLoss: 3.974003\n",
      "Train Epoche: 1 [362/96 (377%)]\tLoss: 60.130295\n",
      "Train Epoche: 1 [363/96 (378%)]\tLoss: 1.844355\n",
      "Train Epoche: 1 [364/96 (379%)]\tLoss: 0.145674\n",
      "Train Epoche: 1 [365/96 (380%)]\tLoss: 55.956726\n",
      "Train Epoche: 1 [366/96 (381%)]\tLoss: 1.380610\n",
      "Train Epoche: 1 [367/96 (382%)]\tLoss: 0.954192\n",
      "Train Epoche: 1 [368/96 (383%)]\tLoss: 3.448521\n",
      "Train Epoche: 1 [369/96 (384%)]\tLoss: 0.114016\n",
      "Train Epoche: 1 [370/96 (385%)]\tLoss: 18.795904\n",
      "Train Epoche: 1 [371/96 (386%)]\tLoss: 11.285131\n",
      "Train Epoche: 1 [372/96 (388%)]\tLoss: 16.529503\n",
      "Train Epoche: 1 [373/96 (389%)]\tLoss: 44.919437\n",
      "Train Epoche: 1 [374/96 (390%)]\tLoss: 49.408482\n",
      "Train Epoche: 1 [375/96 (391%)]\tLoss: 21.848236\n",
      "Train Epoche: 1 [376/96 (392%)]\tLoss: 59.584373\n",
      "Train Epoche: 1 [377/96 (393%)]\tLoss: 0.482420\n",
      "Train Epoche: 1 [378/96 (394%)]\tLoss: 6.459126\n",
      "Train Epoche: 1 [379/96 (395%)]\tLoss: 110.206825\n",
      "Train Epoche: 1 [380/96 (396%)]\tLoss: 3.342716\n",
      "Train Epoche: 1 [381/96 (397%)]\tLoss: 13.369147\n",
      "Train Epoche: 1 [382/96 (398%)]\tLoss: 97.286125\n",
      "Train Epoche: 1 [383/96 (399%)]\tLoss: 32.766529\n",
      "Train Epoche: 1 [384/96 (400%)]\tLoss: 0.509022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [385/96 (401%)]\tLoss: 13.385541\n",
      "Train Epoche: 1 [386/96 (402%)]\tLoss: 1.008066\n",
      "Train Epoche: 1 [387/96 (403%)]\tLoss: 234.450134\n",
      "Train Epoche: 1 [388/96 (404%)]\tLoss: 10.620797\n",
      "Train Epoche: 1 [389/96 (405%)]\tLoss: 11.427537\n",
      "Train Epoche: 1 [390/96 (406%)]\tLoss: 34.632710\n",
      "Train Epoche: 1 [391/96 (407%)]\tLoss: 8.281985\n",
      "Train Epoche: 1 [392/96 (408%)]\tLoss: 9.300157\n",
      "Train Epoche: 1 [393/96 (409%)]\tLoss: 0.424898\n",
      "Train Epoche: 1 [394/96 (410%)]\tLoss: 155.079056\n",
      "Train Epoche: 1 [395/96 (411%)]\tLoss: 10.281392\n",
      "Train Epoche: 1 [396/96 (412%)]\tLoss: 6.639551\n",
      "Train Epoche: 1 [397/96 (414%)]\tLoss: 448.911926\n",
      "Train Epoche: 1 [398/96 (415%)]\tLoss: 0.036248\n",
      "Train Epoche: 1 [399/96 (416%)]\tLoss: 35.738178\n",
      "Train Epoche: 1 [400/96 (417%)]\tLoss: 46.793823\n",
      "Train Epoche: 1 [401/96 (418%)]\tLoss: 0.064482\n",
      "Train Epoche: 1 [402/96 (419%)]\tLoss: 4.259655\n",
      "Train Epoche: 1 [403/96 (420%)]\tLoss: 11.213974\n",
      "Train Epoche: 1 [404/96 (421%)]\tLoss: 0.352768\n",
      "Train Epoche: 1 [405/96 (422%)]\tLoss: 0.097441\n",
      "Train Epoche: 1 [406/96 (423%)]\tLoss: 17.836077\n",
      "Train Epoche: 1 [407/96 (424%)]\tLoss: 9.436944\n",
      "Train Epoche: 1 [408/96 (425%)]\tLoss: 4.155221\n",
      "Train Epoche: 1 [409/96 (426%)]\tLoss: 261.450897\n",
      "Train Epoche: 1 [410/96 (427%)]\tLoss: 243.306152\n",
      "Train Epoche: 1 [411/96 (428%)]\tLoss: 2.192626\n",
      "Train Epoche: 1 [412/96 (429%)]\tLoss: 13.335645\n",
      "Train Epoche: 1 [413/96 (430%)]\tLoss: 9.705959\n",
      "Train Epoche: 1 [414/96 (431%)]\tLoss: 148.357544\n",
      "Train Epoche: 1 [415/96 (432%)]\tLoss: 72.218246\n",
      "Train Epoche: 1 [416/96 (433%)]\tLoss: 43.597565\n",
      "Train Epoche: 1 [417/96 (434%)]\tLoss: 132.188705\n",
      "Train Epoche: 1 [418/96 (435%)]\tLoss: 91.810806\n",
      "Train Epoche: 1 [419/96 (436%)]\tLoss: 102.800522\n",
      "Train Epoche: 1 [420/96 (438%)]\tLoss: 2.943791\n",
      "Train Epoche: 1 [421/96 (439%)]\tLoss: 5.150214\n",
      "Train Epoche: 1 [422/96 (440%)]\tLoss: 9.090026\n",
      "Train Epoche: 1 [423/96 (441%)]\tLoss: 1.146224\n",
      "Train Epoche: 1 [424/96 (442%)]\tLoss: 0.059292\n",
      "Train Epoche: 1 [425/96 (443%)]\tLoss: 38.442646\n",
      "Train Epoche: 1 [426/96 (444%)]\tLoss: 9.343717\n",
      "Train Epoche: 1 [427/96 (445%)]\tLoss: 7.104878\n",
      "Train Epoche: 1 [428/96 (446%)]\tLoss: 1.026116\n",
      "Train Epoche: 1 [429/96 (447%)]\tLoss: 53.826447\n",
      "Train Epoche: 1 [430/96 (448%)]\tLoss: 3.446024\n",
      "Train Epoche: 1 [431/96 (449%)]\tLoss: 2.074807\n",
      "Train Epoche: 1 [432/96 (450%)]\tLoss: 121.307816\n",
      "Train Epoche: 1 [433/96 (451%)]\tLoss: 7.682958\n",
      "Train Epoche: 1 [434/96 (452%)]\tLoss: 21.663607\n",
      "Train Epoche: 1 [435/96 (453%)]\tLoss: 0.008681\n",
      "Train Epoche: 1 [436/96 (454%)]\tLoss: 7.329288\n",
      "Train Epoche: 1 [437/96 (455%)]\tLoss: 100.776588\n",
      "Train Epoche: 1 [438/96 (456%)]\tLoss: 32.103485\n",
      "Train Epoche: 1 [439/96 (457%)]\tLoss: 35.230740\n",
      "Train Epoche: 1 [440/96 (458%)]\tLoss: 0.232457\n",
      "Train Epoche: 1 [441/96 (459%)]\tLoss: 53.798042\n",
      "Train Epoche: 1 [442/96 (460%)]\tLoss: 68.230820\n",
      "Train Epoche: 1 [443/96 (461%)]\tLoss: 0.833350\n",
      "Train Epoche: 1 [444/96 (462%)]\tLoss: 4.780168\n",
      "Train Epoche: 1 [445/96 (464%)]\tLoss: 4.564178\n",
      "Train Epoche: 1 [446/96 (465%)]\tLoss: 7.072692\n",
      "Train Epoche: 1 [447/96 (466%)]\tLoss: 166.188644\n",
      "Train Epoche: 1 [448/96 (467%)]\tLoss: 0.061033\n",
      "Train Epoche: 1 [449/96 (468%)]\tLoss: 8.628799\n",
      "Train Epoche: 1 [450/96 (469%)]\tLoss: 0.004938\n",
      "Train Epoche: 1 [451/96 (470%)]\tLoss: 0.314459\n",
      "Train Epoche: 1 [452/96 (471%)]\tLoss: 3.634345\n",
      "Train Epoche: 1 [453/96 (472%)]\tLoss: 0.350140\n",
      "Train Epoche: 1 [454/96 (473%)]\tLoss: 10.001541\n",
      "Train Epoche: 1 [455/96 (474%)]\tLoss: 6.432540\n",
      "Train Epoche: 1 [456/96 (475%)]\tLoss: 4.229367\n",
      "Train Epoche: 1 [457/96 (476%)]\tLoss: 12.340318\n",
      "Train Epoche: 1 [458/96 (477%)]\tLoss: 11.556852\n",
      "Train Epoche: 1 [459/96 (478%)]\tLoss: 3.205688\n",
      "Train Epoche: 1 [460/96 (479%)]\tLoss: 34.565216\n",
      "Train Epoche: 1 [461/96 (480%)]\tLoss: 75.961998\n",
      "Train Epoche: 1 [462/96 (481%)]\tLoss: 30.560627\n",
      "Train Epoche: 1 [463/96 (482%)]\tLoss: 15.137497\n",
      "Train Epoche: 1 [464/96 (483%)]\tLoss: 5.293621\n",
      "Train Epoche: 1 [465/96 (484%)]\tLoss: 49.879444\n",
      "Train Epoche: 1 [466/96 (485%)]\tLoss: 31.945604\n",
      "Train Epoche: 1 [467/96 (486%)]\tLoss: 89.859467\n",
      "Train Epoche: 1 [468/96 (488%)]\tLoss: 155.750977\n",
      "Train Epoche: 1 [469/96 (489%)]\tLoss: 1.473666\n",
      "Train Epoche: 1 [470/96 (490%)]\tLoss: 3.103747\n",
      "Train Epoche: 1 [471/96 (491%)]\tLoss: 61.875309\n",
      "Train Epoche: 1 [472/96 (492%)]\tLoss: 0.128212\n",
      "Train Epoche: 1 [473/96 (493%)]\tLoss: 1.197390\n",
      "Train Epoche: 1 [474/96 (494%)]\tLoss: 2.429042\n",
      "Train Epoche: 1 [475/96 (495%)]\tLoss: 40.870457\n",
      "Train Epoche: 1 [476/96 (496%)]\tLoss: 28.834103\n",
      "Train Epoche: 1 [477/96 (497%)]\tLoss: 31.390068\n",
      "Train Epoche: 1 [478/96 (498%)]\tLoss: 26.411860\n",
      "Train Epoche: 1 [479/96 (499%)]\tLoss: 119.160545\n",
      "Train Epoche: 1 [480/96 (500%)]\tLoss: 80.522614\n",
      "Train Epoche: 1 [481/96 (501%)]\tLoss: 45.289555\n",
      "Train Epoche: 1 [482/96 (502%)]\tLoss: 48.155174\n",
      "Train Epoche: 1 [483/96 (503%)]\tLoss: 1.142460\n",
      "Train Epoche: 1 [484/96 (504%)]\tLoss: 29.639112\n",
      "Train Epoche: 1 [485/96 (505%)]\tLoss: 1.446688\n",
      "Train Epoche: 1 [486/96 (506%)]\tLoss: 1.335492\n",
      "Train Epoche: 1 [487/96 (507%)]\tLoss: 10.016881\n",
      "Train Epoche: 1 [488/96 (508%)]\tLoss: 0.989101\n",
      "Train Epoche: 1 [489/96 (509%)]\tLoss: 32.268589\n",
      "Train Epoche: 1 [490/96 (510%)]\tLoss: 2.588173\n",
      "Train Epoche: 1 [491/96 (511%)]\tLoss: 10.467809\n",
      "Train Epoche: 1 [492/96 (512%)]\tLoss: 31.290701\n",
      "Train Epoche: 1 [493/96 (514%)]\tLoss: 113.323387\n",
      "Train Epoche: 1 [494/96 (515%)]\tLoss: 0.526236\n",
      "Train Epoche: 1 [495/96 (516%)]\tLoss: 19.172506\n",
      "Train Epoche: 1 [496/96 (517%)]\tLoss: 2.939459\n",
      "Train Epoche: 1 [497/96 (518%)]\tLoss: 5.110798\n",
      "Train Epoche: 1 [498/96 (519%)]\tLoss: 38.383030\n",
      "Train Epoche: 1 [499/96 (520%)]\tLoss: 201.452972\n",
      "Train Epoche: 1 [500/96 (521%)]\tLoss: 48.491436\n",
      "Train Epoche: 1 [501/96 (522%)]\tLoss: 86.126114\n",
      "Train Epoche: 1 [502/96 (523%)]\tLoss: 30.864614\n",
      "Train Epoche: 1 [503/96 (524%)]\tLoss: 62.920849\n",
      "Train Epoche: 1 [504/96 (525%)]\tLoss: 0.001664\n",
      "Train Epoche: 1 [505/96 (526%)]\tLoss: 0.711190\n",
      "Train Epoche: 1 [506/96 (527%)]\tLoss: 10.654766\n",
      "Train Epoche: 1 [507/96 (528%)]\tLoss: 61.634079\n",
      "Train Epoche: 1 [508/96 (529%)]\tLoss: 5.435008\n",
      "Train Epoche: 1 [509/96 (530%)]\tLoss: 74.510880\n",
      "Train Epoche: 1 [510/96 (531%)]\tLoss: 43.143795\n",
      "Train Epoche: 1 [511/96 (532%)]\tLoss: 288.662628\n",
      "Train Epoche: 1 [512/96 (533%)]\tLoss: 45.535881\n",
      "Train Epoche: 1 [513/96 (534%)]\tLoss: 13.919927\n",
      "Train Epoche: 1 [514/96 (535%)]\tLoss: 4.535966\n",
      "Train Epoche: 1 [515/96 (536%)]\tLoss: 100.314331\n",
      "Train Epoche: 1 [516/96 (538%)]\tLoss: 49.927898\n",
      "Train Epoche: 1 [517/96 (539%)]\tLoss: 28.864510\n",
      "Train Epoche: 1 [518/96 (540%)]\tLoss: 15.567597\n",
      "Train Epoche: 1 [519/96 (541%)]\tLoss: 34.043198\n",
      "Train Epoche: 1 [520/96 (542%)]\tLoss: 79.440620\n",
      "Train Epoche: 1 [521/96 (543%)]\tLoss: 164.128342\n",
      "Train Epoche: 1 [522/96 (544%)]\tLoss: 32.664421\n",
      "Train Epoche: 1 [523/96 (545%)]\tLoss: 119.766800\n",
      "Train Epoche: 1 [524/96 (546%)]\tLoss: 16.778362\n",
      "Train Epoche: 1 [525/96 (547%)]\tLoss: 20.624435\n",
      "Train Epoche: 1 [526/96 (548%)]\tLoss: 0.251054\n",
      "Train Epoche: 1 [527/96 (549%)]\tLoss: 56.782459\n",
      "Train Epoche: 1 [528/96 (550%)]\tLoss: 0.091879\n",
      "Train Epoche: 1 [529/96 (551%)]\tLoss: 9.885546\n",
      "Train Epoche: 1 [530/96 (552%)]\tLoss: 93.234047\n",
      "Train Epoche: 1 [531/96 (553%)]\tLoss: 0.765814\n",
      "Train Epoche: 1 [532/96 (554%)]\tLoss: 0.304100\n",
      "Train Epoche: 1 [533/96 (555%)]\tLoss: 22.094749\n",
      "Train Epoche: 1 [534/96 (556%)]\tLoss: 4.963800\n",
      "Train Epoche: 1 [535/96 (557%)]\tLoss: 16.154339\n",
      "Train Epoche: 1 [536/96 (558%)]\tLoss: 0.151902\n",
      "Train Epoche: 1 [537/96 (559%)]\tLoss: 20.889439\n",
      "Train Epoche: 1 [538/96 (560%)]\tLoss: 2.683636\n",
      "Train Epoche: 1 [539/96 (561%)]\tLoss: 0.079467\n",
      "Train Epoche: 1 [540/96 (562%)]\tLoss: 4.427910\n",
      "Train Epoche: 1 [541/96 (564%)]\tLoss: 11.636228\n",
      "Train Epoche: 1 [542/96 (565%)]\tLoss: 3.023970\n",
      "Train Epoche: 1 [543/96 (566%)]\tLoss: 265.986786\n",
      "Train Epoche: 1 [544/96 (567%)]\tLoss: 32.629436\n",
      "Train Epoche: 1 [545/96 (568%)]\tLoss: 18.334793\n",
      "Train Epoche: 1 [546/96 (569%)]\tLoss: 0.295709\n",
      "Train Epoche: 1 [547/96 (570%)]\tLoss: 291.046326\n",
      "Train Epoche: 1 [548/96 (571%)]\tLoss: 54.109444\n",
      "Train Epoche: 1 [549/96 (572%)]\tLoss: 1.615572\n",
      "Train Epoche: 1 [550/96 (573%)]\tLoss: 49.784805\n",
      "Train Epoche: 1 [551/96 (574%)]\tLoss: 27.688334\n",
      "Train Epoche: 1 [552/96 (575%)]\tLoss: 13.571540\n",
      "Train Epoche: 1 [553/96 (576%)]\tLoss: 57.900906\n",
      "Train Epoche: 1 [554/96 (577%)]\tLoss: 13.578884\n",
      "Train Epoche: 1 [555/96 (578%)]\tLoss: 27.800745\n",
      "Train Epoche: 1 [556/96 (579%)]\tLoss: 1.806310\n",
      "Train Epoche: 1 [557/96 (580%)]\tLoss: 2.039177\n",
      "Train Epoche: 1 [558/96 (581%)]\tLoss: 16.050524\n",
      "Train Epoche: 1 [559/96 (582%)]\tLoss: 68.659935\n",
      "Train Epoche: 1 [560/96 (583%)]\tLoss: 45.939758\n",
      "Train Epoche: 1 [561/96 (584%)]\tLoss: 87.596939\n",
      "Train Epoche: 1 [562/96 (585%)]\tLoss: 100.476562\n",
      "Train Epoche: 1 [563/96 (586%)]\tLoss: 3.488512\n",
      "Train Epoche: 1 [564/96 (588%)]\tLoss: 55.159592\n",
      "Train Epoche: 1 [565/96 (589%)]\tLoss: 42.324333\n",
      "Train Epoche: 1 [566/96 (590%)]\tLoss: 33.503330\n",
      "Train Epoche: 1 [567/96 (591%)]\tLoss: 9.530087\n",
      "Train Epoche: 1 [568/96 (592%)]\tLoss: 10.597044\n",
      "Train Epoche: 1 [569/96 (593%)]\tLoss: 16.154558\n",
      "Train Epoche: 1 [570/96 (594%)]\tLoss: 3.156594\n",
      "Train Epoche: 1 [571/96 (595%)]\tLoss: 29.948370\n",
      "Train Epoche: 1 [572/96 (596%)]\tLoss: 54.108517\n",
      "Train Epoche: 1 [573/96 (597%)]\tLoss: 5.700811\n",
      "Train Epoche: 1 [574/96 (598%)]\tLoss: 10.133281\n",
      "Train Epoche: 1 [575/96 (599%)]\tLoss: 0.186733\n",
      "Train Epoche: 1 [576/96 (600%)]\tLoss: 10.944963\n",
      "Train Epoche: 1 [577/96 (601%)]\tLoss: 0.326869\n",
      "Train Epoche: 1 [578/96 (602%)]\tLoss: 9.431961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [579/96 (603%)]\tLoss: 8.911937\n",
      "Train Epoche: 1 [580/96 (604%)]\tLoss: 8.544115\n",
      "Train Epoche: 1 [581/96 (605%)]\tLoss: 53.269104\n",
      "Train Epoche: 1 [582/96 (606%)]\tLoss: 29.108273\n",
      "Train Epoche: 1 [583/96 (607%)]\tLoss: 0.538212\n",
      "Train Epoche: 1 [584/96 (608%)]\tLoss: 0.013294\n",
      "Train Epoche: 1 [585/96 (609%)]\tLoss: 6.541989\n",
      "Train Epoche: 1 [586/96 (610%)]\tLoss: 33.568779\n",
      "Train Epoche: 1 [587/96 (611%)]\tLoss: 0.810262\n",
      "Train Epoche: 1 [588/96 (612%)]\tLoss: 7.711075\n",
      "Train Epoche: 1 [589/96 (614%)]\tLoss: 14.492742\n",
      "Train Epoche: 1 [590/96 (615%)]\tLoss: 11.858143\n",
      "Train Epoche: 1 [591/96 (616%)]\tLoss: 0.201088\n",
      "Train Epoche: 1 [592/96 (617%)]\tLoss: 27.353403\n",
      "Train Epoche: 1 [593/96 (618%)]\tLoss: 0.216259\n",
      "Train Epoche: 1 [594/96 (619%)]\tLoss: 53.214062\n",
      "Train Epoche: 1 [595/96 (620%)]\tLoss: 20.328749\n",
      "Train Epoche: 1 [596/96 (621%)]\tLoss: 38.221157\n",
      "Train Epoche: 1 [597/96 (622%)]\tLoss: 42.589100\n",
      "Train Epoche: 1 [598/96 (623%)]\tLoss: 0.049265\n",
      "Train Epoche: 1 [599/96 (624%)]\tLoss: 46.621479\n",
      "Train Epoche: 1 [600/96 (625%)]\tLoss: 8.876530\n",
      "Train Epoche: 1 [601/96 (626%)]\tLoss: 14.494020\n",
      "Train Epoche: 1 [602/96 (627%)]\tLoss: 3.635738\n",
      "Train Epoche: 1 [603/96 (628%)]\tLoss: 0.018530\n",
      "Train Epoche: 1 [604/96 (629%)]\tLoss: 16.935295\n",
      "Train Epoche: 1 [605/96 (630%)]\tLoss: 15.676178\n",
      "Train Epoche: 1 [606/96 (631%)]\tLoss: 0.007915\n",
      "Train Epoche: 1 [607/96 (632%)]\tLoss: 2.835021\n",
      "Train Epoche: 1 [608/96 (633%)]\tLoss: 210.589645\n",
      "Train Epoche: 1 [609/96 (634%)]\tLoss: 304.034668\n",
      "Train Epoche: 1 [610/96 (635%)]\tLoss: 0.395175\n",
      "Train Epoche: 1 [611/96 (636%)]\tLoss: 4.780493\n",
      "Train Epoche: 1 [612/96 (638%)]\tLoss: 147.800461\n",
      "Train Epoche: 1 [613/96 (639%)]\tLoss: 119.444458\n",
      "Train Epoche: 1 [614/96 (640%)]\tLoss: 2.910037\n",
      "Train Epoche: 1 [615/96 (641%)]\tLoss: 21.193304\n",
      "Train Epoche: 1 [616/96 (642%)]\tLoss: 47.811565\n",
      "Train Epoche: 1 [617/96 (643%)]\tLoss: 8.320007\n",
      "Train Epoche: 1 [618/96 (644%)]\tLoss: 22.072010\n",
      "Train Epoche: 1 [619/96 (645%)]\tLoss: 37.542797\n",
      "Train Epoche: 1 [620/96 (646%)]\tLoss: 33.624317\n",
      "Train Epoche: 1 [621/96 (647%)]\tLoss: 11.819278\n",
      "Train Epoche: 1 [622/96 (648%)]\tLoss: 0.226492\n",
      "Train Epoche: 1 [623/96 (649%)]\tLoss: 11.512200\n",
      "Train Epoche: 1 [624/96 (650%)]\tLoss: 1.957121\n",
      "Train Epoche: 1 [625/96 (651%)]\tLoss: 3.858681\n",
      "Train Epoche: 1 [626/96 (652%)]\tLoss: 121.226723\n",
      "Train Epoche: 1 [627/96 (653%)]\tLoss: 1.631388\n",
      "Train Epoche: 1 [628/96 (654%)]\tLoss: 1.932241\n",
      "Train Epoche: 1 [629/96 (655%)]\tLoss: 16.100262\n",
      "Train Epoche: 1 [630/96 (656%)]\tLoss: 1.198232\n",
      "Train Epoche: 1 [631/96 (657%)]\tLoss: 128.656647\n",
      "Train Epoche: 1 [632/96 (658%)]\tLoss: 14.675069\n",
      "Train Epoche: 1 [633/96 (659%)]\tLoss: 0.888531\n",
      "Train Epoche: 1 [634/96 (660%)]\tLoss: 13.392255\n",
      "Train Epoche: 1 [635/96 (661%)]\tLoss: 202.585510\n",
      "Train Epoche: 1 [636/96 (662%)]\tLoss: 22.010643\n",
      "Train Epoche: 1 [637/96 (664%)]\tLoss: 91.916527\n",
      "Train Epoche: 1 [638/96 (665%)]\tLoss: 7.374416\n",
      "Train Epoche: 1 [639/96 (666%)]\tLoss: 28.629128\n",
      "Train Epoche: 1 [640/96 (667%)]\tLoss: 4.984993\n",
      "Train Epoche: 1 [641/96 (668%)]\tLoss: 18.542225\n",
      "Train Epoche: 1 [642/96 (669%)]\tLoss: 41.719910\n",
      "Train Epoche: 1 [643/96 (670%)]\tLoss: 117.556427\n",
      "Train Epoche: 1 [644/96 (671%)]\tLoss: 11.344783\n",
      "Train Epoche: 1 [645/96 (672%)]\tLoss: 73.745163\n",
      "Train Epoche: 1 [646/96 (673%)]\tLoss: 6.336135\n",
      "Train Epoche: 1 [647/96 (674%)]\tLoss: 47.441689\n",
      "Train Epoche: 1 [648/96 (675%)]\tLoss: 16.635281\n",
      "Train Epoche: 1 [649/96 (676%)]\tLoss: 405.441040\n",
      "Train Epoche: 1 [650/96 (677%)]\tLoss: 0.097269\n",
      "Train Epoche: 1 [651/96 (678%)]\tLoss: 117.747032\n",
      "Train Epoche: 1 [652/96 (679%)]\tLoss: 65.325630\n",
      "Train Epoche: 1 [653/96 (680%)]\tLoss: 22.570288\n",
      "Train Epoche: 1 [654/96 (681%)]\tLoss: 10.730508\n",
      "Train Epoche: 1 [655/96 (682%)]\tLoss: 0.049796\n",
      "Train Epoche: 1 [656/96 (683%)]\tLoss: 8.269130\n",
      "Train Epoche: 1 [657/96 (684%)]\tLoss: 32.293285\n",
      "Train Epoche: 1 [658/96 (685%)]\tLoss: 42.514610\n",
      "Train Epoche: 1 [659/96 (686%)]\tLoss: 8.015615\n",
      "Train Epoche: 1 [660/96 (688%)]\tLoss: 36.800381\n",
      "Train Epoche: 1 [661/96 (689%)]\tLoss: 124.855331\n",
      "Train Epoche: 1 [662/96 (690%)]\tLoss: 44.575886\n",
      "Train Epoche: 1 [663/96 (691%)]\tLoss: 8.768484\n",
      "Train Epoche: 1 [664/96 (692%)]\tLoss: 2.536797\n",
      "Train Epoche: 1 [665/96 (693%)]\tLoss: 30.387823\n",
      "Train Epoche: 1 [666/96 (694%)]\tLoss: 4.260112\n",
      "Train Epoche: 1 [667/96 (695%)]\tLoss: 2.306085\n",
      "Train Epoche: 1 [668/96 (696%)]\tLoss: 8.587266\n",
      "Train Epoche: 1 [669/96 (697%)]\tLoss: 2.029481\n",
      "Train Epoche: 1 [670/96 (698%)]\tLoss: 0.071821\n",
      "Train Epoche: 1 [671/96 (699%)]\tLoss: 1.993554\n",
      "Train Epoche: 1 [672/96 (700%)]\tLoss: 5.265209\n",
      "Train Epoche: 1 [673/96 (701%)]\tLoss: 1.393135\n",
      "Train Epoche: 1 [674/96 (702%)]\tLoss: 0.048081\n",
      "Train Epoche: 1 [675/96 (703%)]\tLoss: 0.286149\n",
      "Train Epoche: 1 [676/96 (704%)]\tLoss: 1.100477\n",
      "Train Epoche: 1 [677/96 (705%)]\tLoss: 7.864688\n",
      "Train Epoche: 1 [678/96 (706%)]\tLoss: 18.007137\n",
      "Train Epoche: 1 [679/96 (707%)]\tLoss: 3.008158\n",
      "Train Epoche: 1 [680/96 (708%)]\tLoss: 221.637772\n",
      "Train Epoche: 1 [681/96 (709%)]\tLoss: 197.648422\n",
      "Train Epoche: 1 [682/96 (710%)]\tLoss: 217.921692\n",
      "Train Epoche: 1 [683/96 (711%)]\tLoss: 73.803223\n",
      "Train Epoche: 1 [684/96 (712%)]\tLoss: 124.247047\n",
      "Train Epoche: 1 [685/96 (714%)]\tLoss: 1.092565\n",
      "Train Epoche: 1 [686/96 (715%)]\tLoss: 71.300255\n",
      "Train Epoche: 1 [687/96 (716%)]\tLoss: 132.180984\n",
      "Train Epoche: 1 [688/96 (717%)]\tLoss: 2.479973\n",
      "Train Epoche: 1 [689/96 (718%)]\tLoss: 134.552734\n",
      "Train Epoche: 1 [690/96 (719%)]\tLoss: 39.080215\n",
      "Train Epoche: 1 [691/96 (720%)]\tLoss: 0.004082\n",
      "Train Epoche: 1 [692/96 (721%)]\tLoss: 53.332256\n",
      "Train Epoche: 1 [693/96 (722%)]\tLoss: 32.362888\n",
      "Train Epoche: 1 [694/96 (723%)]\tLoss: 8.445690\n",
      "Train Epoche: 1 [695/96 (724%)]\tLoss: 12.510171\n",
      "Train Epoche: 1 [696/96 (725%)]\tLoss: 10.918530\n",
      "Train Epoche: 1 [697/96 (726%)]\tLoss: 3.038411\n",
      "Train Epoche: 1 [698/96 (727%)]\tLoss: 38.859085\n",
      "Train Epoche: 1 [699/96 (728%)]\tLoss: 5.126946\n",
      "Train Epoche: 1 [700/96 (729%)]\tLoss: 17.778999\n",
      "Train Epoche: 1 [701/96 (730%)]\tLoss: 47.930283\n",
      "Train Epoche: 1 [702/96 (731%)]\tLoss: 23.983143\n",
      "Train Epoche: 1 [703/96 (732%)]\tLoss: 25.252989\n",
      "Train Epoche: 1 [704/96 (733%)]\tLoss: 102.603920\n",
      "Train Epoche: 1 [705/96 (734%)]\tLoss: 2.042848\n",
      "Train Epoche: 1 [706/96 (735%)]\tLoss: 32.664692\n",
      "Train Epoche: 1 [707/96 (736%)]\tLoss: 4.945035\n",
      "Train Epoche: 1 [708/96 (738%)]\tLoss: 1.125696\n",
      "Train Epoche: 1 [709/96 (739%)]\tLoss: 13.183528\n",
      "Train Epoche: 1 [710/96 (740%)]\tLoss: 0.019572\n",
      "Train Epoche: 1 [711/96 (741%)]\tLoss: 34.038078\n",
      "Train Epoche: 1 [712/96 (742%)]\tLoss: 3.995759\n",
      "Train Epoche: 1 [713/96 (743%)]\tLoss: 11.456412\n",
      "Train Epoche: 1 [714/96 (744%)]\tLoss: 6.490119\n",
      "Train Epoche: 1 [715/96 (745%)]\tLoss: 0.869711\n",
      "Train Epoche: 1 [716/96 (746%)]\tLoss: 3.448929\n",
      "Train Epoche: 1 [717/96 (747%)]\tLoss: 24.467766\n",
      "Train Epoche: 1 [718/96 (748%)]\tLoss: 5.737103\n",
      "Train Epoche: 1 [719/96 (749%)]\tLoss: 0.129147\n",
      "Train Epoche: 1 [720/96 (750%)]\tLoss: 7.237555\n",
      "Train Epoche: 1 [721/96 (751%)]\tLoss: 69.272659\n",
      "Train Epoche: 1 [722/96 (752%)]\tLoss: 78.076370\n",
      "Train Epoche: 1 [723/96 (753%)]\tLoss: 0.199817\n",
      "Train Epoche: 1 [724/96 (754%)]\tLoss: 7.209130\n",
      "Train Epoche: 1 [725/96 (755%)]\tLoss: 11.886971\n",
      "Train Epoche: 1 [726/96 (756%)]\tLoss: 3.442844\n",
      "Train Epoche: 1 [727/96 (757%)]\tLoss: 11.475956\n",
      "Train Epoche: 1 [728/96 (758%)]\tLoss: 75.688469\n",
      "Train Epoche: 1 [729/96 (759%)]\tLoss: 165.356659\n",
      "Train Epoche: 1 [730/96 (760%)]\tLoss: 0.650628\n",
      "Train Epoche: 1 [731/96 (761%)]\tLoss: 9.700212\n",
      "Train Epoche: 1 [732/96 (762%)]\tLoss: 9.796518\n",
      "Train Epoche: 1 [733/96 (764%)]\tLoss: 0.513221\n",
      "Train Epoche: 1 [734/96 (765%)]\tLoss: 2.981708\n",
      "Train Epoche: 1 [735/96 (766%)]\tLoss: 3.739161\n",
      "Train Epoche: 1 [736/96 (767%)]\tLoss: 3.190335\n",
      "Train Epoche: 1 [737/96 (768%)]\tLoss: 7.403647\n",
      "Train Epoche: 1 [738/96 (769%)]\tLoss: 4.238127\n",
      "Train Epoche: 1 [739/96 (770%)]\tLoss: 0.021021\n",
      "Train Epoche: 1 [740/96 (771%)]\tLoss: 9.423399\n",
      "Train Epoche: 1 [741/96 (772%)]\tLoss: 2.219478\n",
      "Train Epoche: 1 [742/96 (773%)]\tLoss: 112.902321\n",
      "Train Epoche: 1 [743/96 (774%)]\tLoss: 0.290143\n",
      "Train Epoche: 1 [744/96 (775%)]\tLoss: 3.891045\n",
      "Train Epoche: 1 [745/96 (776%)]\tLoss: 0.050367\n",
      "Train Epoche: 1 [746/96 (777%)]\tLoss: 3.844356\n",
      "Train Epoche: 1 [747/96 (778%)]\tLoss: 1.117671\n",
      "Train Epoche: 1 [748/96 (779%)]\tLoss: 1.598810\n",
      "Train Epoche: 1 [749/96 (780%)]\tLoss: 26.041111\n",
      "Train Epoche: 1 [750/96 (781%)]\tLoss: 7.851661\n",
      "Train Epoche: 1 [751/96 (782%)]\tLoss: 1.527295\n",
      "Train Epoche: 1 [752/96 (783%)]\tLoss: 78.173996\n",
      "Train Epoche: 1 [753/96 (784%)]\tLoss: 70.906487\n",
      "Train Epoche: 1 [754/96 (785%)]\tLoss: 0.162710\n",
      "Train Epoche: 1 [755/96 (786%)]\tLoss: 31.500479\n",
      "Train Epoche: 1 [756/96 (788%)]\tLoss: 2.036234\n",
      "Train Epoche: 1 [757/96 (789%)]\tLoss: 17.234396\n",
      "Train Epoche: 1 [758/96 (790%)]\tLoss: 10.162127\n",
      "Train Epoche: 1 [759/96 (791%)]\tLoss: 19.462864\n",
      "Train Epoche: 1 [760/96 (792%)]\tLoss: 118.075699\n",
      "Train Epoche: 1 [761/96 (793%)]\tLoss: 93.666000\n",
      "Train Epoche: 1 [762/96 (794%)]\tLoss: 99.568359\n",
      "Train Epoche: 1 [763/96 (795%)]\tLoss: 60.999577\n",
      "Train Epoche: 1 [764/96 (796%)]\tLoss: 138.078857\n",
      "Train Epoche: 1 [765/96 (797%)]\tLoss: 10.678925\n",
      "Train Epoche: 1 [766/96 (798%)]\tLoss: 19.956774\n",
      "Train Epoche: 1 [767/96 (799%)]\tLoss: 0.560530\n",
      "Train Epoche: 1 [768/96 (800%)]\tLoss: 0.863448\n",
      "Train Epoche: 1 [769/96 (801%)]\tLoss: 10.731539\n",
      "Train Epoche: 1 [770/96 (802%)]\tLoss: 8.464287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [771/96 (803%)]\tLoss: 2.782630\n",
      "Train Epoche: 1 [772/96 (804%)]\tLoss: 6.161230\n",
      "Train Epoche: 1 [773/96 (805%)]\tLoss: 15.544180\n",
      "Train Epoche: 1 [774/96 (806%)]\tLoss: 27.980169\n",
      "Train Epoche: 1 [775/96 (807%)]\tLoss: 5.494349\n",
      "Train Epoche: 1 [776/96 (808%)]\tLoss: 5.001890\n",
      "Train Epoche: 1 [777/96 (809%)]\tLoss: 5.900622\n",
      "Train Epoche: 1 [778/96 (810%)]\tLoss: 9.074968\n",
      "Train Epoche: 1 [779/96 (811%)]\tLoss: 3.661347\n",
      "Train Epoche: 1 [780/96 (812%)]\tLoss: 425.978241\n",
      "Train Epoche: 1 [781/96 (814%)]\tLoss: 195.687561\n",
      "Train Epoche: 1 [782/96 (815%)]\tLoss: 136.446823\n",
      "Train Epoche: 1 [783/96 (816%)]\tLoss: 165.716644\n",
      "Train Epoche: 1 [784/96 (817%)]\tLoss: 92.396072\n",
      "Train Epoche: 1 [785/96 (818%)]\tLoss: 0.867984\n",
      "Train Epoche: 1 [786/96 (819%)]\tLoss: 85.403313\n",
      "Train Epoche: 1 [787/96 (820%)]\tLoss: 137.601654\n",
      "Train Epoche: 1 [788/96 (821%)]\tLoss: 27.451662\n",
      "Train Epoche: 1 [789/96 (822%)]\tLoss: 1.987250\n",
      "Train Epoche: 1 [790/96 (823%)]\tLoss: 4.374409\n",
      "Train Epoche: 1 [791/96 (824%)]\tLoss: 36.546719\n",
      "Train Epoche: 1 [792/96 (825%)]\tLoss: 4.107934\n",
      "Train Epoche: 1 [793/96 (826%)]\tLoss: 0.356957\n",
      "Train Epoche: 1 [794/96 (827%)]\tLoss: 2.885024\n",
      "Train Epoche: 1 [795/96 (828%)]\tLoss: 54.632854\n",
      "Train Epoche: 1 [796/96 (829%)]\tLoss: 8.525378\n",
      "Train Epoche: 1 [797/96 (830%)]\tLoss: 13.537005\n",
      "Train Epoche: 1 [798/96 (831%)]\tLoss: 9.769709\n",
      "Train Epoche: 1 [799/96 (832%)]\tLoss: 1.922603\n",
      "Train Epoche: 1 [800/96 (833%)]\tLoss: 5.740704\n",
      "Train Epoche: 1 [801/96 (834%)]\tLoss: 217.465195\n",
      "Train Epoche: 1 [802/96 (835%)]\tLoss: 28.711105\n",
      "Train Epoche: 1 [803/96 (836%)]\tLoss: 0.983658\n",
      "Train Epoche: 1 [804/96 (838%)]\tLoss: 89.224327\n",
      "Train Epoche: 1 [805/96 (839%)]\tLoss: 166.838882\n",
      "Train Epoche: 1 [806/96 (840%)]\tLoss: 45.940819\n",
      "Train Epoche: 1 [807/96 (841%)]\tLoss: 11.156569\n",
      "Train Epoche: 1 [808/96 (842%)]\tLoss: 13.202989\n",
      "Train Epoche: 1 [809/96 (843%)]\tLoss: 3.285765\n",
      "Train Epoche: 1 [810/96 (844%)]\tLoss: 8.497833\n",
      "Train Epoche: 1 [811/96 (845%)]\tLoss: 3.088810\n",
      "Train Epoche: 1 [812/96 (846%)]\tLoss: 0.670499\n",
      "Train Epoche: 1 [813/96 (847%)]\tLoss: 121.702682\n",
      "Train Epoche: 1 [814/96 (848%)]\tLoss: 1.006788\n",
      "Train Epoche: 1 [815/96 (849%)]\tLoss: 0.030172\n",
      "Train Epoche: 1 [816/96 (850%)]\tLoss: 16.820210\n",
      "Train Epoche: 1 [817/96 (851%)]\tLoss: 1.076874\n",
      "Train Epoche: 1 [818/96 (852%)]\tLoss: 14.637613\n",
      "Train Epoche: 1 [819/96 (853%)]\tLoss: 19.697094\n",
      "Train Epoche: 1 [820/96 (854%)]\tLoss: 10.890952\n",
      "Train Epoche: 1 [821/96 (855%)]\tLoss: 21.313841\n",
      "Train Epoche: 1 [822/96 (856%)]\tLoss: 11.066176\n",
      "Train Epoche: 1 [823/96 (857%)]\tLoss: 88.130325\n",
      "Train Epoche: 1 [824/96 (858%)]\tLoss: 89.711922\n",
      "Train Epoche: 1 [825/96 (859%)]\tLoss: 75.599205\n",
      "Train Epoche: 1 [826/96 (860%)]\tLoss: 0.004049\n",
      "Train Epoche: 1 [827/96 (861%)]\tLoss: 114.778183\n",
      "Train Epoche: 1 [828/96 (862%)]\tLoss: 0.770727\n",
      "Train Epoche: 1 [829/96 (864%)]\tLoss: 2.750207\n",
      "Train Epoche: 1 [830/96 (865%)]\tLoss: 1.450699\n",
      "Train Epoche: 1 [831/96 (866%)]\tLoss: 74.668571\n",
      "Train Epoche: 1 [832/96 (867%)]\tLoss: 29.425858\n",
      "Train Epoche: 1 [833/96 (868%)]\tLoss: 3.218844\n",
      "Train Epoche: 1 [834/96 (869%)]\tLoss: 0.522426\n",
      "Train Epoche: 1 [835/96 (870%)]\tLoss: 0.690574\n",
      "Train Epoche: 1 [836/96 (871%)]\tLoss: 1.123203\n",
      "Train Epoche: 1 [837/96 (872%)]\tLoss: 3.267690\n",
      "Train Epoche: 1 [838/96 (873%)]\tLoss: 31.853638\n",
      "Train Epoche: 1 [839/96 (874%)]\tLoss: 0.808541\n",
      "Train Epoche: 1 [840/96 (875%)]\tLoss: 31.654079\n",
      "Train Epoche: 1 [841/96 (876%)]\tLoss: 24.476900\n",
      "Train Epoche: 1 [842/96 (877%)]\tLoss: 7.065167\n",
      "Train Epoche: 1 [843/96 (878%)]\tLoss: 75.953819\n",
      "Train Epoche: 1 [844/96 (879%)]\tLoss: 5.364634\n",
      "Train Epoche: 1 [845/96 (880%)]\tLoss: 10.864833\n",
      "Train Epoche: 1 [846/96 (881%)]\tLoss: 1.263004\n",
      "Train Epoche: 1 [847/96 (882%)]\tLoss: 2.269737\n",
      "Train Epoche: 1 [848/96 (883%)]\tLoss: 0.277359\n",
      "Train Epoche: 1 [849/96 (884%)]\tLoss: 46.296867\n",
      "Train Epoche: 1 [850/96 (885%)]\tLoss: 5.361471\n",
      "Train Epoche: 1 [851/96 (886%)]\tLoss: 160.368835\n",
      "Train Epoche: 1 [852/96 (888%)]\tLoss: 11.011047\n",
      "Train Epoche: 1 [853/96 (889%)]\tLoss: 0.702250\n",
      "Train Epoche: 1 [854/96 (890%)]\tLoss: 0.056828\n",
      "Train Epoche: 1 [855/96 (891%)]\tLoss: 0.146415\n",
      "Train Epoche: 1 [856/96 (892%)]\tLoss: 1.806489\n",
      "Train Epoche: 1 [857/96 (893%)]\tLoss: 0.929618\n",
      "Train Epoche: 1 [858/96 (894%)]\tLoss: 0.047030\n",
      "Train Epoche: 1 [859/96 (895%)]\tLoss: 11.765419\n",
      "Train Epoche: 1 [860/96 (896%)]\tLoss: 8.328306\n",
      "Train Epoche: 1 [861/96 (897%)]\tLoss: 235.682587\n",
      "Train Epoche: 1 [862/96 (898%)]\tLoss: 28.247559\n",
      "Train Epoche: 1 [863/96 (899%)]\tLoss: 72.371384\n",
      "Train Epoche: 1 [864/96 (900%)]\tLoss: 130.937317\n",
      "Train Epoche: 1 [865/96 (901%)]\tLoss: 82.396217\n",
      "Train Epoche: 1 [866/96 (902%)]\tLoss: 15.208316\n",
      "Train Epoche: 1 [867/96 (903%)]\tLoss: 0.878205\n",
      "Train Epoche: 1 [868/96 (904%)]\tLoss: 60.507740\n",
      "Train Epoche: 1 [869/96 (905%)]\tLoss: 54.371849\n",
      "Train Epoche: 1 [870/96 (906%)]\tLoss: 31.689310\n",
      "Train Epoche: 1 [871/96 (907%)]\tLoss: 56.058247\n",
      "Train Epoche: 1 [872/96 (908%)]\tLoss: 35.837585\n",
      "Train Epoche: 1 [873/96 (909%)]\tLoss: 2.067747\n",
      "Train Epoche: 1 [874/96 (910%)]\tLoss: 147.326187\n",
      "Train Epoche: 1 [875/96 (911%)]\tLoss: 9.479980\n",
      "Train Epoche: 1 [876/96 (912%)]\tLoss: 6.912264\n",
      "Train Epoche: 1 [877/96 (914%)]\tLoss: 352.194214\n",
      "Train Epoche: 1 [878/96 (915%)]\tLoss: 3.630870\n",
      "Train Epoche: 1 [879/96 (916%)]\tLoss: 13.643038\n",
      "Train Epoche: 1 [880/96 (917%)]\tLoss: 3.776977\n",
      "Train Epoche: 1 [881/96 (918%)]\tLoss: 33.348633\n",
      "Train Epoche: 1 [882/96 (919%)]\tLoss: 4.364450\n",
      "Train Epoche: 1 [883/96 (920%)]\tLoss: 4.256700\n",
      "Train Epoche: 1 [884/96 (921%)]\tLoss: 41.704216\n",
      "Train Epoche: 1 [885/96 (922%)]\tLoss: 6.686757\n",
      "Train Epoche: 1 [886/96 (923%)]\tLoss: 4.099975\n",
      "Train Epoche: 1 [887/96 (924%)]\tLoss: 128.807816\n",
      "Train Epoche: 1 [888/96 (925%)]\tLoss: 2.075711\n",
      "Train Epoche: 1 [889/96 (926%)]\tLoss: 2.473927\n",
      "Train Epoche: 1 [890/96 (927%)]\tLoss: 2.946291\n",
      "Train Epoche: 1 [891/96 (928%)]\tLoss: 163.425568\n",
      "Train Epoche: 1 [892/96 (929%)]\tLoss: 7.137459\n",
      "Train Epoche: 1 [893/96 (930%)]\tLoss: 0.005034\n",
      "Train Epoche: 1 [894/96 (931%)]\tLoss: 88.162903\n",
      "Train Epoche: 1 [895/96 (932%)]\tLoss: 20.757317\n",
      "Train Epoche: 1 [896/96 (933%)]\tLoss: 38.977192\n",
      "Train Epoche: 1 [897/96 (934%)]\tLoss: 13.115394\n",
      "Train Epoche: 1 [898/96 (935%)]\tLoss: 0.340439\n",
      "Train Epoche: 1 [899/96 (936%)]\tLoss: 2.398560\n",
      "Train Epoche: 1 [900/96 (938%)]\tLoss: 6.942546\n",
      "Train Epoche: 1 [901/96 (939%)]\tLoss: 2.040414\n",
      "Train Epoche: 1 [902/96 (940%)]\tLoss: 0.847188\n",
      "Train Epoche: 1 [903/96 (941%)]\tLoss: 53.615978\n",
      "Train Epoche: 1 [904/96 (942%)]\tLoss: 0.663515\n",
      "Train Epoche: 1 [905/96 (943%)]\tLoss: 1.251737\n",
      "Train Epoche: 1 [906/96 (944%)]\tLoss: 10.438584\n",
      "Train Epoche: 1 [907/96 (945%)]\tLoss: 55.098824\n",
      "Train Epoche: 1 [908/96 (946%)]\tLoss: 9.731082\n",
      "Train Epoche: 1 [909/96 (947%)]\tLoss: 18.033670\n",
      "Train Epoche: 1 [910/96 (948%)]\tLoss: 8.158528\n",
      "Train Epoche: 1 [911/96 (949%)]\tLoss: 69.108818\n",
      "Train Epoche: 1 [912/96 (950%)]\tLoss: 9.227776\n",
      "Train Epoche: 1 [913/96 (951%)]\tLoss: 28.522631\n",
      "Train Epoche: 1 [914/96 (952%)]\tLoss: 127.199135\n",
      "Train Epoche: 1 [915/96 (953%)]\tLoss: 80.648834\n",
      "Train Epoche: 1 [916/96 (954%)]\tLoss: 21.788794\n",
      "Train Epoche: 1 [917/96 (955%)]\tLoss: 13.731873\n",
      "Train Epoche: 1 [918/96 (956%)]\tLoss: 0.773002\n",
      "Train Epoche: 1 [919/96 (957%)]\tLoss: 119.786171\n",
      "Train Epoche: 1 [920/96 (958%)]\tLoss: 4.520912\n",
      "Train Epoche: 1 [921/96 (959%)]\tLoss: 107.886879\n",
      "Train Epoche: 1 [922/96 (960%)]\tLoss: 0.013858\n",
      "Train Epoche: 1 [923/96 (961%)]\tLoss: 20.114161\n",
      "Train Epoche: 1 [924/96 (962%)]\tLoss: 115.170959\n",
      "Train Epoche: 1 [925/96 (964%)]\tLoss: 10.616670\n",
      "Train Epoche: 1 [926/96 (965%)]\tLoss: 172.053360\n",
      "Train Epoche: 1 [927/96 (966%)]\tLoss: 28.566092\n",
      "Train Epoche: 1 [928/96 (967%)]\tLoss: 13.806441\n",
      "Train Epoche: 1 [929/96 (968%)]\tLoss: 154.163986\n",
      "Train Epoche: 1 [930/96 (969%)]\tLoss: 14.153987\n",
      "Train Epoche: 1 [931/96 (970%)]\tLoss: 5.684174\n",
      "Train Epoche: 1 [932/96 (971%)]\tLoss: 0.079659\n",
      "Train Epoche: 1 [933/96 (972%)]\tLoss: 4.293585\n",
      "Train Epoche: 1 [934/96 (973%)]\tLoss: 3.764276\n",
      "Train Epoche: 1 [935/96 (974%)]\tLoss: 12.498840\n",
      "Train Epoche: 1 [936/96 (975%)]\tLoss: 0.584399\n",
      "Train Epoche: 1 [937/96 (976%)]\tLoss: 192.698242\n",
      "Train Epoche: 1 [938/96 (977%)]\tLoss: 35.047970\n",
      "Train Epoche: 1 [939/96 (978%)]\tLoss: 0.943989\n",
      "Train Epoche: 1 [940/96 (979%)]\tLoss: 35.192879\n",
      "Train Epoche: 1 [941/96 (980%)]\tLoss: 6.015305\n",
      "Train Epoche: 1 [942/96 (981%)]\tLoss: 2.292144\n",
      "Train Epoche: 1 [943/96 (982%)]\tLoss: 0.580056\n",
      "Train Epoche: 1 [944/96 (983%)]\tLoss: 6.477135\n",
      "Train Epoche: 1 [945/96 (984%)]\tLoss: 2.725389\n",
      "Train Epoche: 1 [946/96 (985%)]\tLoss: 9.970720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [947/96 (986%)]\tLoss: 23.073999\n",
      "Train Epoche: 1 [948/96 (988%)]\tLoss: 0.299041\n",
      "Train Epoche: 1 [949/96 (989%)]\tLoss: 6.297971\n",
      "Train Epoche: 1 [950/96 (990%)]\tLoss: 7.780253\n",
      "Train Epoche: 1 [951/96 (991%)]\tLoss: 23.555447\n",
      "Train Epoche: 1 [952/96 (992%)]\tLoss: 9.420161\n",
      "Train Epoche: 1 [953/96 (993%)]\tLoss: 4.646083\n",
      "Train Epoche: 1 [954/96 (994%)]\tLoss: 7.739766\n",
      "Train Epoche: 1 [955/96 (995%)]\tLoss: 3.994280\n",
      "Train Epoche: 1 [956/96 (996%)]\tLoss: 3.248492\n",
      "Train Epoche: 1 [957/96 (997%)]\tLoss: 4.305680\n",
      "Train Epoche: 1 [958/96 (998%)]\tLoss: 1.381453\n",
      "Train Epoche: 1 [959/96 (999%)]\tLoss: 4.121486\n",
      "Train Epoche: 1 [960/96 (1000%)]\tLoss: 4.645742\n",
      "Train Epoche: 1 [961/96 (1001%)]\tLoss: 55.881649\n",
      "Train Epoche: 1 [962/96 (1002%)]\tLoss: 23.744968\n",
      "Train Epoche: 1 [963/96 (1003%)]\tLoss: 1.152262\n",
      "Train Epoche: 1 [964/96 (1004%)]\tLoss: 0.081519\n",
      "Train Epoche: 1 [965/96 (1005%)]\tLoss: 0.583804\n",
      "Train Epoche: 1 [966/96 (1006%)]\tLoss: 1.456211\n",
      "Train Epoche: 1 [967/96 (1007%)]\tLoss: 3.173754\n",
      "Train Epoche: 1 [968/96 (1008%)]\tLoss: 13.178282\n",
      "Train Epoche: 1 [969/96 (1009%)]\tLoss: 0.419495\n",
      "Train Epoche: 1 [970/96 (1010%)]\tLoss: 117.565987\n",
      "Train Epoche: 1 [971/96 (1011%)]\tLoss: 37.570808\n",
      "Train Epoche: 1 [972/96 (1012%)]\tLoss: 33.801823\n",
      "Train Epoche: 1 [973/96 (1014%)]\tLoss: 0.053350\n",
      "Train Epoche: 1 [974/96 (1015%)]\tLoss: 159.454422\n",
      "Train Epoche: 1 [975/96 (1016%)]\tLoss: 280.673767\n",
      "Train Epoche: 1 [976/96 (1017%)]\tLoss: 6.861494\n",
      "Train Epoche: 1 [977/96 (1018%)]\tLoss: 15.603908\n",
      "Train Epoche: 1 [978/96 (1019%)]\tLoss: 81.721344\n",
      "Train Epoche: 1 [979/96 (1020%)]\tLoss: 2.196220\n",
      "Train Epoche: 1 [980/96 (1021%)]\tLoss: 8.196217\n",
      "Train Epoche: 1 [981/96 (1022%)]\tLoss: 7.109282\n",
      "Train Epoche: 1 [982/96 (1023%)]\tLoss: 2.415906\n",
      "Train Epoche: 1 [983/96 (1024%)]\tLoss: 24.539881\n",
      "Train Epoche: 1 [984/96 (1025%)]\tLoss: 0.877457\n",
      "Train Epoche: 1 [985/96 (1026%)]\tLoss: 4.111598\n",
      "Train Epoche: 1 [986/96 (1027%)]\tLoss: 0.302723\n",
      "Train Epoche: 1 [987/96 (1028%)]\tLoss: 64.227280\n",
      "Train Epoche: 1 [988/96 (1029%)]\tLoss: 31.393147\n",
      "Train Epoche: 1 [989/96 (1030%)]\tLoss: 14.910312\n",
      "Train Epoche: 1 [990/96 (1031%)]\tLoss: 8.549819\n",
      "Train Epoche: 1 [991/96 (1032%)]\tLoss: 6.446044\n",
      "Train Epoche: 1 [992/96 (1033%)]\tLoss: 5.813065\n",
      "Train Epoche: 1 [993/96 (1034%)]\tLoss: 23.348425\n",
      "Train Epoche: 1 [994/96 (1035%)]\tLoss: 2.122529\n",
      "Train Epoche: 1 [995/96 (1036%)]\tLoss: 0.106242\n",
      "Train Epoche: 1 [996/96 (1038%)]\tLoss: 0.123750\n",
      "Train Epoche: 1 [997/96 (1039%)]\tLoss: 0.610736\n",
      "Train Epoche: 1 [998/96 (1040%)]\tLoss: 18.465504\n",
      "Train Epoche: 1 [999/96 (1041%)]\tLoss: 48.705166\n",
      "Train Epoche: 1 [1000/96 (1042%)]\tLoss: 25.569435\n",
      "Train Epoche: 1 [1001/96 (1043%)]\tLoss: 14.608534\n",
      "Train Epoche: 1 [1002/96 (1044%)]\tLoss: 9.702849\n",
      "Train Epoche: 1 [1003/96 (1045%)]\tLoss: 10.612393\n",
      "Train Epoche: 1 [1004/96 (1046%)]\tLoss: 328.677307\n",
      "Train Epoche: 1 [1005/96 (1047%)]\tLoss: 8.905760\n",
      "Train Epoche: 1 [1006/96 (1048%)]\tLoss: 1.707945\n",
      "Train Epoche: 1 [1007/96 (1049%)]\tLoss: 0.730633\n",
      "Train Epoche: 1 [1008/96 (1050%)]\tLoss: 1.352626\n",
      "Train Epoche: 1 [1009/96 (1051%)]\tLoss: 13.700191\n",
      "Train Epoche: 1 [1010/96 (1052%)]\tLoss: 11.479969\n",
      "Train Epoche: 1 [1011/96 (1053%)]\tLoss: 0.274113\n",
      "Train Epoche: 1 [1012/96 (1054%)]\tLoss: 0.206894\n",
      "Train Epoche: 1 [1013/96 (1055%)]\tLoss: 183.105850\n",
      "Train Epoche: 1 [1014/96 (1056%)]\tLoss: 3.031543\n",
      "Train Epoche: 1 [1015/96 (1057%)]\tLoss: 12.707134\n",
      "Train Epoche: 1 [1016/96 (1058%)]\tLoss: 15.075374\n",
      "Train Epoche: 1 [1017/96 (1059%)]\tLoss: 7.000266\n",
      "Train Epoche: 1 [1018/96 (1060%)]\tLoss: 0.168267\n",
      "Train Epoche: 1 [1019/96 (1061%)]\tLoss: 6.715701\n",
      "Train Epoche: 1 [1020/96 (1062%)]\tLoss: 63.079475\n",
      "Train Epoche: 1 [1021/96 (1064%)]\tLoss: 3.049609\n",
      "Train Epoche: 1 [1022/96 (1065%)]\tLoss: 164.352249\n",
      "Train Epoche: 1 [1023/96 (1066%)]\tLoss: 17.035261\n",
      "Train Epoche: 1 [1024/96 (1067%)]\tLoss: 20.552299\n",
      "Train Epoche: 1 [1025/96 (1068%)]\tLoss: 0.109677\n",
      "Train Epoche: 1 [1026/96 (1069%)]\tLoss: 1.043777\n",
      "Train Epoche: 1 [1027/96 (1070%)]\tLoss: 32.143063\n",
      "Train Epoche: 1 [1028/96 (1071%)]\tLoss: 87.288376\n",
      "Train Epoche: 1 [1029/96 (1072%)]\tLoss: 60.071945\n",
      "Train Epoche: 1 [1030/96 (1073%)]\tLoss: 2.736006\n",
      "Train Epoche: 1 [1031/96 (1074%)]\tLoss: 3.653881\n",
      "Train Epoche: 1 [1032/96 (1075%)]\tLoss: 6.179675\n",
      "Train Epoche: 1 [1033/96 (1076%)]\tLoss: 21.311216\n",
      "Train Epoche: 1 [1034/96 (1077%)]\tLoss: 10.986017\n",
      "Train Epoche: 1 [1035/96 (1078%)]\tLoss: 12.598063\n",
      "Train Epoche: 1 [1036/96 (1079%)]\tLoss: 1.575540\n",
      "Train Epoche: 1 [1037/96 (1080%)]\tLoss: 6.132247\n",
      "Train Epoche: 1 [1038/96 (1081%)]\tLoss: 2.608378\n",
      "Train Epoche: 1 [1039/96 (1082%)]\tLoss: 121.245834\n",
      "Train Epoche: 1 [1040/96 (1083%)]\tLoss: 42.573902\n",
      "Train Epoche: 1 [1041/96 (1084%)]\tLoss: 9.373245\n",
      "Train Epoche: 1 [1042/96 (1085%)]\tLoss: 0.023054\n",
      "Train Epoche: 1 [1043/96 (1086%)]\tLoss: 1.556521\n",
      "Train Epoche: 1 [1044/96 (1088%)]\tLoss: 0.035887\n",
      "Train Epoche: 1 [1045/96 (1089%)]\tLoss: 1.539834\n",
      "Train Epoche: 1 [1046/96 (1090%)]\tLoss: 1.103445\n",
      "Train Epoche: 1 [1047/96 (1091%)]\tLoss: 59.602402\n",
      "Train Epoche: 1 [1048/96 (1092%)]\tLoss: 21.153214\n",
      "Train Epoche: 1 [1049/96 (1093%)]\tLoss: 28.642595\n",
      "Train Epoche: 1 [1050/96 (1094%)]\tLoss: 13.790844\n",
      "Train Epoche: 1 [1051/96 (1095%)]\tLoss: 211.833099\n",
      "Train Epoche: 1 [1052/96 (1096%)]\tLoss: 6.952115\n",
      "Train Epoche: 1 [1053/96 (1097%)]\tLoss: 1.888081\n",
      "Train Epoche: 1 [1054/96 (1098%)]\tLoss: 0.090729\n",
      "Train Epoche: 1 [1055/96 (1099%)]\tLoss: 0.179401\n",
      "Train Epoche: 1 [1056/96 (1100%)]\tLoss: 97.332405\n",
      "Train Epoche: 1 [1057/96 (1101%)]\tLoss: 13.720651\n",
      "Train Epoche: 1 [1058/96 (1102%)]\tLoss: 34.842560\n",
      "Train Epoche: 1 [1059/96 (1103%)]\tLoss: 17.922865\n",
      "Train Epoche: 1 [1060/96 (1104%)]\tLoss: 11.549079\n",
      "Train Epoche: 1 [1061/96 (1105%)]\tLoss: 1.105754\n",
      "Train Epoche: 1 [1062/96 (1106%)]\tLoss: 14.827540\n",
      "Train Epoche: 1 [1063/96 (1107%)]\tLoss: 4.740214\n",
      "Train Epoche: 1 [1064/96 (1108%)]\tLoss: 0.371769\n",
      "Train Epoche: 1 [1065/96 (1109%)]\tLoss: 33.932377\n",
      "Train Epoche: 1 [1066/96 (1110%)]\tLoss: 25.619184\n",
      "Train Epoche: 1 [1067/96 (1111%)]\tLoss: 1.546216\n",
      "Train Epoche: 1 [1068/96 (1112%)]\tLoss: 9.767991\n",
      "Train Epoche: 1 [1069/96 (1114%)]\tLoss: 12.090625\n",
      "Train Epoche: 1 [1070/96 (1115%)]\tLoss: 5.639175\n",
      "Train Epoche: 1 [1071/96 (1116%)]\tLoss: 4.205375\n",
      "Train Epoche: 1 [1072/96 (1117%)]\tLoss: 80.274712\n",
      "Train Epoche: 1 [1073/96 (1118%)]\tLoss: 53.222271\n",
      "Train Epoche: 1 [1074/96 (1119%)]\tLoss: 4.829305\n",
      "Train Epoche: 1 [1075/96 (1120%)]\tLoss: 14.884280\n",
      "Train Epoche: 1 [1076/96 (1121%)]\tLoss: 28.154696\n",
      "Train Epoche: 1 [1077/96 (1122%)]\tLoss: 8.451312\n",
      "Train Epoche: 1 [1078/96 (1123%)]\tLoss: 0.014772\n",
      "Train Epoche: 1 [1079/96 (1124%)]\tLoss: 1.719924\n",
      "Train Epoche: 1 [1080/96 (1125%)]\tLoss: 29.694880\n",
      "Train Epoche: 1 [1081/96 (1126%)]\tLoss: 0.133141\n",
      "Train Epoche: 1 [1082/96 (1127%)]\tLoss: 6.175891\n",
      "Train Epoche: 1 [1083/96 (1128%)]\tLoss: 4.298978\n",
      "Train Epoche: 1 [1084/96 (1129%)]\tLoss: 1.850813\n",
      "Train Epoche: 1 [1085/96 (1130%)]\tLoss: 0.018011\n",
      "Train Epoche: 1 [1086/96 (1131%)]\tLoss: 4.688385\n",
      "Train Epoche: 1 [1087/96 (1132%)]\tLoss: 0.574620\n",
      "Train Epoche: 1 [1088/96 (1133%)]\tLoss: 12.362469\n",
      "Train Epoche: 1 [1089/96 (1134%)]\tLoss: 0.035249\n",
      "Train Epoche: 1 [1090/96 (1135%)]\tLoss: 0.775476\n",
      "Train Epoche: 1 [1091/96 (1136%)]\tLoss: 59.279408\n",
      "Train Epoche: 1 [1092/96 (1138%)]\tLoss: 21.451973\n",
      "Train Epoche: 1 [1093/96 (1139%)]\tLoss: 38.927551\n",
      "Train Epoche: 1 [1094/96 (1140%)]\tLoss: 4.262797\n",
      "Train Epoche: 1 [1095/96 (1141%)]\tLoss: 53.843212\n",
      "Train Epoche: 1 [1096/96 (1142%)]\tLoss: 10.944856\n",
      "Train Epoche: 1 [1097/96 (1143%)]\tLoss: 47.730320\n",
      "Train Epoche: 1 [1098/96 (1144%)]\tLoss: 450.388184\n",
      "Train Epoche: 1 [1099/96 (1145%)]\tLoss: 0.969689\n",
      "Train Epoche: 1 [1100/96 (1146%)]\tLoss: 5.622131\n",
      "Train Epoche: 1 [1101/96 (1147%)]\tLoss: 313.376190\n",
      "Train Epoche: 1 [1102/96 (1148%)]\tLoss: 45.752235\n",
      "Train Epoche: 1 [1103/96 (1149%)]\tLoss: 15.100267\n",
      "Train Epoche: 1 [1104/96 (1150%)]\tLoss: 14.290702\n",
      "Train Epoche: 1 [1105/96 (1151%)]\tLoss: 23.714010\n",
      "Train Epoche: 1 [1106/96 (1152%)]\tLoss: 7.347362\n",
      "Train Epoche: 1 [1107/96 (1153%)]\tLoss: 17.074284\n",
      "Train Epoche: 1 [1108/96 (1154%)]\tLoss: 0.019423\n",
      "Train Epoche: 1 [1109/96 (1155%)]\tLoss: 8.568694\n",
      "Train Epoche: 1 [1110/96 (1156%)]\tLoss: 51.303734\n",
      "Train Epoche: 1 [1111/96 (1157%)]\tLoss: 35.884064\n",
      "Train Epoche: 1 [1112/96 (1158%)]\tLoss: 15.611488\n",
      "Train Epoche: 1 [1113/96 (1159%)]\tLoss: 210.753174\n",
      "Train Epoche: 1 [1114/96 (1160%)]\tLoss: 99.391190\n",
      "Train Epoche: 1 [1115/96 (1161%)]\tLoss: 35.713509\n",
      "Train Epoche: 1 [1116/96 (1162%)]\tLoss: 0.596015\n",
      "Train Epoche: 1 [1117/96 (1164%)]\tLoss: 6.734497\n",
      "Train Epoche: 1 [1118/96 (1165%)]\tLoss: 4.633438\n",
      "Train Epoche: 1 [1119/96 (1166%)]\tLoss: 33.416115\n",
      "Train Epoche: 1 [1120/96 (1167%)]\tLoss: 4.668150\n",
      "Train Epoche: 1 [1121/96 (1168%)]\tLoss: 2.023286\n",
      "Train Epoche: 1 [1122/96 (1169%)]\tLoss: 66.729691\n",
      "Train Epoche: 1 [1123/96 (1170%)]\tLoss: 100.699783\n",
      "Train Epoche: 1 [1124/96 (1171%)]\tLoss: 68.499535\n",
      "Train Epoche: 1 [1125/96 (1172%)]\tLoss: 7.287746\n",
      "Train Epoche: 1 [1126/96 (1173%)]\tLoss: 0.560506\n",
      "Train Epoche: 1 [1127/96 (1174%)]\tLoss: 14.636804\n",
      "Train Epoche: 1 [1128/96 (1175%)]\tLoss: 12.056890\n",
      "Train Epoche: 1 [1129/96 (1176%)]\tLoss: 4.850511\n",
      "Train Epoche: 1 [1130/96 (1177%)]\tLoss: 85.458473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1131/96 (1178%)]\tLoss: 27.587931\n",
      "Train Epoche: 1 [1132/96 (1179%)]\tLoss: 14.073341\n",
      "Train Epoche: 1 [1133/96 (1180%)]\tLoss: 5.512045\n",
      "Train Epoche: 1 [1134/96 (1181%)]\tLoss: 4.064870\n",
      "Train Epoche: 1 [1135/96 (1182%)]\tLoss: 8.197473\n",
      "Train Epoche: 1 [1136/96 (1183%)]\tLoss: 140.681656\n",
      "Train Epoche: 1 [1137/96 (1184%)]\tLoss: 13.800064\n",
      "Train Epoche: 1 [1138/96 (1185%)]\tLoss: 0.183251\n",
      "Train Epoche: 1 [1139/96 (1186%)]\tLoss: 0.116469\n",
      "Train Epoche: 1 [1140/96 (1188%)]\tLoss: 1.521582\n",
      "Train Epoche: 1 [1141/96 (1189%)]\tLoss: 4.331154\n",
      "Train Epoche: 1 [1142/96 (1190%)]\tLoss: 9.365400\n",
      "Train Epoche: 1 [1143/96 (1191%)]\tLoss: 1.268483\n",
      "Train Epoche: 1 [1144/96 (1192%)]\tLoss: 41.853699\n",
      "Train Epoche: 1 [1145/96 (1193%)]\tLoss: 0.674520\n",
      "Train Epoche: 1 [1146/96 (1194%)]\tLoss: 9.782632\n",
      "Train Epoche: 1 [1147/96 (1195%)]\tLoss: 3.061429\n",
      "Train Epoche: 1 [1148/96 (1196%)]\tLoss: 6.462563\n",
      "Train Epoche: 1 [1149/96 (1197%)]\tLoss: 1.051092\n",
      "Train Epoche: 1 [1150/96 (1198%)]\tLoss: 10.699059\n",
      "Train Epoche: 1 [1151/96 (1199%)]\tLoss: 11.880301\n",
      "Train Epoche: 1 [1152/96 (1200%)]\tLoss: 1.795986\n",
      "Train Epoche: 1 [1153/96 (1201%)]\tLoss: 0.362294\n",
      "Train Epoche: 1 [1154/96 (1202%)]\tLoss: 0.277757\n",
      "Train Epoche: 1 [1155/96 (1203%)]\tLoss: 0.405351\n",
      "Train Epoche: 1 [1156/96 (1204%)]\tLoss: 28.672956\n",
      "Train Epoche: 1 [1157/96 (1205%)]\tLoss: 2.368791\n",
      "Train Epoche: 1 [1158/96 (1206%)]\tLoss: 3.456001\n",
      "Train Epoche: 1 [1159/96 (1207%)]\tLoss: 23.417377\n",
      "Train Epoche: 1 [1160/96 (1208%)]\tLoss: 7.354980\n",
      "Train Epoche: 1 [1161/96 (1209%)]\tLoss: 21.407080\n",
      "Train Epoche: 1 [1162/96 (1210%)]\tLoss: 1.347645\n",
      "Train Epoche: 1 [1163/96 (1211%)]\tLoss: 12.842333\n",
      "Train Epoche: 1 [1164/96 (1212%)]\tLoss: 2.028264\n",
      "Train Epoche: 1 [1165/96 (1214%)]\tLoss: 15.667131\n",
      "Train Epoche: 1 [1166/96 (1215%)]\tLoss: 143.364731\n",
      "Train Epoche: 1 [1167/96 (1216%)]\tLoss: 2.250781\n",
      "Train Epoche: 1 [1168/96 (1217%)]\tLoss: 0.204523\n",
      "Train Epoche: 1 [1169/96 (1218%)]\tLoss: 1.525383\n",
      "Train Epoche: 1 [1170/96 (1219%)]\tLoss: 0.773015\n",
      "Train Epoche: 1 [1171/96 (1220%)]\tLoss: 0.543133\n",
      "Train Epoche: 1 [1172/96 (1221%)]\tLoss: 237.097397\n",
      "Train Epoche: 1 [1173/96 (1222%)]\tLoss: 0.137926\n",
      "Train Epoche: 1 [1174/96 (1223%)]\tLoss: 0.492251\n",
      "Train Epoche: 1 [1175/96 (1224%)]\tLoss: 15.082622\n",
      "Train Epoche: 1 [1176/96 (1225%)]\tLoss: 12.284932\n",
      "Train Epoche: 1 [1177/96 (1226%)]\tLoss: 14.243948\n",
      "Train Epoche: 1 [1178/96 (1227%)]\tLoss: 1.264861\n",
      "Train Epoche: 1 [1179/96 (1228%)]\tLoss: 4.615208\n",
      "Train Epoche: 1 [1180/96 (1229%)]\tLoss: 1.950725\n",
      "Train Epoche: 1 [1181/96 (1230%)]\tLoss: 4.361637\n",
      "Train Epoche: 1 [1182/96 (1231%)]\tLoss: 0.820491\n",
      "Train Epoche: 1 [1183/96 (1232%)]\tLoss: 0.526899\n",
      "Train Epoche: 1 [1184/96 (1233%)]\tLoss: 10.723536\n",
      "Train Epoche: 1 [1185/96 (1234%)]\tLoss: 3.125984\n",
      "Train Epoche: 1 [1186/96 (1235%)]\tLoss: 7.264756\n",
      "Train Epoche: 1 [1187/96 (1236%)]\tLoss: 0.001763\n",
      "Train Epoche: 1 [1188/96 (1238%)]\tLoss: 0.251065\n",
      "Train Epoche: 1 [1189/96 (1239%)]\tLoss: 0.150346\n",
      "Train Epoche: 1 [1190/96 (1240%)]\tLoss: 0.716890\n",
      "Train Epoche: 1 [1191/96 (1241%)]\tLoss: 53.690178\n",
      "Train Epoche: 1 [1192/96 (1242%)]\tLoss: 8.753500\n",
      "Train Epoche: 1 [1193/96 (1243%)]\tLoss: 0.546518\n",
      "Train Epoche: 1 [1194/96 (1244%)]\tLoss: 55.242977\n",
      "Train Epoche: 1 [1195/96 (1245%)]\tLoss: 7.795050\n",
      "Train Epoche: 1 [1196/96 (1246%)]\tLoss: 0.769338\n",
      "Train Epoche: 1 [1197/96 (1247%)]\tLoss: 1.738760\n",
      "Train Epoche: 1 [1198/96 (1248%)]\tLoss: 2.921703\n",
      "Train Epoche: 1 [1199/96 (1249%)]\tLoss: 82.238068\n",
      "Train Epoche: 1 [1200/96 (1250%)]\tLoss: 64.612587\n",
      "Train Epoche: 1 [1201/96 (1251%)]\tLoss: 20.196426\n",
      "Train Epoche: 1 [1202/96 (1252%)]\tLoss: 0.912673\n",
      "Train Epoche: 1 [1203/96 (1253%)]\tLoss: 41.617641\n",
      "Train Epoche: 1 [1204/96 (1254%)]\tLoss: 6.244017\n",
      "Train Epoche: 1 [1205/96 (1255%)]\tLoss: 52.118034\n",
      "Train Epoche: 1 [1206/96 (1256%)]\tLoss: 6.095028\n",
      "Train Epoche: 1 [1207/96 (1257%)]\tLoss: 43.997097\n",
      "Train Epoche: 1 [1208/96 (1258%)]\tLoss: 4.703467\n",
      "Train Epoche: 1 [1209/96 (1259%)]\tLoss: 10.339151\n",
      "Train Epoche: 1 [1210/96 (1260%)]\tLoss: 2.132660\n",
      "Train Epoche: 1 [1211/96 (1261%)]\tLoss: 0.263295\n",
      "Train Epoche: 1 [1212/96 (1262%)]\tLoss: 20.536177\n",
      "Train Epoche: 1 [1213/96 (1264%)]\tLoss: 0.035810\n",
      "Train Epoche: 1 [1214/96 (1265%)]\tLoss: 1.391721\n",
      "Train Epoche: 1 [1215/96 (1266%)]\tLoss: 86.080559\n",
      "Train Epoche: 1 [1216/96 (1267%)]\tLoss: 6.069175\n",
      "Train Epoche: 1 [1217/96 (1268%)]\tLoss: 17.409718\n",
      "Train Epoche: 1 [1218/96 (1269%)]\tLoss: 2.328765\n",
      "Train Epoche: 1 [1219/96 (1270%)]\tLoss: 76.057686\n",
      "Train Epoche: 1 [1220/96 (1271%)]\tLoss: 57.361233\n",
      "Train Epoche: 1 [1221/96 (1272%)]\tLoss: 6.506738\n",
      "Train Epoche: 1 [1222/96 (1273%)]\tLoss: 100.241486\n",
      "Train Epoche: 1 [1223/96 (1274%)]\tLoss: 88.989868\n",
      "Train Epoche: 1 [1224/96 (1275%)]\tLoss: 4.258292\n",
      "Train Epoche: 1 [1225/96 (1276%)]\tLoss: 0.027667\n",
      "Train Epoche: 1 [1226/96 (1277%)]\tLoss: 5.474231\n",
      "Train Epoche: 1 [1227/96 (1278%)]\tLoss: 37.105553\n",
      "Train Epoche: 1 [1228/96 (1279%)]\tLoss: 13.182433\n",
      "Train Epoche: 1 [1229/96 (1280%)]\tLoss: 231.273575\n",
      "Train Epoche: 1 [1230/96 (1281%)]\tLoss: 8.637761\n",
      "Train Epoche: 1 [1231/96 (1282%)]\tLoss: 38.241337\n",
      "Train Epoche: 1 [1232/96 (1283%)]\tLoss: 10.006563\n",
      "Train Epoche: 1 [1233/96 (1284%)]\tLoss: 214.260193\n",
      "Train Epoche: 1 [1234/96 (1285%)]\tLoss: 17.463346\n",
      "Train Epoche: 1 [1235/96 (1286%)]\tLoss: 53.377102\n",
      "Train Epoche: 1 [1236/96 (1288%)]\tLoss: 12.735336\n",
      "Train Epoche: 1 [1237/96 (1289%)]\tLoss: 3.293028\n",
      "Train Epoche: 1 [1238/96 (1290%)]\tLoss: 55.907986\n",
      "Train Epoche: 1 [1239/96 (1291%)]\tLoss: 2.827867\n",
      "Train Epoche: 1 [1240/96 (1292%)]\tLoss: 179.586014\n",
      "Train Epoche: 1 [1241/96 (1293%)]\tLoss: 103.519157\n",
      "Train Epoche: 1 [1242/96 (1294%)]\tLoss: 102.352196\n",
      "Train Epoche: 1 [1243/96 (1295%)]\tLoss: 4.819849\n",
      "Train Epoche: 1 [1244/96 (1296%)]\tLoss: 13.969100\n",
      "Train Epoche: 1 [1245/96 (1297%)]\tLoss: 9.170897\n",
      "Train Epoche: 1 [1246/96 (1298%)]\tLoss: 4.277258\n",
      "Train Epoche: 1 [1247/96 (1299%)]\tLoss: 3.570105\n",
      "Train Epoche: 1 [1248/96 (1300%)]\tLoss: 0.663340\n",
      "Train Epoche: 1 [1249/96 (1301%)]\tLoss: 173.306320\n",
      "Train Epoche: 1 [1250/96 (1302%)]\tLoss: 1.266696\n",
      "Train Epoche: 1 [1251/96 (1303%)]\tLoss: 0.001687\n",
      "Train Epoche: 1 [1252/96 (1304%)]\tLoss: 9.606207\n",
      "Train Epoche: 1 [1253/96 (1305%)]\tLoss: 12.552932\n",
      "Train Epoche: 1 [1254/96 (1306%)]\tLoss: 0.000093\n",
      "Train Epoche: 1 [1255/96 (1307%)]\tLoss: 0.289113\n",
      "Train Epoche: 1 [1256/96 (1308%)]\tLoss: 3.533595\n",
      "Train Epoche: 1 [1257/96 (1309%)]\tLoss: 41.719242\n",
      "Train Epoche: 1 [1258/96 (1310%)]\tLoss: 3.784760\n",
      "Train Epoche: 1 [1259/96 (1311%)]\tLoss: 1.933903\n",
      "Train Epoche: 1 [1260/96 (1312%)]\tLoss: 28.949881\n",
      "Train Epoche: 1 [1261/96 (1314%)]\tLoss: 224.411591\n",
      "Train Epoche: 1 [1262/96 (1315%)]\tLoss: 42.683090\n",
      "Train Epoche: 1 [1263/96 (1316%)]\tLoss: 21.154608\n",
      "Train Epoche: 1 [1264/96 (1317%)]\tLoss: 3.572237\n",
      "Train Epoche: 1 [1265/96 (1318%)]\tLoss: 10.548024\n",
      "Train Epoche: 1 [1266/96 (1319%)]\tLoss: 0.223369\n",
      "Train Epoche: 1 [1267/96 (1320%)]\tLoss: 3.239046\n",
      "Train Epoche: 1 [1268/96 (1321%)]\tLoss: 2.107251\n",
      "Train Epoche: 1 [1269/96 (1322%)]\tLoss: 1.200241\n",
      "Train Epoche: 1 [1270/96 (1323%)]\tLoss: 60.924194\n",
      "Train Epoche: 1 [1271/96 (1324%)]\tLoss: 2.737885\n",
      "Train Epoche: 1 [1272/96 (1325%)]\tLoss: 7.647732\n",
      "Train Epoche: 1 [1273/96 (1326%)]\tLoss: 0.015174\n",
      "Train Epoche: 1 [1274/96 (1327%)]\tLoss: 1.858653\n",
      "Train Epoche: 1 [1275/96 (1328%)]\tLoss: 103.215096\n",
      "Train Epoche: 1 [1276/96 (1329%)]\tLoss: 0.021109\n",
      "Train Epoche: 1 [1277/96 (1330%)]\tLoss: 19.313498\n",
      "Train Epoche: 1 [1278/96 (1331%)]\tLoss: 0.452927\n",
      "Train Epoche: 1 [1279/96 (1332%)]\tLoss: 14.355643\n",
      "Train Epoche: 1 [1280/96 (1333%)]\tLoss: 12.430450\n",
      "Train Epoche: 1 [1281/96 (1334%)]\tLoss: 43.780910\n",
      "Train Epoche: 1 [1282/96 (1335%)]\tLoss: 70.550827\n",
      "Train Epoche: 1 [1283/96 (1336%)]\tLoss: 0.414134\n",
      "Train Epoche: 1 [1284/96 (1338%)]\tLoss: 49.101391\n",
      "Train Epoche: 1 [1285/96 (1339%)]\tLoss: 44.019520\n",
      "Train Epoche: 1 [1286/96 (1340%)]\tLoss: 38.008823\n",
      "Train Epoche: 1 [1287/96 (1341%)]\tLoss: 5.068493\n",
      "Train Epoche: 1 [1288/96 (1342%)]\tLoss: 6.279895\n",
      "Train Epoche: 1 [1289/96 (1343%)]\tLoss: 15.631684\n",
      "Train Epoche: 1 [1290/96 (1344%)]\tLoss: 19.232201\n",
      "Train Epoche: 1 [1291/96 (1345%)]\tLoss: 49.660748\n",
      "Train Epoche: 1 [1292/96 (1346%)]\tLoss: 12.679253\n",
      "Train Epoche: 1 [1293/96 (1347%)]\tLoss: 55.477020\n",
      "Train Epoche: 1 [1294/96 (1348%)]\tLoss: 61.417850\n",
      "Train Epoche: 1 [1295/96 (1349%)]\tLoss: 18.515837\n",
      "Train Epoche: 1 [1296/96 (1350%)]\tLoss: 90.617554\n",
      "Train Epoche: 1 [1297/96 (1351%)]\tLoss: 15.292771\n",
      "Train Epoche: 1 [1298/96 (1352%)]\tLoss: 180.429642\n",
      "Train Epoche: 1 [1299/96 (1353%)]\tLoss: 129.921463\n",
      "Train Epoche: 1 [1300/96 (1354%)]\tLoss: 79.858009\n",
      "Train Epoche: 1 [1301/96 (1355%)]\tLoss: 0.754692\n",
      "Train Epoche: 1 [1302/96 (1356%)]\tLoss: 6.671452\n",
      "Train Epoche: 1 [1303/96 (1357%)]\tLoss: 15.347109\n",
      "Train Epoche: 1 [1304/96 (1358%)]\tLoss: 4.993744\n",
      "Train Epoche: 1 [1305/96 (1359%)]\tLoss: 32.212971\n",
      "Train Epoche: 1 [1306/96 (1360%)]\tLoss: 2.451588\n",
      "Train Epoche: 1 [1307/96 (1361%)]\tLoss: 4.613968\n",
      "Train Epoche: 1 [1308/96 (1362%)]\tLoss: 27.653318\n",
      "Train Epoche: 1 [1309/96 (1364%)]\tLoss: 3.445805\n",
      "Train Epoche: 1 [1310/96 (1365%)]\tLoss: 3.342474\n",
      "Train Epoche: 1 [1311/96 (1366%)]\tLoss: 12.510833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1312/96 (1367%)]\tLoss: 57.868675\n",
      "Train Epoche: 1 [1313/96 (1368%)]\tLoss: 19.991348\n",
      "Train Epoche: 1 [1314/96 (1369%)]\tLoss: 21.921169\n",
      "Train Epoche: 1 [1315/96 (1370%)]\tLoss: 3.902305\n",
      "Train Epoche: 1 [1316/96 (1371%)]\tLoss: 1.347647\n",
      "Train Epoche: 1 [1317/96 (1372%)]\tLoss: 1.820138\n",
      "Train Epoche: 1 [1318/96 (1373%)]\tLoss: 0.029197\n",
      "Train Epoche: 1 [1319/96 (1374%)]\tLoss: 0.076097\n",
      "Train Epoche: 1 [1320/96 (1375%)]\tLoss: 0.256606\n",
      "Train Epoche: 1 [1321/96 (1376%)]\tLoss: 1.677523\n",
      "Train Epoche: 1 [1322/96 (1377%)]\tLoss: 0.750062\n",
      "Train Epoche: 1 [1323/96 (1378%)]\tLoss: 153.638962\n",
      "Train Epoche: 1 [1324/96 (1379%)]\tLoss: 0.468014\n",
      "Train Epoche: 1 [1325/96 (1380%)]\tLoss: 1.910246\n",
      "Train Epoche: 1 [1326/96 (1381%)]\tLoss: 8.564837\n",
      "Train Epoche: 1 [1327/96 (1382%)]\tLoss: 285.437103\n",
      "Train Epoche: 1 [1328/96 (1383%)]\tLoss: 0.234011\n",
      "Train Epoche: 1 [1329/96 (1384%)]\tLoss: 22.145147\n",
      "Train Epoche: 1 [1330/96 (1385%)]\tLoss: 21.043516\n",
      "Train Epoche: 1 [1331/96 (1386%)]\tLoss: 5.074626\n",
      "Train Epoche: 1 [1332/96 (1388%)]\tLoss: 2.148721\n",
      "Train Epoche: 1 [1333/96 (1389%)]\tLoss: 0.006357\n",
      "Train Epoche: 1 [1334/96 (1390%)]\tLoss: 156.172897\n",
      "Train Epoche: 1 [1335/96 (1391%)]\tLoss: 3.790191\n",
      "Train Epoche: 1 [1336/96 (1392%)]\tLoss: 41.711334\n",
      "Train Epoche: 1 [1337/96 (1393%)]\tLoss: 52.301113\n",
      "Train Epoche: 1 [1338/96 (1394%)]\tLoss: 31.572758\n",
      "Train Epoche: 1 [1339/96 (1395%)]\tLoss: 22.909870\n",
      "Train Epoche: 1 [1340/96 (1396%)]\tLoss: 3.449440\n",
      "Train Epoche: 1 [1341/96 (1397%)]\tLoss: 2.040092\n",
      "Train Epoche: 1 [1342/96 (1398%)]\tLoss: 10.901774\n",
      "Train Epoche: 1 [1343/96 (1399%)]\tLoss: 6.807250\n",
      "Train Epoche: 1 [1344/96 (1400%)]\tLoss: 4.687128\n",
      "Train Epoche: 1 [1345/96 (1401%)]\tLoss: 19.167988\n",
      "Train Epoche: 1 [1346/96 (1402%)]\tLoss: 46.137440\n",
      "Train Epoche: 1 [1347/96 (1403%)]\tLoss: 17.316872\n",
      "Train Epoche: 1 [1348/96 (1404%)]\tLoss: 0.058601\n",
      "Train Epoche: 1 [1349/96 (1405%)]\tLoss: 8.533157\n",
      "Train Epoche: 1 [1350/96 (1406%)]\tLoss: 0.649589\n",
      "Train Epoche: 1 [1351/96 (1407%)]\tLoss: 15.068502\n",
      "Train Epoche: 1 [1352/96 (1408%)]\tLoss: 0.318384\n",
      "Train Epoche: 1 [1353/96 (1409%)]\tLoss: 0.981577\n",
      "Train Epoche: 1 [1354/96 (1410%)]\tLoss: 2.245276\n",
      "Train Epoche: 1 [1355/96 (1411%)]\tLoss: 4.959059\n",
      "Train Epoche: 1 [1356/96 (1412%)]\tLoss: 26.026592\n",
      "Train Epoche: 1 [1357/96 (1414%)]\tLoss: 49.197453\n",
      "Train Epoche: 1 [1358/96 (1415%)]\tLoss: 0.406486\n",
      "Train Epoche: 1 [1359/96 (1416%)]\tLoss: 6.418017\n",
      "Train Epoche: 1 [1360/96 (1417%)]\tLoss: 0.293760\n",
      "Train Epoche: 1 [1361/96 (1418%)]\tLoss: 7.539724\n",
      "Train Epoche: 1 [1362/96 (1419%)]\tLoss: 23.706102\n",
      "Train Epoche: 1 [1363/96 (1420%)]\tLoss: 56.159672\n",
      "Train Epoche: 1 [1364/96 (1421%)]\tLoss: 18.719311\n",
      "Train Epoche: 1 [1365/96 (1422%)]\tLoss: 0.134955\n",
      "Train Epoche: 1 [1366/96 (1423%)]\tLoss: 1.277394\n",
      "Train Epoche: 1 [1367/96 (1424%)]\tLoss: 0.943976\n",
      "Train Epoche: 1 [1368/96 (1425%)]\tLoss: 18.591784\n",
      "Train Epoche: 1 [1369/96 (1426%)]\tLoss: 54.330494\n",
      "Train Epoche: 1 [1370/96 (1427%)]\tLoss: 22.378309\n",
      "Train Epoche: 1 [1371/96 (1428%)]\tLoss: 0.041965\n",
      "Train Epoche: 1 [1372/96 (1429%)]\tLoss: 71.572716\n",
      "Train Epoche: 1 [1373/96 (1430%)]\tLoss: 18.559641\n",
      "Train Epoche: 1 [1374/96 (1431%)]\tLoss: 58.916843\n",
      "Train Epoche: 1 [1375/96 (1432%)]\tLoss: 229.187241\n",
      "Train Epoche: 1 [1376/96 (1433%)]\tLoss: 4.891098\n",
      "Train Epoche: 1 [1377/96 (1434%)]\tLoss: 99.972687\n",
      "Train Epoche: 1 [1378/96 (1435%)]\tLoss: 45.250210\n",
      "Train Epoche: 1 [1379/96 (1436%)]\tLoss: 11.426009\n",
      "Train Epoche: 1 [1380/96 (1438%)]\tLoss: 71.430992\n",
      "Train Epoche: 1 [1381/96 (1439%)]\tLoss: 106.994110\n",
      "Train Epoche: 1 [1382/96 (1440%)]\tLoss: 80.426842\n",
      "Train Epoche: 1 [1383/96 (1441%)]\tLoss: 2.785760\n",
      "Train Epoche: 1 [1384/96 (1442%)]\tLoss: 0.210359\n",
      "Train Epoche: 1 [1385/96 (1443%)]\tLoss: 148.173294\n",
      "Train Epoche: 1 [1386/96 (1444%)]\tLoss: 0.010986\n",
      "Train Epoche: 1 [1387/96 (1445%)]\tLoss: 7.085161\n",
      "Train Epoche: 1 [1388/96 (1446%)]\tLoss: 12.524099\n",
      "Train Epoche: 1 [1389/96 (1447%)]\tLoss: 1.170626\n",
      "Train Epoche: 1 [1390/96 (1448%)]\tLoss: 3.339637\n",
      "Train Epoche: 1 [1391/96 (1449%)]\tLoss: 36.726078\n",
      "Train Epoche: 1 [1392/96 (1450%)]\tLoss: 48.222622\n",
      "Train Epoche: 1 [1393/96 (1451%)]\tLoss: 3.290863\n",
      "Train Epoche: 1 [1394/96 (1452%)]\tLoss: 23.930565\n",
      "Train Epoche: 1 [1395/96 (1453%)]\tLoss: 1.056347\n",
      "Train Epoche: 1 [1396/96 (1454%)]\tLoss: 24.481117\n",
      "Train Epoche: 1 [1397/96 (1455%)]\tLoss: 0.468743\n",
      "Train Epoche: 1 [1398/96 (1456%)]\tLoss: 46.964664\n",
      "Train Epoche: 1 [1399/96 (1457%)]\tLoss: 20.239891\n",
      "Train Epoche: 1 [1400/96 (1458%)]\tLoss: 246.413376\n",
      "Train Epoche: 1 [1401/96 (1459%)]\tLoss: 0.202368\n",
      "Train Epoche: 1 [1402/96 (1460%)]\tLoss: 0.044036\n",
      "Train Epoche: 1 [1403/96 (1461%)]\tLoss: 2.784166\n",
      "Train Epoche: 1 [1404/96 (1462%)]\tLoss: 4.475274\n",
      "Train Epoche: 1 [1405/96 (1464%)]\tLoss: 4.018681\n",
      "Train Epoche: 1 [1406/96 (1465%)]\tLoss: 16.442873\n",
      "Train Epoche: 1 [1407/96 (1466%)]\tLoss: 78.911118\n",
      "Train Epoche: 1 [1408/96 (1467%)]\tLoss: 6.970242\n",
      "Train Epoche: 1 [1409/96 (1468%)]\tLoss: 46.154140\n",
      "Train Epoche: 1 [1410/96 (1469%)]\tLoss: 99.434090\n",
      "Train Epoche: 1 [1411/96 (1470%)]\tLoss: 8.987142\n",
      "Train Epoche: 1 [1412/96 (1471%)]\tLoss: 0.921750\n",
      "Train Epoche: 1 [1413/96 (1472%)]\tLoss: 19.351646\n",
      "Train Epoche: 1 [1414/96 (1473%)]\tLoss: 92.659317\n",
      "Train Epoche: 1 [1415/96 (1474%)]\tLoss: 8.762131\n",
      "Train Epoche: 1 [1416/96 (1475%)]\tLoss: 39.668003\n",
      "Train Epoche: 1 [1417/96 (1476%)]\tLoss: 17.550320\n",
      "Train Epoche: 1 [1418/96 (1477%)]\tLoss: 2.851472\n",
      "Train Epoche: 1 [1419/96 (1478%)]\tLoss: 31.052444\n",
      "Train Epoche: 1 [1420/96 (1479%)]\tLoss: 44.680393\n",
      "Train Epoche: 1 [1421/96 (1480%)]\tLoss: 2.579708\n",
      "Train Epoche: 1 [1422/96 (1481%)]\tLoss: 20.483572\n",
      "Train Epoche: 1 [1423/96 (1482%)]\tLoss: 54.470074\n",
      "Train Epoche: 1 [1424/96 (1483%)]\tLoss: 89.974518\n",
      "Train Epoche: 1 [1425/96 (1484%)]\tLoss: 68.138336\n",
      "Train Epoche: 1 [1426/96 (1485%)]\tLoss: 2.285219\n",
      "Train Epoche: 1 [1427/96 (1486%)]\tLoss: 37.198360\n",
      "Train Epoche: 1 [1428/96 (1488%)]\tLoss: 20.923826\n",
      "Train Epoche: 1 [1429/96 (1489%)]\tLoss: 75.128357\n",
      "Train Epoche: 1 [1430/96 (1490%)]\tLoss: 8.449060\n",
      "Train Epoche: 1 [1431/96 (1491%)]\tLoss: 1.926495\n",
      "Train Epoche: 1 [1432/96 (1492%)]\tLoss: 5.725633\n",
      "Train Epoche: 1 [1433/96 (1493%)]\tLoss: 3.412028\n",
      "Train Epoche: 1 [1434/96 (1494%)]\tLoss: 28.994053\n",
      "Train Epoche: 1 [1435/96 (1495%)]\tLoss: 27.347139\n",
      "Train Epoche: 1 [1436/96 (1496%)]\tLoss: 124.994003\n",
      "Train Epoche: 1 [1437/96 (1497%)]\tLoss: 295.528778\n",
      "Train Epoche: 1 [1438/96 (1498%)]\tLoss: 4.201330\n",
      "Train Epoche: 1 [1439/96 (1499%)]\tLoss: 6.233393\n",
      "Train Epoche: 1 [1440/96 (1500%)]\tLoss: 3.517032\n",
      "Train Epoche: 1 [1441/96 (1501%)]\tLoss: 0.564053\n",
      "Train Epoche: 1 [1442/96 (1502%)]\tLoss: 3.835251\n",
      "Train Epoche: 1 [1443/96 (1503%)]\tLoss: 107.997932\n",
      "Train Epoche: 1 [1444/96 (1504%)]\tLoss: 4.121118\n",
      "Train Epoche: 1 [1445/96 (1505%)]\tLoss: 6.964321\n",
      "Train Epoche: 1 [1446/96 (1506%)]\tLoss: 0.206043\n",
      "Train Epoche: 1 [1447/96 (1507%)]\tLoss: 12.439786\n",
      "Train Epoche: 1 [1448/96 (1508%)]\tLoss: 41.417370\n",
      "Train Epoche: 1 [1449/96 (1509%)]\tLoss: 9.136400\n",
      "Train Epoche: 1 [1450/96 (1510%)]\tLoss: 0.137129\n",
      "Train Epoche: 1 [1451/96 (1511%)]\tLoss: 14.325018\n",
      "Train Epoche: 1 [1452/96 (1512%)]\tLoss: 3.100707\n",
      "Train Epoche: 1 [1453/96 (1514%)]\tLoss: 222.302017\n",
      "Train Epoche: 1 [1454/96 (1515%)]\tLoss: 0.722797\n",
      "Train Epoche: 1 [1455/96 (1516%)]\tLoss: 1.684572\n",
      "Train Epoche: 1 [1456/96 (1517%)]\tLoss: 8.516334\n",
      "Train Epoche: 1 [1457/96 (1518%)]\tLoss: 4.598552\n",
      "Train Epoche: 1 [1458/96 (1519%)]\tLoss: 30.892673\n",
      "Train Epoche: 1 [1459/96 (1520%)]\tLoss: 10.849297\n",
      "Train Epoche: 1 [1460/96 (1521%)]\tLoss: 4.424934\n",
      "Train Epoche: 1 [1461/96 (1522%)]\tLoss: 48.979736\n",
      "Train Epoche: 1 [1462/96 (1523%)]\tLoss: 18.644835\n",
      "Train Epoche: 1 [1463/96 (1524%)]\tLoss: 12.930693\n",
      "Train Epoche: 1 [1464/96 (1525%)]\tLoss: 194.468658\n",
      "Train Epoche: 1 [1465/96 (1526%)]\tLoss: 18.532633\n",
      "Train Epoche: 1 [1466/96 (1527%)]\tLoss: 56.772800\n",
      "Train Epoche: 1 [1467/96 (1528%)]\tLoss: 0.501796\n",
      "Train Epoche: 1 [1468/96 (1529%)]\tLoss: 9.008270\n",
      "Train Epoche: 1 [1469/96 (1530%)]\tLoss: 44.280582\n",
      "Train Epoche: 1 [1470/96 (1531%)]\tLoss: 3.422671\n",
      "Train Epoche: 1 [1471/96 (1532%)]\tLoss: 33.907860\n",
      "Train Epoche: 1 [1472/96 (1533%)]\tLoss: 26.907772\n",
      "Train Epoche: 1 [1473/96 (1534%)]\tLoss: 34.547558\n",
      "Train Epoche: 1 [1474/96 (1535%)]\tLoss: 28.457098\n",
      "Train Epoche: 1 [1475/96 (1536%)]\tLoss: 0.600722\n",
      "Train Epoche: 1 [1476/96 (1538%)]\tLoss: 0.943596\n",
      "Train Epoche: 1 [1477/96 (1539%)]\tLoss: 26.124701\n",
      "Train Epoche: 1 [1478/96 (1540%)]\tLoss: 0.182994\n",
      "Train Epoche: 1 [1479/96 (1541%)]\tLoss: 5.454733\n",
      "Train Epoche: 1 [1480/96 (1542%)]\tLoss: 6.144439\n",
      "Train Epoche: 1 [1481/96 (1543%)]\tLoss: 21.412287\n",
      "Train Epoche: 1 [1482/96 (1544%)]\tLoss: 0.598166\n",
      "Train Epoche: 1 [1483/96 (1545%)]\tLoss: 13.647145\n",
      "Train Epoche: 1 [1484/96 (1546%)]\tLoss: 33.348694\n",
      "Train Epoche: 1 [1485/96 (1547%)]\tLoss: 7.680659\n",
      "Train Epoche: 1 [1486/96 (1548%)]\tLoss: 380.424225\n",
      "Train Epoche: 1 [1487/96 (1549%)]\tLoss: 45.517014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1488/96 (1550%)]\tLoss: 48.304844\n",
      "Train Epoche: 1 [1489/96 (1551%)]\tLoss: 131.755051\n",
      "Train Epoche: 1 [1490/96 (1552%)]\tLoss: 10.000112\n",
      "Train Epoche: 1 [1491/96 (1553%)]\tLoss: 67.154594\n",
      "Train Epoche: 1 [1492/96 (1554%)]\tLoss: 7.677077\n",
      "Train Epoche: 1 [1493/96 (1555%)]\tLoss: 18.118380\n",
      "Train Epoche: 1 [1494/96 (1556%)]\tLoss: 0.876262\n",
      "Train Epoche: 1 [1495/96 (1557%)]\tLoss: 20.853399\n",
      "Train Epoche: 1 [1496/96 (1558%)]\tLoss: 74.015541\n",
      "Train Epoche: 1 [1497/96 (1559%)]\tLoss: 9.987009\n",
      "Train Epoche: 1 [1498/96 (1560%)]\tLoss: 72.992088\n",
      "Train Epoche: 1 [1499/96 (1561%)]\tLoss: 0.004534\n",
      "Train Epoche: 1 [1500/96 (1562%)]\tLoss: 68.189384\n",
      "Train Epoche: 1 [1501/96 (1564%)]\tLoss: 17.063618\n",
      "Train Epoche: 1 [1502/96 (1565%)]\tLoss: 120.122459\n",
      "Train Epoche: 1 [1503/96 (1566%)]\tLoss: 0.183511\n",
      "Train Epoche: 1 [1504/96 (1567%)]\tLoss: 39.164539\n",
      "Train Epoche: 1 [1505/96 (1568%)]\tLoss: 34.777042\n",
      "Train Epoche: 1 [1506/96 (1569%)]\tLoss: 376.013428\n",
      "Train Epoche: 1 [1507/96 (1570%)]\tLoss: 14.145879\n",
      "Train Epoche: 1 [1508/96 (1571%)]\tLoss: 15.140956\n",
      "Train Epoche: 1 [1509/96 (1572%)]\tLoss: 85.933807\n",
      "Train Epoche: 1 [1510/96 (1573%)]\tLoss: 53.035645\n",
      "Train Epoche: 1 [1511/96 (1574%)]\tLoss: 67.180344\n",
      "Train Epoche: 1 [1512/96 (1575%)]\tLoss: 27.532608\n",
      "Train Epoche: 1 [1513/96 (1576%)]\tLoss: 27.343798\n",
      "Train Epoche: 1 [1514/96 (1577%)]\tLoss: 53.267475\n",
      "Train Epoche: 1 [1515/96 (1578%)]\tLoss: 12.945134\n",
      "Train Epoche: 1 [1516/96 (1579%)]\tLoss: 0.037774\n",
      "Train Epoche: 1 [1517/96 (1580%)]\tLoss: 0.208204\n",
      "Train Epoche: 1 [1518/96 (1581%)]\tLoss: 54.481407\n",
      "Train Epoche: 1 [1519/96 (1582%)]\tLoss: 6.604977\n",
      "Train Epoche: 1 [1520/96 (1583%)]\tLoss: 119.876450\n",
      "Train Epoche: 1 [1521/96 (1584%)]\tLoss: 14.440559\n",
      "Train Epoche: 1 [1522/96 (1585%)]\tLoss: 0.103001\n",
      "Train Epoche: 1 [1523/96 (1586%)]\tLoss: 3.072952\n",
      "Train Epoche: 1 [1524/96 (1588%)]\tLoss: 76.229111\n",
      "Train Epoche: 1 [1525/96 (1589%)]\tLoss: 23.922398\n",
      "Train Epoche: 1 [1526/96 (1590%)]\tLoss: 1.008785\n",
      "Train Epoche: 1 [1527/96 (1591%)]\tLoss: 0.090113\n",
      "Train Epoche: 1 [1528/96 (1592%)]\tLoss: 63.108070\n",
      "Train Epoche: 1 [1529/96 (1593%)]\tLoss: 225.589844\n",
      "Train Epoche: 1 [1530/96 (1594%)]\tLoss: 3.956866\n",
      "Train Epoche: 1 [1531/96 (1595%)]\tLoss: 47.463215\n",
      "Train Epoche: 1 [1532/96 (1596%)]\tLoss: 26.085100\n",
      "Train Epoche: 1 [1533/96 (1597%)]\tLoss: 159.324081\n",
      "Train Epoche: 1 [1534/96 (1598%)]\tLoss: 121.851761\n",
      "Train Epoche: 1 [1535/96 (1599%)]\tLoss: 2.758209\n",
      "Train Epoche: 1 [1536/96 (1600%)]\tLoss: 0.463262\n",
      "Train Epoche: 1 [1537/96 (1601%)]\tLoss: 35.172855\n",
      "Train Epoche: 1 [1538/96 (1602%)]\tLoss: 150.488846\n",
      "Train Epoche: 1 [1539/96 (1603%)]\tLoss: 1.071146\n",
      "Train Epoche: 1 [1540/96 (1604%)]\tLoss: 21.222063\n",
      "Train Epoche: 1 [1541/96 (1605%)]\tLoss: 369.870270\n",
      "Train Epoche: 1 [1542/96 (1606%)]\tLoss: 19.662796\n",
      "Train Epoche: 1 [1543/96 (1607%)]\tLoss: 87.124283\n",
      "Train Epoche: 1 [1544/96 (1608%)]\tLoss: 5.040817\n",
      "Train Epoche: 1 [1545/96 (1609%)]\tLoss: 219.312012\n",
      "Train Epoche: 1 [1546/96 (1610%)]\tLoss: 9.439513\n",
      "Train Epoche: 1 [1547/96 (1611%)]\tLoss: 21.610498\n",
      "Train Epoche: 1 [1548/96 (1612%)]\tLoss: 0.291242\n",
      "Train Epoche: 1 [1549/96 (1614%)]\tLoss: 66.823677\n",
      "Train Epoche: 1 [1550/96 (1615%)]\tLoss: 19.710014\n",
      "Train Epoche: 1 [1551/96 (1616%)]\tLoss: 0.002025\n",
      "Train Epoche: 1 [1552/96 (1617%)]\tLoss: 0.023730\n",
      "Train Epoche: 1 [1553/96 (1618%)]\tLoss: 4.517157\n",
      "Train Epoche: 1 [1554/96 (1619%)]\tLoss: 92.880501\n",
      "Train Epoche: 1 [1555/96 (1620%)]\tLoss: 4.658442\n",
      "Train Epoche: 1 [1556/96 (1621%)]\tLoss: 10.688469\n",
      "Train Epoche: 1 [1557/96 (1622%)]\tLoss: 11.079539\n",
      "Train Epoche: 1 [1558/96 (1623%)]\tLoss: 1.266187\n",
      "Train Epoche: 1 [1559/96 (1624%)]\tLoss: 0.876760\n",
      "Train Epoche: 1 [1560/96 (1625%)]\tLoss: 81.332794\n",
      "Train Epoche: 1 [1561/96 (1626%)]\tLoss: 0.430284\n",
      "Train Epoche: 1 [1562/96 (1627%)]\tLoss: 16.170082\n",
      "Train Epoche: 1 [1563/96 (1628%)]\tLoss: 0.119909\n",
      "Train Epoche: 1 [1564/96 (1629%)]\tLoss: 2.075538\n",
      "Train Epoche: 1 [1565/96 (1630%)]\tLoss: 3.217723\n",
      "Train Epoche: 1 [1566/96 (1631%)]\tLoss: 3.727688\n",
      "Train Epoche: 1 [1567/96 (1632%)]\tLoss: 0.828461\n",
      "Train Epoche: 1 [1568/96 (1633%)]\tLoss: 9.197666\n",
      "Train Epoche: 1 [1569/96 (1634%)]\tLoss: 26.470234\n",
      "Train Epoche: 1 [1570/96 (1635%)]\tLoss: 0.019527\n",
      "Train Epoche: 1 [1571/96 (1636%)]\tLoss: 1.301453\n",
      "Train Epoche: 1 [1572/96 (1638%)]\tLoss: 161.695068\n",
      "Train Epoche: 1 [1573/96 (1639%)]\tLoss: 0.651917\n",
      "Train Epoche: 1 [1574/96 (1640%)]\tLoss: 389.050140\n",
      "Train Epoche: 1 [1575/96 (1641%)]\tLoss: 5.710715\n",
      "Train Epoche: 1 [1576/96 (1642%)]\tLoss: 165.371613\n",
      "Train Epoche: 1 [1577/96 (1643%)]\tLoss: 229.905350\n",
      "Train Epoche: 1 [1578/96 (1644%)]\tLoss: 10.812588\n",
      "Train Epoche: 1 [1579/96 (1645%)]\tLoss: 15.941916\n",
      "Train Epoche: 1 [1580/96 (1646%)]\tLoss: 58.121777\n",
      "Train Epoche: 1 [1581/96 (1647%)]\tLoss: 0.044684\n",
      "Train Epoche: 1 [1582/96 (1648%)]\tLoss: 88.466095\n",
      "Train Epoche: 1 [1583/96 (1649%)]\tLoss: 23.312468\n",
      "Train Epoche: 1 [1584/96 (1650%)]\tLoss: 4.151120\n",
      "Train Epoche: 1 [1585/96 (1651%)]\tLoss: 35.493767\n",
      "Train Epoche: 1 [1586/96 (1652%)]\tLoss: 0.232121\n",
      "Train Epoche: 1 [1587/96 (1653%)]\tLoss: 15.958431\n",
      "Train Epoche: 1 [1588/96 (1654%)]\tLoss: 2.605915\n",
      "Train Epoche: 1 [1589/96 (1655%)]\tLoss: 5.098001\n",
      "Train Epoche: 1 [1590/96 (1656%)]\tLoss: 1.683833\n",
      "Train Epoche: 1 [1591/96 (1657%)]\tLoss: 213.783752\n",
      "Train Epoche: 1 [1592/96 (1658%)]\tLoss: 0.568586\n",
      "Train Epoche: 1 [1593/96 (1659%)]\tLoss: 0.249761\n",
      "Train Epoche: 1 [1594/96 (1660%)]\tLoss: 53.602600\n",
      "Train Epoche: 1 [1595/96 (1661%)]\tLoss: 11.034211\n",
      "Train Epoche: 1 [1596/96 (1662%)]\tLoss: 0.340370\n",
      "Train Epoche: 1 [1597/96 (1664%)]\tLoss: 1.501329\n",
      "Train Epoche: 1 [1598/96 (1665%)]\tLoss: 17.894743\n",
      "Train Epoche: 1 [1599/96 (1666%)]\tLoss: 3.559261\n",
      "Train Epoche: 1 [1600/96 (1667%)]\tLoss: 72.451302\n",
      "Train Epoche: 1 [1601/96 (1668%)]\tLoss: 0.055011\n",
      "Train Epoche: 1 [1602/96 (1669%)]\tLoss: 35.564640\n",
      "Train Epoche: 1 [1603/96 (1670%)]\tLoss: 40.239063\n",
      "Train Epoche: 1 [1604/96 (1671%)]\tLoss: 13.456205\n",
      "Train Epoche: 1 [1605/96 (1672%)]\tLoss: 0.338068\n",
      "Train Epoche: 1 [1606/96 (1673%)]\tLoss: 2.321089\n",
      "Train Epoche: 1 [1607/96 (1674%)]\tLoss: 36.537178\n",
      "Train Epoche: 1 [1608/96 (1675%)]\tLoss: 2.522267\n",
      "Train Epoche: 1 [1609/96 (1676%)]\tLoss: 0.530283\n",
      "Train Epoche: 1 [1610/96 (1677%)]\tLoss: 8.125977\n",
      "Train Epoche: 1 [1611/96 (1678%)]\tLoss: 4.799449\n",
      "Train Epoche: 1 [1612/96 (1679%)]\tLoss: 268.674896\n",
      "Train Epoche: 1 [1613/96 (1680%)]\tLoss: 1.305690\n",
      "Train Epoche: 1 [1614/96 (1681%)]\tLoss: 25.568525\n",
      "Train Epoche: 1 [1615/96 (1682%)]\tLoss: 0.005016\n",
      "Train Epoche: 1 [1616/96 (1683%)]\tLoss: 0.002667\n",
      "Train Epoche: 1 [1617/96 (1684%)]\tLoss: 0.527752\n",
      "Train Epoche: 1 [1618/96 (1685%)]\tLoss: 2.709046\n",
      "Train Epoche: 1 [1619/96 (1686%)]\tLoss: 48.524235\n",
      "Train Epoche: 1 [1620/96 (1688%)]\tLoss: 1.789447\n",
      "Train Epoche: 1 [1621/96 (1689%)]\tLoss: 3.184833\n",
      "Train Epoche: 1 [1622/96 (1690%)]\tLoss: 5.027084\n",
      "Train Epoche: 1 [1623/96 (1691%)]\tLoss: 35.533775\n",
      "Train Epoche: 1 [1624/96 (1692%)]\tLoss: 2.069604\n",
      "Train Epoche: 1 [1625/96 (1693%)]\tLoss: 25.116245\n",
      "Train Epoche: 1 [1626/96 (1694%)]\tLoss: 61.372700\n",
      "Train Epoche: 1 [1627/96 (1695%)]\tLoss: 9.957743\n",
      "Train Epoche: 1 [1628/96 (1696%)]\tLoss: 6.357835\n",
      "Train Epoche: 1 [1629/96 (1697%)]\tLoss: 8.669640\n",
      "Train Epoche: 1 [1630/96 (1698%)]\tLoss: 0.047469\n",
      "Train Epoche: 1 [1631/96 (1699%)]\tLoss: 18.896402\n",
      "Train Epoche: 1 [1632/96 (1700%)]\tLoss: 8.086155\n",
      "Train Epoche: 1 [1633/96 (1701%)]\tLoss: 44.536163\n",
      "Train Epoche: 1 [1634/96 (1702%)]\tLoss: 10.242676\n",
      "Train Epoche: 1 [1635/96 (1703%)]\tLoss: 5.098566\n",
      "Train Epoche: 1 [1636/96 (1704%)]\tLoss: 16.914051\n",
      "Train Epoche: 1 [1637/96 (1705%)]\tLoss: 6.314234\n",
      "Train Epoche: 1 [1638/96 (1706%)]\tLoss: 9.124876\n",
      "Train Epoche: 1 [1639/96 (1707%)]\tLoss: 2.113936\n",
      "Train Epoche: 1 [1640/96 (1708%)]\tLoss: 251.055023\n",
      "Train Epoche: 1 [1641/96 (1709%)]\tLoss: 19.698965\n",
      "Train Epoche: 1 [1642/96 (1710%)]\tLoss: 0.006244\n",
      "Train Epoche: 1 [1643/96 (1711%)]\tLoss: 22.145622\n",
      "Train Epoche: 1 [1644/96 (1712%)]\tLoss: 6.736883\n",
      "Train Epoche: 1 [1645/96 (1714%)]\tLoss: 10.784609\n",
      "Train Epoche: 1 [1646/96 (1715%)]\tLoss: 10.484458\n",
      "Train Epoche: 1 [1647/96 (1716%)]\tLoss: 40.927116\n",
      "Train Epoche: 1 [1648/96 (1717%)]\tLoss: 4.692458\n",
      "Train Epoche: 1 [1649/96 (1718%)]\tLoss: 11.836832\n",
      "Train Epoche: 1 [1650/96 (1719%)]\tLoss: 203.862122\n",
      "Train Epoche: 1 [1651/96 (1720%)]\tLoss: 24.133390\n",
      "Train Epoche: 1 [1652/96 (1721%)]\tLoss: 0.586117\n",
      "Train Epoche: 1 [1653/96 (1722%)]\tLoss: 31.403877\n",
      "Train Epoche: 1 [1654/96 (1723%)]\tLoss: 0.121784\n",
      "Train Epoche: 1 [1655/96 (1724%)]\tLoss: 1.043705\n",
      "Train Epoche: 1 [1656/96 (1725%)]\tLoss: 11.645940\n",
      "Train Epoche: 1 [1657/96 (1726%)]\tLoss: 8.210645\n",
      "Train Epoche: 1 [1658/96 (1727%)]\tLoss: 4.745821\n",
      "Train Epoche: 1 [1659/96 (1728%)]\tLoss: 0.352570\n",
      "Train Epoche: 1 [1660/96 (1729%)]\tLoss: 0.993940\n",
      "Train Epoche: 1 [1661/96 (1730%)]\tLoss: 1.883644\n",
      "Train Epoche: 1 [1662/96 (1731%)]\tLoss: 76.366356\n",
      "Train Epoche: 1 [1663/96 (1732%)]\tLoss: 31.368593\n",
      "Train Epoche: 1 [1664/96 (1733%)]\tLoss: 0.999230\n",
      "Train Epoche: 1 [1665/96 (1734%)]\tLoss: 2.712952\n",
      "Train Epoche: 1 [1666/96 (1735%)]\tLoss: 53.011826\n",
      "Train Epoche: 1 [1667/96 (1736%)]\tLoss: 1.629615\n",
      "Train Epoche: 1 [1668/96 (1738%)]\tLoss: 8.080027\n",
      "Train Epoche: 1 [1669/96 (1739%)]\tLoss: 0.086778\n",
      "Train Epoche: 1 [1670/96 (1740%)]\tLoss: 31.318233\n",
      "Train Epoche: 1 [1671/96 (1741%)]\tLoss: 1.024057\n",
      "Train Epoche: 1 [1672/96 (1742%)]\tLoss: 34.571991\n",
      "Train Epoche: 1 [1673/96 (1743%)]\tLoss: 30.087973\n",
      "Train Epoche: 1 [1674/96 (1744%)]\tLoss: 0.049965\n",
      "Train Epoche: 1 [1675/96 (1745%)]\tLoss: 17.757271\n",
      "Train Epoche: 1 [1676/96 (1746%)]\tLoss: 18.056223\n",
      "Train Epoche: 1 [1677/96 (1747%)]\tLoss: 58.466709\n",
      "Train Epoche: 1 [1678/96 (1748%)]\tLoss: 36.323460\n",
      "Train Epoche: 1 [1679/96 (1749%)]\tLoss: 4.112130\n",
      "Train Epoche: 1 [1680/96 (1750%)]\tLoss: 3.233656\n",
      "Train Epoche: 1 [1681/96 (1751%)]\tLoss: 9.415508\n",
      "Train Epoche: 1 [1682/96 (1752%)]\tLoss: 2.353564\n",
      "Train Epoche: 1 [1683/96 (1753%)]\tLoss: 7.834592\n",
      "Train Epoche: 1 [1684/96 (1754%)]\tLoss: 11.482651\n",
      "Train Epoche: 1 [1685/96 (1755%)]\tLoss: 6.046499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1686/96 (1756%)]\tLoss: 1.429727\n",
      "Train Epoche: 1 [1687/96 (1757%)]\tLoss: 5.311494\n",
      "Train Epoche: 1 [1688/96 (1758%)]\tLoss: 36.910385\n",
      "Train Epoche: 1 [1689/96 (1759%)]\tLoss: 6.646197\n",
      "Train Epoche: 1 [1690/96 (1760%)]\tLoss: 11.685275\n",
      "Train Epoche: 1 [1691/96 (1761%)]\tLoss: 20.479725\n",
      "Train Epoche: 1 [1692/96 (1762%)]\tLoss: 0.599574\n",
      "Train Epoche: 1 [1693/96 (1764%)]\tLoss: 14.443039\n",
      "Train Epoche: 1 [1694/96 (1765%)]\tLoss: 25.372816\n",
      "Train Epoche: 1 [1695/96 (1766%)]\tLoss: 1.356578\n",
      "Train Epoche: 1 [1696/96 (1767%)]\tLoss: 11.030441\n",
      "Train Epoche: 1 [1697/96 (1768%)]\tLoss: 12.960552\n",
      "Train Epoche: 1 [1698/96 (1769%)]\tLoss: 87.949211\n",
      "Train Epoche: 1 [1699/96 (1770%)]\tLoss: 0.747064\n",
      "Train Epoche: 1 [1700/96 (1771%)]\tLoss: 12.003484\n",
      "Train Epoche: 1 [1701/96 (1772%)]\tLoss: 51.329041\n",
      "Train Epoche: 1 [1702/96 (1773%)]\tLoss: 0.668472\n",
      "Train Epoche: 1 [1703/96 (1774%)]\tLoss: 108.369209\n",
      "Train Epoche: 1 [1704/96 (1775%)]\tLoss: 9.547523\n",
      "Train Epoche: 1 [1705/96 (1776%)]\tLoss: 0.724917\n",
      "Train Epoche: 1 [1706/96 (1777%)]\tLoss: 2.481089\n",
      "Train Epoche: 1 [1707/96 (1778%)]\tLoss: 1.995353\n",
      "Train Epoche: 1 [1708/96 (1779%)]\tLoss: 5.830719\n",
      "Train Epoche: 1 [1709/96 (1780%)]\tLoss: 15.118936\n",
      "Train Epoche: 1 [1710/96 (1781%)]\tLoss: 0.097718\n",
      "Train Epoche: 1 [1711/96 (1782%)]\tLoss: 18.964979\n",
      "Train Epoche: 1 [1712/96 (1783%)]\tLoss: 24.605810\n",
      "Train Epoche: 1 [1713/96 (1784%)]\tLoss: 38.523945\n",
      "Train Epoche: 1 [1714/96 (1785%)]\tLoss: 18.846119\n",
      "Train Epoche: 1 [1715/96 (1786%)]\tLoss: 33.825966\n",
      "Train Epoche: 1 [1716/96 (1788%)]\tLoss: 52.339260\n",
      "Train Epoche: 1 [1717/96 (1789%)]\tLoss: 2.850271\n",
      "Train Epoche: 1 [1718/96 (1790%)]\tLoss: 1.561349\n",
      "Train Epoche: 1 [1719/96 (1791%)]\tLoss: 0.422590\n",
      "Train Epoche: 1 [1720/96 (1792%)]\tLoss: 7.153856\n",
      "Train Epoche: 1 [1721/96 (1793%)]\tLoss: 0.512706\n",
      "Train Epoche: 1 [1722/96 (1794%)]\tLoss: 7.683499\n",
      "Train Epoche: 1 [1723/96 (1795%)]\tLoss: 1.055208\n",
      "Train Epoche: 1 [1724/96 (1796%)]\tLoss: 4.680538\n",
      "Train Epoche: 1 [1725/96 (1797%)]\tLoss: 1.708908\n",
      "Train Epoche: 1 [1726/96 (1798%)]\tLoss: 0.225105\n",
      "Train Epoche: 1 [1727/96 (1799%)]\tLoss: 0.041149\n",
      "Train Epoche: 1 [1728/96 (1800%)]\tLoss: 1.310302\n",
      "Train Epoche: 1 [1729/96 (1801%)]\tLoss: 6.436416\n",
      "Train Epoche: 1 [1730/96 (1802%)]\tLoss: 0.142533\n",
      "Train Epoche: 1 [1731/96 (1803%)]\tLoss: 0.337515\n",
      "Train Epoche: 1 [1732/96 (1804%)]\tLoss: 7.751176\n",
      "Train Epoche: 1 [1733/96 (1805%)]\tLoss: 9.297751\n",
      "Train Epoche: 1 [1734/96 (1806%)]\tLoss: 86.184837\n",
      "Train Epoche: 1 [1735/96 (1807%)]\tLoss: 1.219414\n",
      "Train Epoche: 1 [1736/96 (1808%)]\tLoss: 0.027616\n",
      "Train Epoche: 1 [1737/96 (1809%)]\tLoss: 0.588270\n",
      "Train Epoche: 1 [1738/96 (1810%)]\tLoss: 0.668400\n",
      "Train Epoche: 1 [1739/96 (1811%)]\tLoss: 8.376048\n",
      "Train Epoche: 1 [1740/96 (1812%)]\tLoss: 10.983810\n",
      "Train Epoche: 1 [1741/96 (1814%)]\tLoss: 2.521297\n",
      "Train Epoche: 1 [1742/96 (1815%)]\tLoss: 0.248155\n",
      "Train Epoche: 1 [1743/96 (1816%)]\tLoss: 2.577071\n",
      "Train Epoche: 1 [1744/96 (1817%)]\tLoss: 0.457742\n",
      "Train Epoche: 1 [1745/96 (1818%)]\tLoss: 2.050962\n",
      "Train Epoche: 1 [1746/96 (1819%)]\tLoss: 19.062565\n",
      "Train Epoche: 1 [1747/96 (1820%)]\tLoss: 6.471949\n",
      "Train Epoche: 1 [1748/96 (1821%)]\tLoss: 3.182341\n",
      "Train Epoche: 1 [1749/96 (1822%)]\tLoss: 2.683124\n",
      "Train Epoche: 1 [1750/96 (1823%)]\tLoss: 20.274874\n",
      "Train Epoche: 1 [1751/96 (1824%)]\tLoss: 0.146239\n",
      "Train Epoche: 1 [1752/96 (1825%)]\tLoss: 0.860446\n",
      "Train Epoche: 1 [1753/96 (1826%)]\tLoss: 14.192734\n",
      "Train Epoche: 1 [1754/96 (1827%)]\tLoss: 0.979310\n",
      "Train Epoche: 1 [1755/96 (1828%)]\tLoss: 6.819674\n",
      "Train Epoche: 1 [1756/96 (1829%)]\tLoss: 1.349047\n",
      "Train Epoche: 1 [1757/96 (1830%)]\tLoss: 163.845505\n",
      "Train Epoche: 1 [1758/96 (1831%)]\tLoss: 170.590363\n",
      "Train Epoche: 1 [1759/96 (1832%)]\tLoss: 104.432854\n",
      "Train Epoche: 1 [1760/96 (1833%)]\tLoss: 2.907405\n",
      "Train Epoche: 1 [1761/96 (1834%)]\tLoss: 8.526891\n",
      "Train Epoche: 1 [1762/96 (1835%)]\tLoss: 27.167467\n",
      "Train Epoche: 1 [1763/96 (1836%)]\tLoss: 111.499352\n",
      "Train Epoche: 1 [1764/96 (1838%)]\tLoss: 1.385298\n",
      "Train Epoche: 1 [1765/96 (1839%)]\tLoss: 2.790895\n",
      "Train Epoche: 1 [1766/96 (1840%)]\tLoss: 29.056555\n",
      "Train Epoche: 1 [1767/96 (1841%)]\tLoss: 3.316224\n",
      "Train Epoche: 1 [1768/96 (1842%)]\tLoss: 7.779218\n",
      "Train Epoche: 1 [1769/96 (1843%)]\tLoss: 3.895938\n",
      "Train Epoche: 1 [1770/96 (1844%)]\tLoss: 125.294365\n",
      "Train Epoche: 1 [1771/96 (1845%)]\tLoss: 63.285423\n",
      "Train Epoche: 1 [1772/96 (1846%)]\tLoss: 6.686757\n",
      "Train Epoche: 1 [1773/96 (1847%)]\tLoss: 48.983631\n",
      "Train Epoche: 1 [1774/96 (1848%)]\tLoss: 377.956421\n",
      "Train Epoche: 1 [1775/96 (1849%)]\tLoss: 4.324488\n",
      "Train Epoche: 1 [1776/96 (1850%)]\tLoss: 0.121963\n",
      "Train Epoche: 1 [1777/96 (1851%)]\tLoss: 1.906225\n",
      "Train Epoche: 1 [1778/96 (1852%)]\tLoss: 87.478729\n",
      "Train Epoche: 1 [1779/96 (1853%)]\tLoss: 13.719386\n",
      "Train Epoche: 1 [1780/96 (1854%)]\tLoss: 1.765754\n",
      "Train Epoche: 1 [1781/96 (1855%)]\tLoss: 1.419051\n",
      "Train Epoche: 1 [1782/96 (1856%)]\tLoss: 113.068466\n",
      "Train Epoche: 1 [1783/96 (1857%)]\tLoss: 8.222618\n",
      "Train Epoche: 1 [1784/96 (1858%)]\tLoss: 16.152884\n",
      "Train Epoche: 1 [1785/96 (1859%)]\tLoss: 234.491989\n",
      "Train Epoche: 1 [1786/96 (1860%)]\tLoss: 16.402834\n",
      "Train Epoche: 1 [1787/96 (1861%)]\tLoss: 10.489436\n",
      "Train Epoche: 1 [1788/96 (1862%)]\tLoss: 1.081699\n",
      "Train Epoche: 1 [1789/96 (1864%)]\tLoss: 0.568037\n",
      "Train Epoche: 1 [1790/96 (1865%)]\tLoss: 24.622101\n",
      "Train Epoche: 1 [1791/96 (1866%)]\tLoss: 312.756836\n",
      "Train Epoche: 1 [1792/96 (1867%)]\tLoss: 153.808289\n",
      "Train Epoche: 1 [1793/96 (1868%)]\tLoss: 101.516014\n",
      "Train Epoche: 1 [1794/96 (1869%)]\tLoss: 0.301766\n",
      "Train Epoche: 1 [1795/96 (1870%)]\tLoss: 50.396832\n",
      "Train Epoche: 1 [1796/96 (1871%)]\tLoss: 10.219069\n",
      "Train Epoche: 1 [1797/96 (1872%)]\tLoss: 0.584583\n",
      "Train Epoche: 1 [1798/96 (1873%)]\tLoss: 20.312704\n",
      "Train Epoche: 1 [1799/96 (1874%)]\tLoss: 8.642165\n",
      "Train Epoche: 1 [1800/96 (1875%)]\tLoss: 17.617094\n",
      "Train Epoche: 1 [1801/96 (1876%)]\tLoss: 0.229198\n",
      "Train Epoche: 1 [1802/96 (1877%)]\tLoss: 1.701177\n",
      "Train Epoche: 1 [1803/96 (1878%)]\tLoss: 0.040767\n",
      "Train Epoche: 1 [1804/96 (1879%)]\tLoss: 4.827710\n",
      "Train Epoche: 1 [1805/96 (1880%)]\tLoss: 2.389439\n",
      "Train Epoche: 1 [1806/96 (1881%)]\tLoss: 6.694967\n",
      "Train Epoche: 1 [1807/96 (1882%)]\tLoss: 0.603504\n",
      "Train Epoche: 1 [1808/96 (1883%)]\tLoss: 325.722687\n",
      "Train Epoche: 1 [1809/96 (1884%)]\tLoss: 0.809658\n",
      "Train Epoche: 1 [1810/96 (1885%)]\tLoss: 214.278015\n",
      "Train Epoche: 1 [1811/96 (1886%)]\tLoss: 47.619141\n",
      "Train Epoche: 1 [1812/96 (1888%)]\tLoss: 176.784286\n",
      "Train Epoche: 1 [1813/96 (1889%)]\tLoss: 97.458809\n",
      "Train Epoche: 1 [1814/96 (1890%)]\tLoss: 42.759689\n",
      "Train Epoche: 1 [1815/96 (1891%)]\tLoss: 120.058708\n",
      "Train Epoche: 1 [1816/96 (1892%)]\tLoss: 30.763992\n",
      "Train Epoche: 1 [1817/96 (1893%)]\tLoss: 19.889856\n",
      "Train Epoche: 1 [1818/96 (1894%)]\tLoss: 0.639858\n",
      "Train Epoche: 1 [1819/96 (1895%)]\tLoss: 0.160550\n",
      "Train Epoche: 1 [1820/96 (1896%)]\tLoss: 323.669342\n",
      "Train Epoche: 1 [1821/96 (1897%)]\tLoss: 39.561352\n",
      "Train Epoche: 1 [1822/96 (1898%)]\tLoss: 7.581510\n",
      "Train Epoche: 1 [1823/96 (1899%)]\tLoss: 13.798916\n",
      "Train Epoche: 1 [1824/96 (1900%)]\tLoss: 2.038627\n",
      "Train Epoche: 1 [1825/96 (1901%)]\tLoss: 18.588724\n",
      "Train Epoche: 1 [1826/96 (1902%)]\tLoss: 1.979366\n",
      "Train Epoche: 1 [1827/96 (1903%)]\tLoss: 19.169199\n",
      "Train Epoche: 1 [1828/96 (1904%)]\tLoss: 54.145241\n",
      "Train Epoche: 1 [1829/96 (1905%)]\tLoss: 28.569059\n",
      "Train Epoche: 1 [1830/96 (1906%)]\tLoss: 8.146890\n",
      "Train Epoche: 1 [1831/96 (1907%)]\tLoss: 31.618093\n",
      "Train Epoche: 1 [1832/96 (1908%)]\tLoss: 0.557378\n",
      "Train Epoche: 1 [1833/96 (1909%)]\tLoss: 7.758689\n",
      "Train Epoche: 1 [1834/96 (1910%)]\tLoss: 71.712181\n",
      "Train Epoche: 1 [1835/96 (1911%)]\tLoss: 78.450203\n",
      "Train Epoche: 1 [1836/96 (1912%)]\tLoss: 279.565643\n",
      "Train Epoche: 1 [1837/96 (1914%)]\tLoss: 6.728731\n",
      "Train Epoche: 1 [1838/96 (1915%)]\tLoss: 1.502912\n",
      "Train Epoche: 1 [1839/96 (1916%)]\tLoss: 22.943579\n",
      "Train Epoche: 1 [1840/96 (1917%)]\tLoss: 0.558332\n",
      "Train Epoche: 1 [1841/96 (1918%)]\tLoss: 47.818127\n",
      "Train Epoche: 1 [1842/96 (1919%)]\tLoss: 1.511055\n",
      "Train Epoche: 1 [1843/96 (1920%)]\tLoss: 17.164904\n",
      "Train Epoche: 1 [1844/96 (1921%)]\tLoss: 0.275892\n",
      "Train Epoche: 1 [1845/96 (1922%)]\tLoss: 8.043402\n",
      "Train Epoche: 1 [1846/96 (1923%)]\tLoss: 1.588976\n",
      "Train Epoche: 1 [1847/96 (1924%)]\tLoss: 0.685850\n",
      "Train Epoche: 1 [1848/96 (1925%)]\tLoss: 126.885643\n",
      "Train Epoche: 1 [1849/96 (1926%)]\tLoss: 35.826443\n",
      "Train Epoche: 1 [1850/96 (1927%)]\tLoss: 7.864430\n",
      "Train Epoche: 1 [1851/96 (1928%)]\tLoss: 84.178787\n",
      "Train Epoche: 1 [1852/96 (1929%)]\tLoss: 1.639359\n",
      "Train Epoche: 1 [1853/96 (1930%)]\tLoss: 36.666088\n",
      "Train Epoche: 1 [1854/96 (1931%)]\tLoss: 4.905250\n",
      "Train Epoche: 1 [1855/96 (1932%)]\tLoss: 6.371289\n",
      "Train Epoche: 1 [1856/96 (1933%)]\tLoss: 2.936628\n",
      "Train Epoche: 1 [1857/96 (1934%)]\tLoss: 46.154270\n",
      "Train Epoche: 1 [1858/96 (1935%)]\tLoss: 13.974005\n",
      "Train Epoche: 1 [1859/96 (1936%)]\tLoss: 2.601902\n",
      "Train Epoche: 1 [1860/96 (1938%)]\tLoss: 3.683316\n",
      "Train Epoche: 1 [1861/96 (1939%)]\tLoss: 54.562191\n",
      "Train Epoche: 1 [1862/96 (1940%)]\tLoss: 0.380032\n",
      "Train Epoche: 1 [1863/96 (1941%)]\tLoss: 178.204895\n",
      "Train Epoche: 1 [1864/96 (1942%)]\tLoss: 1.107649\n",
      "Train Epoche: 1 [1865/96 (1943%)]\tLoss: 16.377089\n",
      "Train Epoche: 1 [1866/96 (1944%)]\tLoss: 0.100187\n",
      "Train Epoche: 1 [1867/96 (1945%)]\tLoss: 19.838583\n",
      "Train Epoche: 1 [1868/96 (1946%)]\tLoss: 12.439429\n",
      "Train Epoche: 1 [1869/96 (1947%)]\tLoss: 0.035655\n",
      "Train Epoche: 1 [1870/96 (1948%)]\tLoss: 3.525173\n",
      "Train Epoche: 1 [1871/96 (1949%)]\tLoss: 111.917953\n",
      "Train Epoche: 1 [1872/96 (1950%)]\tLoss: 26.362572\n",
      "Train Epoche: 1 [1873/96 (1951%)]\tLoss: 1.976171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1874/96 (1952%)]\tLoss: 89.412674\n",
      "Train Epoche: 1 [1875/96 (1953%)]\tLoss: 5.150755\n",
      "Train Epoche: 1 [1876/96 (1954%)]\tLoss: 26.766747\n",
      "Train Epoche: 1 [1877/96 (1955%)]\tLoss: 14.485605\n",
      "Train Epoche: 1 [1878/96 (1956%)]\tLoss: 21.540358\n",
      "Train Epoche: 1 [1879/96 (1957%)]\tLoss: 0.039303\n",
      "Train Epoche: 1 [1880/96 (1958%)]\tLoss: 0.573709\n",
      "Train Epoche: 1 [1881/96 (1959%)]\tLoss: 1.976265\n",
      "Train Epoche: 1 [1882/96 (1960%)]\tLoss: 0.034084\n",
      "Train Epoche: 1 [1883/96 (1961%)]\tLoss: 0.140285\n",
      "Train Epoche: 1 [1884/96 (1962%)]\tLoss: 12.739608\n",
      "Train Epoche: 1 [1885/96 (1964%)]\tLoss: 1.204324\n",
      "Train Epoche: 1 [1886/96 (1965%)]\tLoss: 1.293297\n",
      "Train Epoche: 1 [1887/96 (1966%)]\tLoss: 154.717453\n",
      "Train Epoche: 1 [1888/96 (1967%)]\tLoss: 4.821901\n",
      "Train Epoche: 1 [1889/96 (1968%)]\tLoss: 273.308960\n",
      "Train Epoche: 1 [1890/96 (1969%)]\tLoss: 25.040968\n",
      "Train Epoche: 1 [1891/96 (1970%)]\tLoss: 91.579582\n",
      "Train Epoche: 1 [1892/96 (1971%)]\tLoss: 87.267479\n",
      "Train Epoche: 1 [1893/96 (1972%)]\tLoss: 0.035451\n",
      "Train Epoche: 1 [1894/96 (1973%)]\tLoss: 0.195575\n",
      "Train Epoche: 1 [1895/96 (1974%)]\tLoss: 34.602345\n",
      "Train Epoche: 1 [1896/96 (1975%)]\tLoss: 68.131065\n",
      "Train Epoche: 1 [1897/96 (1976%)]\tLoss: 8.575627\n",
      "Train Epoche: 1 [1898/96 (1977%)]\tLoss: 1.509269\n",
      "Train Epoche: 1 [1899/96 (1978%)]\tLoss: 17.400858\n",
      "Train Epoche: 1 [1900/96 (1979%)]\tLoss: 93.946648\n",
      "Train Epoche: 1 [1901/96 (1980%)]\tLoss: 2.815125\n",
      "Train Epoche: 1 [1902/96 (1981%)]\tLoss: 5.089474\n",
      "Train Epoche: 1 [1903/96 (1982%)]\tLoss: 17.178120\n",
      "Train Epoche: 1 [1904/96 (1983%)]\tLoss: 18.922287\n",
      "Train Epoche: 1 [1905/96 (1984%)]\tLoss: 12.276677\n",
      "Train Epoche: 1 [1906/96 (1985%)]\tLoss: 18.974228\n",
      "Train Epoche: 1 [1907/96 (1986%)]\tLoss: 270.610168\n",
      "Train Epoche: 1 [1908/96 (1988%)]\tLoss: 0.003362\n",
      "Train Epoche: 1 [1909/96 (1989%)]\tLoss: 29.660547\n",
      "Train Epoche: 1 [1910/96 (1990%)]\tLoss: 3.471762\n",
      "Train Epoche: 1 [1911/96 (1991%)]\tLoss: 2.033648\n",
      "Train Epoche: 1 [1912/96 (1992%)]\tLoss: 277.111420\n",
      "Train Epoche: 1 [1913/96 (1993%)]\tLoss: 0.849736\n",
      "Train Epoche: 1 [1914/96 (1994%)]\tLoss: 15.379309\n",
      "Train Epoche: 1 [1915/96 (1995%)]\tLoss: 12.951051\n",
      "Train Epoche: 1 [1916/96 (1996%)]\tLoss: 15.789087\n",
      "Train Epoche: 1 [1917/96 (1997%)]\tLoss: 5.405726\n",
      "Train Epoche: 1 [1918/96 (1998%)]\tLoss: 1.469304\n",
      "Train Epoche: 1 [1919/96 (1999%)]\tLoss: 1.015421\n",
      "Train Epoche: 1 [1920/96 (2000%)]\tLoss: 6.527919\n",
      "Train Epoche: 1 [1921/96 (2001%)]\tLoss: 0.102557\n",
      "Train Epoche: 1 [1922/96 (2002%)]\tLoss: 24.656715\n",
      "Train Epoche: 1 [1923/96 (2003%)]\tLoss: 188.568512\n",
      "Train Epoche: 1 [1924/96 (2004%)]\tLoss: 5.000964\n",
      "Train Epoche: 1 [1925/96 (2005%)]\tLoss: 1.917605\n",
      "Train Epoche: 1 [1926/96 (2006%)]\tLoss: 0.469378\n",
      "Train Epoche: 1 [1927/96 (2007%)]\tLoss: 2.567055\n",
      "Train Epoche: 1 [1928/96 (2008%)]\tLoss: 10.319737\n",
      "Train Epoche: 1 [1929/96 (2009%)]\tLoss: 2.363705\n",
      "Train Epoche: 1 [1930/96 (2010%)]\tLoss: 26.243675\n",
      "Train Epoche: 1 [1931/96 (2011%)]\tLoss: 11.561894\n",
      "Train Epoche: 1 [1932/96 (2012%)]\tLoss: 1.100027\n",
      "Train Epoche: 1 [1933/96 (2014%)]\tLoss: 9.444512\n",
      "Train Epoche: 1 [1934/96 (2015%)]\tLoss: 216.697220\n",
      "Train Epoche: 1 [1935/96 (2016%)]\tLoss: 1.103238\n",
      "Train Epoche: 1 [1936/96 (2017%)]\tLoss: 230.282883\n",
      "Train Epoche: 1 [1937/96 (2018%)]\tLoss: 0.560179\n",
      "Train Epoche: 1 [1938/96 (2019%)]\tLoss: 0.605411\n",
      "Train Epoche: 1 [1939/96 (2020%)]\tLoss: 7.324090\n",
      "Train Epoche: 1 [1940/96 (2021%)]\tLoss: 22.178360\n",
      "Train Epoche: 1 [1941/96 (2022%)]\tLoss: 57.730438\n",
      "Train Epoche: 1 [1942/96 (2023%)]\tLoss: 52.169876\n",
      "Train Epoche: 1 [1943/96 (2024%)]\tLoss: 0.128344\n",
      "Train Epoche: 1 [1944/96 (2025%)]\tLoss: 13.637889\n",
      "Train Epoche: 1 [1945/96 (2026%)]\tLoss: 4.064967\n",
      "Train Epoche: 1 [1946/96 (2027%)]\tLoss: 5.005592\n",
      "Train Epoche: 1 [1947/96 (2028%)]\tLoss: 8.272531\n",
      "Train Epoche: 1 [1948/96 (2029%)]\tLoss: 1.616551\n",
      "Train Epoche: 1 [1949/96 (2030%)]\tLoss: 5.986206\n",
      "Train Epoche: 1 [1950/96 (2031%)]\tLoss: 0.009226\n",
      "Train Epoche: 1 [1951/96 (2032%)]\tLoss: 6.016592\n",
      "Train Epoche: 1 [1952/96 (2033%)]\tLoss: 6.156861\n",
      "Train Epoche: 1 [1953/96 (2034%)]\tLoss: 0.037007\n",
      "Train Epoche: 1 [1954/96 (2035%)]\tLoss: 2.134807\n",
      "Train Epoche: 1 [1955/96 (2036%)]\tLoss: 2.174940\n",
      "Train Epoche: 1 [1956/96 (2038%)]\tLoss: 1.722273\n",
      "Train Epoche: 1 [1957/96 (2039%)]\tLoss: 36.543526\n",
      "Train Epoche: 1 [1958/96 (2040%)]\tLoss: 7.243354\n",
      "Train Epoche: 1 [1959/96 (2041%)]\tLoss: 0.187570\n",
      "Train Epoche: 1 [1960/96 (2042%)]\tLoss: 3.681883\n",
      "Train Epoche: 1 [1961/96 (2043%)]\tLoss: 1.068015\n",
      "Train Epoche: 1 [1962/96 (2044%)]\tLoss: 4.944632\n",
      "Train Epoche: 1 [1963/96 (2045%)]\tLoss: 1.221167\n",
      "Train Epoche: 1 [1964/96 (2046%)]\tLoss: 33.871418\n",
      "Train Epoche: 1 [1965/96 (2047%)]\tLoss: 2.244004\n",
      "Train Epoche: 1 [1966/96 (2048%)]\tLoss: 0.632743\n",
      "Train Epoche: 1 [1967/96 (2049%)]\tLoss: 4.203502\n",
      "Train Epoche: 1 [1968/96 (2050%)]\tLoss: 1.646253\n",
      "Train Epoche: 1 [1969/96 (2051%)]\tLoss: 6.545109\n",
      "Train Epoche: 1 [1970/96 (2052%)]\tLoss: 6.845538\n",
      "Train Epoche: 1 [1971/96 (2053%)]\tLoss: 102.232941\n",
      "Train Epoche: 1 [1972/96 (2054%)]\tLoss: 8.208623\n",
      "Train Epoche: 1 [1973/96 (2055%)]\tLoss: 1.852407\n",
      "Train Epoche: 1 [1974/96 (2056%)]\tLoss: 1.118087\n",
      "Train Epoche: 1 [1975/96 (2057%)]\tLoss: 7.321163\n",
      "Train Epoche: 1 [1976/96 (2058%)]\tLoss: 16.487455\n",
      "Train Epoche: 1 [1977/96 (2059%)]\tLoss: 0.713891\n",
      "Train Epoche: 1 [1978/96 (2060%)]\tLoss: 59.501392\n",
      "Train Epoche: 1 [1979/96 (2061%)]\tLoss: 0.152315\n",
      "Train Epoche: 1 [1980/96 (2062%)]\tLoss: 0.429178\n",
      "Train Epoche: 1 [1981/96 (2064%)]\tLoss: 0.993207\n",
      "Train Epoche: 1 [1982/96 (2065%)]\tLoss: 0.488022\n",
      "Train Epoche: 1 [1983/96 (2066%)]\tLoss: 11.714215\n",
      "Train Epoche: 1 [1984/96 (2067%)]\tLoss: 0.005175\n",
      "Train Epoche: 1 [1985/96 (2068%)]\tLoss: 0.778382\n",
      "Train Epoche: 1 [1986/96 (2069%)]\tLoss: 0.068359\n",
      "Train Epoche: 1 [1987/96 (2070%)]\tLoss: 0.677800\n",
      "Train Epoche: 1 [1988/96 (2071%)]\tLoss: 4.309183\n",
      "Train Epoche: 1 [1989/96 (2072%)]\tLoss: 183.622025\n",
      "Train Epoche: 1 [1990/96 (2073%)]\tLoss: 4.429168\n",
      "Train Epoche: 1 [1991/96 (2074%)]\tLoss: 8.787889\n",
      "Train Epoche: 1 [1992/96 (2075%)]\tLoss: 13.017497\n",
      "Train Epoche: 1 [1993/96 (2076%)]\tLoss: 2.476537\n",
      "Train Epoche: 1 [1994/96 (2077%)]\tLoss: 30.068590\n",
      "Train Epoche: 1 [1995/96 (2078%)]\tLoss: 9.882308\n",
      "Train Epoche: 1 [1996/96 (2079%)]\tLoss: 4.919630\n",
      "Train Epoche: 1 [1997/96 (2080%)]\tLoss: 0.403757\n",
      "Train Epoche: 1 [1998/96 (2081%)]\tLoss: 3.260853\n",
      "Train Epoche: 1 [1999/96 (2082%)]\tLoss: 1.153113\n",
      "Train Epoche: 1 [2000/96 (2083%)]\tLoss: 0.280994\n",
      "Train Epoche: 1 [2001/96 (2084%)]\tLoss: 39.701660\n",
      "Train Epoche: 1 [2002/96 (2085%)]\tLoss: 12.783461\n",
      "Train Epoche: 1 [2003/96 (2086%)]\tLoss: 27.486355\n",
      "Train Epoche: 1 [2004/96 (2088%)]\tLoss: 1.650061\n",
      "Train Epoche: 1 [2005/96 (2089%)]\tLoss: 0.204409\n",
      "Train Epoche: 1 [2006/96 (2090%)]\tLoss: 13.183084\n",
      "Train Epoche: 1 [2007/96 (2091%)]\tLoss: 27.109888\n",
      "Train Epoche: 1 [2008/96 (2092%)]\tLoss: 84.750725\n",
      "Train Epoche: 1 [2009/96 (2093%)]\tLoss: 14.135917\n",
      "Train Epoche: 1 [2010/96 (2094%)]\tLoss: 7.500869\n",
      "Train Epoche: 1 [2011/96 (2095%)]\tLoss: 439.841827\n",
      "Train Epoche: 1 [2012/96 (2096%)]\tLoss: 30.730465\n",
      "Train Epoche: 1 [2013/96 (2097%)]\tLoss: 9.228094\n",
      "Train Epoche: 1 [2014/96 (2098%)]\tLoss: 4.728850\n",
      "Train Epoche: 1 [2015/96 (2099%)]\tLoss: 0.003306\n",
      "Train Epoche: 1 [2016/96 (2100%)]\tLoss: 4.551208\n",
      "Train Epoche: 1 [2017/96 (2101%)]\tLoss: 31.886835\n",
      "Train Epoche: 1 [2018/96 (2102%)]\tLoss: 0.000268\n",
      "Train Epoche: 1 [2019/96 (2103%)]\tLoss: 9.738154\n",
      "Train Epoche: 1 [2020/96 (2104%)]\tLoss: 3.734209\n",
      "Train Epoche: 1 [2021/96 (2105%)]\tLoss: 31.169764\n",
      "Train Epoche: 1 [2022/96 (2106%)]\tLoss: 3.454583\n",
      "Train Epoche: 1 [2023/96 (2107%)]\tLoss: 2.843254\n",
      "Train Epoche: 1 [2024/96 (2108%)]\tLoss: 2.228719\n",
      "Train Epoche: 1 [2025/96 (2109%)]\tLoss: 197.370834\n",
      "Train Epoche: 1 [2026/96 (2110%)]\tLoss: 0.692393\n",
      "Train Epoche: 1 [2027/96 (2111%)]\tLoss: 90.151711\n",
      "Train Epoche: 1 [2028/96 (2112%)]\tLoss: 25.069075\n",
      "Train Epoche: 1 [2029/96 (2114%)]\tLoss: 91.663216\n",
      "Train Epoche: 1 [2030/96 (2115%)]\tLoss: 21.682049\n",
      "Train Epoche: 1 [2031/96 (2116%)]\tLoss: 9.773781\n",
      "Train Epoche: 1 [2032/96 (2117%)]\tLoss: 1.527970\n",
      "Train Epoche: 1 [2033/96 (2118%)]\tLoss: 64.673210\n",
      "Train Epoche: 2 [0/96 (0%)]\tLoss: 50.097233\n",
      "Train Epoche: 2 [1/96 (1%)]\tLoss: 65.865356\n",
      "Train Epoche: 2 [2/96 (2%)]\tLoss: 0.027156\n",
      "Train Epoche: 2 [3/96 (3%)]\tLoss: 4.641562\n",
      "Train Epoche: 2 [4/96 (4%)]\tLoss: 3.210272\n",
      "Train Epoche: 2 [5/96 (5%)]\tLoss: 4.064417\n",
      "Train Epoche: 2 [6/96 (6%)]\tLoss: 1.364935\n",
      "Train Epoche: 2 [7/96 (7%)]\tLoss: 19.209414\n",
      "Train Epoche: 2 [8/96 (8%)]\tLoss: 7.959077\n",
      "Train Epoche: 2 [9/96 (9%)]\tLoss: 4.633154\n",
      "Train Epoche: 2 [10/96 (10%)]\tLoss: 0.185022\n",
      "Train Epoche: 2 [11/96 (11%)]\tLoss: 2.324745\n",
      "Train Epoche: 2 [12/96 (12%)]\tLoss: 165.284515\n",
      "Train Epoche: 2 [13/96 (14%)]\tLoss: 11.858839\n",
      "Train Epoche: 2 [14/96 (15%)]\tLoss: 3.702972\n",
      "Train Epoche: 2 [15/96 (16%)]\tLoss: 11.519269\n",
      "Train Epoche: 2 [16/96 (17%)]\tLoss: 4.294196\n",
      "Train Epoche: 2 [17/96 (18%)]\tLoss: 4.086622\n",
      "Train Epoche: 2 [18/96 (19%)]\tLoss: 35.483879\n",
      "Train Epoche: 2 [19/96 (20%)]\tLoss: 42.241421\n",
      "Train Epoche: 2 [20/96 (21%)]\tLoss: 9.930063\n",
      "Train Epoche: 2 [21/96 (22%)]\tLoss: 196.461319\n",
      "Train Epoche: 2 [22/96 (23%)]\tLoss: 110.872040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [23/96 (24%)]\tLoss: 73.685081\n",
      "Train Epoche: 2 [24/96 (25%)]\tLoss: 24.688078\n",
      "Train Epoche: 2 [25/96 (26%)]\tLoss: 0.923712\n",
      "Train Epoche: 2 [26/96 (27%)]\tLoss: 0.547809\n",
      "Train Epoche: 2 [27/96 (28%)]\tLoss: 7.435828\n",
      "Train Epoche: 2 [28/96 (29%)]\tLoss: 9.434873\n",
      "Train Epoche: 2 [29/96 (30%)]\tLoss: 0.383991\n",
      "Train Epoche: 2 [30/96 (31%)]\tLoss: 0.401596\n",
      "Train Epoche: 2 [31/96 (32%)]\tLoss: 8.112268\n",
      "Train Epoche: 2 [32/96 (33%)]\tLoss: 6.209188\n",
      "Train Epoche: 2 [33/96 (34%)]\tLoss: 0.127242\n",
      "Train Epoche: 2 [34/96 (35%)]\tLoss: 0.373689\n",
      "Train Epoche: 2 [35/96 (36%)]\tLoss: 1.744462\n",
      "Train Epoche: 2 [36/96 (38%)]\tLoss: 57.884476\n",
      "Train Epoche: 2 [37/96 (39%)]\tLoss: 3.880867\n",
      "Train Epoche: 2 [38/96 (40%)]\tLoss: 0.131849\n",
      "Train Epoche: 2 [39/96 (41%)]\tLoss: 24.265106\n",
      "Train Epoche: 2 [40/96 (42%)]\tLoss: 46.078369\n",
      "Train Epoche: 2 [41/96 (43%)]\tLoss: 79.382149\n",
      "Train Epoche: 2 [42/96 (44%)]\tLoss: 62.669601\n",
      "Train Epoche: 2 [43/96 (45%)]\tLoss: 8.510479\n",
      "Train Epoche: 2 [44/96 (46%)]\tLoss: 2.389890\n",
      "Train Epoche: 2 [45/96 (47%)]\tLoss: 62.439800\n",
      "Train Epoche: 2 [46/96 (48%)]\tLoss: 45.595749\n",
      "Train Epoche: 2 [47/96 (49%)]\tLoss: 23.775631\n",
      "Train Epoche: 2 [48/96 (50%)]\tLoss: 1.612884\n",
      "Train Epoche: 2 [49/96 (51%)]\tLoss: 19.260792\n",
      "Train Epoche: 2 [50/96 (52%)]\tLoss: 72.742561\n",
      "Train Epoche: 2 [51/96 (53%)]\tLoss: 2.997837\n",
      "Train Epoche: 2 [52/96 (54%)]\tLoss: 1.525202\n",
      "Train Epoche: 2 [53/96 (55%)]\tLoss: 2.870532\n",
      "Train Epoche: 2 [54/96 (56%)]\tLoss: 85.644188\n",
      "Train Epoche: 2 [55/96 (57%)]\tLoss: 14.328195\n",
      "Train Epoche: 2 [56/96 (58%)]\tLoss: 3.159217\n",
      "Train Epoche: 2 [57/96 (59%)]\tLoss: 221.404785\n",
      "Train Epoche: 2 [58/96 (60%)]\tLoss: 3.422381\n",
      "Train Epoche: 2 [59/96 (61%)]\tLoss: 4.787593\n",
      "Train Epoche: 2 [60/96 (62%)]\tLoss: 10.984313\n",
      "Train Epoche: 2 [61/96 (64%)]\tLoss: 2.408307\n",
      "Train Epoche: 2 [62/96 (65%)]\tLoss: 20.264019\n",
      "Train Epoche: 2 [63/96 (66%)]\tLoss: 64.552368\n",
      "Train Epoche: 2 [64/96 (67%)]\tLoss: 3.005583\n",
      "Train Epoche: 2 [65/96 (68%)]\tLoss: 0.964942\n",
      "Train Epoche: 2 [66/96 (69%)]\tLoss: 5.429671\n",
      "Train Epoche: 2 [67/96 (70%)]\tLoss: 0.024075\n",
      "Train Epoche: 2 [68/96 (71%)]\tLoss: 13.196332\n",
      "Train Epoche: 2 [69/96 (72%)]\tLoss: 0.468926\n",
      "Train Epoche: 2 [70/96 (73%)]\tLoss: 0.509587\n",
      "Train Epoche: 2 [71/96 (74%)]\tLoss: 16.230192\n",
      "Train Epoche: 2 [72/96 (75%)]\tLoss: 15.599138\n",
      "Train Epoche: 2 [73/96 (76%)]\tLoss: 1.615379\n",
      "Train Epoche: 2 [74/96 (77%)]\tLoss: 180.084732\n",
      "Train Epoche: 2 [75/96 (78%)]\tLoss: 11.917895\n",
      "Train Epoche: 2 [76/96 (79%)]\tLoss: 1.572145\n",
      "Train Epoche: 2 [77/96 (80%)]\tLoss: 0.817645\n",
      "Train Epoche: 2 [78/96 (81%)]\tLoss: 8.953155\n",
      "Train Epoche: 2 [79/96 (82%)]\tLoss: 0.670927\n",
      "Train Epoche: 2 [80/96 (83%)]\tLoss: 3.628944\n",
      "Train Epoche: 2 [81/96 (84%)]\tLoss: 13.377971\n",
      "Train Epoche: 2 [82/96 (85%)]\tLoss: 12.378523\n",
      "Train Epoche: 2 [83/96 (86%)]\tLoss: 6.496369\n",
      "Train Epoche: 2 [84/96 (88%)]\tLoss: 9.891355\n",
      "Train Epoche: 2 [85/96 (89%)]\tLoss: 9.493251\n",
      "Train Epoche: 2 [86/96 (90%)]\tLoss: 33.017067\n",
      "Train Epoche: 2 [87/96 (91%)]\tLoss: 13.811133\n",
      "Train Epoche: 2 [88/96 (92%)]\tLoss: 3.157137\n",
      "Train Epoche: 2 [89/96 (93%)]\tLoss: 3.306403\n",
      "Train Epoche: 2 [90/96 (94%)]\tLoss: 117.411964\n",
      "Train Epoche: 2 [91/96 (95%)]\tLoss: 2.834841\n",
      "Train Epoche: 2 [92/96 (96%)]\tLoss: 5.395239\n",
      "Train Epoche: 2 [93/96 (97%)]\tLoss: 16.052441\n",
      "Train Epoche: 2 [94/96 (98%)]\tLoss: 1.147987\n",
      "Train Epoche: 2 [95/96 (99%)]\tLoss: 20.566807\n",
      "Train Epoche: 2 [96/96 (100%)]\tLoss: 25.051712\n",
      "Train Epoche: 2 [97/96 (101%)]\tLoss: 5.914463\n",
      "Train Epoche: 2 [98/96 (102%)]\tLoss: 2.868190\n",
      "Train Epoche: 2 [99/96 (103%)]\tLoss: 5.662072\n",
      "Train Epoche: 2 [100/96 (104%)]\tLoss: 4.174937\n",
      "Train Epoche: 2 [101/96 (105%)]\tLoss: 11.895064\n",
      "Train Epoche: 2 [102/96 (106%)]\tLoss: 2.984872\n",
      "Train Epoche: 2 [103/96 (107%)]\tLoss: 0.972275\n",
      "Train Epoche: 2 [104/96 (108%)]\tLoss: 13.874410\n",
      "Train Epoche: 2 [105/96 (109%)]\tLoss: 18.117607\n",
      "Train Epoche: 2 [106/96 (110%)]\tLoss: 24.300718\n",
      "Train Epoche: 2 [107/96 (111%)]\tLoss: 23.279032\n",
      "Train Epoche: 2 [108/96 (112%)]\tLoss: 1.275494\n",
      "Train Epoche: 2 [109/96 (114%)]\tLoss: 0.910041\n",
      "Train Epoche: 2 [110/96 (115%)]\tLoss: 0.428056\n",
      "Train Epoche: 2 [111/96 (116%)]\tLoss: 9.267336\n",
      "Train Epoche: 2 [112/96 (117%)]\tLoss: 0.000494\n",
      "Train Epoche: 2 [113/96 (118%)]\tLoss: 0.093080\n",
      "Train Epoche: 2 [114/96 (119%)]\tLoss: 2.105540\n",
      "Train Epoche: 2 [115/96 (120%)]\tLoss: 140.558701\n",
      "Train Epoche: 2 [116/96 (121%)]\tLoss: 35.848732\n",
      "Train Epoche: 2 [117/96 (122%)]\tLoss: 0.000783\n",
      "Train Epoche: 2 [118/96 (123%)]\tLoss: 1.733626\n",
      "Train Epoche: 2 [119/96 (124%)]\tLoss: 25.872959\n",
      "Train Epoche: 2 [120/96 (125%)]\tLoss: 14.685589\n",
      "Train Epoche: 2 [121/96 (126%)]\tLoss: 3.339688\n",
      "Train Epoche: 2 [122/96 (127%)]\tLoss: 10.352176\n",
      "Train Epoche: 2 [123/96 (128%)]\tLoss: 0.380137\n",
      "Train Epoche: 2 [124/96 (129%)]\tLoss: 0.199324\n",
      "Train Epoche: 2 [125/96 (130%)]\tLoss: 24.572159\n",
      "Train Epoche: 2 [126/96 (131%)]\tLoss: 16.257513\n",
      "Train Epoche: 2 [127/96 (132%)]\tLoss: 1.852513\n",
      "Train Epoche: 2 [128/96 (133%)]\tLoss: 0.027835\n",
      "Train Epoche: 2 [129/96 (134%)]\tLoss: 7.408272\n",
      "Train Epoche: 2 [130/96 (135%)]\tLoss: 87.163414\n",
      "Train Epoche: 2 [131/96 (136%)]\tLoss: 3.577219\n",
      "Train Epoche: 2 [132/96 (138%)]\tLoss: 61.190556\n",
      "Train Epoche: 2 [133/96 (139%)]\tLoss: 1.074325\n",
      "Train Epoche: 2 [134/96 (140%)]\tLoss: 1.387294\n",
      "Train Epoche: 2 [135/96 (141%)]\tLoss: 19.863148\n",
      "Train Epoche: 2 [136/96 (142%)]\tLoss: 0.117103\n",
      "Train Epoche: 2 [137/96 (143%)]\tLoss: 197.674957\n",
      "Train Epoche: 2 [138/96 (144%)]\tLoss: 1.203719\n",
      "Train Epoche: 2 [139/96 (145%)]\tLoss: 1.245373\n",
      "Train Epoche: 2 [140/96 (146%)]\tLoss: 0.010637\n",
      "Train Epoche: 2 [141/96 (147%)]\tLoss: 0.650676\n",
      "Train Epoche: 2 [142/96 (148%)]\tLoss: 6.117982\n",
      "Train Epoche: 2 [143/96 (149%)]\tLoss: 10.288916\n",
      "Train Epoche: 2 [144/96 (150%)]\tLoss: 0.812380\n",
      "Train Epoche: 2 [145/96 (151%)]\tLoss: 78.482597\n",
      "Train Epoche: 2 [146/96 (152%)]\tLoss: 40.070408\n",
      "Train Epoche: 2 [147/96 (153%)]\tLoss: 5.236035\n",
      "Train Epoche: 2 [148/96 (154%)]\tLoss: 48.871098\n",
      "Train Epoche: 2 [149/96 (155%)]\tLoss: 0.008090\n",
      "Train Epoche: 2 [150/96 (156%)]\tLoss: 1.176892\n",
      "Train Epoche: 2 [151/96 (157%)]\tLoss: 0.101579\n",
      "Train Epoche: 2 [152/96 (158%)]\tLoss: 1.263756\n",
      "Train Epoche: 2 [153/96 (159%)]\tLoss: 0.414144\n",
      "Train Epoche: 2 [154/96 (160%)]\tLoss: 1.004178\n",
      "Train Epoche: 2 [155/96 (161%)]\tLoss: 0.749094\n",
      "Train Epoche: 2 [156/96 (162%)]\tLoss: 0.336209\n",
      "Train Epoche: 2 [157/96 (164%)]\tLoss: 0.404534\n",
      "Train Epoche: 2 [158/96 (165%)]\tLoss: 2.212163\n",
      "Train Epoche: 2 [159/96 (166%)]\tLoss: 36.727448\n",
      "Train Epoche: 2 [160/96 (167%)]\tLoss: 3.290385\n",
      "Train Epoche: 2 [161/96 (168%)]\tLoss: 28.930807\n",
      "Train Epoche: 2 [162/96 (169%)]\tLoss: 1.200950\n",
      "Train Epoche: 2 [163/96 (170%)]\tLoss: 38.661633\n",
      "Train Epoche: 2 [164/96 (171%)]\tLoss: 178.088547\n",
      "Train Epoche: 2 [165/96 (172%)]\tLoss: 4.457715\n",
      "Train Epoche: 2 [166/96 (173%)]\tLoss: 5.429975\n",
      "Train Epoche: 2 [167/96 (174%)]\tLoss: 2.203574\n",
      "Train Epoche: 2 [168/96 (175%)]\tLoss: 0.390637\n",
      "Train Epoche: 2 [169/96 (176%)]\tLoss: 2.672271\n",
      "Train Epoche: 2 [170/96 (177%)]\tLoss: 23.265596\n",
      "Train Epoche: 2 [171/96 (178%)]\tLoss: 3.671438\n",
      "Train Epoche: 2 [172/96 (179%)]\tLoss: 0.000000\n",
      "Train Epoche: 2 [173/96 (180%)]\tLoss: 0.234121\n",
      "Train Epoche: 2 [174/96 (181%)]\tLoss: 52.495483\n",
      "Train Epoche: 2 [175/96 (182%)]\tLoss: 0.335020\n",
      "Train Epoche: 2 [176/96 (183%)]\tLoss: 9.816534\n",
      "Train Epoche: 2 [177/96 (184%)]\tLoss: 0.781444\n",
      "Train Epoche: 2 [178/96 (185%)]\tLoss: 4.218070\n",
      "Train Epoche: 2 [179/96 (186%)]\tLoss: 0.354580\n",
      "Train Epoche: 2 [180/96 (188%)]\tLoss: 7.161192\n",
      "Train Epoche: 2 [181/96 (189%)]\tLoss: 7.823822\n",
      "Train Epoche: 2 [182/96 (190%)]\tLoss: 7.347838\n",
      "Train Epoche: 2 [183/96 (191%)]\tLoss: 0.000119\n",
      "Train Epoche: 2 [184/96 (192%)]\tLoss: 280.800507\n",
      "Train Epoche: 2 [185/96 (193%)]\tLoss: 4.818995\n",
      "Train Epoche: 2 [186/96 (194%)]\tLoss: 19.235487\n",
      "Train Epoche: 2 [187/96 (195%)]\tLoss: 55.793537\n",
      "Train Epoche: 2 [188/96 (196%)]\tLoss: 44.980995\n",
      "Train Epoche: 2 [189/96 (197%)]\tLoss: 4.603752\n",
      "Train Epoche: 2 [190/96 (198%)]\tLoss: 64.681305\n",
      "Train Epoche: 2 [191/96 (199%)]\tLoss: 28.108353\n",
      "Train Epoche: 2 [192/96 (200%)]\tLoss: 6.904817\n",
      "Train Epoche: 2 [193/96 (201%)]\tLoss: 42.139706\n",
      "Train Epoche: 2 [194/96 (202%)]\tLoss: 4.501454\n",
      "Train Epoche: 2 [195/96 (203%)]\tLoss: 1.180027\n",
      "Train Epoche: 2 [196/96 (204%)]\tLoss: 120.490532\n",
      "Train Epoche: 2 [197/96 (205%)]\tLoss: 3.163862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [198/96 (206%)]\tLoss: 3.571730\n",
      "Train Epoche: 2 [199/96 (207%)]\tLoss: 0.185677\n",
      "Train Epoche: 2 [200/96 (208%)]\tLoss: 17.978054\n",
      "Train Epoche: 2 [201/96 (209%)]\tLoss: 0.051651\n",
      "Train Epoche: 2 [202/96 (210%)]\tLoss: 5.723492\n",
      "Train Epoche: 2 [203/96 (211%)]\tLoss: 1.736719\n",
      "Train Epoche: 2 [204/96 (212%)]\tLoss: 0.002698\n",
      "Train Epoche: 2 [205/96 (214%)]\tLoss: 375.288208\n",
      "Train Epoche: 2 [206/96 (215%)]\tLoss: 2.927131\n",
      "Train Epoche: 2 [207/96 (216%)]\tLoss: 60.523827\n",
      "Train Epoche: 2 [208/96 (217%)]\tLoss: 50.428913\n",
      "Train Epoche: 2 [209/96 (218%)]\tLoss: 66.546135\n",
      "Train Epoche: 2 [210/96 (219%)]\tLoss: 107.077049\n",
      "Train Epoche: 2 [211/96 (220%)]\tLoss: 0.074574\n",
      "Train Epoche: 2 [212/96 (221%)]\tLoss: 2.597116\n",
      "Train Epoche: 2 [213/96 (222%)]\tLoss: 110.760963\n",
      "Train Epoche: 2 [214/96 (223%)]\tLoss: 28.876806\n",
      "Train Epoche: 2 [215/96 (224%)]\tLoss: 96.788315\n",
      "Train Epoche: 2 [216/96 (225%)]\tLoss: 13.493664\n",
      "Train Epoche: 2 [217/96 (226%)]\tLoss: 62.857079\n",
      "Train Epoche: 2 [218/96 (227%)]\tLoss: 40.164192\n",
      "Train Epoche: 2 [219/96 (228%)]\tLoss: 0.395829\n",
      "Train Epoche: 2 [220/96 (229%)]\tLoss: 12.453526\n",
      "Train Epoche: 2 [221/96 (230%)]\tLoss: 23.283300\n",
      "Train Epoche: 2 [222/96 (231%)]\tLoss: 11.994855\n",
      "Train Epoche: 2 [223/96 (232%)]\tLoss: 60.486809\n",
      "Train Epoche: 2 [224/96 (233%)]\tLoss: 69.856369\n",
      "Train Epoche: 2 [225/96 (234%)]\tLoss: 33.293060\n",
      "Train Epoche: 2 [226/96 (235%)]\tLoss: 79.744804\n",
      "Train Epoche: 2 [227/96 (236%)]\tLoss: 23.862911\n",
      "Train Epoche: 2 [228/96 (238%)]\tLoss: 119.503769\n",
      "Train Epoche: 2 [229/96 (239%)]\tLoss: 98.773888\n",
      "Train Epoche: 2 [230/96 (240%)]\tLoss: 73.479385\n",
      "Train Epoche: 2 [231/96 (241%)]\tLoss: 0.295820\n",
      "Train Epoche: 2 [232/96 (242%)]\tLoss: 1.093140\n",
      "Train Epoche: 2 [233/96 (243%)]\tLoss: 23.997063\n",
      "Train Epoche: 2 [234/96 (244%)]\tLoss: 18.595533\n",
      "Train Epoche: 2 [235/96 (245%)]\tLoss: 5.446533\n",
      "Train Epoche: 2 [236/96 (246%)]\tLoss: 20.850508\n",
      "Train Epoche: 2 [237/96 (247%)]\tLoss: 211.026840\n",
      "Train Epoche: 2 [238/96 (248%)]\tLoss: 17.499880\n",
      "Train Epoche: 2 [239/96 (249%)]\tLoss: 0.235533\n",
      "Train Epoche: 2 [240/96 (250%)]\tLoss: 14.649115\n",
      "Train Epoche: 2 [241/96 (251%)]\tLoss: 3.606705\n",
      "Train Epoche: 2 [242/96 (252%)]\tLoss: 19.281073\n",
      "Train Epoche: 2 [243/96 (253%)]\tLoss: 4.242013\n",
      "Train Epoche: 2 [244/96 (254%)]\tLoss: 3.084940\n",
      "Train Epoche: 2 [245/96 (255%)]\tLoss: 9.643646\n",
      "Train Epoche: 2 [246/96 (256%)]\tLoss: 4.816048\n",
      "Train Epoche: 2 [247/96 (257%)]\tLoss: 0.125778\n",
      "Train Epoche: 2 [248/96 (258%)]\tLoss: 0.355992\n",
      "Train Epoche: 2 [249/96 (259%)]\tLoss: 0.662290\n",
      "Train Epoche: 2 [250/96 (260%)]\tLoss: 9.991717\n",
      "Train Epoche: 2 [251/96 (261%)]\tLoss: 7.730493\n",
      "Train Epoche: 2 [252/96 (262%)]\tLoss: 5.132080\n",
      "Train Epoche: 2 [253/96 (264%)]\tLoss: 0.978612\n",
      "Train Epoche: 2 [254/96 (265%)]\tLoss: 3.766518\n",
      "Train Epoche: 2 [255/96 (266%)]\tLoss: 54.983059\n",
      "Train Epoche: 2 [256/96 (267%)]\tLoss: 1.408506\n",
      "Train Epoche: 2 [257/96 (268%)]\tLoss: 0.000442\n",
      "Train Epoche: 2 [258/96 (269%)]\tLoss: 70.105202\n",
      "Train Epoche: 2 [259/96 (270%)]\tLoss: 145.602203\n",
      "Train Epoche: 2 [260/96 (271%)]\tLoss: 1.602101\n",
      "Train Epoche: 2 [261/96 (272%)]\tLoss: 3.738619\n",
      "Train Epoche: 2 [262/96 (273%)]\tLoss: 6.179988\n",
      "Train Epoche: 2 [263/96 (274%)]\tLoss: 45.676434\n",
      "Train Epoche: 2 [264/96 (275%)]\tLoss: 0.671295\n",
      "Train Epoche: 2 [265/96 (276%)]\tLoss: 7.516711\n",
      "Train Epoche: 2 [266/96 (277%)]\tLoss: 3.428658\n",
      "Train Epoche: 2 [267/96 (278%)]\tLoss: 4.101958\n",
      "Train Epoche: 2 [268/96 (279%)]\tLoss: 16.845518\n",
      "Train Epoche: 2 [269/96 (280%)]\tLoss: 8.882057\n",
      "Train Epoche: 2 [270/96 (281%)]\tLoss: 13.424791\n",
      "Train Epoche: 2 [271/96 (282%)]\tLoss: 4.224502\n",
      "Train Epoche: 2 [272/96 (283%)]\tLoss: 61.774784\n",
      "Train Epoche: 2 [273/96 (284%)]\tLoss: 88.358917\n",
      "Train Epoche: 2 [274/96 (285%)]\tLoss: 3.022331\n",
      "Train Epoche: 2 [275/96 (286%)]\tLoss: 17.943119\n",
      "Train Epoche: 2 [276/96 (288%)]\tLoss: 17.234213\n",
      "Train Epoche: 2 [277/96 (289%)]\tLoss: 0.267278\n",
      "Train Epoche: 2 [278/96 (290%)]\tLoss: 32.115189\n",
      "Train Epoche: 2 [279/96 (291%)]\tLoss: 0.473409\n",
      "Train Epoche: 2 [280/96 (292%)]\tLoss: 79.332130\n",
      "Train Epoche: 2 [281/96 (293%)]\tLoss: 8.457662\n",
      "Train Epoche: 2 [282/96 (294%)]\tLoss: 0.230619\n",
      "Train Epoche: 2 [283/96 (295%)]\tLoss: 4.862701\n",
      "Train Epoche: 2 [284/96 (296%)]\tLoss: 24.435236\n",
      "Train Epoche: 2 [285/96 (297%)]\tLoss: 6.475957\n",
      "Train Epoche: 2 [286/96 (298%)]\tLoss: 3.360940\n",
      "Train Epoche: 2 [287/96 (299%)]\tLoss: 0.090060\n",
      "Train Epoche: 2 [288/96 (300%)]\tLoss: 4.339104\n",
      "Train Epoche: 2 [289/96 (301%)]\tLoss: 12.608995\n",
      "Train Epoche: 2 [290/96 (302%)]\tLoss: 22.350334\n",
      "Train Epoche: 2 [291/96 (303%)]\tLoss: 345.398071\n",
      "Train Epoche: 2 [292/96 (304%)]\tLoss: 166.222214\n",
      "Train Epoche: 2 [293/96 (305%)]\tLoss: 38.619587\n",
      "Train Epoche: 2 [294/96 (306%)]\tLoss: 46.040184\n",
      "Train Epoche: 2 [295/96 (307%)]\tLoss: 33.285820\n",
      "Train Epoche: 2 [296/96 (308%)]\tLoss: 90.781380\n",
      "Train Epoche: 2 [297/96 (309%)]\tLoss: 1.917967\n",
      "Train Epoche: 2 [298/96 (310%)]\tLoss: 7.415563\n",
      "Train Epoche: 2 [299/96 (311%)]\tLoss: 0.119195\n",
      "Train Epoche: 2 [300/96 (312%)]\tLoss: 28.218805\n",
      "Train Epoche: 2 [301/96 (314%)]\tLoss: 2.178592\n",
      "Train Epoche: 2 [302/96 (315%)]\tLoss: 36.779675\n",
      "Train Epoche: 2 [303/96 (316%)]\tLoss: 34.327625\n",
      "Train Epoche: 2 [304/96 (317%)]\tLoss: 5.559639\n",
      "Train Epoche: 2 [305/96 (318%)]\tLoss: 15.684772\n",
      "Train Epoche: 2 [306/96 (319%)]\tLoss: 14.111881\n",
      "Train Epoche: 2 [307/96 (320%)]\tLoss: 7.203405\n",
      "Train Epoche: 2 [308/96 (321%)]\tLoss: 5.686221\n",
      "Train Epoche: 2 [309/96 (322%)]\tLoss: 32.148651\n",
      "Train Epoche: 2 [310/96 (323%)]\tLoss: 5.635059\n",
      "Train Epoche: 2 [311/96 (324%)]\tLoss: 18.619600\n",
      "Train Epoche: 2 [312/96 (325%)]\tLoss: 19.310974\n",
      "Train Epoche: 2 [313/96 (326%)]\tLoss: 121.881706\n",
      "Train Epoche: 2 [314/96 (327%)]\tLoss: 249.434738\n",
      "Train Epoche: 2 [315/96 (328%)]\tLoss: 61.907887\n",
      "Train Epoche: 2 [316/96 (329%)]\tLoss: 106.574722\n",
      "Train Epoche: 2 [317/96 (330%)]\tLoss: 12.456986\n",
      "Train Epoche: 2 [318/96 (331%)]\tLoss: 47.113335\n",
      "Train Epoche: 2 [319/96 (332%)]\tLoss: 1.573128\n",
      "Train Epoche: 2 [320/96 (333%)]\tLoss: 2.199293\n",
      "Train Epoche: 2 [321/96 (334%)]\tLoss: 5.871624\n",
      "Train Epoche: 2 [322/96 (335%)]\tLoss: 16.805311\n",
      "Train Epoche: 2 [323/96 (336%)]\tLoss: 75.578125\n",
      "Train Epoche: 2 [324/96 (338%)]\tLoss: 50.098782\n",
      "Train Epoche: 2 [325/96 (339%)]\tLoss: 191.321259\n",
      "Train Epoche: 2 [326/96 (340%)]\tLoss: 54.974030\n",
      "Train Epoche: 2 [327/96 (341%)]\tLoss: 3.291275\n",
      "Train Epoche: 2 [328/96 (342%)]\tLoss: 5.575063\n",
      "Train Epoche: 2 [329/96 (343%)]\tLoss: 25.750967\n",
      "Train Epoche: 2 [330/96 (344%)]\tLoss: 2.432293\n",
      "Train Epoche: 2 [331/96 (345%)]\tLoss: 0.393455\n",
      "Train Epoche: 2 [332/96 (346%)]\tLoss: 87.170731\n",
      "Train Epoche: 2 [333/96 (347%)]\tLoss: 46.982403\n",
      "Train Epoche: 2 [334/96 (348%)]\tLoss: 0.312778\n",
      "Train Epoche: 2 [335/96 (349%)]\tLoss: 8.955990\n",
      "Train Epoche: 2 [336/96 (350%)]\tLoss: 58.291241\n",
      "Train Epoche: 2 [337/96 (351%)]\tLoss: 30.811283\n",
      "Train Epoche: 2 [338/96 (352%)]\tLoss: 1.041847\n",
      "Train Epoche: 2 [339/96 (353%)]\tLoss: 18.703325\n",
      "Train Epoche: 2 [340/96 (354%)]\tLoss: 9.925736\n",
      "Train Epoche: 2 [341/96 (355%)]\tLoss: 1.900884\n",
      "Train Epoche: 2 [342/96 (356%)]\tLoss: 96.415260\n",
      "Train Epoche: 2 [343/96 (357%)]\tLoss: 0.961508\n",
      "Train Epoche: 2 [344/96 (358%)]\tLoss: 2.860784\n",
      "Train Epoche: 2 [345/96 (359%)]\tLoss: 0.339875\n",
      "Train Epoche: 2 [346/96 (360%)]\tLoss: 1.071671\n",
      "Train Epoche: 2 [347/96 (361%)]\tLoss: 4.101114\n",
      "Train Epoche: 2 [348/96 (362%)]\tLoss: 0.510265\n",
      "Train Epoche: 2 [349/96 (364%)]\tLoss: 7.423073\n",
      "Train Epoche: 2 [350/96 (365%)]\tLoss: 0.277840\n",
      "Train Epoche: 2 [351/96 (366%)]\tLoss: 9.796817\n",
      "Train Epoche: 2 [352/96 (367%)]\tLoss: 2.806581\n",
      "Train Epoche: 2 [353/96 (368%)]\tLoss: 390.907135\n",
      "Train Epoche: 2 [354/96 (369%)]\tLoss: 0.237214\n",
      "Train Epoche: 2 [355/96 (370%)]\tLoss: 29.071650\n",
      "Train Epoche: 2 [356/96 (371%)]\tLoss: 5.023078\n",
      "Train Epoche: 2 [357/96 (372%)]\tLoss: 123.763649\n",
      "Train Epoche: 2 [358/96 (373%)]\tLoss: 15.730045\n",
      "Train Epoche: 2 [359/96 (374%)]\tLoss: 6.382977\n",
      "Train Epoche: 2 [360/96 (375%)]\tLoss: 13.704434\n",
      "Train Epoche: 2 [361/96 (376%)]\tLoss: 1.103570\n",
      "Train Epoche: 2 [362/96 (377%)]\tLoss: 1.762103\n",
      "Train Epoche: 2 [363/96 (378%)]\tLoss: 0.112699\n",
      "Train Epoche: 2 [364/96 (379%)]\tLoss: 11.581005\n",
      "Train Epoche: 2 [365/96 (380%)]\tLoss: 0.001821\n",
      "Train Epoche: 2 [366/96 (381%)]\tLoss: 0.077868\n",
      "Train Epoche: 2 [367/96 (382%)]\tLoss: 86.721626\n",
      "Train Epoche: 2 [368/96 (383%)]\tLoss: 13.348875\n",
      "Train Epoche: 2 [369/96 (384%)]\tLoss: 0.128867\n",
      "Train Epoche: 2 [370/96 (385%)]\tLoss: 0.525673\n",
      "Train Epoche: 2 [371/96 (386%)]\tLoss: 13.565125\n",
      "Train Epoche: 2 [372/96 (388%)]\tLoss: 6.341374\n",
      "Train Epoche: 2 [373/96 (389%)]\tLoss: 16.376432\n",
      "Train Epoche: 2 [374/96 (390%)]\tLoss: 39.796032\n",
      "Train Epoche: 2 [375/96 (391%)]\tLoss: 0.456157\n",
      "Train Epoche: 2 [376/96 (392%)]\tLoss: 18.545134\n",
      "Train Epoche: 2 [377/96 (393%)]\tLoss: 1.952473\n",
      "Train Epoche: 2 [378/96 (394%)]\tLoss: 5.626356\n",
      "Train Epoche: 2 [379/96 (395%)]\tLoss: 29.052752\n",
      "Train Epoche: 2 [380/96 (396%)]\tLoss: 0.004738\n",
      "Train Epoche: 2 [381/96 (397%)]\tLoss: 1.818192\n",
      "Train Epoche: 2 [382/96 (398%)]\tLoss: 0.412477\n",
      "Train Epoche: 2 [383/96 (399%)]\tLoss: 44.962509\n",
      "Train Epoche: 2 [384/96 (400%)]\tLoss: 26.335604\n",
      "Train Epoche: 2 [385/96 (401%)]\tLoss: 1.080382\n",
      "Train Epoche: 2 [386/96 (402%)]\tLoss: 0.407954\n",
      "Train Epoche: 2 [387/96 (403%)]\tLoss: 103.923920\n",
      "Train Epoche: 2 [388/96 (404%)]\tLoss: 2.207128\n",
      "Train Epoche: 2 [389/96 (405%)]\tLoss: 0.062153\n",
      "Train Epoche: 2 [390/96 (406%)]\tLoss: 1.356402\n",
      "Train Epoche: 2 [391/96 (407%)]\tLoss: 8.922371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [392/96 (408%)]\tLoss: 42.426842\n",
      "Train Epoche: 2 [393/96 (409%)]\tLoss: 35.897018\n",
      "Train Epoche: 2 [394/96 (410%)]\tLoss: 311.830719\n",
      "Train Epoche: 2 [395/96 (411%)]\tLoss: 89.127853\n",
      "Train Epoche: 2 [396/96 (412%)]\tLoss: 100.784271\n",
      "Train Epoche: 2 [397/96 (414%)]\tLoss: 14.398609\n",
      "Train Epoche: 2 [398/96 (415%)]\tLoss: 162.129791\n",
      "Train Epoche: 2 [399/96 (416%)]\tLoss: 4.502567\n",
      "Train Epoche: 2 [400/96 (417%)]\tLoss: 25.417723\n",
      "Train Epoche: 2 [401/96 (418%)]\tLoss: 8.608876\n",
      "Train Epoche: 2 [402/96 (419%)]\tLoss: 71.705284\n",
      "Train Epoche: 2 [403/96 (420%)]\tLoss: 19.868408\n",
      "Train Epoche: 2 [404/96 (421%)]\tLoss: 6.865979\n",
      "Train Epoche: 2 [405/96 (422%)]\tLoss: 0.002367\n",
      "Train Epoche: 2 [406/96 (423%)]\tLoss: 0.035422\n",
      "Train Epoche: 2 [407/96 (424%)]\tLoss: 5.433507\n",
      "Train Epoche: 2 [408/96 (425%)]\tLoss: 17.149183\n",
      "Train Epoche: 2 [409/96 (426%)]\tLoss: 176.085953\n",
      "Train Epoche: 2 [410/96 (427%)]\tLoss: 117.815987\n",
      "Train Epoche: 2 [411/96 (428%)]\tLoss: 6.259306\n",
      "Train Epoche: 2 [412/96 (429%)]\tLoss: 12.210722\n",
      "Train Epoche: 2 [413/96 (430%)]\tLoss: 1.053244\n",
      "Train Epoche: 2 [414/96 (431%)]\tLoss: 0.006665\n",
      "Train Epoche: 2 [415/96 (432%)]\tLoss: 110.310371\n",
      "Train Epoche: 2 [416/96 (433%)]\tLoss: 52.466228\n",
      "Train Epoche: 2 [417/96 (434%)]\tLoss: 9.578230\n",
      "Train Epoche: 2 [418/96 (435%)]\tLoss: 64.040840\n",
      "Train Epoche: 2 [419/96 (436%)]\tLoss: 0.006209\n",
      "Train Epoche: 2 [420/96 (438%)]\tLoss: 7.877735\n",
      "Train Epoche: 2 [421/96 (439%)]\tLoss: 5.228819\n",
      "Train Epoche: 2 [422/96 (440%)]\tLoss: 17.725241\n",
      "Train Epoche: 2 [423/96 (441%)]\tLoss: 0.736855\n",
      "Train Epoche: 2 [424/96 (442%)]\tLoss: 0.227840\n",
      "Train Epoche: 2 [425/96 (443%)]\tLoss: 12.485081\n",
      "Train Epoche: 2 [426/96 (444%)]\tLoss: 1.157142\n",
      "Train Epoche: 2 [427/96 (445%)]\tLoss: 30.187677\n",
      "Train Epoche: 2 [428/96 (446%)]\tLoss: 0.290818\n",
      "Train Epoche: 2 [429/96 (447%)]\tLoss: 4.514257\n",
      "Train Epoche: 2 [430/96 (448%)]\tLoss: 1.628322\n",
      "Train Epoche: 2 [431/96 (449%)]\tLoss: 20.675442\n",
      "Train Epoche: 2 [432/96 (450%)]\tLoss: 230.690979\n",
      "Train Epoche: 2 [433/96 (451%)]\tLoss: 6.182106\n",
      "Train Epoche: 2 [434/96 (452%)]\tLoss: 0.108409\n",
      "Train Epoche: 2 [435/96 (453%)]\tLoss: 22.024015\n",
      "Train Epoche: 2 [436/96 (454%)]\tLoss: 0.248954\n",
      "Train Epoche: 2 [437/96 (455%)]\tLoss: 1.372610\n",
      "Train Epoche: 2 [438/96 (456%)]\tLoss: 1.623733\n",
      "Train Epoche: 2 [439/96 (457%)]\tLoss: 16.975035\n",
      "Train Epoche: 2 [440/96 (458%)]\tLoss: 31.457342\n",
      "Train Epoche: 2 [441/96 (459%)]\tLoss: 179.604034\n",
      "Train Epoche: 2 [442/96 (460%)]\tLoss: 7.344560\n",
      "Train Epoche: 2 [443/96 (461%)]\tLoss: 4.908905\n",
      "Train Epoche: 2 [444/96 (462%)]\tLoss: 3.209657\n",
      "Train Epoche: 2 [445/96 (464%)]\tLoss: 0.328200\n",
      "Train Epoche: 2 [446/96 (465%)]\tLoss: 9.435646\n",
      "Train Epoche: 2 [447/96 (466%)]\tLoss: 24.260033\n",
      "Train Epoche: 2 [448/96 (467%)]\tLoss: 1.601086\n",
      "Train Epoche: 2 [449/96 (468%)]\tLoss: 2.954852\n",
      "Train Epoche: 2 [450/96 (469%)]\tLoss: 20.190289\n",
      "Train Epoche: 2 [451/96 (470%)]\tLoss: 16.690756\n",
      "Train Epoche: 2 [452/96 (471%)]\tLoss: 0.038387\n",
      "Train Epoche: 2 [453/96 (472%)]\tLoss: 0.970304\n",
      "Train Epoche: 2 [454/96 (473%)]\tLoss: 1.508061\n",
      "Train Epoche: 2 [455/96 (474%)]\tLoss: 0.382209\n",
      "Train Epoche: 2 [456/96 (475%)]\tLoss: 0.377005\n",
      "Train Epoche: 2 [457/96 (476%)]\tLoss: 0.398059\n",
      "Train Epoche: 2 [458/96 (477%)]\tLoss: 0.607451\n",
      "Train Epoche: 2 [459/96 (478%)]\tLoss: 18.138193\n",
      "Train Epoche: 2 [460/96 (479%)]\tLoss: 4.185373\n",
      "Train Epoche: 2 [461/96 (480%)]\tLoss: 19.726797\n",
      "Train Epoche: 2 [462/96 (481%)]\tLoss: 2.798040\n",
      "Train Epoche: 2 [463/96 (482%)]\tLoss: 38.500484\n",
      "Train Epoche: 2 [464/96 (483%)]\tLoss: 12.543588\n",
      "Train Epoche: 2 [465/96 (484%)]\tLoss: 0.151658\n",
      "Train Epoche: 2 [466/96 (485%)]\tLoss: 76.727142\n",
      "Train Epoche: 2 [467/96 (486%)]\tLoss: 117.908791\n",
      "Train Epoche: 2 [468/96 (488%)]\tLoss: 167.452271\n",
      "Train Epoche: 2 [469/96 (489%)]\tLoss: 8.886739\n",
      "Train Epoche: 2 [470/96 (490%)]\tLoss: 1.924145\n",
      "Train Epoche: 2 [471/96 (491%)]\tLoss: 36.937019\n",
      "Train Epoche: 2 [472/96 (492%)]\tLoss: 0.978699\n",
      "Train Epoche: 2 [473/96 (493%)]\tLoss: 7.980937\n",
      "Train Epoche: 2 [474/96 (494%)]\tLoss: 11.165376\n",
      "Train Epoche: 2 [475/96 (495%)]\tLoss: 0.003538\n",
      "Train Epoche: 2 [476/96 (496%)]\tLoss: 4.460640\n",
      "Train Epoche: 2 [477/96 (497%)]\tLoss: 0.306197\n",
      "Train Epoche: 2 [478/96 (498%)]\tLoss: 35.257576\n",
      "Train Epoche: 2 [479/96 (499%)]\tLoss: 2.035948\n",
      "Train Epoche: 2 [480/96 (500%)]\tLoss: 49.752914\n",
      "Train Epoche: 2 [481/96 (501%)]\tLoss: 4.807580\n",
      "Train Epoche: 2 [482/96 (502%)]\tLoss: 3.328547\n",
      "Train Epoche: 2 [483/96 (503%)]\tLoss: 9.395813\n",
      "Train Epoche: 2 [484/96 (504%)]\tLoss: 39.708401\n",
      "Train Epoche: 2 [485/96 (505%)]\tLoss: 0.995654\n",
      "Train Epoche: 2 [486/96 (506%)]\tLoss: 0.003408\n",
      "Train Epoche: 2 [487/96 (507%)]\tLoss: 1.573376\n",
      "Train Epoche: 2 [488/96 (508%)]\tLoss: 17.387501\n",
      "Train Epoche: 2 [489/96 (509%)]\tLoss: 6.530132\n",
      "Train Epoche: 2 [490/96 (510%)]\tLoss: 5.783005\n",
      "Train Epoche: 2 [491/96 (511%)]\tLoss: 0.862929\n",
      "Train Epoche: 2 [492/96 (512%)]\tLoss: 29.152138\n",
      "Train Epoche: 2 [493/96 (514%)]\tLoss: 28.239681\n",
      "Train Epoche: 2 [494/96 (515%)]\tLoss: 22.648996\n",
      "Train Epoche: 2 [495/96 (516%)]\tLoss: 0.689749\n",
      "Train Epoche: 2 [496/96 (517%)]\tLoss: 4.739030\n",
      "Train Epoche: 2 [497/96 (518%)]\tLoss: 2.955006\n",
      "Train Epoche: 2 [498/96 (519%)]\tLoss: 3.771380\n",
      "Train Epoche: 2 [499/96 (520%)]\tLoss: 325.445221\n",
      "Train Epoche: 2 [500/96 (521%)]\tLoss: 51.826469\n",
      "Train Epoche: 2 [501/96 (522%)]\tLoss: 11.253488\n",
      "Train Epoche: 2 [502/96 (523%)]\tLoss: 18.080063\n",
      "Train Epoche: 2 [503/96 (524%)]\tLoss: 4.110119\n",
      "Train Epoche: 2 [504/96 (525%)]\tLoss: 20.097475\n",
      "Train Epoche: 2 [505/96 (526%)]\tLoss: 63.581131\n",
      "Train Epoche: 2 [506/96 (527%)]\tLoss: 41.107906\n",
      "Train Epoche: 2 [507/96 (528%)]\tLoss: 34.656063\n",
      "Train Epoche: 2 [508/96 (529%)]\tLoss: 55.505096\n",
      "Train Epoche: 2 [509/96 (530%)]\tLoss: 8.013002\n",
      "Train Epoche: 2 [510/96 (531%)]\tLoss: 4.557428\n",
      "Train Epoche: 2 [511/96 (532%)]\tLoss: 450.957825\n",
      "Train Epoche: 2 [512/96 (533%)]\tLoss: 6.117755\n",
      "Train Epoche: 2 [513/96 (534%)]\tLoss: 0.224572\n",
      "Train Epoche: 2 [514/96 (535%)]\tLoss: 15.859107\n",
      "Train Epoche: 2 [515/96 (536%)]\tLoss: 107.768677\n",
      "Train Epoche: 2 [516/96 (538%)]\tLoss: 17.081171\n",
      "Train Epoche: 2 [517/96 (539%)]\tLoss: 0.982144\n",
      "Train Epoche: 2 [518/96 (540%)]\tLoss: 63.992447\n",
      "Train Epoche: 2 [519/96 (541%)]\tLoss: 5.716447\n",
      "Train Epoche: 2 [520/96 (542%)]\tLoss: 97.636658\n",
      "Train Epoche: 2 [521/96 (543%)]\tLoss: 350.120300\n",
      "Train Epoche: 2 [522/96 (544%)]\tLoss: 2.021639\n",
      "Train Epoche: 2 [523/96 (545%)]\tLoss: 16.697460\n",
      "Train Epoche: 2 [524/96 (546%)]\tLoss: 2.295199\n",
      "Train Epoche: 2 [525/96 (547%)]\tLoss: 7.059156\n",
      "Train Epoche: 2 [526/96 (548%)]\tLoss: 9.096047\n",
      "Train Epoche: 2 [527/96 (549%)]\tLoss: 55.019966\n",
      "Train Epoche: 2 [528/96 (550%)]\tLoss: 0.040253\n",
      "Train Epoche: 2 [529/96 (551%)]\tLoss: 10.248137\n",
      "Train Epoche: 2 [530/96 (552%)]\tLoss: 1.956306\n",
      "Train Epoche: 2 [531/96 (553%)]\tLoss: 10.256393\n",
      "Train Epoche: 2 [532/96 (554%)]\tLoss: 0.488915\n",
      "Train Epoche: 2 [533/96 (555%)]\tLoss: 10.706673\n",
      "Train Epoche: 2 [534/96 (556%)]\tLoss: 0.490662\n",
      "Train Epoche: 2 [535/96 (557%)]\tLoss: 30.720749\n",
      "Train Epoche: 2 [536/96 (558%)]\tLoss: 16.405691\n",
      "Train Epoche: 2 [537/96 (559%)]\tLoss: 3.135525\n",
      "Train Epoche: 2 [538/96 (560%)]\tLoss: 4.966095\n",
      "Train Epoche: 2 [539/96 (561%)]\tLoss: 58.738613\n",
      "Train Epoche: 2 [540/96 (562%)]\tLoss: 1.168562\n",
      "Train Epoche: 2 [541/96 (564%)]\tLoss: 9.342936\n",
      "Train Epoche: 2 [542/96 (565%)]\tLoss: 4.507437\n",
      "Train Epoche: 2 [543/96 (566%)]\tLoss: 24.215523\n",
      "Train Epoche: 2 [544/96 (567%)]\tLoss: 5.521245\n",
      "Train Epoche: 2 [545/96 (568%)]\tLoss: 2.308261\n",
      "Train Epoche: 2 [546/96 (569%)]\tLoss: 2.263829\n",
      "Train Epoche: 2 [547/96 (570%)]\tLoss: 185.153030\n",
      "Train Epoche: 2 [548/96 (571%)]\tLoss: 25.166876\n",
      "Train Epoche: 2 [549/96 (572%)]\tLoss: 1.744876\n",
      "Train Epoche: 2 [550/96 (573%)]\tLoss: 7.882565\n",
      "Train Epoche: 2 [551/96 (574%)]\tLoss: 12.513261\n",
      "Train Epoche: 2 [552/96 (575%)]\tLoss: 2.085498\n",
      "Train Epoche: 2 [553/96 (576%)]\tLoss: 4.737747\n",
      "Train Epoche: 2 [554/96 (577%)]\tLoss: 9.605704\n",
      "Train Epoche: 2 [555/96 (578%)]\tLoss: 6.336606\n",
      "Train Epoche: 2 [556/96 (579%)]\tLoss: 1.345980\n",
      "Train Epoche: 2 [557/96 (580%)]\tLoss: 0.288741\n",
      "Train Epoche: 2 [558/96 (581%)]\tLoss: 33.107483\n",
      "Train Epoche: 2 [559/96 (582%)]\tLoss: 58.051712\n",
      "Train Epoche: 2 [560/96 (583%)]\tLoss: 16.403591\n",
      "Train Epoche: 2 [561/96 (584%)]\tLoss: 40.265598\n",
      "Train Epoche: 2 [562/96 (585%)]\tLoss: 13.265499\n",
      "Train Epoche: 2 [563/96 (586%)]\tLoss: 10.928758\n",
      "Train Epoche: 2 [564/96 (588%)]\tLoss: 35.487801\n",
      "Train Epoche: 2 [565/96 (589%)]\tLoss: 4.758372\n",
      "Train Epoche: 2 [566/96 (590%)]\tLoss: 4.556859\n",
      "Train Epoche: 2 [567/96 (591%)]\tLoss: 0.088129\n",
      "Train Epoche: 2 [568/96 (592%)]\tLoss: 54.997486\n",
      "Train Epoche: 2 [569/96 (593%)]\tLoss: 2.388823\n",
      "Train Epoche: 2 [570/96 (594%)]\tLoss: 2.203203\n",
      "Train Epoche: 2 [571/96 (595%)]\tLoss: 1.362797\n",
      "Train Epoche: 2 [572/96 (596%)]\tLoss: 1.117360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [573/96 (597%)]\tLoss: 0.612054\n",
      "Train Epoche: 2 [574/96 (598%)]\tLoss: 0.882115\n",
      "Train Epoche: 2 [575/96 (599%)]\tLoss: 0.004702\n",
      "Train Epoche: 2 [576/96 (600%)]\tLoss: 2.985317\n",
      "Train Epoche: 2 [577/96 (601%)]\tLoss: 0.039514\n",
      "Train Epoche: 2 [578/96 (602%)]\tLoss: 0.127546\n",
      "Train Epoche: 2 [579/96 (603%)]\tLoss: 0.305368\n",
      "Train Epoche: 2 [580/96 (604%)]\tLoss: 25.854795\n",
      "Train Epoche: 2 [581/96 (605%)]\tLoss: 2.400754\n",
      "Train Epoche: 2 [582/96 (606%)]\tLoss: 2.020297\n",
      "Train Epoche: 2 [583/96 (607%)]\tLoss: 2.629770\n",
      "Train Epoche: 2 [584/96 (608%)]\tLoss: 4.264517\n",
      "Train Epoche: 2 [585/96 (609%)]\tLoss: 0.549332\n",
      "Train Epoche: 2 [586/96 (610%)]\tLoss: 8.048498\n",
      "Train Epoche: 2 [587/96 (611%)]\tLoss: 0.031496\n",
      "Train Epoche: 2 [588/96 (612%)]\tLoss: 35.616096\n",
      "Train Epoche: 2 [589/96 (614%)]\tLoss: 0.252214\n",
      "Train Epoche: 2 [590/96 (615%)]\tLoss: 0.047253\n",
      "Train Epoche: 2 [591/96 (616%)]\tLoss: 12.666216\n",
      "Train Epoche: 2 [592/96 (617%)]\tLoss: 7.523327\n",
      "Train Epoche: 2 [593/96 (618%)]\tLoss: 0.032716\n",
      "Train Epoche: 2 [594/96 (619%)]\tLoss: 0.606558\n",
      "Train Epoche: 2 [595/96 (620%)]\tLoss: 50.351440\n",
      "Train Epoche: 2 [596/96 (621%)]\tLoss: 62.879383\n",
      "Train Epoche: 2 [597/96 (622%)]\tLoss: 72.814415\n",
      "Train Epoche: 2 [598/96 (623%)]\tLoss: 0.628598\n",
      "Train Epoche: 2 [599/96 (624%)]\tLoss: 16.617382\n",
      "Train Epoche: 2 [600/96 (625%)]\tLoss: 4.197702\n",
      "Train Epoche: 2 [601/96 (626%)]\tLoss: 26.423065\n",
      "Train Epoche: 2 [602/96 (627%)]\tLoss: 12.532025\n",
      "Train Epoche: 2 [603/96 (628%)]\tLoss: 0.013311\n",
      "Train Epoche: 2 [604/96 (629%)]\tLoss: 1.030740\n",
      "Train Epoche: 2 [605/96 (630%)]\tLoss: 1.344138\n",
      "Train Epoche: 2 [606/96 (631%)]\tLoss: 0.026906\n",
      "Train Epoche: 2 [607/96 (632%)]\tLoss: 3.798602\n",
      "Train Epoche: 2 [608/96 (633%)]\tLoss: 92.913353\n",
      "Train Epoche: 2 [609/96 (634%)]\tLoss: 311.823242\n",
      "Train Epoche: 2 [610/96 (635%)]\tLoss: 0.609193\n",
      "Train Epoche: 2 [611/96 (636%)]\tLoss: 8.631266\n",
      "Train Epoche: 2 [612/96 (638%)]\tLoss: 144.756821\n",
      "Train Epoche: 2 [613/96 (639%)]\tLoss: 67.485573\n",
      "Train Epoche: 2 [614/96 (640%)]\tLoss: 0.366068\n",
      "Train Epoche: 2 [615/96 (641%)]\tLoss: 0.090523\n",
      "Train Epoche: 2 [616/96 (642%)]\tLoss: 12.639126\n",
      "Train Epoche: 2 [617/96 (643%)]\tLoss: 22.132303\n",
      "Train Epoche: 2 [618/96 (644%)]\tLoss: 5.631017\n",
      "Train Epoche: 2 [619/96 (645%)]\tLoss: 26.786703\n",
      "Train Epoche: 2 [620/96 (646%)]\tLoss: 0.223777\n",
      "Train Epoche: 2 [621/96 (647%)]\tLoss: 27.115479\n",
      "Train Epoche: 2 [622/96 (648%)]\tLoss: 12.412287\n",
      "Train Epoche: 2 [623/96 (649%)]\tLoss: 0.415317\n",
      "Train Epoche: 2 [624/96 (650%)]\tLoss: 0.044404\n",
      "Train Epoche: 2 [625/96 (651%)]\tLoss: 6.770787\n",
      "Train Epoche: 2 [626/96 (652%)]\tLoss: 37.378036\n",
      "Train Epoche: 2 [627/96 (653%)]\tLoss: 6.308451\n",
      "Train Epoche: 2 [628/96 (654%)]\tLoss: 4.246470\n",
      "Train Epoche: 2 [629/96 (655%)]\tLoss: 35.039062\n",
      "Train Epoche: 2 [630/96 (656%)]\tLoss: 0.681442\n",
      "Train Epoche: 2 [631/96 (657%)]\tLoss: 68.640846\n",
      "Train Epoche: 2 [632/96 (658%)]\tLoss: 0.370560\n",
      "Train Epoche: 2 [633/96 (659%)]\tLoss: 3.111726\n",
      "Train Epoche: 2 [634/96 (660%)]\tLoss: 7.675867\n",
      "Train Epoche: 2 [635/96 (661%)]\tLoss: 2.110388\n",
      "Train Epoche: 2 [636/96 (662%)]\tLoss: 18.567949\n",
      "Train Epoche: 2 [637/96 (664%)]\tLoss: 1.550493\n",
      "Train Epoche: 2 [638/96 (665%)]\tLoss: 16.879416\n",
      "Train Epoche: 2 [639/96 (666%)]\tLoss: 4.363175\n",
      "Train Epoche: 2 [640/96 (667%)]\tLoss: 0.193451\n",
      "Train Epoche: 2 [641/96 (668%)]\tLoss: 4.587405\n",
      "Train Epoche: 2 [642/96 (669%)]\tLoss: 0.732898\n",
      "Train Epoche: 2 [643/96 (670%)]\tLoss: 102.631027\n",
      "Train Epoche: 2 [644/96 (671%)]\tLoss: 1.240625\n",
      "Train Epoche: 2 [645/96 (672%)]\tLoss: 5.010740\n",
      "Train Epoche: 2 [646/96 (673%)]\tLoss: 14.093638\n",
      "Train Epoche: 2 [647/96 (674%)]\tLoss: 9.079787\n",
      "Train Epoche: 2 [648/96 (675%)]\tLoss: 75.556892\n",
      "Train Epoche: 2 [649/96 (676%)]\tLoss: 347.594635\n",
      "Train Epoche: 2 [650/96 (677%)]\tLoss: 0.306855\n",
      "Train Epoche: 2 [651/96 (678%)]\tLoss: 36.559402\n",
      "Train Epoche: 2 [652/96 (679%)]\tLoss: 0.487473\n",
      "Train Epoche: 2 [653/96 (680%)]\tLoss: 7.237386\n",
      "Train Epoche: 2 [654/96 (681%)]\tLoss: 6.870461\n",
      "Train Epoche: 2 [655/96 (682%)]\tLoss: 0.351185\n",
      "Train Epoche: 2 [656/96 (683%)]\tLoss: 1.049997\n",
      "Train Epoche: 2 [657/96 (684%)]\tLoss: 6.769063\n",
      "Train Epoche: 2 [658/96 (685%)]\tLoss: 1.675465\n",
      "Train Epoche: 2 [659/96 (686%)]\tLoss: 1.989340\n",
      "Train Epoche: 2 [660/96 (688%)]\tLoss: 14.871774\n",
      "Train Epoche: 2 [661/96 (689%)]\tLoss: 26.020685\n",
      "Train Epoche: 2 [662/96 (690%)]\tLoss: 3.882545\n",
      "Train Epoche: 2 [663/96 (691%)]\tLoss: 12.167507\n",
      "Train Epoche: 2 [664/96 (692%)]\tLoss: 0.353928\n",
      "Train Epoche: 2 [665/96 (693%)]\tLoss: 19.278048\n",
      "Train Epoche: 2 [666/96 (694%)]\tLoss: 0.161770\n",
      "Train Epoche: 2 [667/96 (695%)]\tLoss: 7.546932\n",
      "Train Epoche: 2 [668/96 (696%)]\tLoss: 0.669172\n",
      "Train Epoche: 2 [669/96 (697%)]\tLoss: 2.411994\n",
      "Train Epoche: 2 [670/96 (698%)]\tLoss: 12.937456\n",
      "Train Epoche: 2 [671/96 (699%)]\tLoss: 4.958735\n",
      "Train Epoche: 2 [672/96 (700%)]\tLoss: 0.349503\n",
      "Train Epoche: 2 [673/96 (701%)]\tLoss: 7.323491\n",
      "Train Epoche: 2 [674/96 (702%)]\tLoss: 0.021518\n",
      "Train Epoche: 2 [675/96 (703%)]\tLoss: 1.171921\n",
      "Train Epoche: 2 [676/96 (704%)]\tLoss: 3.732536\n",
      "Train Epoche: 2 [677/96 (705%)]\tLoss: 1.242520\n",
      "Train Epoche: 2 [678/96 (706%)]\tLoss: 0.000042\n",
      "Train Epoche: 2 [679/96 (707%)]\tLoss: 0.446195\n",
      "Train Epoche: 2 [680/96 (708%)]\tLoss: 182.919113\n",
      "Train Epoche: 2 [681/96 (709%)]\tLoss: 70.490784\n",
      "Train Epoche: 2 [682/96 (710%)]\tLoss: 48.887245\n",
      "Train Epoche: 2 [683/96 (711%)]\tLoss: 7.979223\n",
      "Train Epoche: 2 [684/96 (712%)]\tLoss: 37.560791\n",
      "Train Epoche: 2 [685/96 (714%)]\tLoss: 0.377585\n",
      "Train Epoche: 2 [686/96 (715%)]\tLoss: 53.445656\n",
      "Train Epoche: 2 [687/96 (716%)]\tLoss: 4.193923\n",
      "Train Epoche: 2 [688/96 (717%)]\tLoss: 6.237813\n",
      "Train Epoche: 2 [689/96 (718%)]\tLoss: 72.425476\n",
      "Train Epoche: 2 [690/96 (719%)]\tLoss: 15.869104\n",
      "Train Epoche: 2 [691/96 (720%)]\tLoss: 0.860090\n",
      "Train Epoche: 2 [692/96 (721%)]\tLoss: 1.048198\n",
      "Train Epoche: 2 [693/96 (722%)]\tLoss: 30.958914\n",
      "Train Epoche: 2 [694/96 (723%)]\tLoss: 28.818138\n",
      "Train Epoche: 2 [695/96 (724%)]\tLoss: 4.479644\n",
      "Train Epoche: 2 [696/96 (725%)]\tLoss: 2.833394\n",
      "Train Epoche: 2 [697/96 (726%)]\tLoss: 0.766396\n",
      "Train Epoche: 2 [698/96 (727%)]\tLoss: 48.134174\n",
      "Train Epoche: 2 [699/96 (728%)]\tLoss: 6.752045\n",
      "Train Epoche: 2 [700/96 (729%)]\tLoss: 10.625578\n",
      "Train Epoche: 2 [701/96 (730%)]\tLoss: 5.658732\n",
      "Train Epoche: 2 [702/96 (731%)]\tLoss: 0.829440\n",
      "Train Epoche: 2 [703/96 (732%)]\tLoss: 10.592978\n",
      "Train Epoche: 2 [704/96 (733%)]\tLoss: 164.019836\n",
      "Train Epoche: 2 [705/96 (734%)]\tLoss: 0.401423\n",
      "Train Epoche: 2 [706/96 (735%)]\tLoss: 106.058929\n",
      "Train Epoche: 2 [707/96 (736%)]\tLoss: 67.040169\n",
      "Train Epoche: 2 [708/96 (738%)]\tLoss: 3.764338\n",
      "Train Epoche: 2 [709/96 (739%)]\tLoss: 2.563597\n",
      "Train Epoche: 2 [710/96 (740%)]\tLoss: 0.349853\n",
      "Train Epoche: 2 [711/96 (741%)]\tLoss: 2.343750\n",
      "Train Epoche: 2 [712/96 (742%)]\tLoss: 7.403388\n",
      "Train Epoche: 2 [713/96 (743%)]\tLoss: 0.148614\n",
      "Train Epoche: 2 [714/96 (744%)]\tLoss: 13.029838\n",
      "Train Epoche: 2 [715/96 (745%)]\tLoss: 5.791544\n",
      "Train Epoche: 2 [716/96 (746%)]\tLoss: 6.359278\n",
      "Train Epoche: 2 [717/96 (747%)]\tLoss: 29.340332\n",
      "Train Epoche: 2 [718/96 (748%)]\tLoss: 5.558643\n",
      "Train Epoche: 2 [719/96 (749%)]\tLoss: 1.637943\n",
      "Train Epoche: 2 [720/96 (750%)]\tLoss: 1.208870\n",
      "Train Epoche: 2 [721/96 (751%)]\tLoss: 31.815052\n",
      "Train Epoche: 2 [722/96 (752%)]\tLoss: 22.813183\n",
      "Train Epoche: 2 [723/96 (753%)]\tLoss: 1.439853\n",
      "Train Epoche: 2 [724/96 (754%)]\tLoss: 64.019745\n",
      "Train Epoche: 2 [725/96 (755%)]\tLoss: 0.028430\n",
      "Train Epoche: 2 [726/96 (756%)]\tLoss: 2.464067\n",
      "Train Epoche: 2 [727/96 (757%)]\tLoss: 0.349523\n",
      "Train Epoche: 2 [728/96 (758%)]\tLoss: 64.454185\n",
      "Train Epoche: 2 [729/96 (759%)]\tLoss: 53.114613\n",
      "Train Epoche: 2 [730/96 (760%)]\tLoss: 1.509715\n",
      "Train Epoche: 2 [731/96 (761%)]\tLoss: 0.206792\n",
      "Train Epoche: 2 [732/96 (762%)]\tLoss: 1.199474\n",
      "Train Epoche: 2 [733/96 (764%)]\tLoss: 1.597055\n",
      "Train Epoche: 2 [734/96 (765%)]\tLoss: 10.126722\n",
      "Train Epoche: 2 [735/96 (766%)]\tLoss: 8.500260\n",
      "Train Epoche: 2 [736/96 (767%)]\tLoss: 2.768415\n",
      "Train Epoche: 2 [737/96 (768%)]\tLoss: 0.107390\n",
      "Train Epoche: 2 [738/96 (769%)]\tLoss: 1.718117\n",
      "Train Epoche: 2 [739/96 (770%)]\tLoss: 71.576912\n",
      "Train Epoche: 2 [740/96 (771%)]\tLoss: 47.676270\n",
      "Train Epoche: 2 [741/96 (772%)]\tLoss: 1.689663\n",
      "Train Epoche: 2 [742/96 (773%)]\tLoss: 198.010147\n",
      "Train Epoche: 2 [743/96 (774%)]\tLoss: 0.015406\n",
      "Train Epoche: 2 [744/96 (775%)]\tLoss: 10.079045\n",
      "Train Epoche: 2 [745/96 (776%)]\tLoss: 2.521572\n",
      "Train Epoche: 2 [746/96 (777%)]\tLoss: 82.124443\n",
      "Train Epoche: 2 [747/96 (778%)]\tLoss: 9.483274\n",
      "Train Epoche: 2 [748/96 (779%)]\tLoss: 14.373275\n",
      "Train Epoche: 2 [749/96 (780%)]\tLoss: 9.298901\n",
      "Train Epoche: 2 [750/96 (781%)]\tLoss: 0.556653\n",
      "Train Epoche: 2 [751/96 (782%)]\tLoss: 0.619191\n",
      "Train Epoche: 2 [752/96 (783%)]\tLoss: 4.882697\n",
      "Train Epoche: 2 [753/96 (784%)]\tLoss: 110.354263\n",
      "Train Epoche: 2 [754/96 (785%)]\tLoss: 2.486723\n",
      "Train Epoche: 2 [755/96 (786%)]\tLoss: 6.479724\n",
      "Train Epoche: 2 [756/96 (788%)]\tLoss: 25.091856\n",
      "Train Epoche: 2 [757/96 (789%)]\tLoss: 14.873480\n",
      "Train Epoche: 2 [758/96 (790%)]\tLoss: 39.778057\n",
      "Train Epoche: 2 [759/96 (791%)]\tLoss: 4.028286\n",
      "Train Epoche: 2 [760/96 (792%)]\tLoss: 28.900221\n",
      "Train Epoche: 2 [761/96 (793%)]\tLoss: 37.875729\n",
      "Train Epoche: 2 [762/96 (794%)]\tLoss: 18.567190\n",
      "Train Epoche: 2 [763/96 (795%)]\tLoss: 50.983433\n",
      "Train Epoche: 2 [764/96 (796%)]\tLoss: 24.072487\n",
      "Train Epoche: 2 [765/96 (797%)]\tLoss: 4.836621\n",
      "Train Epoche: 2 [766/96 (798%)]\tLoss: 7.225496\n",
      "Train Epoche: 2 [767/96 (799%)]\tLoss: 9.957074\n",
      "Train Epoche: 2 [768/96 (800%)]\tLoss: 12.060295\n",
      "Train Epoche: 2 [769/96 (801%)]\tLoss: 9.398444\n",
      "Train Epoche: 2 [770/96 (802%)]\tLoss: 17.984013\n",
      "Train Epoche: 2 [771/96 (803%)]\tLoss: 5.263394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [772/96 (804%)]\tLoss: 13.267545\n",
      "Train Epoche: 2 [773/96 (805%)]\tLoss: 0.025536\n",
      "Train Epoche: 2 [774/96 (806%)]\tLoss: 6.651253\n",
      "Train Epoche: 2 [775/96 (807%)]\tLoss: 11.218118\n",
      "Train Epoche: 2 [776/96 (808%)]\tLoss: 2.595652\n",
      "Train Epoche: 2 [777/96 (809%)]\tLoss: 1.133919\n",
      "Train Epoche: 2 [778/96 (810%)]\tLoss: 8.979216\n",
      "Train Epoche: 2 [779/96 (811%)]\tLoss: 4.939480\n",
      "Train Epoche: 2 [780/96 (812%)]\tLoss: 304.650848\n",
      "Train Epoche: 2 [781/96 (814%)]\tLoss: 90.629265\n",
      "Train Epoche: 2 [782/96 (815%)]\tLoss: 1.264445\n",
      "Train Epoche: 2 [783/96 (816%)]\tLoss: 13.467731\n",
      "Train Epoche: 2 [784/96 (817%)]\tLoss: 24.165888\n",
      "Train Epoche: 2 [785/96 (818%)]\tLoss: 7.397390\n",
      "Train Epoche: 2 [786/96 (819%)]\tLoss: 67.521202\n",
      "Train Epoche: 2 [787/96 (820%)]\tLoss: 10.944912\n",
      "Train Epoche: 2 [788/96 (821%)]\tLoss: 1.446431\n",
      "Train Epoche: 2 [789/96 (822%)]\tLoss: 1.816586\n",
      "Train Epoche: 2 [790/96 (823%)]\tLoss: 0.533064\n",
      "Train Epoche: 2 [791/96 (824%)]\tLoss: 11.817692\n",
      "Train Epoche: 2 [792/96 (825%)]\tLoss: 5.848076\n",
      "Train Epoche: 2 [793/96 (826%)]\tLoss: 1.888433\n",
      "Train Epoche: 2 [794/96 (827%)]\tLoss: 1.031316\n",
      "Train Epoche: 2 [795/96 (828%)]\tLoss: 94.998055\n",
      "Train Epoche: 2 [796/96 (829%)]\tLoss: 0.017829\n",
      "Train Epoche: 2 [797/96 (830%)]\tLoss: 3.402069\n",
      "Train Epoche: 2 [798/96 (831%)]\tLoss: 6.833257\n",
      "Train Epoche: 2 [799/96 (832%)]\tLoss: 3.033324\n",
      "Train Epoche: 2 [800/96 (833%)]\tLoss: 0.048538\n",
      "Train Epoche: 2 [801/96 (834%)]\tLoss: 220.491699\n",
      "Train Epoche: 2 [802/96 (835%)]\tLoss: 6.758633\n",
      "Train Epoche: 2 [803/96 (836%)]\tLoss: 0.139640\n",
      "Train Epoche: 2 [804/96 (838%)]\tLoss: 6.693957\n",
      "Train Epoche: 2 [805/96 (839%)]\tLoss: 1.963549\n",
      "Train Epoche: 2 [806/96 (840%)]\tLoss: 55.568436\n",
      "Train Epoche: 2 [807/96 (841%)]\tLoss: 2.809899\n",
      "Train Epoche: 2 [808/96 (842%)]\tLoss: 1.518582\n",
      "Train Epoche: 2 [809/96 (843%)]\tLoss: 0.528118\n",
      "Train Epoche: 2 [810/96 (844%)]\tLoss: 1.318326\n",
      "Train Epoche: 2 [811/96 (845%)]\tLoss: 3.408007\n",
      "Train Epoche: 2 [812/96 (846%)]\tLoss: 10.414115\n",
      "Train Epoche: 2 [813/96 (847%)]\tLoss: 125.462791\n",
      "Train Epoche: 2 [814/96 (848%)]\tLoss: 16.952030\n",
      "Train Epoche: 2 [815/96 (849%)]\tLoss: 0.338966\n",
      "Train Epoche: 2 [816/96 (850%)]\tLoss: 13.915024\n",
      "Train Epoche: 2 [817/96 (851%)]\tLoss: 13.591032\n",
      "Train Epoche: 2 [818/96 (852%)]\tLoss: 10.352206\n",
      "Train Epoche: 2 [819/96 (853%)]\tLoss: 14.052892\n",
      "Train Epoche: 2 [820/96 (854%)]\tLoss: 0.168492\n",
      "Train Epoche: 2 [821/96 (855%)]\tLoss: 6.181676\n",
      "Train Epoche: 2 [822/96 (856%)]\tLoss: 1.627664\n",
      "Train Epoche: 2 [823/96 (857%)]\tLoss: 114.667824\n",
      "Train Epoche: 2 [824/96 (858%)]\tLoss: 71.692062\n",
      "Train Epoche: 2 [825/96 (859%)]\tLoss: 22.718300\n",
      "Train Epoche: 2 [826/96 (860%)]\tLoss: 27.691404\n",
      "Train Epoche: 2 [827/96 (861%)]\tLoss: 38.690193\n",
      "Train Epoche: 2 [828/96 (862%)]\tLoss: 22.573183\n",
      "Train Epoche: 2 [829/96 (864%)]\tLoss: 0.031994\n",
      "Train Epoche: 2 [830/96 (865%)]\tLoss: 3.422371\n",
      "Train Epoche: 2 [831/96 (866%)]\tLoss: 0.375758\n",
      "Train Epoche: 2 [832/96 (867%)]\tLoss: 4.936708\n",
      "Train Epoche: 2 [833/96 (868%)]\tLoss: 12.563760\n",
      "Train Epoche: 2 [834/96 (869%)]\tLoss: 1.359420\n",
      "Train Epoche: 2 [835/96 (870%)]\tLoss: 0.526860\n",
      "Train Epoche: 2 [836/96 (871%)]\tLoss: 2.425013\n",
      "Train Epoche: 2 [837/96 (872%)]\tLoss: 12.192983\n",
      "Train Epoche: 2 [838/96 (873%)]\tLoss: 57.518845\n",
      "Train Epoche: 2 [839/96 (874%)]\tLoss: 4.321632\n",
      "Train Epoche: 2 [840/96 (875%)]\tLoss: 0.287601\n",
      "Train Epoche: 2 [841/96 (876%)]\tLoss: 9.110279\n",
      "Train Epoche: 2 [842/96 (877%)]\tLoss: 0.142361\n",
      "Train Epoche: 2 [843/96 (878%)]\tLoss: 24.593266\n",
      "Train Epoche: 2 [844/96 (879%)]\tLoss: 0.738289\n",
      "Train Epoche: 2 [845/96 (880%)]\tLoss: 1.312835\n",
      "Train Epoche: 2 [846/96 (881%)]\tLoss: 7.727513\n",
      "Train Epoche: 2 [847/96 (882%)]\tLoss: 4.870341\n",
      "Train Epoche: 2 [848/96 (883%)]\tLoss: 1.083164\n",
      "Train Epoche: 2 [849/96 (884%)]\tLoss: 150.029541\n",
      "Train Epoche: 2 [850/96 (885%)]\tLoss: 27.337206\n",
      "Train Epoche: 2 [851/96 (886%)]\tLoss: 162.045456\n",
      "Train Epoche: 2 [852/96 (888%)]\tLoss: 29.752905\n",
      "Train Epoche: 2 [853/96 (889%)]\tLoss: 2.171708\n",
      "Train Epoche: 2 [854/96 (890%)]\tLoss: 0.893211\n",
      "Train Epoche: 2 [855/96 (891%)]\tLoss: 1.749562\n",
      "Train Epoche: 2 [856/96 (892%)]\tLoss: 2.264173\n",
      "Train Epoche: 2 [857/96 (893%)]\tLoss: 4.122579\n",
      "Train Epoche: 2 [858/96 (894%)]\tLoss: 0.643194\n",
      "Train Epoche: 2 [859/96 (895%)]\tLoss: 20.757690\n",
      "Train Epoche: 2 [860/96 (896%)]\tLoss: 0.079180\n",
      "Train Epoche: 2 [861/96 (897%)]\tLoss: 323.711609\n",
      "Train Epoche: 2 [862/96 (898%)]\tLoss: 3.788969\n",
      "Train Epoche: 2 [863/96 (899%)]\tLoss: 28.044672\n",
      "Train Epoche: 2 [864/96 (900%)]\tLoss: 76.699715\n",
      "Train Epoche: 2 [865/96 (901%)]\tLoss: 6.730087\n",
      "Train Epoche: 2 [866/96 (902%)]\tLoss: 18.378881\n",
      "Train Epoche: 2 [867/96 (903%)]\tLoss: 8.691446\n",
      "Train Epoche: 2 [868/96 (904%)]\tLoss: 0.730663\n",
      "Train Epoche: 2 [869/96 (905%)]\tLoss: 1.724100\n",
      "Train Epoche: 2 [870/96 (906%)]\tLoss: 0.121751\n",
      "Train Epoche: 2 [871/96 (907%)]\tLoss: 75.412186\n",
      "Train Epoche: 2 [872/96 (908%)]\tLoss: 15.793563\n",
      "Train Epoche: 2 [873/96 (909%)]\tLoss: 3.696856\n",
      "Train Epoche: 2 [874/96 (910%)]\tLoss: 177.625092\n",
      "Train Epoche: 2 [875/96 (911%)]\tLoss: 2.602443\n",
      "Train Epoche: 2 [876/96 (912%)]\tLoss: 0.185620\n",
      "Train Epoche: 2 [877/96 (914%)]\tLoss: 266.992310\n",
      "Train Epoche: 2 [878/96 (915%)]\tLoss: 8.069835\n",
      "Train Epoche: 2 [879/96 (916%)]\tLoss: 0.031423\n",
      "Train Epoche: 2 [880/96 (917%)]\tLoss: 0.113207\n",
      "Train Epoche: 2 [881/96 (918%)]\tLoss: 13.359609\n",
      "Train Epoche: 2 [882/96 (919%)]\tLoss: 109.120132\n",
      "Train Epoche: 2 [883/96 (920%)]\tLoss: 1.484521\n",
      "Train Epoche: 2 [884/96 (921%)]\tLoss: 6.897614\n",
      "Train Epoche: 2 [885/96 (922%)]\tLoss: 3.396236\n",
      "Train Epoche: 2 [886/96 (923%)]\tLoss: 4.811679\n",
      "Train Epoche: 2 [887/96 (924%)]\tLoss: 26.986673\n",
      "Train Epoche: 2 [888/96 (925%)]\tLoss: 4.707030\n",
      "Train Epoche: 2 [889/96 (926%)]\tLoss: 41.109470\n",
      "Train Epoche: 2 [890/96 (927%)]\tLoss: 0.105854\n",
      "Train Epoche: 2 [891/96 (928%)]\tLoss: 120.377815\n",
      "Train Epoche: 2 [892/96 (929%)]\tLoss: 2.356186\n",
      "Train Epoche: 2 [893/96 (930%)]\tLoss: 2.185822\n",
      "Train Epoche: 2 [894/96 (931%)]\tLoss: 64.145699\n",
      "Train Epoche: 2 [895/96 (932%)]\tLoss: 0.759361\n",
      "Train Epoche: 2 [896/96 (933%)]\tLoss: 37.352970\n",
      "Train Epoche: 2 [897/96 (934%)]\tLoss: 9.180818\n",
      "Train Epoche: 2 [898/96 (935%)]\tLoss: 1.110593\n",
      "Train Epoche: 2 [899/96 (936%)]\tLoss: 0.017322\n",
      "Train Epoche: 2 [900/96 (938%)]\tLoss: 1.756641\n",
      "Train Epoche: 2 [901/96 (939%)]\tLoss: 0.941841\n",
      "Train Epoche: 2 [902/96 (940%)]\tLoss: 0.050698\n",
      "Train Epoche: 2 [903/96 (941%)]\tLoss: 74.615562\n",
      "Train Epoche: 2 [904/96 (942%)]\tLoss: 0.000051\n",
      "Train Epoche: 2 [905/96 (943%)]\tLoss: 57.029724\n",
      "Train Epoche: 2 [906/96 (944%)]\tLoss: 2.731286\n",
      "Train Epoche: 2 [907/96 (945%)]\tLoss: 1.650350\n",
      "Train Epoche: 2 [908/96 (946%)]\tLoss: 0.538855\n",
      "Train Epoche: 2 [909/96 (947%)]\tLoss: 184.607391\n",
      "Train Epoche: 2 [910/96 (948%)]\tLoss: 128.661316\n",
      "Train Epoche: 2 [911/96 (949%)]\tLoss: 14.296070\n",
      "Train Epoche: 2 [912/96 (950%)]\tLoss: 12.780597\n",
      "Train Epoche: 2 [913/96 (951%)]\tLoss: 57.316532\n",
      "Train Epoche: 2 [914/96 (952%)]\tLoss: 2.186513\n",
      "Train Epoche: 2 [915/96 (953%)]\tLoss: 23.485912\n",
      "Train Epoche: 2 [916/96 (954%)]\tLoss: 3.311545\n",
      "Train Epoche: 2 [917/96 (955%)]\tLoss: 9.215897\n",
      "Train Epoche: 2 [918/96 (956%)]\tLoss: 8.621181\n",
      "Train Epoche: 2 [919/96 (957%)]\tLoss: 23.264456\n",
      "Train Epoche: 2 [920/96 (958%)]\tLoss: 8.755199\n",
      "Train Epoche: 2 [921/96 (959%)]\tLoss: 23.718208\n",
      "Train Epoche: 2 [922/96 (960%)]\tLoss: 0.278581\n",
      "Train Epoche: 2 [923/96 (961%)]\tLoss: 16.972509\n",
      "Train Epoche: 2 [924/96 (962%)]\tLoss: 6.967764\n",
      "Train Epoche: 2 [925/96 (964%)]\tLoss: 2.803518\n",
      "Train Epoche: 2 [926/96 (965%)]\tLoss: 72.062965\n",
      "Train Epoche: 2 [927/96 (966%)]\tLoss: 35.747097\n",
      "Train Epoche: 2 [928/96 (967%)]\tLoss: 2.919898\n",
      "Train Epoche: 2 [929/96 (968%)]\tLoss: 44.002274\n",
      "Train Epoche: 2 [930/96 (969%)]\tLoss: 12.970455\n",
      "Train Epoche: 2 [931/96 (970%)]\tLoss: 5.378284\n",
      "Train Epoche: 2 [932/96 (971%)]\tLoss: 3.164920\n",
      "Train Epoche: 2 [933/96 (972%)]\tLoss: 0.305191\n",
      "Train Epoche: 2 [934/96 (973%)]\tLoss: 3.334044\n",
      "Train Epoche: 2 [935/96 (974%)]\tLoss: 4.400842\n",
      "Train Epoche: 2 [936/96 (975%)]\tLoss: 2.535254\n",
      "Train Epoche: 2 [937/96 (976%)]\tLoss: 151.127914\n",
      "Train Epoche: 2 [938/96 (977%)]\tLoss: 21.842503\n",
      "Train Epoche: 2 [939/96 (978%)]\tLoss: 85.565712\n",
      "Train Epoche: 2 [940/96 (979%)]\tLoss: 3.987322\n",
      "Train Epoche: 2 [941/96 (980%)]\tLoss: 2.617936\n",
      "Train Epoche: 2 [942/96 (981%)]\tLoss: 8.519985\n",
      "Train Epoche: 2 [943/96 (982%)]\tLoss: 0.136708\n",
      "Train Epoche: 2 [944/96 (983%)]\tLoss: 0.518853\n",
      "Train Epoche: 2 [945/96 (984%)]\tLoss: 9.122166\n",
      "Train Epoche: 2 [946/96 (985%)]\tLoss: 2.971144\n",
      "Train Epoche: 2 [947/96 (986%)]\tLoss: 50.135540\n",
      "Train Epoche: 2 [948/96 (988%)]\tLoss: 7.094892\n",
      "Train Epoche: 2 [949/96 (989%)]\tLoss: 0.641422\n",
      "Train Epoche: 2 [950/96 (990%)]\tLoss: 2.529252\n",
      "Train Epoche: 2 [951/96 (991%)]\tLoss: 0.099955\n",
      "Train Epoche: 2 [952/96 (992%)]\tLoss: 0.382619\n",
      "Train Epoche: 2 [953/96 (993%)]\tLoss: 6.120228\n",
      "Train Epoche: 2 [954/96 (994%)]\tLoss: 13.049324\n",
      "Train Epoche: 2 [955/96 (995%)]\tLoss: 6.046215\n",
      "Train Epoche: 2 [956/96 (996%)]\tLoss: 3.649622\n",
      "Train Epoche: 2 [957/96 (997%)]\tLoss: 2.838069\n",
      "Train Epoche: 2 [958/96 (998%)]\tLoss: 8.885860\n",
      "Train Epoche: 2 [959/96 (999%)]\tLoss: 6.391820\n",
      "Train Epoche: 2 [960/96 (1000%)]\tLoss: 1.469999\n",
      "Train Epoche: 2 [961/96 (1001%)]\tLoss: 0.334532\n",
      "Train Epoche: 2 [962/96 (1002%)]\tLoss: 2.072478\n",
      "Train Epoche: 2 [963/96 (1003%)]\tLoss: 1.335637\n",
      "Train Epoche: 2 [964/96 (1004%)]\tLoss: 0.592628\n",
      "Train Epoche: 2 [965/96 (1005%)]\tLoss: 1.615950\n",
      "Train Epoche: 2 [966/96 (1006%)]\tLoss: 0.437026\n",
      "Train Epoche: 2 [967/96 (1007%)]\tLoss: 2.666580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [968/96 (1008%)]\tLoss: 4.720025\n",
      "Train Epoche: 2 [969/96 (1009%)]\tLoss: 3.385997\n",
      "Train Epoche: 2 [970/96 (1010%)]\tLoss: 40.766853\n",
      "Train Epoche: 2 [971/96 (1011%)]\tLoss: 0.002987\n",
      "Train Epoche: 2 [972/96 (1012%)]\tLoss: 44.082096\n",
      "Train Epoche: 2 [973/96 (1014%)]\tLoss: 21.790564\n",
      "Train Epoche: 2 [974/96 (1015%)]\tLoss: 57.024998\n",
      "Train Epoche: 2 [975/96 (1016%)]\tLoss: 178.799133\n",
      "Train Epoche: 2 [976/96 (1017%)]\tLoss: 11.494498\n",
      "Train Epoche: 2 [977/96 (1018%)]\tLoss: 68.685760\n",
      "Train Epoche: 2 [978/96 (1019%)]\tLoss: 0.814396\n",
      "Train Epoche: 2 [979/96 (1020%)]\tLoss: 2.380917\n",
      "Train Epoche: 2 [980/96 (1021%)]\tLoss: 52.548149\n",
      "Train Epoche: 2 [981/96 (1022%)]\tLoss: 7.837721\n",
      "Train Epoche: 2 [982/96 (1023%)]\tLoss: 4.370796\n",
      "Train Epoche: 2 [983/96 (1024%)]\tLoss: 19.166119\n",
      "Train Epoche: 2 [984/96 (1025%)]\tLoss: 8.288903\n",
      "Train Epoche: 2 [985/96 (1026%)]\tLoss: 9.624182\n",
      "Train Epoche: 2 [986/96 (1027%)]\tLoss: 0.188219\n",
      "Train Epoche: 2 [987/96 (1028%)]\tLoss: 7.263897\n",
      "Train Epoche: 2 [988/96 (1029%)]\tLoss: 0.336263\n",
      "Train Epoche: 2 [989/96 (1030%)]\tLoss: 9.432773\n",
      "Train Epoche: 2 [990/96 (1031%)]\tLoss: 0.017028\n",
      "Train Epoche: 2 [991/96 (1032%)]\tLoss: 0.738637\n",
      "Train Epoche: 2 [992/96 (1033%)]\tLoss: 4.073793\n",
      "Train Epoche: 2 [993/96 (1034%)]\tLoss: 4.561248\n",
      "Train Epoche: 2 [994/96 (1035%)]\tLoss: 0.016026\n",
      "Train Epoche: 2 [995/96 (1036%)]\tLoss: 0.849200\n",
      "Train Epoche: 2 [996/96 (1038%)]\tLoss: 1.242897\n",
      "Train Epoche: 2 [997/96 (1039%)]\tLoss: 2.560133\n",
      "Train Epoche: 2 [998/96 (1040%)]\tLoss: 34.864540\n",
      "Train Epoche: 2 [999/96 (1041%)]\tLoss: 44.960373\n",
      "Train Epoche: 2 [1000/96 (1042%)]\tLoss: 24.061758\n",
      "Train Epoche: 2 [1001/96 (1043%)]\tLoss: 30.249025\n",
      "Train Epoche: 2 [1002/96 (1044%)]\tLoss: 30.432589\n",
      "Train Epoche: 2 [1003/96 (1045%)]\tLoss: 3.621669\n",
      "Train Epoche: 2 [1004/96 (1046%)]\tLoss: 352.882599\n",
      "Train Epoche: 2 [1005/96 (1047%)]\tLoss: 0.120059\n",
      "Train Epoche: 2 [1006/96 (1048%)]\tLoss: 0.018521\n",
      "Train Epoche: 2 [1007/96 (1049%)]\tLoss: 1.520827\n",
      "Train Epoche: 2 [1008/96 (1050%)]\tLoss: 1.024486\n",
      "Train Epoche: 2 [1009/96 (1051%)]\tLoss: 29.449194\n",
      "Train Epoche: 2 [1010/96 (1052%)]\tLoss: 39.678997\n",
      "Train Epoche: 2 [1011/96 (1053%)]\tLoss: 0.050865\n",
      "Train Epoche: 2 [1012/96 (1054%)]\tLoss: 3.137760\n",
      "Train Epoche: 2 [1013/96 (1055%)]\tLoss: 24.683170\n",
      "Train Epoche: 2 [1014/96 (1056%)]\tLoss: 0.282541\n",
      "Train Epoche: 2 [1015/96 (1057%)]\tLoss: 6.094708\n",
      "Train Epoche: 2 [1016/96 (1058%)]\tLoss: 17.383127\n",
      "Train Epoche: 2 [1017/96 (1059%)]\tLoss: 32.608425\n",
      "Train Epoche: 2 [1018/96 (1060%)]\tLoss: 3.069681\n",
      "Train Epoche: 2 [1019/96 (1061%)]\tLoss: 0.121265\n",
      "Train Epoche: 2 [1020/96 (1062%)]\tLoss: 19.249779\n",
      "Train Epoche: 2 [1021/96 (1064%)]\tLoss: 13.028311\n",
      "Train Epoche: 2 [1022/96 (1065%)]\tLoss: 76.529121\n",
      "Train Epoche: 2 [1023/96 (1066%)]\tLoss: 2.111866\n",
      "Train Epoche: 2 [1024/96 (1067%)]\tLoss: 4.504340\n",
      "Train Epoche: 2 [1025/96 (1068%)]\tLoss: 1.367786\n",
      "Train Epoche: 2 [1026/96 (1069%)]\tLoss: 12.601574\n",
      "Train Epoche: 2 [1027/96 (1070%)]\tLoss: 0.910403\n",
      "Train Epoche: 2 [1028/96 (1071%)]\tLoss: 38.723896\n",
      "Train Epoche: 2 [1029/96 (1072%)]\tLoss: 48.182884\n",
      "Train Epoche: 2 [1030/96 (1073%)]\tLoss: 3.693044\n",
      "Train Epoche: 2 [1031/96 (1074%)]\tLoss: 22.716755\n",
      "Train Epoche: 2 [1032/96 (1075%)]\tLoss: 0.335129\n",
      "Train Epoche: 2 [1033/96 (1076%)]\tLoss: 11.896570\n",
      "Train Epoche: 2 [1034/96 (1077%)]\tLoss: 0.005858\n",
      "Train Epoche: 2 [1035/96 (1078%)]\tLoss: 9.087774\n",
      "Train Epoche: 2 [1036/96 (1079%)]\tLoss: 0.836114\n",
      "Train Epoche: 2 [1037/96 (1080%)]\tLoss: 12.390430\n",
      "Train Epoche: 2 [1038/96 (1081%)]\tLoss: 2.832754\n",
      "Train Epoche: 2 [1039/96 (1082%)]\tLoss: 112.238190\n",
      "Train Epoche: 2 [1040/96 (1083%)]\tLoss: 17.057390\n",
      "Train Epoche: 2 [1041/96 (1084%)]\tLoss: 0.038723\n",
      "Train Epoche: 2 [1042/96 (1085%)]\tLoss: 5.299692\n",
      "Train Epoche: 2 [1043/96 (1086%)]\tLoss: 0.238594\n",
      "Train Epoche: 2 [1044/96 (1088%)]\tLoss: 0.140190\n",
      "Train Epoche: 2 [1045/96 (1089%)]\tLoss: 1.006915\n",
      "Train Epoche: 2 [1046/96 (1090%)]\tLoss: 2.688373\n",
      "Train Epoche: 2 [1047/96 (1091%)]\tLoss: 24.338409\n",
      "Train Epoche: 2 [1048/96 (1092%)]\tLoss: 0.817579\n",
      "Train Epoche: 2 [1049/96 (1093%)]\tLoss: 11.390725\n",
      "Train Epoche: 2 [1050/96 (1094%)]\tLoss: 3.624723\n",
      "Train Epoche: 2 [1051/96 (1095%)]\tLoss: 63.107430\n",
      "Train Epoche: 2 [1052/96 (1096%)]\tLoss: 0.671513\n",
      "Train Epoche: 2 [1053/96 (1097%)]\tLoss: 4.898375\n",
      "Train Epoche: 2 [1054/96 (1098%)]\tLoss: 9.490269\n",
      "Train Epoche: 2 [1055/96 (1099%)]\tLoss: 0.154018\n",
      "Train Epoche: 2 [1056/96 (1100%)]\tLoss: 28.406631\n",
      "Train Epoche: 2 [1057/96 (1101%)]\tLoss: 0.004295\n",
      "Train Epoche: 2 [1058/96 (1102%)]\tLoss: 4.691260\n",
      "Train Epoche: 2 [1059/96 (1103%)]\tLoss: 3.928174\n",
      "Train Epoche: 2 [1060/96 (1104%)]\tLoss: 19.286030\n",
      "Train Epoche: 2 [1061/96 (1105%)]\tLoss: 2.169084\n",
      "Train Epoche: 2 [1062/96 (1106%)]\tLoss: 0.841515\n",
      "Train Epoche: 2 [1063/96 (1107%)]\tLoss: 0.155910\n",
      "Train Epoche: 2 [1064/96 (1108%)]\tLoss: 0.055747\n",
      "Train Epoche: 2 [1065/96 (1109%)]\tLoss: 26.153633\n",
      "Train Epoche: 2 [1066/96 (1110%)]\tLoss: 10.906530\n",
      "Train Epoche: 2 [1067/96 (1111%)]\tLoss: 7.652105\n",
      "Train Epoche: 2 [1068/96 (1112%)]\tLoss: 0.409967\n",
      "Train Epoche: 2 [1069/96 (1114%)]\tLoss: 0.174690\n",
      "Train Epoche: 2 [1070/96 (1115%)]\tLoss: 0.438011\n",
      "Train Epoche: 2 [1071/96 (1116%)]\tLoss: 0.640858\n",
      "Train Epoche: 2 [1072/96 (1117%)]\tLoss: 7.098136\n",
      "Train Epoche: 2 [1073/96 (1118%)]\tLoss: 3.400503\n",
      "Train Epoche: 2 [1074/96 (1119%)]\tLoss: 0.593261\n",
      "Train Epoche: 2 [1075/96 (1120%)]\tLoss: 1.727454\n",
      "Train Epoche: 2 [1076/96 (1121%)]\tLoss: 8.899813\n",
      "Train Epoche: 2 [1077/96 (1122%)]\tLoss: 13.998743\n",
      "Train Epoche: 2 [1078/96 (1123%)]\tLoss: 0.504620\n",
      "Train Epoche: 2 [1079/96 (1124%)]\tLoss: 14.648160\n",
      "Train Epoche: 2 [1080/96 (1125%)]\tLoss: 15.177976\n",
      "Train Epoche: 2 [1081/96 (1126%)]\tLoss: 3.065638\n",
      "Train Epoche: 2 [1082/96 (1127%)]\tLoss: 2.539756\n",
      "Train Epoche: 2 [1083/96 (1128%)]\tLoss: 3.527713\n",
      "Train Epoche: 2 [1084/96 (1129%)]\tLoss: 0.995272\n",
      "Train Epoche: 2 [1085/96 (1130%)]\tLoss: 0.526243\n",
      "Train Epoche: 2 [1086/96 (1131%)]\tLoss: 14.695422\n",
      "Train Epoche: 2 [1087/96 (1132%)]\tLoss: 12.133713\n",
      "Train Epoche: 2 [1088/96 (1133%)]\tLoss: 17.176767\n",
      "Train Epoche: 2 [1089/96 (1134%)]\tLoss: 2.538687\n",
      "Train Epoche: 2 [1090/96 (1135%)]\tLoss: 8.210421\n",
      "Train Epoche: 2 [1091/96 (1136%)]\tLoss: 15.893397\n",
      "Train Epoche: 2 [1092/96 (1138%)]\tLoss: 2.580934\n",
      "Train Epoche: 2 [1093/96 (1139%)]\tLoss: 0.847183\n",
      "Train Epoche: 2 [1094/96 (1140%)]\tLoss: 0.325558\n",
      "Train Epoche: 2 [1095/96 (1141%)]\tLoss: 1.114288\n",
      "Train Epoche: 2 [1096/96 (1142%)]\tLoss: 12.852362\n",
      "Train Epoche: 2 [1097/96 (1143%)]\tLoss: 0.083377\n",
      "Train Epoche: 2 [1098/96 (1144%)]\tLoss: 418.226959\n",
      "Train Epoche: 2 [1099/96 (1145%)]\tLoss: 6.909651\n",
      "Train Epoche: 2 [1100/96 (1146%)]\tLoss: 1.043450\n",
      "Train Epoche: 2 [1101/96 (1147%)]\tLoss: 358.730560\n",
      "Train Epoche: 2 [1102/96 (1148%)]\tLoss: 28.101416\n",
      "Train Epoche: 2 [1103/96 (1149%)]\tLoss: 33.656830\n",
      "Train Epoche: 2 [1104/96 (1150%)]\tLoss: 19.000725\n",
      "Train Epoche: 2 [1105/96 (1151%)]\tLoss: 148.368759\n",
      "Train Epoche: 2 [1106/96 (1152%)]\tLoss: 0.215314\n",
      "Train Epoche: 2 [1107/96 (1153%)]\tLoss: 3.340617\n",
      "Train Epoche: 2 [1108/96 (1154%)]\tLoss: 0.004239\n",
      "Train Epoche: 2 [1109/96 (1155%)]\tLoss: 6.371634\n",
      "Train Epoche: 2 [1110/96 (1156%)]\tLoss: 8.645835\n",
      "Train Epoche: 2 [1111/96 (1157%)]\tLoss: 65.109840\n",
      "Train Epoche: 2 [1112/96 (1158%)]\tLoss: 0.318862\n",
      "Train Epoche: 2 [1113/96 (1159%)]\tLoss: 379.697052\n",
      "Train Epoche: 2 [1114/96 (1160%)]\tLoss: 10.219831\n",
      "Train Epoche: 2 [1115/96 (1161%)]\tLoss: 3.332118\n",
      "Train Epoche: 2 [1116/96 (1162%)]\tLoss: 6.047449\n",
      "Train Epoche: 2 [1117/96 (1164%)]\tLoss: 3.095802\n",
      "Train Epoche: 2 [1118/96 (1165%)]\tLoss: 13.866819\n",
      "Train Epoche: 2 [1119/96 (1166%)]\tLoss: 9.009770\n",
      "Train Epoche: 2 [1120/96 (1167%)]\tLoss: 29.627918\n",
      "Train Epoche: 2 [1121/96 (1168%)]\tLoss: 42.223549\n",
      "Train Epoche: 2 [1122/96 (1169%)]\tLoss: 75.507553\n",
      "Train Epoche: 2 [1123/96 (1170%)]\tLoss: 60.278332\n",
      "Train Epoche: 2 [1124/96 (1171%)]\tLoss: 47.904102\n",
      "Train Epoche: 2 [1125/96 (1172%)]\tLoss: 1.891312\n",
      "Train Epoche: 2 [1126/96 (1173%)]\tLoss: 5.866569\n",
      "Train Epoche: 2 [1127/96 (1174%)]\tLoss: 31.018555\n",
      "Train Epoche: 2 [1128/96 (1175%)]\tLoss: 1.650067\n",
      "Train Epoche: 2 [1129/96 (1176%)]\tLoss: 8.705231\n",
      "Train Epoche: 2 [1130/96 (1177%)]\tLoss: 87.981827\n",
      "Train Epoche: 2 [1131/96 (1178%)]\tLoss: 0.014442\n",
      "Train Epoche: 2 [1132/96 (1179%)]\tLoss: 2.036139\n",
      "Train Epoche: 2 [1133/96 (1180%)]\tLoss: 0.139221\n",
      "Train Epoche: 2 [1134/96 (1181%)]\tLoss: 2.946786\n",
      "Train Epoche: 2 [1135/96 (1182%)]\tLoss: 0.719018\n",
      "Train Epoche: 2 [1136/96 (1183%)]\tLoss: 206.901398\n",
      "Train Epoche: 2 [1137/96 (1184%)]\tLoss: 0.000817\n",
      "Train Epoche: 2 [1138/96 (1185%)]\tLoss: 26.228800\n",
      "Train Epoche: 2 [1139/96 (1186%)]\tLoss: 27.420067\n",
      "Train Epoche: 2 [1140/96 (1188%)]\tLoss: 0.015710\n",
      "Train Epoche: 2 [1141/96 (1189%)]\tLoss: 1.377816\n",
      "Train Epoche: 2 [1142/96 (1190%)]\tLoss: 6.153601\n",
      "Train Epoche: 2 [1143/96 (1191%)]\tLoss: 2.238356\n",
      "Train Epoche: 2 [1144/96 (1192%)]\tLoss: 10.970230\n",
      "Train Epoche: 2 [1145/96 (1193%)]\tLoss: 3.889499\n",
      "Train Epoche: 2 [1146/96 (1194%)]\tLoss: 18.365932\n",
      "Train Epoche: 2 [1147/96 (1195%)]\tLoss: 1.667721\n",
      "Train Epoche: 2 [1148/96 (1196%)]\tLoss: 13.927151\n",
      "Train Epoche: 2 [1149/96 (1197%)]\tLoss: 5.210465\n",
      "Train Epoche: 2 [1150/96 (1198%)]\tLoss: 5.541427\n",
      "Train Epoche: 2 [1151/96 (1199%)]\tLoss: 5.723044\n",
      "Train Epoche: 2 [1152/96 (1200%)]\tLoss: 0.000017\n",
      "Train Epoche: 2 [1153/96 (1201%)]\tLoss: 2.257577\n",
      "Train Epoche: 2 [1154/96 (1202%)]\tLoss: 0.459407\n",
      "Train Epoche: 2 [1155/96 (1203%)]\tLoss: 0.038679\n",
      "Train Epoche: 2 [1156/96 (1204%)]\tLoss: 10.666400\n",
      "Train Epoche: 2 [1157/96 (1205%)]\tLoss: 0.290086\n",
      "Train Epoche: 2 [1158/96 (1206%)]\tLoss: 0.839156\n",
      "Train Epoche: 2 [1159/96 (1207%)]\tLoss: 18.809023\n",
      "Train Epoche: 2 [1160/96 (1208%)]\tLoss: 0.244035\n",
      "Train Epoche: 2 [1161/96 (1209%)]\tLoss: 8.419432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1162/96 (1210%)]\tLoss: 1.596541\n",
      "Train Epoche: 2 [1163/96 (1211%)]\tLoss: 103.305450\n",
      "Train Epoche: 2 [1164/96 (1212%)]\tLoss: 3.077398\n",
      "Train Epoche: 2 [1165/96 (1214%)]\tLoss: 17.833048\n",
      "Train Epoche: 2 [1166/96 (1215%)]\tLoss: 78.553947\n",
      "Train Epoche: 2 [1167/96 (1216%)]\tLoss: 5.460628\n",
      "Train Epoche: 2 [1168/96 (1217%)]\tLoss: 0.486211\n",
      "Train Epoche: 2 [1169/96 (1218%)]\tLoss: 1.630750\n",
      "Train Epoche: 2 [1170/96 (1219%)]\tLoss: 1.590455\n",
      "Train Epoche: 2 [1171/96 (1220%)]\tLoss: 0.000649\n",
      "Train Epoche: 2 [1172/96 (1221%)]\tLoss: 268.203918\n",
      "Train Epoche: 2 [1173/96 (1222%)]\tLoss: 7.768223\n",
      "Train Epoche: 2 [1174/96 (1223%)]\tLoss: 0.145900\n",
      "Train Epoche: 2 [1175/96 (1224%)]\tLoss: 11.721933\n",
      "Train Epoche: 2 [1176/96 (1225%)]\tLoss: 2.437363\n",
      "Train Epoche: 2 [1177/96 (1226%)]\tLoss: 29.381136\n",
      "Train Epoche: 2 [1178/96 (1227%)]\tLoss: 0.019346\n",
      "Train Epoche: 2 [1179/96 (1228%)]\tLoss: 1.701582\n",
      "Train Epoche: 2 [1180/96 (1229%)]\tLoss: 1.388232\n",
      "Train Epoche: 2 [1181/96 (1230%)]\tLoss: 4.391216\n",
      "Train Epoche: 2 [1182/96 (1231%)]\tLoss: 5.000852\n",
      "Train Epoche: 2 [1183/96 (1232%)]\tLoss: 6.620608\n",
      "Train Epoche: 2 [1184/96 (1233%)]\tLoss: 9.908174\n",
      "Train Epoche: 2 [1185/96 (1234%)]\tLoss: 0.717070\n",
      "Train Epoche: 2 [1186/96 (1235%)]\tLoss: 6.040909\n",
      "Train Epoche: 2 [1187/96 (1236%)]\tLoss: 0.020291\n",
      "Train Epoche: 2 [1188/96 (1238%)]\tLoss: 9.015227\n",
      "Train Epoche: 2 [1189/96 (1239%)]\tLoss: 1.895343\n",
      "Train Epoche: 2 [1190/96 (1240%)]\tLoss: 1.045267\n",
      "Train Epoche: 2 [1191/96 (1241%)]\tLoss: 21.337227\n",
      "Train Epoche: 2 [1192/96 (1242%)]\tLoss: 3.433442\n",
      "Train Epoche: 2 [1193/96 (1243%)]\tLoss: 3.852517\n",
      "Train Epoche: 2 [1194/96 (1244%)]\tLoss: 67.561005\n",
      "Train Epoche: 2 [1195/96 (1245%)]\tLoss: 6.199140\n",
      "Train Epoche: 2 [1196/96 (1246%)]\tLoss: 5.485469\n",
      "Train Epoche: 2 [1197/96 (1247%)]\tLoss: 0.753328\n",
      "Train Epoche: 2 [1198/96 (1248%)]\tLoss: 0.494685\n",
      "Train Epoche: 2 [1199/96 (1249%)]\tLoss: 107.672905\n",
      "Train Epoche: 2 [1200/96 (1250%)]\tLoss: 34.824852\n",
      "Train Epoche: 2 [1201/96 (1251%)]\tLoss: 1.200636\n",
      "Train Epoche: 2 [1202/96 (1252%)]\tLoss: 1.422441\n",
      "Train Epoche: 2 [1203/96 (1253%)]\tLoss: 9.806043\n",
      "Train Epoche: 2 [1204/96 (1254%)]\tLoss: 7.602279\n",
      "Train Epoche: 2 [1205/96 (1255%)]\tLoss: 22.608530\n",
      "Train Epoche: 2 [1206/96 (1256%)]\tLoss: 5.315858\n",
      "Train Epoche: 2 [1207/96 (1257%)]\tLoss: 5.903428\n",
      "Train Epoche: 2 [1208/96 (1258%)]\tLoss: 18.399616\n",
      "Train Epoche: 2 [1209/96 (1259%)]\tLoss: 9.461642\n",
      "Train Epoche: 2 [1210/96 (1260%)]\tLoss: 5.240868\n",
      "Train Epoche: 2 [1211/96 (1261%)]\tLoss: 1.830861\n",
      "Train Epoche: 2 [1212/96 (1262%)]\tLoss: 33.383320\n",
      "Train Epoche: 2 [1213/96 (1264%)]\tLoss: 1.226157\n",
      "Train Epoche: 2 [1214/96 (1265%)]\tLoss: 1.401994\n",
      "Train Epoche: 2 [1215/96 (1266%)]\tLoss: 89.414085\n",
      "Train Epoche: 2 [1216/96 (1267%)]\tLoss: 0.130446\n",
      "Train Epoche: 2 [1217/96 (1268%)]\tLoss: 8.161884\n",
      "Train Epoche: 2 [1218/96 (1269%)]\tLoss: 0.050204\n",
      "Train Epoche: 2 [1219/96 (1270%)]\tLoss: 2.755631\n",
      "Train Epoche: 2 [1220/96 (1271%)]\tLoss: 13.365968\n",
      "Train Epoche: 2 [1221/96 (1272%)]\tLoss: 2.956684\n",
      "Train Epoche: 2 [1222/96 (1273%)]\tLoss: 9.146711\n",
      "Train Epoche: 2 [1223/96 (1274%)]\tLoss: 27.713320\n",
      "Train Epoche: 2 [1224/96 (1275%)]\tLoss: 4.919987\n",
      "Train Epoche: 2 [1225/96 (1276%)]\tLoss: 0.144453\n",
      "Train Epoche: 2 [1226/96 (1277%)]\tLoss: 17.015728\n",
      "Train Epoche: 2 [1227/96 (1278%)]\tLoss: 44.181103\n",
      "Train Epoche: 2 [1228/96 (1279%)]\tLoss: 33.600754\n",
      "Train Epoche: 2 [1229/96 (1280%)]\tLoss: 185.287643\n",
      "Train Epoche: 2 [1230/96 (1281%)]\tLoss: 24.260370\n",
      "Train Epoche: 2 [1231/96 (1282%)]\tLoss: 56.511372\n",
      "Train Epoche: 2 [1232/96 (1283%)]\tLoss: 3.034923\n",
      "Train Epoche: 2 [1233/96 (1284%)]\tLoss: 167.563110\n",
      "Train Epoche: 2 [1234/96 (1285%)]\tLoss: 2.060694\n",
      "Train Epoche: 2 [1235/96 (1286%)]\tLoss: 14.258652\n",
      "Train Epoche: 2 [1236/96 (1288%)]\tLoss: 20.986917\n",
      "Train Epoche: 2 [1237/96 (1289%)]\tLoss: 7.823282\n",
      "Train Epoche: 2 [1238/96 (1290%)]\tLoss: 24.634136\n",
      "Train Epoche: 2 [1239/96 (1291%)]\tLoss: 5.039669\n",
      "Train Epoche: 2 [1240/96 (1292%)]\tLoss: 42.930256\n",
      "Train Epoche: 2 [1241/96 (1293%)]\tLoss: 200.873886\n",
      "Train Epoche: 2 [1242/96 (1294%)]\tLoss: 123.085930\n",
      "Train Epoche: 2 [1243/96 (1295%)]\tLoss: 2.410273\n",
      "Train Epoche: 2 [1244/96 (1296%)]\tLoss: 14.304560\n",
      "Train Epoche: 2 [1245/96 (1297%)]\tLoss: 19.570171\n",
      "Train Epoche: 2 [1246/96 (1298%)]\tLoss: 34.645218\n",
      "Train Epoche: 2 [1247/96 (1299%)]\tLoss: 4.247169\n",
      "Train Epoche: 2 [1248/96 (1300%)]\tLoss: 3.443702\n",
      "Train Epoche: 2 [1249/96 (1301%)]\tLoss: 150.324493\n",
      "Train Epoche: 2 [1250/96 (1302%)]\tLoss: 1.376080\n",
      "Train Epoche: 2 [1251/96 (1303%)]\tLoss: 6.968680\n",
      "Train Epoche: 2 [1252/96 (1304%)]\tLoss: 0.169498\n",
      "Train Epoche: 2 [1253/96 (1305%)]\tLoss: 0.791208\n",
      "Train Epoche: 2 [1254/96 (1306%)]\tLoss: 17.891819\n",
      "Train Epoche: 2 [1255/96 (1307%)]\tLoss: 4.656853\n",
      "Train Epoche: 2 [1256/96 (1308%)]\tLoss: 2.333658\n",
      "Train Epoche: 2 [1257/96 (1309%)]\tLoss: 66.490036\n",
      "Train Epoche: 2 [1258/96 (1310%)]\tLoss: 31.340652\n",
      "Train Epoche: 2 [1259/96 (1311%)]\tLoss: 12.753956\n",
      "Train Epoche: 2 [1260/96 (1312%)]\tLoss: 100.017319\n",
      "Train Epoche: 2 [1261/96 (1314%)]\tLoss: 180.271683\n",
      "Train Epoche: 2 [1262/96 (1315%)]\tLoss: 66.928146\n",
      "Train Epoche: 2 [1263/96 (1316%)]\tLoss: 48.827251\n",
      "Train Epoche: 2 [1264/96 (1317%)]\tLoss: 0.195264\n",
      "Train Epoche: 2 [1265/96 (1318%)]\tLoss: 20.491152\n",
      "Train Epoche: 2 [1266/96 (1319%)]\tLoss: 30.528202\n",
      "Train Epoche: 2 [1267/96 (1320%)]\tLoss: 4.247441\n",
      "Train Epoche: 2 [1268/96 (1321%)]\tLoss: 3.279765\n",
      "Train Epoche: 2 [1269/96 (1322%)]\tLoss: 4.079932\n",
      "Train Epoche: 2 [1270/96 (1323%)]\tLoss: 86.725945\n",
      "Train Epoche: 2 [1271/96 (1324%)]\tLoss: 0.093397\n",
      "Train Epoche: 2 [1272/96 (1325%)]\tLoss: 1.736535\n",
      "Train Epoche: 2 [1273/96 (1326%)]\tLoss: 0.345367\n",
      "Train Epoche: 2 [1274/96 (1327%)]\tLoss: 2.191830\n",
      "Train Epoche: 2 [1275/96 (1328%)]\tLoss: 117.676529\n",
      "Train Epoche: 2 [1276/96 (1329%)]\tLoss: 4.764992\n",
      "Train Epoche: 2 [1277/96 (1330%)]\tLoss: 3.582630\n",
      "Train Epoche: 2 [1278/96 (1331%)]\tLoss: 1.244609\n",
      "Train Epoche: 2 [1279/96 (1332%)]\tLoss: 0.408776\n",
      "Train Epoche: 2 [1280/96 (1333%)]\tLoss: 15.439925\n",
      "Train Epoche: 2 [1281/96 (1334%)]\tLoss: 16.583561\n",
      "Train Epoche: 2 [1282/96 (1335%)]\tLoss: 94.037109\n",
      "Train Epoche: 2 [1283/96 (1336%)]\tLoss: 76.751503\n",
      "Train Epoche: 2 [1284/96 (1338%)]\tLoss: 45.540951\n",
      "Train Epoche: 2 [1285/96 (1339%)]\tLoss: 43.601810\n",
      "Train Epoche: 2 [1286/96 (1340%)]\tLoss: 16.039362\n",
      "Train Epoche: 2 [1287/96 (1341%)]\tLoss: 22.378128\n",
      "Train Epoche: 2 [1288/96 (1342%)]\tLoss: 20.360477\n",
      "Train Epoche: 2 [1289/96 (1343%)]\tLoss: 57.194218\n",
      "Train Epoche: 2 [1290/96 (1344%)]\tLoss: 0.837445\n",
      "Train Epoche: 2 [1291/96 (1345%)]\tLoss: 59.640892\n",
      "Train Epoche: 2 [1292/96 (1346%)]\tLoss: 1.963632\n",
      "Train Epoche: 2 [1293/96 (1347%)]\tLoss: 0.883739\n",
      "Train Epoche: 2 [1294/96 (1348%)]\tLoss: 67.519135\n",
      "Train Epoche: 2 [1295/96 (1349%)]\tLoss: 86.078590\n",
      "Train Epoche: 2 [1296/96 (1350%)]\tLoss: 49.162727\n",
      "Train Epoche: 2 [1297/96 (1351%)]\tLoss: 2.361622\n",
      "Train Epoche: 2 [1298/96 (1352%)]\tLoss: 273.880890\n",
      "Train Epoche: 2 [1299/96 (1353%)]\tLoss: 108.755409\n",
      "Train Epoche: 2 [1300/96 (1354%)]\tLoss: 81.845863\n",
      "Train Epoche: 2 [1301/96 (1355%)]\tLoss: 0.073409\n",
      "Train Epoche: 2 [1302/96 (1356%)]\tLoss: 4.808562\n",
      "Train Epoche: 2 [1303/96 (1357%)]\tLoss: 1.958087\n",
      "Train Epoche: 2 [1304/96 (1358%)]\tLoss: 16.653202\n",
      "Train Epoche: 2 [1305/96 (1359%)]\tLoss: 2.132299\n",
      "Train Epoche: 2 [1306/96 (1360%)]\tLoss: 2.371623\n",
      "Train Epoche: 2 [1307/96 (1361%)]\tLoss: 7.638220\n",
      "Train Epoche: 2 [1308/96 (1362%)]\tLoss: 37.775730\n",
      "Train Epoche: 2 [1309/96 (1364%)]\tLoss: 2.385935\n",
      "Train Epoche: 2 [1310/96 (1365%)]\tLoss: 16.238480\n",
      "Train Epoche: 2 [1311/96 (1366%)]\tLoss: 0.594451\n",
      "Train Epoche: 2 [1312/96 (1367%)]\tLoss: 28.550364\n",
      "Train Epoche: 2 [1313/96 (1368%)]\tLoss: 4.857942\n",
      "Train Epoche: 2 [1314/96 (1369%)]\tLoss: 16.445124\n",
      "Train Epoche: 2 [1315/96 (1370%)]\tLoss: 8.747610\n",
      "Train Epoche: 2 [1316/96 (1371%)]\tLoss: 0.403454\n",
      "Train Epoche: 2 [1317/96 (1372%)]\tLoss: 16.254982\n",
      "Train Epoche: 2 [1318/96 (1373%)]\tLoss: 22.942583\n",
      "Train Epoche: 2 [1319/96 (1374%)]\tLoss: 0.061475\n",
      "Train Epoche: 2 [1320/96 (1375%)]\tLoss: 4.522252\n",
      "Train Epoche: 2 [1321/96 (1376%)]\tLoss: 1.048372\n",
      "Train Epoche: 2 [1322/96 (1377%)]\tLoss: 0.195930\n",
      "Train Epoche: 2 [1323/96 (1378%)]\tLoss: 31.220333\n",
      "Train Epoche: 2 [1324/96 (1379%)]\tLoss: 3.903317\n",
      "Train Epoche: 2 [1325/96 (1380%)]\tLoss: 0.003032\n",
      "Train Epoche: 2 [1326/96 (1381%)]\tLoss: 0.967958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1327/96 (1382%)]\tLoss: 327.843231\n",
      "Train Epoche: 2 [1328/96 (1383%)]\tLoss: 24.317463\n",
      "Train Epoche: 2 [1329/96 (1384%)]\tLoss: 61.838444\n",
      "Train Epoche: 2 [1330/96 (1385%)]\tLoss: 47.151833\n",
      "Train Epoche: 2 [1331/96 (1386%)]\tLoss: 14.102948\n",
      "Train Epoche: 2 [1332/96 (1388%)]\tLoss: 2.675643\n",
      "Train Epoche: 2 [1333/96 (1389%)]\tLoss: 10.608559\n",
      "Train Epoche: 2 [1334/96 (1390%)]\tLoss: 174.995041\n",
      "Train Epoche: 2 [1335/96 (1391%)]\tLoss: 8.813278\n",
      "Train Epoche: 2 [1336/96 (1392%)]\tLoss: 19.404881\n",
      "Train Epoche: 2 [1337/96 (1393%)]\tLoss: 37.307465\n",
      "Train Epoche: 2 [1338/96 (1394%)]\tLoss: 15.470451\n",
      "Train Epoche: 2 [1339/96 (1395%)]\tLoss: 21.854441\n",
      "Train Epoche: 2 [1340/96 (1396%)]\tLoss: 9.009149\n",
      "Train Epoche: 2 [1341/96 (1397%)]\tLoss: 28.026361\n",
      "Train Epoche: 2 [1342/96 (1398%)]\tLoss: 37.708961\n",
      "Train Epoche: 2 [1343/96 (1399%)]\tLoss: 0.644561\n",
      "Train Epoche: 2 [1344/96 (1400%)]\tLoss: 0.248473\n",
      "Train Epoche: 2 [1345/96 (1401%)]\tLoss: 0.021626\n",
      "Train Epoche: 2 [1346/96 (1402%)]\tLoss: 234.567856\n",
      "Train Epoche: 2 [1347/96 (1403%)]\tLoss: 1.305718\n",
      "Train Epoche: 2 [1348/96 (1404%)]\tLoss: 0.399249\n",
      "Train Epoche: 2 [1349/96 (1405%)]\tLoss: 0.076261\n",
      "Train Epoche: 2 [1350/96 (1406%)]\tLoss: 0.169167\n",
      "Train Epoche: 2 [1351/96 (1407%)]\tLoss: 52.667843\n",
      "Train Epoche: 2 [1352/96 (1408%)]\tLoss: 0.522892\n",
      "Train Epoche: 2 [1353/96 (1409%)]\tLoss: 3.219111\n",
      "Train Epoche: 2 [1354/96 (1410%)]\tLoss: 3.697520\n",
      "Train Epoche: 2 [1355/96 (1411%)]\tLoss: 0.822148\n",
      "Train Epoche: 2 [1356/96 (1412%)]\tLoss: 0.715632\n",
      "Train Epoche: 2 [1357/96 (1414%)]\tLoss: 22.384861\n",
      "Train Epoche: 2 [1358/96 (1415%)]\tLoss: 0.669533\n",
      "Train Epoche: 2 [1359/96 (1416%)]\tLoss: 0.172383\n",
      "Train Epoche: 2 [1360/96 (1417%)]\tLoss: 0.457045\n",
      "Train Epoche: 2 [1361/96 (1418%)]\tLoss: 1.898105\n",
      "Train Epoche: 2 [1362/96 (1419%)]\tLoss: 1.241565\n",
      "Train Epoche: 2 [1363/96 (1420%)]\tLoss: 46.473160\n",
      "Train Epoche: 2 [1364/96 (1421%)]\tLoss: 0.171620\n",
      "Train Epoche: 2 [1365/96 (1422%)]\tLoss: 4.338115\n",
      "Train Epoche: 2 [1366/96 (1423%)]\tLoss: 12.397777\n",
      "Train Epoche: 2 [1367/96 (1424%)]\tLoss: 30.576183\n",
      "Train Epoche: 2 [1368/96 (1425%)]\tLoss: 1.829728\n",
      "Train Epoche: 2 [1369/96 (1426%)]\tLoss: 70.263390\n",
      "Train Epoche: 2 [1370/96 (1427%)]\tLoss: 22.064905\n",
      "Train Epoche: 2 [1371/96 (1428%)]\tLoss: 6.503041\n",
      "Train Epoche: 2 [1372/96 (1429%)]\tLoss: 19.236776\n",
      "Train Epoche: 2 [1373/96 (1430%)]\tLoss: 44.157021\n",
      "Train Epoche: 2 [1374/96 (1431%)]\tLoss: 59.797604\n",
      "Train Epoche: 2 [1375/96 (1432%)]\tLoss: 197.353943\n",
      "Train Epoche: 2 [1376/96 (1433%)]\tLoss: 7.805403\n",
      "Train Epoche: 2 [1377/96 (1434%)]\tLoss: 125.074089\n",
      "Train Epoche: 2 [1378/96 (1435%)]\tLoss: 42.082584\n",
      "Train Epoche: 2 [1379/96 (1436%)]\tLoss: 18.389284\n",
      "Train Epoche: 2 [1380/96 (1438%)]\tLoss: 170.503830\n",
      "Train Epoche: 2 [1381/96 (1439%)]\tLoss: 78.673431\n",
      "Train Epoche: 2 [1382/96 (1440%)]\tLoss: 52.125278\n",
      "Train Epoche: 2 [1383/96 (1441%)]\tLoss: 4.609388\n",
      "Train Epoche: 2 [1384/96 (1442%)]\tLoss: 12.795418\n",
      "Train Epoche: 2 [1385/96 (1443%)]\tLoss: 0.382294\n",
      "Train Epoche: 2 [1386/96 (1444%)]\tLoss: 0.154915\n",
      "Train Epoche: 2 [1387/96 (1445%)]\tLoss: 0.599144\n",
      "Train Epoche: 2 [1388/96 (1446%)]\tLoss: 0.114631\n",
      "Train Epoche: 2 [1389/96 (1447%)]\tLoss: 0.047123\n",
      "Train Epoche: 2 [1390/96 (1448%)]\tLoss: 0.001101\n",
      "Train Epoche: 2 [1391/96 (1449%)]\tLoss: 1.967486\n",
      "Train Epoche: 2 [1392/96 (1450%)]\tLoss: 0.827326\n",
      "Train Epoche: 2 [1393/96 (1451%)]\tLoss: 0.003553\n",
      "Train Epoche: 2 [1394/96 (1452%)]\tLoss: 10.478991\n",
      "Train Epoche: 2 [1395/96 (1453%)]\tLoss: 5.905833\n",
      "Train Epoche: 2 [1396/96 (1454%)]\tLoss: 20.064201\n",
      "Train Epoche: 2 [1397/96 (1455%)]\tLoss: 0.507274\n",
      "Train Epoche: 2 [1398/96 (1456%)]\tLoss: 53.320564\n",
      "Train Epoche: 2 [1399/96 (1457%)]\tLoss: 8.120839\n",
      "Train Epoche: 2 [1400/96 (1458%)]\tLoss: 123.130119\n",
      "Train Epoche: 2 [1401/96 (1459%)]\tLoss: 0.334567\n",
      "Train Epoche: 2 [1402/96 (1460%)]\tLoss: 1.970082\n",
      "Train Epoche: 2 [1403/96 (1461%)]\tLoss: 0.226658\n",
      "Train Epoche: 2 [1404/96 (1462%)]\tLoss: 1.413107\n",
      "Train Epoche: 2 [1405/96 (1464%)]\tLoss: 0.718329\n",
      "Train Epoche: 2 [1406/96 (1465%)]\tLoss: 17.011511\n",
      "Train Epoche: 2 [1407/96 (1466%)]\tLoss: 8.182211\n",
      "Train Epoche: 2 [1408/96 (1467%)]\tLoss: 3.783602\n",
      "Train Epoche: 2 [1409/96 (1468%)]\tLoss: 0.080325\n",
      "Train Epoche: 2 [1410/96 (1469%)]\tLoss: 0.933973\n",
      "Train Epoche: 2 [1411/96 (1470%)]\tLoss: 12.658092\n",
      "Train Epoche: 2 [1412/96 (1471%)]\tLoss: 33.289581\n",
      "Train Epoche: 2 [1413/96 (1472%)]\tLoss: 11.912286\n",
      "Train Epoche: 2 [1414/96 (1473%)]\tLoss: 38.361435\n",
      "Train Epoche: 2 [1415/96 (1474%)]\tLoss: 7.408984\n",
      "Train Epoche: 2 [1416/96 (1475%)]\tLoss: 0.354807\n",
      "Train Epoche: 2 [1417/96 (1476%)]\tLoss: 52.460773\n",
      "Train Epoche: 2 [1418/96 (1477%)]\tLoss: 2.207064\n",
      "Train Epoche: 2 [1419/96 (1478%)]\tLoss: 14.655322\n",
      "Train Epoche: 2 [1420/96 (1479%)]\tLoss: 23.566223\n",
      "Train Epoche: 2 [1421/96 (1480%)]\tLoss: 5.718481\n",
      "Train Epoche: 2 [1422/96 (1481%)]\tLoss: 21.436840\n",
      "Train Epoche: 2 [1423/96 (1482%)]\tLoss: 3.214385\n",
      "Train Epoche: 2 [1424/96 (1483%)]\tLoss: 10.277814\n",
      "Train Epoche: 2 [1425/96 (1484%)]\tLoss: 10.933393\n",
      "Train Epoche: 2 [1426/96 (1485%)]\tLoss: 0.542676\n",
      "Train Epoche: 2 [1427/96 (1486%)]\tLoss: 57.180515\n",
      "Train Epoche: 2 [1428/96 (1488%)]\tLoss: 20.759533\n",
      "Train Epoche: 2 [1429/96 (1489%)]\tLoss: 29.099609\n",
      "Train Epoche: 2 [1430/96 (1490%)]\tLoss: 15.060833\n",
      "Train Epoche: 2 [1431/96 (1491%)]\tLoss: 0.139081\n",
      "Train Epoche: 2 [1432/96 (1492%)]\tLoss: 1.536775\n",
      "Train Epoche: 2 [1433/96 (1493%)]\tLoss: 5.638530\n",
      "Train Epoche: 2 [1434/96 (1494%)]\tLoss: 14.671273\n",
      "Train Epoche: 2 [1435/96 (1495%)]\tLoss: 35.093208\n",
      "Train Epoche: 2 [1436/96 (1496%)]\tLoss: 143.897919\n",
      "Train Epoche: 2 [1437/96 (1497%)]\tLoss: 220.114014\n",
      "Train Epoche: 2 [1438/96 (1498%)]\tLoss: 0.056001\n",
      "Train Epoche: 2 [1439/96 (1499%)]\tLoss: 0.812370\n",
      "Train Epoche: 2 [1440/96 (1500%)]\tLoss: 0.237225\n",
      "Train Epoche: 2 [1441/96 (1501%)]\tLoss: 0.129628\n",
      "Train Epoche: 2 [1442/96 (1502%)]\tLoss: 0.640681\n",
      "Train Epoche: 2 [1443/96 (1503%)]\tLoss: 70.367592\n",
      "Train Epoche: 2 [1444/96 (1504%)]\tLoss: 3.841940\n",
      "Train Epoche: 2 [1445/96 (1505%)]\tLoss: 3.757934\n",
      "Train Epoche: 2 [1446/96 (1506%)]\tLoss: 3.397783\n",
      "Train Epoche: 2 [1447/96 (1507%)]\tLoss: 3.636247\n",
      "Train Epoche: 2 [1448/96 (1508%)]\tLoss: 0.519237\n",
      "Train Epoche: 2 [1449/96 (1509%)]\tLoss: 1.212289\n",
      "Train Epoche: 2 [1450/96 (1510%)]\tLoss: 0.089965\n",
      "Train Epoche: 2 [1451/96 (1511%)]\tLoss: 2.770806\n",
      "Train Epoche: 2 [1452/96 (1512%)]\tLoss: 7.431070\n",
      "Train Epoche: 2 [1453/96 (1514%)]\tLoss: 165.297043\n",
      "Train Epoche: 2 [1454/96 (1515%)]\tLoss: 8.428144\n",
      "Train Epoche: 2 [1455/96 (1516%)]\tLoss: 11.567868\n",
      "Train Epoche: 2 [1456/96 (1517%)]\tLoss: 14.748363\n",
      "Train Epoche: 2 [1457/96 (1518%)]\tLoss: 32.280346\n",
      "Train Epoche: 2 [1458/96 (1519%)]\tLoss: 1.296291\n",
      "Train Epoche: 2 [1459/96 (1520%)]\tLoss: 9.883388\n",
      "Train Epoche: 2 [1460/96 (1521%)]\tLoss: 2.792682\n",
      "Train Epoche: 2 [1461/96 (1522%)]\tLoss: 30.185015\n",
      "Train Epoche: 2 [1462/96 (1523%)]\tLoss: 91.527000\n",
      "Train Epoche: 2 [1463/96 (1524%)]\tLoss: 3.387821\n",
      "Train Epoche: 2 [1464/96 (1525%)]\tLoss: 306.432800\n",
      "Train Epoche: 2 [1465/96 (1526%)]\tLoss: 29.388725\n",
      "Train Epoche: 2 [1466/96 (1527%)]\tLoss: 0.000386\n",
      "Train Epoche: 2 [1467/96 (1528%)]\tLoss: 7.742483\n",
      "Train Epoche: 2 [1468/96 (1529%)]\tLoss: 0.902375\n",
      "Train Epoche: 2 [1469/96 (1530%)]\tLoss: 2.095638\n",
      "Train Epoche: 2 [1470/96 (1531%)]\tLoss: 69.035820\n",
      "Train Epoche: 2 [1471/96 (1532%)]\tLoss: 18.592491\n",
      "Train Epoche: 2 [1472/96 (1533%)]\tLoss: 20.460051\n",
      "Train Epoche: 2 [1473/96 (1534%)]\tLoss: 0.340342\n",
      "Train Epoche: 2 [1474/96 (1535%)]\tLoss: 8.288507\n",
      "Train Epoche: 2 [1475/96 (1536%)]\tLoss: 4.976165\n",
      "Train Epoche: 2 [1476/96 (1538%)]\tLoss: 7.646456\n",
      "Train Epoche: 2 [1477/96 (1539%)]\tLoss: 16.558529\n",
      "Train Epoche: 2 [1478/96 (1540%)]\tLoss: 2.810062\n",
      "Train Epoche: 2 [1479/96 (1541%)]\tLoss: 2.798611\n",
      "Train Epoche: 2 [1480/96 (1542%)]\tLoss: 10.475554\n",
      "Train Epoche: 2 [1481/96 (1543%)]\tLoss: 41.218288\n",
      "Train Epoche: 2 [1482/96 (1544%)]\tLoss: 0.183829\n",
      "Train Epoche: 2 [1483/96 (1545%)]\tLoss: 1.838826\n",
      "Train Epoche: 2 [1484/96 (1546%)]\tLoss: 23.374376\n",
      "Train Epoche: 2 [1485/96 (1547%)]\tLoss: 11.094642\n",
      "Train Epoche: 2 [1486/96 (1548%)]\tLoss: 302.021332\n",
      "Train Epoche: 2 [1487/96 (1549%)]\tLoss: 3.243212\n",
      "Train Epoche: 2 [1488/96 (1550%)]\tLoss: 1.097110\n",
      "Train Epoche: 2 [1489/96 (1551%)]\tLoss: 6.700821\n",
      "Train Epoche: 2 [1490/96 (1552%)]\tLoss: 4.811177\n",
      "Train Epoche: 2 [1491/96 (1553%)]\tLoss: 29.112461\n",
      "Train Epoche: 2 [1492/96 (1554%)]\tLoss: 47.518322\n",
      "Train Epoche: 2 [1493/96 (1555%)]\tLoss: 27.184134\n",
      "Train Epoche: 2 [1494/96 (1556%)]\tLoss: 11.326523\n",
      "Train Epoche: 2 [1495/96 (1557%)]\tLoss: 18.585337\n",
      "Train Epoche: 2 [1496/96 (1558%)]\tLoss: 100.311005\n",
      "Train Epoche: 2 [1497/96 (1559%)]\tLoss: 12.190192\n",
      "Train Epoche: 2 [1498/96 (1560%)]\tLoss: 8.016745\n",
      "Train Epoche: 2 [1499/96 (1561%)]\tLoss: 0.137572\n",
      "Train Epoche: 2 [1500/96 (1562%)]\tLoss: 78.465683\n",
      "Train Epoche: 2 [1501/96 (1564%)]\tLoss: 13.131548\n",
      "Train Epoche: 2 [1502/96 (1565%)]\tLoss: 72.518387\n",
      "Train Epoche: 2 [1503/96 (1566%)]\tLoss: 0.712942\n",
      "Train Epoche: 2 [1504/96 (1567%)]\tLoss: 43.400856\n",
      "Train Epoche: 2 [1505/96 (1568%)]\tLoss: 30.235945\n",
      "Train Epoche: 2 [1506/96 (1569%)]\tLoss: 342.863464\n",
      "Train Epoche: 2 [1507/96 (1570%)]\tLoss: 0.400924\n",
      "Train Epoche: 2 [1508/96 (1571%)]\tLoss: 0.030487\n",
      "Train Epoche: 2 [1509/96 (1572%)]\tLoss: 45.325645\n",
      "Train Epoche: 2 [1510/96 (1573%)]\tLoss: 8.808928\n",
      "Train Epoche: 2 [1511/96 (1574%)]\tLoss: 21.488356\n",
      "Train Epoche: 2 [1512/96 (1575%)]\tLoss: 6.108729\n",
      "Train Epoche: 2 [1513/96 (1576%)]\tLoss: 31.390388\n",
      "Train Epoche: 2 [1514/96 (1577%)]\tLoss: 145.077164\n",
      "Train Epoche: 2 [1515/96 (1578%)]\tLoss: 1.377658\n",
      "Train Epoche: 2 [1516/96 (1579%)]\tLoss: 0.565784\n",
      "Train Epoche: 2 [1517/96 (1580%)]\tLoss: 5.057654\n",
      "Train Epoche: 2 [1518/96 (1581%)]\tLoss: 29.702883\n",
      "Train Epoche: 2 [1519/96 (1582%)]\tLoss: 7.138009\n",
      "Train Epoche: 2 [1520/96 (1583%)]\tLoss: 48.442982\n",
      "Train Epoche: 2 [1521/96 (1584%)]\tLoss: 0.218881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1522/96 (1585%)]\tLoss: 7.709433\n",
      "Train Epoche: 2 [1523/96 (1586%)]\tLoss: 19.955627\n",
      "Train Epoche: 2 [1524/96 (1588%)]\tLoss: 1.880675\n",
      "Train Epoche: 2 [1525/96 (1589%)]\tLoss: 4.476428\n",
      "Train Epoche: 2 [1526/96 (1590%)]\tLoss: 11.967905\n",
      "Train Epoche: 2 [1527/96 (1591%)]\tLoss: 1.864091\n",
      "Train Epoche: 2 [1528/96 (1592%)]\tLoss: 44.545734\n",
      "Train Epoche: 2 [1529/96 (1593%)]\tLoss: 208.206985\n",
      "Train Epoche: 2 [1530/96 (1594%)]\tLoss: 16.885826\n",
      "Train Epoche: 2 [1531/96 (1595%)]\tLoss: 9.637925\n",
      "Train Epoche: 2 [1532/96 (1596%)]\tLoss: 2.002662\n",
      "Train Epoche: 2 [1533/96 (1597%)]\tLoss: 54.762154\n",
      "Train Epoche: 2 [1534/96 (1598%)]\tLoss: 57.229393\n",
      "Train Epoche: 2 [1535/96 (1599%)]\tLoss: 2.204541\n",
      "Train Epoche: 2 [1536/96 (1600%)]\tLoss: 7.829633\n",
      "Train Epoche: 2 [1537/96 (1601%)]\tLoss: 9.883081\n",
      "Train Epoche: 2 [1538/96 (1602%)]\tLoss: 23.896633\n",
      "Train Epoche: 2 [1539/96 (1603%)]\tLoss: 0.008714\n",
      "Train Epoche: 2 [1540/96 (1604%)]\tLoss: 0.020483\n",
      "Train Epoche: 2 [1541/96 (1605%)]\tLoss: 400.602112\n",
      "Train Epoche: 2 [1542/96 (1606%)]\tLoss: 47.862000\n",
      "Train Epoche: 2 [1543/96 (1607%)]\tLoss: 82.208611\n",
      "Train Epoche: 2 [1544/96 (1608%)]\tLoss: 0.773809\n",
      "Train Epoche: 2 [1545/96 (1609%)]\tLoss: 294.677338\n",
      "Train Epoche: 2 [1546/96 (1610%)]\tLoss: 14.899840\n",
      "Train Epoche: 2 [1547/96 (1611%)]\tLoss: 19.884651\n",
      "Train Epoche: 2 [1548/96 (1612%)]\tLoss: 0.107517\n",
      "Train Epoche: 2 [1549/96 (1614%)]\tLoss: 18.965548\n",
      "Train Epoche: 2 [1550/96 (1615%)]\tLoss: 18.838903\n",
      "Train Epoche: 2 [1551/96 (1616%)]\tLoss: 0.188063\n",
      "Train Epoche: 2 [1552/96 (1617%)]\tLoss: 2.016983\n",
      "Train Epoche: 2 [1553/96 (1618%)]\tLoss: 5.087224\n",
      "Train Epoche: 2 [1554/96 (1619%)]\tLoss: 65.205162\n",
      "Train Epoche: 2 [1555/96 (1620%)]\tLoss: 51.025085\n",
      "Train Epoche: 2 [1556/96 (1621%)]\tLoss: 1.237582\n",
      "Train Epoche: 2 [1557/96 (1622%)]\tLoss: 5.320512\n",
      "Train Epoche: 2 [1558/96 (1623%)]\tLoss: 16.971312\n",
      "Train Epoche: 2 [1559/96 (1624%)]\tLoss: 2.862665\n",
      "Train Epoche: 2 [1560/96 (1625%)]\tLoss: 100.580391\n",
      "Train Epoche: 2 [1561/96 (1626%)]\tLoss: 6.627663\n",
      "Train Epoche: 2 [1562/96 (1627%)]\tLoss: 0.244988\n",
      "Train Epoche: 2 [1563/96 (1628%)]\tLoss: 0.860041\n",
      "Train Epoche: 2 [1564/96 (1629%)]\tLoss: 4.919687\n",
      "Train Epoche: 2 [1565/96 (1630%)]\tLoss: 7.349084\n",
      "Train Epoche: 2 [1566/96 (1631%)]\tLoss: 8.159057\n",
      "Train Epoche: 2 [1567/96 (1632%)]\tLoss: 7.363230\n",
      "Train Epoche: 2 [1568/96 (1633%)]\tLoss: 0.038672\n",
      "Train Epoche: 2 [1569/96 (1634%)]\tLoss: 0.006979\n",
      "Train Epoche: 2 [1570/96 (1635%)]\tLoss: 0.404959\n",
      "Train Epoche: 2 [1571/96 (1636%)]\tLoss: 1.703546\n",
      "Train Epoche: 2 [1572/96 (1638%)]\tLoss: 154.213409\n",
      "Train Epoche: 2 [1573/96 (1639%)]\tLoss: 0.730221\n",
      "Train Epoche: 2 [1574/96 (1640%)]\tLoss: 345.382263\n",
      "Train Epoche: 2 [1575/96 (1641%)]\tLoss: 0.957583\n",
      "Train Epoche: 2 [1576/96 (1642%)]\tLoss: 169.376328\n",
      "Train Epoche: 2 [1577/96 (1643%)]\tLoss: 273.540344\n",
      "Train Epoche: 2 [1578/96 (1644%)]\tLoss: 4.007228\n",
      "Train Epoche: 2 [1579/96 (1645%)]\tLoss: 0.082159\n",
      "Train Epoche: 2 [1580/96 (1646%)]\tLoss: 0.002852\n",
      "Train Epoche: 2 [1581/96 (1647%)]\tLoss: 20.603886\n",
      "Train Epoche: 2 [1582/96 (1648%)]\tLoss: 0.367260\n",
      "Train Epoche: 2 [1583/96 (1649%)]\tLoss: 30.315475\n",
      "Train Epoche: 2 [1584/96 (1650%)]\tLoss: 0.492255\n",
      "Train Epoche: 2 [1585/96 (1651%)]\tLoss: 20.698067\n",
      "Train Epoche: 2 [1586/96 (1652%)]\tLoss: 3.585799\n",
      "Train Epoche: 2 [1587/96 (1653%)]\tLoss: 11.212422\n",
      "Train Epoche: 2 [1588/96 (1654%)]\tLoss: 6.979666\n",
      "Train Epoche: 2 [1589/96 (1655%)]\tLoss: 1.137771\n",
      "Train Epoche: 2 [1590/96 (1656%)]\tLoss: 0.008103\n",
      "Train Epoche: 2 [1591/96 (1657%)]\tLoss: 165.051636\n",
      "Train Epoche: 2 [1592/96 (1658%)]\tLoss: 17.514181\n",
      "Train Epoche: 2 [1593/96 (1659%)]\tLoss: 0.071173\n",
      "Train Epoche: 2 [1594/96 (1660%)]\tLoss: 39.509613\n",
      "Train Epoche: 2 [1595/96 (1661%)]\tLoss: 14.007843\n",
      "Train Epoche: 2 [1596/96 (1662%)]\tLoss: 0.295000\n",
      "Train Epoche: 2 [1597/96 (1664%)]\tLoss: 0.009718\n",
      "Train Epoche: 2 [1598/96 (1665%)]\tLoss: 3.795074\n",
      "Train Epoche: 2 [1599/96 (1666%)]\tLoss: 15.772368\n",
      "Train Epoche: 2 [1600/96 (1667%)]\tLoss: 8.081730\n",
      "Train Epoche: 2 [1601/96 (1668%)]\tLoss: 10.610410\n",
      "Train Epoche: 2 [1602/96 (1669%)]\tLoss: 38.343243\n",
      "Train Epoche: 2 [1603/96 (1670%)]\tLoss: 23.041801\n",
      "Train Epoche: 2 [1604/96 (1671%)]\tLoss: 2.433176\n",
      "Train Epoche: 2 [1605/96 (1672%)]\tLoss: 23.879480\n",
      "Train Epoche: 2 [1606/96 (1673%)]\tLoss: 7.573493\n",
      "Train Epoche: 2 [1607/96 (1674%)]\tLoss: 0.002729\n",
      "Train Epoche: 2 [1608/96 (1675%)]\tLoss: 21.117630\n",
      "Train Epoche: 2 [1609/96 (1676%)]\tLoss: 7.654369\n",
      "Train Epoche: 2 [1610/96 (1677%)]\tLoss: 0.624596\n",
      "Train Epoche: 2 [1611/96 (1678%)]\tLoss: 11.204535\n",
      "Train Epoche: 2 [1612/96 (1679%)]\tLoss: 162.128143\n",
      "Train Epoche: 2 [1613/96 (1680%)]\tLoss: 15.418918\n",
      "Train Epoche: 2 [1614/96 (1681%)]\tLoss: 1.836326\n",
      "Train Epoche: 2 [1615/96 (1682%)]\tLoss: 3.283691\n",
      "Train Epoche: 2 [1616/96 (1683%)]\tLoss: 0.002984\n",
      "Train Epoche: 2 [1617/96 (1684%)]\tLoss: 0.002740\n",
      "Train Epoche: 2 [1618/96 (1685%)]\tLoss: 0.345737\n",
      "Train Epoche: 2 [1619/96 (1686%)]\tLoss: 10.685788\n",
      "Train Epoche: 2 [1620/96 (1688%)]\tLoss: 3.950718\n",
      "Train Epoche: 2 [1621/96 (1689%)]\tLoss: 5.157120\n",
      "Train Epoche: 2 [1622/96 (1690%)]\tLoss: 0.663365\n",
      "Train Epoche: 2 [1623/96 (1691%)]\tLoss: 17.851540\n",
      "Train Epoche: 2 [1624/96 (1692%)]\tLoss: 1.922037\n",
      "Train Epoche: 2 [1625/96 (1693%)]\tLoss: 8.069826\n",
      "Train Epoche: 2 [1626/96 (1694%)]\tLoss: 0.631055\n",
      "Train Epoche: 2 [1627/96 (1695%)]\tLoss: 23.381865\n",
      "Train Epoche: 2 [1628/96 (1696%)]\tLoss: 0.347188\n",
      "Train Epoche: 2 [1629/96 (1697%)]\tLoss: 0.321956\n",
      "Train Epoche: 2 [1630/96 (1698%)]\tLoss: 1.235546\n",
      "Train Epoche: 2 [1631/96 (1699%)]\tLoss: 0.092826\n",
      "Train Epoche: 2 [1632/96 (1700%)]\tLoss: 11.455748\n",
      "Train Epoche: 2 [1633/96 (1701%)]\tLoss: 0.335115\n",
      "Train Epoche: 2 [1634/96 (1702%)]\tLoss: 10.821796\n",
      "Train Epoche: 2 [1635/96 (1703%)]\tLoss: 1.597274\n",
      "Train Epoche: 2 [1636/96 (1704%)]\tLoss: 8.911835\n",
      "Train Epoche: 2 [1637/96 (1705%)]\tLoss: 1.837624\n",
      "Train Epoche: 2 [1638/96 (1706%)]\tLoss: 4.877112\n",
      "Train Epoche: 2 [1639/96 (1707%)]\tLoss: 0.533483\n",
      "Train Epoche: 2 [1640/96 (1708%)]\tLoss: 255.586044\n",
      "Train Epoche: 2 [1641/96 (1709%)]\tLoss: 16.204561\n",
      "Train Epoche: 2 [1642/96 (1710%)]\tLoss: 0.040052\n",
      "Train Epoche: 2 [1643/96 (1711%)]\tLoss: 11.322531\n",
      "Train Epoche: 2 [1644/96 (1712%)]\tLoss: 14.164888\n",
      "Train Epoche: 2 [1645/96 (1714%)]\tLoss: 4.889831\n",
      "Train Epoche: 2 [1646/96 (1715%)]\tLoss: 3.962096\n",
      "Train Epoche: 2 [1647/96 (1716%)]\tLoss: 76.688721\n",
      "Train Epoche: 2 [1648/96 (1717%)]\tLoss: 3.149774\n",
      "Train Epoche: 2 [1649/96 (1718%)]\tLoss: 57.767368\n",
      "Train Epoche: 2 [1650/96 (1719%)]\tLoss: 75.181572\n",
      "Train Epoche: 2 [1651/96 (1720%)]\tLoss: 13.991272\n",
      "Train Epoche: 2 [1652/96 (1721%)]\tLoss: 5.235669\n",
      "Train Epoche: 2 [1653/96 (1722%)]\tLoss: 27.202503\n",
      "Train Epoche: 2 [1654/96 (1723%)]\tLoss: 0.000548\n",
      "Train Epoche: 2 [1655/96 (1724%)]\tLoss: 10.344751\n",
      "Train Epoche: 2 [1656/96 (1725%)]\tLoss: 4.800993\n",
      "Train Epoche: 2 [1657/96 (1726%)]\tLoss: 7.232953\n",
      "Train Epoche: 2 [1658/96 (1727%)]\tLoss: 0.002737\n",
      "Train Epoche: 2 [1659/96 (1728%)]\tLoss: 0.631079\n",
      "Train Epoche: 2 [1660/96 (1729%)]\tLoss: 0.541102\n",
      "Train Epoche: 2 [1661/96 (1730%)]\tLoss: 0.084393\n",
      "Train Epoche: 2 [1662/96 (1731%)]\tLoss: 23.602339\n",
      "Train Epoche: 2 [1663/96 (1732%)]\tLoss: 24.867254\n",
      "Train Epoche: 2 [1664/96 (1733%)]\tLoss: 6.952537\n",
      "Train Epoche: 2 [1665/96 (1734%)]\tLoss: 4.971097\n",
      "Train Epoche: 2 [1666/96 (1735%)]\tLoss: 15.142173\n",
      "Train Epoche: 2 [1667/96 (1736%)]\tLoss: 4.348319\n",
      "Train Epoche: 2 [1668/96 (1738%)]\tLoss: 5.938190\n",
      "Train Epoche: 2 [1669/96 (1739%)]\tLoss: 28.110922\n",
      "Train Epoche: 2 [1670/96 (1740%)]\tLoss: 14.195069\n",
      "Train Epoche: 2 [1671/96 (1741%)]\tLoss: 1.436900\n",
      "Train Epoche: 2 [1672/96 (1742%)]\tLoss: 13.197867\n",
      "Train Epoche: 2 [1673/96 (1743%)]\tLoss: 29.843466\n",
      "Train Epoche: 2 [1674/96 (1744%)]\tLoss: 0.173021\n",
      "Train Epoche: 2 [1675/96 (1745%)]\tLoss: 10.180521\n",
      "Train Epoche: 2 [1676/96 (1746%)]\tLoss: 11.933687\n",
      "Train Epoche: 2 [1677/96 (1747%)]\tLoss: 95.770912\n",
      "Train Epoche: 2 [1678/96 (1748%)]\tLoss: 2.978351\n",
      "Train Epoche: 2 [1679/96 (1749%)]\tLoss: 13.317930\n",
      "Train Epoche: 2 [1680/96 (1750%)]\tLoss: 23.965363\n",
      "Train Epoche: 2 [1681/96 (1751%)]\tLoss: 68.305672\n",
      "Train Epoche: 2 [1682/96 (1752%)]\tLoss: 0.842396\n",
      "Train Epoche: 2 [1683/96 (1753%)]\tLoss: 7.535419\n",
      "Train Epoche: 2 [1684/96 (1754%)]\tLoss: 0.386208\n",
      "Train Epoche: 2 [1685/96 (1755%)]\tLoss: 5.952030\n",
      "Train Epoche: 2 [1686/96 (1756%)]\tLoss: 0.825035\n",
      "Train Epoche: 2 [1687/96 (1757%)]\tLoss: 0.721939\n",
      "Train Epoche: 2 [1688/96 (1758%)]\tLoss: 25.525499\n",
      "Train Epoche: 2 [1689/96 (1759%)]\tLoss: 16.587061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1690/96 (1760%)]\tLoss: 4.625928\n",
      "Train Epoche: 2 [1691/96 (1761%)]\tLoss: 0.844813\n",
      "Train Epoche: 2 [1692/96 (1762%)]\tLoss: 21.826612\n",
      "Train Epoche: 2 [1693/96 (1764%)]\tLoss: 15.750129\n",
      "Train Epoche: 2 [1694/96 (1765%)]\tLoss: 5.756854\n",
      "Train Epoche: 2 [1695/96 (1766%)]\tLoss: 2.294180\n",
      "Train Epoche: 2 [1696/96 (1767%)]\tLoss: 4.939777\n",
      "Train Epoche: 2 [1697/96 (1768%)]\tLoss: 7.531329\n",
      "Train Epoche: 2 [1698/96 (1769%)]\tLoss: 11.606334\n",
      "Train Epoche: 2 [1699/96 (1770%)]\tLoss: 0.002326\n",
      "Train Epoche: 2 [1700/96 (1771%)]\tLoss: 1.233743\n",
      "Train Epoche: 2 [1701/96 (1772%)]\tLoss: 0.454993\n",
      "Train Epoche: 2 [1702/96 (1773%)]\tLoss: 3.974703\n",
      "Train Epoche: 2 [1703/96 (1774%)]\tLoss: 70.301384\n",
      "Train Epoche: 2 [1704/96 (1775%)]\tLoss: 0.302380\n",
      "Train Epoche: 2 [1705/96 (1776%)]\tLoss: 2.605043\n",
      "Train Epoche: 2 [1706/96 (1777%)]\tLoss: 0.281711\n",
      "Train Epoche: 2 [1707/96 (1778%)]\tLoss: 4.068999\n",
      "Train Epoche: 2 [1708/96 (1779%)]\tLoss: 0.367600\n",
      "Train Epoche: 2 [1709/96 (1780%)]\tLoss: 8.314590\n",
      "Train Epoche: 2 [1710/96 (1781%)]\tLoss: 2.109925\n",
      "Train Epoche: 2 [1711/96 (1782%)]\tLoss: 22.471594\n",
      "Train Epoche: 2 [1712/96 (1783%)]\tLoss: 56.022877\n",
      "Train Epoche: 2 [1713/96 (1784%)]\tLoss: 50.587402\n",
      "Train Epoche: 2 [1714/96 (1785%)]\tLoss: 8.735181\n",
      "Train Epoche: 2 [1715/96 (1786%)]\tLoss: 13.195723\n",
      "Train Epoche: 2 [1716/96 (1788%)]\tLoss: 23.452999\n",
      "Train Epoche: 2 [1717/96 (1789%)]\tLoss: 29.662844\n",
      "Train Epoche: 2 [1718/96 (1790%)]\tLoss: 1.976842\n",
      "Train Epoche: 2 [1719/96 (1791%)]\tLoss: 6.129895\n",
      "Train Epoche: 2 [1720/96 (1792%)]\tLoss: 0.646980\n",
      "Train Epoche: 2 [1721/96 (1793%)]\tLoss: 2.943002\n",
      "Train Epoche: 2 [1722/96 (1794%)]\tLoss: 22.117680\n",
      "Train Epoche: 2 [1723/96 (1795%)]\tLoss: 2.551369\n",
      "Train Epoche: 2 [1724/96 (1796%)]\tLoss: 7.575230\n",
      "Train Epoche: 2 [1725/96 (1797%)]\tLoss: 12.834173\n",
      "Train Epoche: 2 [1726/96 (1798%)]\tLoss: 1.744719\n",
      "Train Epoche: 2 [1727/96 (1799%)]\tLoss: 0.064352\n",
      "Train Epoche: 2 [1728/96 (1800%)]\tLoss: 3.062096\n",
      "Train Epoche: 2 [1729/96 (1801%)]\tLoss: 2.052911\n",
      "Train Epoche: 2 [1730/96 (1802%)]\tLoss: 15.072486\n",
      "Train Epoche: 2 [1731/96 (1803%)]\tLoss: 3.514770\n",
      "Train Epoche: 2 [1732/96 (1804%)]\tLoss: 0.968392\n",
      "Train Epoche: 2 [1733/96 (1805%)]\tLoss: 10.616247\n",
      "Train Epoche: 2 [1734/96 (1806%)]\tLoss: 3.456609\n",
      "Train Epoche: 2 [1735/96 (1807%)]\tLoss: 1.090212\n",
      "Train Epoche: 2 [1736/96 (1808%)]\tLoss: 19.595728\n",
      "Train Epoche: 2 [1737/96 (1809%)]\tLoss: 2.202421\n",
      "Train Epoche: 2 [1738/96 (1810%)]\tLoss: 3.788895\n",
      "Train Epoche: 2 [1739/96 (1811%)]\tLoss: 4.524826\n",
      "Train Epoche: 2 [1740/96 (1812%)]\tLoss: 6.313110\n",
      "Train Epoche: 2 [1741/96 (1814%)]\tLoss: 20.261648\n",
      "Train Epoche: 2 [1742/96 (1815%)]\tLoss: 1.603057\n",
      "Train Epoche: 2 [1743/96 (1816%)]\tLoss: 9.302682\n",
      "Train Epoche: 2 [1744/96 (1817%)]\tLoss: 11.381952\n",
      "Train Epoche: 2 [1745/96 (1818%)]\tLoss: 4.448141\n",
      "Train Epoche: 2 [1746/96 (1819%)]\tLoss: 9.218925\n",
      "Train Epoche: 2 [1747/96 (1820%)]\tLoss: 16.536497\n",
      "Train Epoche: 2 [1748/96 (1821%)]\tLoss: 0.000020\n",
      "Train Epoche: 2 [1749/96 (1822%)]\tLoss: 25.690142\n",
      "Train Epoche: 2 [1750/96 (1823%)]\tLoss: 0.112160\n",
      "Train Epoche: 2 [1751/96 (1824%)]\tLoss: 0.005119\n",
      "Train Epoche: 2 [1752/96 (1825%)]\tLoss: 6.720845\n",
      "Train Epoche: 2 [1753/96 (1826%)]\tLoss: 0.782404\n",
      "Train Epoche: 2 [1754/96 (1827%)]\tLoss: 15.829513\n",
      "Train Epoche: 2 [1755/96 (1828%)]\tLoss: 5.101019\n",
      "Train Epoche: 2 [1756/96 (1829%)]\tLoss: 4.774193\n",
      "Train Epoche: 2 [1757/96 (1830%)]\tLoss: 162.791107\n",
      "Train Epoche: 2 [1758/96 (1831%)]\tLoss: 118.482445\n",
      "Train Epoche: 2 [1759/96 (1832%)]\tLoss: 57.104675\n",
      "Train Epoche: 2 [1760/96 (1833%)]\tLoss: 1.821231\n",
      "Train Epoche: 2 [1761/96 (1834%)]\tLoss: 0.423704\n",
      "Train Epoche: 2 [1762/96 (1835%)]\tLoss: 0.884902\n",
      "Train Epoche: 2 [1763/96 (1836%)]\tLoss: 28.047045\n",
      "Train Epoche: 2 [1764/96 (1838%)]\tLoss: 5.038911\n",
      "Train Epoche: 2 [1765/96 (1839%)]\tLoss: 0.010250\n",
      "Train Epoche: 2 [1766/96 (1840%)]\tLoss: 57.182175\n",
      "Train Epoche: 2 [1767/96 (1841%)]\tLoss: 1.474827\n",
      "Train Epoche: 2 [1768/96 (1842%)]\tLoss: 11.052405\n",
      "Train Epoche: 2 [1769/96 (1843%)]\tLoss: 21.191196\n",
      "Train Epoche: 2 [1770/96 (1844%)]\tLoss: 46.471039\n",
      "Train Epoche: 2 [1771/96 (1845%)]\tLoss: 19.243803\n",
      "Train Epoche: 2 [1772/96 (1846%)]\tLoss: 59.717041\n",
      "Train Epoche: 2 [1773/96 (1847%)]\tLoss: 63.748966\n",
      "Train Epoche: 2 [1774/96 (1848%)]\tLoss: 122.225807\n",
      "Train Epoche: 2 [1775/96 (1849%)]\tLoss: 2.580458\n",
      "Train Epoche: 2 [1776/96 (1850%)]\tLoss: 0.883328\n",
      "Train Epoche: 2 [1777/96 (1851%)]\tLoss: 4.108885\n",
      "Train Epoche: 2 [1778/96 (1852%)]\tLoss: 48.295895\n",
      "Train Epoche: 2 [1779/96 (1853%)]\tLoss: 1.481498\n",
      "Train Epoche: 2 [1780/96 (1854%)]\tLoss: 0.091301\n",
      "Train Epoche: 2 [1781/96 (1855%)]\tLoss: 0.035263\n",
      "Train Epoche: 2 [1782/96 (1856%)]\tLoss: 36.199780\n",
      "Train Epoche: 2 [1783/96 (1857%)]\tLoss: 1.625084\n",
      "Train Epoche: 2 [1784/96 (1858%)]\tLoss: 0.851006\n",
      "Train Epoche: 2 [1785/96 (1859%)]\tLoss: 168.648804\n",
      "Train Epoche: 2 [1786/96 (1860%)]\tLoss: 2.882210\n",
      "Train Epoche: 2 [1787/96 (1861%)]\tLoss: 0.302928\n",
      "Train Epoche: 2 [1788/96 (1862%)]\tLoss: 0.777307\n",
      "Train Epoche: 2 [1789/96 (1864%)]\tLoss: 0.369782\n",
      "Train Epoche: 2 [1790/96 (1865%)]\tLoss: 7.970510\n",
      "Train Epoche: 2 [1791/96 (1866%)]\tLoss: 282.151031\n",
      "Train Epoche: 2 [1792/96 (1867%)]\tLoss: 57.174416\n",
      "Train Epoche: 2 [1793/96 (1868%)]\tLoss: 70.126549\n",
      "Train Epoche: 2 [1794/96 (1869%)]\tLoss: 2.928764\n",
      "Train Epoche: 2 [1795/96 (1870%)]\tLoss: 54.755737\n",
      "Train Epoche: 2 [1796/96 (1871%)]\tLoss: 13.276110\n",
      "Train Epoche: 2 [1797/96 (1872%)]\tLoss: 1.204409\n",
      "Train Epoche: 2 [1798/96 (1873%)]\tLoss: 1.595809\n",
      "Train Epoche: 2 [1799/96 (1874%)]\tLoss: 0.083867\n",
      "Train Epoche: 2 [1800/96 (1875%)]\tLoss: 4.410068\n",
      "Train Epoche: 2 [1801/96 (1876%)]\tLoss: 0.659012\n",
      "Train Epoche: 2 [1802/96 (1877%)]\tLoss: 0.028713\n",
      "Train Epoche: 2 [1803/96 (1878%)]\tLoss: 0.848618\n",
      "Train Epoche: 2 [1804/96 (1879%)]\tLoss: 0.459755\n",
      "Train Epoche: 2 [1805/96 (1880%)]\tLoss: 4.632453\n",
      "Train Epoche: 2 [1806/96 (1881%)]\tLoss: 0.122394\n",
      "Train Epoche: 2 [1807/96 (1882%)]\tLoss: 0.926484\n",
      "Train Epoche: 2 [1808/96 (1883%)]\tLoss: 207.341583\n",
      "Train Epoche: 2 [1809/96 (1884%)]\tLoss: 10.565321\n",
      "Train Epoche: 2 [1810/96 (1885%)]\tLoss: 17.905565\n",
      "Train Epoche: 2 [1811/96 (1886%)]\tLoss: 1.341565\n",
      "Train Epoche: 2 [1812/96 (1888%)]\tLoss: 68.393379\n",
      "Train Epoche: 2 [1813/96 (1889%)]\tLoss: 7.210073\n",
      "Train Epoche: 2 [1814/96 (1890%)]\tLoss: 4.602267\n",
      "Train Epoche: 2 [1815/96 (1891%)]\tLoss: 5.129436\n",
      "Train Epoche: 2 [1816/96 (1892%)]\tLoss: 34.104019\n",
      "Train Epoche: 2 [1817/96 (1893%)]\tLoss: 4.244934\n",
      "Train Epoche: 2 [1818/96 (1894%)]\tLoss: 0.296341\n",
      "Train Epoche: 2 [1819/96 (1895%)]\tLoss: 1.148756\n",
      "Train Epoche: 2 [1820/96 (1896%)]\tLoss: 103.927483\n",
      "Train Epoche: 2 [1821/96 (1897%)]\tLoss: 84.522804\n",
      "Train Epoche: 2 [1822/96 (1898%)]\tLoss: 33.403820\n",
      "Train Epoche: 2 [1823/96 (1899%)]\tLoss: 5.653368\n",
      "Train Epoche: 2 [1824/96 (1900%)]\tLoss: 4.268151\n",
      "Train Epoche: 2 [1825/96 (1901%)]\tLoss: 0.200050\n",
      "Train Epoche: 2 [1826/96 (1902%)]\tLoss: 1.220326\n",
      "Train Epoche: 2 [1827/96 (1903%)]\tLoss: 1.784755\n",
      "Train Epoche: 2 [1828/96 (1904%)]\tLoss: 1.085817\n",
      "Train Epoche: 2 [1829/96 (1905%)]\tLoss: 0.020623\n",
      "Train Epoche: 2 [1830/96 (1906%)]\tLoss: 0.627559\n",
      "Train Epoche: 2 [1831/96 (1907%)]\tLoss: 4.750947\n",
      "Train Epoche: 2 [1832/96 (1908%)]\tLoss: 24.379868\n",
      "Train Epoche: 2 [1833/96 (1909%)]\tLoss: 6.112209\n",
      "Train Epoche: 2 [1834/96 (1910%)]\tLoss: 19.533737\n",
      "Train Epoche: 2 [1835/96 (1911%)]\tLoss: 4.601501\n",
      "Train Epoche: 2 [1836/96 (1912%)]\tLoss: 298.154510\n",
      "Train Epoche: 2 [1837/96 (1914%)]\tLoss: 0.386194\n",
      "Train Epoche: 2 [1838/96 (1915%)]\tLoss: 2.503485\n",
      "Train Epoche: 2 [1839/96 (1916%)]\tLoss: 22.183256\n",
      "Train Epoche: 2 [1840/96 (1917%)]\tLoss: 8.987690\n",
      "Train Epoche: 2 [1841/96 (1918%)]\tLoss: 0.045051\n",
      "Train Epoche: 2 [1842/96 (1919%)]\tLoss: 1.231529\n",
      "Train Epoche: 2 [1843/96 (1920%)]\tLoss: 12.122911\n",
      "Train Epoche: 2 [1844/96 (1921%)]\tLoss: 11.237383\n",
      "Train Epoche: 2 [1845/96 (1922%)]\tLoss: 44.225159\n",
      "Train Epoche: 2 [1846/96 (1923%)]\tLoss: 6.008730\n",
      "Train Epoche: 2 [1847/96 (1924%)]\tLoss: 1.319648\n",
      "Train Epoche: 2 [1848/96 (1925%)]\tLoss: 52.790470\n",
      "Train Epoche: 2 [1849/96 (1926%)]\tLoss: 6.995513\n",
      "Train Epoche: 2 [1850/96 (1927%)]\tLoss: 2.043959\n",
      "Train Epoche: 2 [1851/96 (1928%)]\tLoss: 144.944885\n",
      "Train Epoche: 2 [1852/96 (1929%)]\tLoss: 7.607576\n",
      "Train Epoche: 2 [1853/96 (1930%)]\tLoss: 8.721229\n",
      "Train Epoche: 2 [1854/96 (1931%)]\tLoss: 9.414677\n",
      "Train Epoche: 2 [1855/96 (1932%)]\tLoss: 3.436422\n",
      "Train Epoche: 2 [1856/96 (1933%)]\tLoss: 0.492133\n",
      "Train Epoche: 2 [1857/96 (1934%)]\tLoss: 41.649494\n",
      "Train Epoche: 2 [1858/96 (1935%)]\tLoss: 39.584007\n",
      "Train Epoche: 2 [1859/96 (1936%)]\tLoss: 3.124081\n",
      "Train Epoche: 2 [1860/96 (1938%)]\tLoss: 10.302324\n",
      "Train Epoche: 2 [1861/96 (1939%)]\tLoss: 64.780701\n",
      "Train Epoche: 2 [1862/96 (1940%)]\tLoss: 11.014851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1863/96 (1941%)]\tLoss: 133.830154\n",
      "Train Epoche: 2 [1864/96 (1942%)]\tLoss: 0.059038\n",
      "Train Epoche: 2 [1865/96 (1943%)]\tLoss: 5.052285\n",
      "Train Epoche: 2 [1866/96 (1944%)]\tLoss: 2.186200\n",
      "Train Epoche: 2 [1867/96 (1945%)]\tLoss: 12.348108\n",
      "Train Epoche: 2 [1868/96 (1946%)]\tLoss: 0.005031\n",
      "Train Epoche: 2 [1869/96 (1947%)]\tLoss: 6.736536\n",
      "Train Epoche: 2 [1870/96 (1948%)]\tLoss: 21.301075\n",
      "Train Epoche: 2 [1871/96 (1949%)]\tLoss: 9.211401\n",
      "Train Epoche: 2 [1872/96 (1950%)]\tLoss: 16.664455\n",
      "Train Epoche: 2 [1873/96 (1951%)]\tLoss: 1.081229\n",
      "Train Epoche: 2 [1874/96 (1952%)]\tLoss: 15.185563\n",
      "Train Epoche: 2 [1875/96 (1953%)]\tLoss: 15.108562\n",
      "Train Epoche: 2 [1876/96 (1954%)]\tLoss: 10.619977\n",
      "Train Epoche: 2 [1877/96 (1955%)]\tLoss: 20.739281\n",
      "Train Epoche: 2 [1878/96 (1956%)]\tLoss: 6.518527\n",
      "Train Epoche: 2 [1879/96 (1957%)]\tLoss: 2.906204\n",
      "Train Epoche: 2 [1880/96 (1958%)]\tLoss: 3.911083\n",
      "Train Epoche: 2 [1881/96 (1959%)]\tLoss: 5.892288\n",
      "Train Epoche: 2 [1882/96 (1960%)]\tLoss: 1.029827\n",
      "Train Epoche: 2 [1883/96 (1961%)]\tLoss: 0.102948\n",
      "Train Epoche: 2 [1884/96 (1962%)]\tLoss: 3.781151\n",
      "Train Epoche: 2 [1885/96 (1964%)]\tLoss: 1.576982\n",
      "Train Epoche: 2 [1886/96 (1965%)]\tLoss: 2.122766\n",
      "Train Epoche: 2 [1887/96 (1966%)]\tLoss: 105.497177\n",
      "Train Epoche: 2 [1888/96 (1967%)]\tLoss: 0.957105\n",
      "Train Epoche: 2 [1889/96 (1968%)]\tLoss: 131.567535\n",
      "Train Epoche: 2 [1890/96 (1969%)]\tLoss: 17.091690\n",
      "Train Epoche: 2 [1891/96 (1970%)]\tLoss: 10.247171\n",
      "Train Epoche: 2 [1892/96 (1971%)]\tLoss: 16.469791\n",
      "Train Epoche: 2 [1893/96 (1972%)]\tLoss: 10.426090\n",
      "Train Epoche: 2 [1894/96 (1973%)]\tLoss: 4.211244\n",
      "Train Epoche: 2 [1895/96 (1974%)]\tLoss: 3.151169\n",
      "Train Epoche: 2 [1896/96 (1975%)]\tLoss: 24.227819\n",
      "Train Epoche: 2 [1897/96 (1976%)]\tLoss: 50.458363\n",
      "Train Epoche: 2 [1898/96 (1977%)]\tLoss: 61.028122\n",
      "Train Epoche: 2 [1899/96 (1978%)]\tLoss: 44.956245\n",
      "Train Epoche: 2 [1900/96 (1979%)]\tLoss: 35.465839\n",
      "Train Epoche: 2 [1901/96 (1980%)]\tLoss: 0.057736\n",
      "Train Epoche: 2 [1902/96 (1981%)]\tLoss: 0.075276\n",
      "Train Epoche: 2 [1903/96 (1982%)]\tLoss: 8.350526\n",
      "Train Epoche: 2 [1904/96 (1983%)]\tLoss: 5.492163\n",
      "Train Epoche: 2 [1905/96 (1984%)]\tLoss: 4.978190\n",
      "Train Epoche: 2 [1906/96 (1985%)]\tLoss: 4.309195\n",
      "Train Epoche: 2 [1907/96 (1986%)]\tLoss: 324.535309\n",
      "Train Epoche: 2 [1908/96 (1988%)]\tLoss: 0.066877\n",
      "Train Epoche: 2 [1909/96 (1989%)]\tLoss: 6.881415\n",
      "Train Epoche: 2 [1910/96 (1990%)]\tLoss: 0.014960\n",
      "Train Epoche: 2 [1911/96 (1991%)]\tLoss: 1.723076\n",
      "Train Epoche: 2 [1912/96 (1992%)]\tLoss: 272.560699\n",
      "Train Epoche: 2 [1913/96 (1993%)]\tLoss: 2.677834\n",
      "Train Epoche: 2 [1914/96 (1994%)]\tLoss: 16.194435\n",
      "Train Epoche: 2 [1915/96 (1995%)]\tLoss: 1.955961\n",
      "Train Epoche: 2 [1916/96 (1996%)]\tLoss: 4.628276\n",
      "Train Epoche: 2 [1917/96 (1997%)]\tLoss: 39.118034\n",
      "Train Epoche: 2 [1918/96 (1998%)]\tLoss: 3.706371\n",
      "Train Epoche: 2 [1919/96 (1999%)]\tLoss: 0.273385\n",
      "Train Epoche: 2 [1920/96 (2000%)]\tLoss: 46.901134\n",
      "Train Epoche: 2 [1921/96 (2001%)]\tLoss: 0.749321\n",
      "Train Epoche: 2 [1922/96 (2002%)]\tLoss: 1.950643\n",
      "Train Epoche: 2 [1923/96 (2003%)]\tLoss: 102.229889\n",
      "Train Epoche: 2 [1924/96 (2004%)]\tLoss: 8.216346\n",
      "Train Epoche: 2 [1925/96 (2005%)]\tLoss: 8.407432\n",
      "Train Epoche: 2 [1926/96 (2006%)]\tLoss: 12.730786\n",
      "Train Epoche: 2 [1927/96 (2007%)]\tLoss: 1.592363\n",
      "Train Epoche: 2 [1928/96 (2008%)]\tLoss: 6.833995\n",
      "Train Epoche: 2 [1929/96 (2009%)]\tLoss: 11.468539\n",
      "Train Epoche: 2 [1930/96 (2010%)]\tLoss: 2.719991\n",
      "Train Epoche: 2 [1931/96 (2011%)]\tLoss: 0.166510\n",
      "Train Epoche: 2 [1932/96 (2012%)]\tLoss: 0.567143\n",
      "Train Epoche: 2 [1933/96 (2014%)]\tLoss: 9.192929\n",
      "Train Epoche: 2 [1934/96 (2015%)]\tLoss: 121.211159\n",
      "Train Epoche: 2 [1935/96 (2016%)]\tLoss: 1.685135\n",
      "Train Epoche: 2 [1936/96 (2017%)]\tLoss: 182.021652\n",
      "Train Epoche: 2 [1937/96 (2018%)]\tLoss: 93.289139\n",
      "Train Epoche: 2 [1938/96 (2019%)]\tLoss: 0.528070\n",
      "Train Epoche: 2 [1939/96 (2020%)]\tLoss: 30.004103\n",
      "Train Epoche: 2 [1940/96 (2021%)]\tLoss: 0.080243\n",
      "Train Epoche: 2 [1941/96 (2022%)]\tLoss: 40.089703\n",
      "Train Epoche: 2 [1942/96 (2023%)]\tLoss: 8.286295\n",
      "Train Epoche: 2 [1943/96 (2024%)]\tLoss: 1.199621\n",
      "Train Epoche: 2 [1944/96 (2025%)]\tLoss: 15.284788\n",
      "Train Epoche: 2 [1945/96 (2026%)]\tLoss: 9.711882\n",
      "Train Epoche: 2 [1946/96 (2027%)]\tLoss: 85.938622\n",
      "Train Epoche: 2 [1947/96 (2028%)]\tLoss: 0.332170\n",
      "Train Epoche: 2 [1948/96 (2029%)]\tLoss: 0.000011\n",
      "Train Epoche: 2 [1949/96 (2030%)]\tLoss: 0.096046\n",
      "Train Epoche: 2 [1950/96 (2031%)]\tLoss: 1.890672\n",
      "Train Epoche: 2 [1951/96 (2032%)]\tLoss: 0.002059\n",
      "Train Epoche: 2 [1952/96 (2033%)]\tLoss: 2.140769\n",
      "Train Epoche: 2 [1953/96 (2034%)]\tLoss: 0.257411\n",
      "Train Epoche: 2 [1954/96 (2035%)]\tLoss: 0.524136\n",
      "Train Epoche: 2 [1955/96 (2036%)]\tLoss: 8.434076\n",
      "Train Epoche: 2 [1956/96 (2038%)]\tLoss: 0.806046\n",
      "Train Epoche: 2 [1957/96 (2039%)]\tLoss: 17.966181\n",
      "Train Epoche: 2 [1958/96 (2040%)]\tLoss: 17.441212\n",
      "Train Epoche: 2 [1959/96 (2041%)]\tLoss: 6.460154\n",
      "Train Epoche: 2 [1960/96 (2042%)]\tLoss: 13.135482\n",
      "Train Epoche: 2 [1961/96 (2043%)]\tLoss: 1.298653\n",
      "Train Epoche: 2 [1962/96 (2044%)]\tLoss: 3.408397\n",
      "Train Epoche: 2 [1963/96 (2045%)]\tLoss: 0.319761\n",
      "Train Epoche: 2 [1964/96 (2046%)]\tLoss: 4.095079\n",
      "Train Epoche: 2 [1965/96 (2047%)]\tLoss: 4.247456\n",
      "Train Epoche: 2 [1966/96 (2048%)]\tLoss: 0.039803\n",
      "Train Epoche: 2 [1967/96 (2049%)]\tLoss: 1.108593\n",
      "Train Epoche: 2 [1968/96 (2050%)]\tLoss: 0.314596\n",
      "Train Epoche: 2 [1969/96 (2051%)]\tLoss: 23.783081\n",
      "Train Epoche: 2 [1970/96 (2052%)]\tLoss: 7.247282\n",
      "Train Epoche: 2 [1971/96 (2053%)]\tLoss: 68.579750\n",
      "Train Epoche: 2 [1972/96 (2054%)]\tLoss: 0.526020\n",
      "Train Epoche: 2 [1973/96 (2055%)]\tLoss: 5.061032\n",
      "Train Epoche: 2 [1974/96 (2056%)]\tLoss: 0.594903\n",
      "Train Epoche: 2 [1975/96 (2057%)]\tLoss: 2.350760\n",
      "Train Epoche: 2 [1976/96 (2058%)]\tLoss: 0.441169\n",
      "Train Epoche: 2 [1977/96 (2059%)]\tLoss: 0.024388\n",
      "Train Epoche: 2 [1978/96 (2060%)]\tLoss: 2.095837\n",
      "Train Epoche: 2 [1979/96 (2061%)]\tLoss: 4.072123\n",
      "Train Epoche: 2 [1980/96 (2062%)]\tLoss: 0.964787\n",
      "Train Epoche: 2 [1981/96 (2064%)]\tLoss: 3.234923\n",
      "Train Epoche: 2 [1982/96 (2065%)]\tLoss: 5.473485\n",
      "Train Epoche: 2 [1983/96 (2066%)]\tLoss: 21.551752\n",
      "Train Epoche: 2 [1984/96 (2067%)]\tLoss: 8.184890\n",
      "Train Epoche: 2 [1985/96 (2068%)]\tLoss: 5.621977\n",
      "Train Epoche: 2 [1986/96 (2069%)]\tLoss: 1.767464\n",
      "Train Epoche: 2 [1987/96 (2070%)]\tLoss: 1.118832\n",
      "Train Epoche: 2 [1988/96 (2071%)]\tLoss: 7.473511\n",
      "Train Epoche: 2 [1989/96 (2072%)]\tLoss: 56.983856\n",
      "Train Epoche: 2 [1990/96 (2073%)]\tLoss: 0.002144\n",
      "Train Epoche: 2 [1991/96 (2074%)]\tLoss: 2.234469\n",
      "Train Epoche: 2 [1992/96 (2075%)]\tLoss: 4.709894\n",
      "Train Epoche: 2 [1993/96 (2076%)]\tLoss: 1.342659\n",
      "Train Epoche: 2 [1994/96 (2077%)]\tLoss: 77.630424\n",
      "Train Epoche: 2 [1995/96 (2078%)]\tLoss: 7.933692\n",
      "Train Epoche: 2 [1996/96 (2079%)]\tLoss: 4.759109\n",
      "Train Epoche: 2 [1997/96 (2080%)]\tLoss: 79.975182\n",
      "Train Epoche: 2 [1998/96 (2081%)]\tLoss: 5.488851\n",
      "Train Epoche: 2 [1999/96 (2082%)]\tLoss: 0.155965\n",
      "Train Epoche: 2 [2000/96 (2083%)]\tLoss: 0.070723\n",
      "Train Epoche: 2 [2001/96 (2084%)]\tLoss: 14.606048\n",
      "Train Epoche: 2 [2002/96 (2085%)]\tLoss: 3.857939\n",
      "Train Epoche: 2 [2003/96 (2086%)]\tLoss: 17.095980\n",
      "Train Epoche: 2 [2004/96 (2088%)]\tLoss: 2.303760\n",
      "Train Epoche: 2 [2005/96 (2089%)]\tLoss: 0.547532\n",
      "Train Epoche: 2 [2006/96 (2090%)]\tLoss: 14.337719\n",
      "Train Epoche: 2 [2007/96 (2091%)]\tLoss: 34.537632\n",
      "Train Epoche: 2 [2008/96 (2092%)]\tLoss: 33.499863\n",
      "Train Epoche: 2 [2009/96 (2093%)]\tLoss: 0.105762\n",
      "Train Epoche: 2 [2010/96 (2094%)]\tLoss: 0.000090\n",
      "Train Epoche: 2 [2011/96 (2095%)]\tLoss: 436.717499\n",
      "Train Epoche: 2 [2012/96 (2096%)]\tLoss: 20.081022\n",
      "Train Epoche: 2 [2013/96 (2097%)]\tLoss: 23.732405\n",
      "Train Epoche: 2 [2014/96 (2098%)]\tLoss: 13.756155\n",
      "Train Epoche: 2 [2015/96 (2099%)]\tLoss: 4.409531\n",
      "Train Epoche: 2 [2016/96 (2100%)]\tLoss: 26.401979\n",
      "Train Epoche: 2 [2017/96 (2101%)]\tLoss: 100.468132\n",
      "Train Epoche: 2 [2018/96 (2102%)]\tLoss: 3.127972\n",
      "Train Epoche: 2 [2019/96 (2103%)]\tLoss: 19.603218\n",
      "Train Epoche: 2 [2020/96 (2104%)]\tLoss: 0.047692\n",
      "Train Epoche: 2 [2021/96 (2105%)]\tLoss: 0.573878\n",
      "Train Epoche: 2 [2022/96 (2106%)]\tLoss: 0.637681\n",
      "Train Epoche: 2 [2023/96 (2107%)]\tLoss: 1.585595\n",
      "Train Epoche: 2 [2024/96 (2108%)]\tLoss: 1.263227\n",
      "Train Epoche: 2 [2025/96 (2109%)]\tLoss: 249.960312\n",
      "Train Epoche: 2 [2026/96 (2110%)]\tLoss: 3.436500\n",
      "Train Epoche: 2 [2027/96 (2111%)]\tLoss: 29.076988\n",
      "Train Epoche: 2 [2028/96 (2112%)]\tLoss: 0.820140\n",
      "Train Epoche: 2 [2029/96 (2114%)]\tLoss: 19.827553\n",
      "Train Epoche: 2 [2030/96 (2115%)]\tLoss: 0.678950\n",
      "Train Epoche: 2 [2031/96 (2116%)]\tLoss: 25.456953\n",
      "Train Epoche: 2 [2032/96 (2117%)]\tLoss: 3.245678\n",
      "Train Epoche: 2 [2033/96 (2118%)]\tLoss: 66.641281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [0/96 (0%)]\tLoss: 9.541961\n",
      "Train Epoche: 3 [1/96 (1%)]\tLoss: 6.704851\n",
      "Train Epoche: 3 [2/96 (2%)]\tLoss: 1.506276\n",
      "Train Epoche: 3 [3/96 (3%)]\tLoss: 60.052006\n",
      "Train Epoche: 3 [4/96 (4%)]\tLoss: 66.929535\n",
      "Train Epoche: 3 [5/96 (5%)]\tLoss: 65.763947\n",
      "Train Epoche: 3 [6/96 (6%)]\tLoss: 13.614626\n",
      "Train Epoche: 3 [7/96 (7%)]\tLoss: 2.226134\n",
      "Train Epoche: 3 [8/96 (8%)]\tLoss: 10.129656\n",
      "Train Epoche: 3 [9/96 (9%)]\tLoss: 14.460006\n",
      "Train Epoche: 3 [10/96 (10%)]\tLoss: 1.415874\n",
      "Train Epoche: 3 [11/96 (11%)]\tLoss: 6.517694\n",
      "Train Epoche: 3 [12/96 (12%)]\tLoss: 146.434097\n",
      "Train Epoche: 3 [13/96 (14%)]\tLoss: 10.883916\n",
      "Train Epoche: 3 [14/96 (15%)]\tLoss: 0.017854\n",
      "Train Epoche: 3 [15/96 (16%)]\tLoss: 4.122142\n",
      "Train Epoche: 3 [16/96 (17%)]\tLoss: 3.422359\n",
      "Train Epoche: 3 [17/96 (18%)]\tLoss: 1.284887\n",
      "Train Epoche: 3 [18/96 (19%)]\tLoss: 63.681290\n",
      "Train Epoche: 3 [19/96 (20%)]\tLoss: 21.223442\n",
      "Train Epoche: 3 [20/96 (21%)]\tLoss: 13.404711\n",
      "Train Epoche: 3 [21/96 (22%)]\tLoss: 163.049927\n",
      "Train Epoche: 3 [22/96 (23%)]\tLoss: 111.355774\n",
      "Train Epoche: 3 [23/96 (24%)]\tLoss: 87.889709\n",
      "Train Epoche: 3 [24/96 (25%)]\tLoss: 15.258916\n",
      "Train Epoche: 3 [25/96 (26%)]\tLoss: 0.024980\n",
      "Train Epoche: 3 [26/96 (27%)]\tLoss: 0.006615\n",
      "Train Epoche: 3 [27/96 (28%)]\tLoss: 3.933275\n",
      "Train Epoche: 3 [28/96 (29%)]\tLoss: 0.300690\n",
      "Train Epoche: 3 [29/96 (30%)]\tLoss: 0.185147\n",
      "Train Epoche: 3 [30/96 (31%)]\tLoss: 0.058639\n",
      "Train Epoche: 3 [31/96 (32%)]\tLoss: 1.731245\n",
      "Train Epoche: 3 [32/96 (33%)]\tLoss: 28.016781\n",
      "Train Epoche: 3 [33/96 (34%)]\tLoss: 0.292058\n",
      "Train Epoche: 3 [34/96 (35%)]\tLoss: 3.261117\n",
      "Train Epoche: 3 [35/96 (36%)]\tLoss: 6.931300\n",
      "Train Epoche: 3 [36/96 (38%)]\tLoss: 14.063973\n",
      "Train Epoche: 3 [37/96 (39%)]\tLoss: 7.950666\n",
      "Train Epoche: 3 [38/96 (40%)]\tLoss: 0.808733\n",
      "Train Epoche: 3 [39/96 (41%)]\tLoss: 0.306370\n",
      "Train Epoche: 3 [40/96 (42%)]\tLoss: 72.436287\n",
      "Train Epoche: 3 [41/96 (43%)]\tLoss: 2.292108\n",
      "Train Epoche: 3 [42/96 (44%)]\tLoss: 1.323574\n",
      "Train Epoche: 3 [43/96 (45%)]\tLoss: 43.686031\n",
      "Train Epoche: 3 [44/96 (46%)]\tLoss: 24.753485\n",
      "Train Epoche: 3 [45/96 (47%)]\tLoss: 41.890438\n",
      "Train Epoche: 3 [46/96 (48%)]\tLoss: 13.824286\n",
      "Train Epoche: 3 [47/96 (49%)]\tLoss: 11.144124\n",
      "Train Epoche: 3 [48/96 (50%)]\tLoss: 1.270510\n",
      "Train Epoche: 3 [49/96 (51%)]\tLoss: 1.052116\n",
      "Train Epoche: 3 [50/96 (52%)]\tLoss: 44.656631\n",
      "Train Epoche: 3 [51/96 (53%)]\tLoss: 34.354630\n",
      "Train Epoche: 3 [52/96 (54%)]\tLoss: 1.557992\n",
      "Train Epoche: 3 [53/96 (55%)]\tLoss: 0.122612\n",
      "Train Epoche: 3 [54/96 (56%)]\tLoss: 75.856857\n",
      "Train Epoche: 3 [55/96 (57%)]\tLoss: 3.698697\n",
      "Train Epoche: 3 [56/96 (58%)]\tLoss: 1.834597\n",
      "Train Epoche: 3 [57/96 (59%)]\tLoss: 175.279938\n",
      "Train Epoche: 3 [58/96 (60%)]\tLoss: 1.268468\n",
      "Train Epoche: 3 [59/96 (61%)]\tLoss: 61.726101\n",
      "Train Epoche: 3 [60/96 (62%)]\tLoss: 3.016306\n",
      "Train Epoche: 3 [61/96 (64%)]\tLoss: 3.296919\n",
      "Train Epoche: 3 [62/96 (65%)]\tLoss: 9.680825\n",
      "Train Epoche: 3 [63/96 (66%)]\tLoss: 38.612274\n",
      "Train Epoche: 3 [64/96 (67%)]\tLoss: 0.587562\n",
      "Train Epoche: 3 [65/96 (68%)]\tLoss: 0.029848\n",
      "Train Epoche: 3 [66/96 (69%)]\tLoss: 10.157761\n",
      "Train Epoche: 3 [67/96 (70%)]\tLoss: 3.101422\n",
      "Train Epoche: 3 [68/96 (71%)]\tLoss: 5.914713\n",
      "Train Epoche: 3 [69/96 (72%)]\tLoss: 0.791970\n",
      "Train Epoche: 3 [70/96 (73%)]\tLoss: 4.007263\n",
      "Train Epoche: 3 [71/96 (74%)]\tLoss: 11.382879\n",
      "Train Epoche: 3 [72/96 (75%)]\tLoss: 10.547813\n",
      "Train Epoche: 3 [73/96 (76%)]\tLoss: 0.871562\n",
      "Train Epoche: 3 [74/96 (77%)]\tLoss: 225.152924\n",
      "Train Epoche: 3 [75/96 (78%)]\tLoss: 24.448317\n",
      "Train Epoche: 3 [76/96 (79%)]\tLoss: 2.669753\n",
      "Train Epoche: 3 [77/96 (80%)]\tLoss: 1.307676\n",
      "Train Epoche: 3 [78/96 (81%)]\tLoss: 1.147341\n",
      "Train Epoche: 3 [79/96 (82%)]\tLoss: 22.728319\n",
      "Train Epoche: 3 [80/96 (83%)]\tLoss: 8.793205\n",
      "Train Epoche: 3 [81/96 (84%)]\tLoss: 3.785146\n",
      "Train Epoche: 3 [82/96 (85%)]\tLoss: 3.489339\n",
      "Train Epoche: 3 [83/96 (86%)]\tLoss: 4.875517\n",
      "Train Epoche: 3 [84/96 (88%)]\tLoss: 11.092562\n",
      "Train Epoche: 3 [85/96 (89%)]\tLoss: 36.448914\n",
      "Train Epoche: 3 [86/96 (90%)]\tLoss: 44.693619\n",
      "Train Epoche: 3 [87/96 (91%)]\tLoss: 4.047285\n",
      "Train Epoche: 3 [88/96 (92%)]\tLoss: 4.028893\n",
      "Train Epoche: 3 [89/96 (93%)]\tLoss: 3.831722\n",
      "Train Epoche: 3 [90/96 (94%)]\tLoss: 49.910835\n",
      "Train Epoche: 3 [91/96 (95%)]\tLoss: 9.888928\n",
      "Train Epoche: 3 [92/96 (96%)]\tLoss: 0.015258\n",
      "Train Epoche: 3 [93/96 (97%)]\tLoss: 67.228409\n",
      "Train Epoche: 3 [94/96 (98%)]\tLoss: 51.372448\n",
      "Train Epoche: 3 [95/96 (99%)]\tLoss: 23.409233\n",
      "Train Epoche: 3 [96/96 (100%)]\tLoss: 2.568188\n",
      "Train Epoche: 3 [97/96 (101%)]\tLoss: 5.031109\n",
      "Train Epoche: 3 [98/96 (102%)]\tLoss: 5.824144\n",
      "Train Epoche: 3 [99/96 (103%)]\tLoss: 25.460957\n",
      "Train Epoche: 3 [100/96 (104%)]\tLoss: 15.315138\n",
      "Train Epoche: 3 [101/96 (105%)]\tLoss: 0.241826\n",
      "Train Epoche: 3 [102/96 (106%)]\tLoss: 0.161258\n",
      "Train Epoche: 3 [103/96 (107%)]\tLoss: 1.066411\n",
      "Train Epoche: 3 [104/96 (108%)]\tLoss: 18.032734\n",
      "Train Epoche: 3 [105/96 (109%)]\tLoss: 11.098969\n",
      "Train Epoche: 3 [106/96 (110%)]\tLoss: 39.764370\n",
      "Train Epoche: 3 [107/96 (111%)]\tLoss: 7.964349\n",
      "Train Epoche: 3 [108/96 (112%)]\tLoss: 0.335321\n",
      "Train Epoche: 3 [109/96 (114%)]\tLoss: 1.428570\n",
      "Train Epoche: 3 [110/96 (115%)]\tLoss: 0.031899\n",
      "Train Epoche: 3 [111/96 (116%)]\tLoss: 18.182953\n",
      "Train Epoche: 3 [112/96 (117%)]\tLoss: 0.264596\n",
      "Train Epoche: 3 [113/96 (118%)]\tLoss: 0.448243\n",
      "Train Epoche: 3 [114/96 (119%)]\tLoss: 1.047134\n",
      "Train Epoche: 3 [115/96 (120%)]\tLoss: 138.202164\n",
      "Train Epoche: 3 [116/96 (121%)]\tLoss: 92.008934\n",
      "Train Epoche: 3 [117/96 (122%)]\tLoss: 1.262397\n",
      "Train Epoche: 3 [118/96 (123%)]\tLoss: 8.870820\n",
      "Train Epoche: 3 [119/96 (124%)]\tLoss: 34.175457\n",
      "Train Epoche: 3 [120/96 (125%)]\tLoss: 75.866508\n",
      "Train Epoche: 3 [121/96 (126%)]\tLoss: 9.806159\n",
      "Train Epoche: 3 [122/96 (127%)]\tLoss: 21.925261\n",
      "Train Epoche: 3 [123/96 (128%)]\tLoss: 8.537800\n",
      "Train Epoche: 3 [124/96 (129%)]\tLoss: 18.388859\n",
      "Train Epoche: 3 [125/96 (130%)]\tLoss: 0.004799\n",
      "Train Epoche: 3 [126/96 (131%)]\tLoss: 18.438572\n",
      "Train Epoche: 3 [127/96 (132%)]\tLoss: 0.471310\n",
      "Train Epoche: 3 [128/96 (133%)]\tLoss: 17.812555\n",
      "Train Epoche: 3 [129/96 (134%)]\tLoss: 16.221165\n",
      "Train Epoche: 3 [130/96 (135%)]\tLoss: 144.688889\n",
      "Train Epoche: 3 [131/96 (136%)]\tLoss: 2.244193\n",
      "Train Epoche: 3 [132/96 (138%)]\tLoss: 67.196007\n",
      "Train Epoche: 3 [133/96 (139%)]\tLoss: 0.073329\n",
      "Train Epoche: 3 [134/96 (140%)]\tLoss: 0.935488\n",
      "Train Epoche: 3 [135/96 (141%)]\tLoss: 8.685717\n",
      "Train Epoche: 3 [136/96 (142%)]\tLoss: 0.948399\n",
      "Train Epoche: 3 [137/96 (143%)]\tLoss: 233.214676\n",
      "Train Epoche: 3 [138/96 (144%)]\tLoss: 0.001999\n",
      "Train Epoche: 3 [139/96 (145%)]\tLoss: 0.670027\n",
      "Train Epoche: 3 [140/96 (146%)]\tLoss: 11.636686\n",
      "Train Epoche: 3 [141/96 (147%)]\tLoss: 3.557791\n",
      "Train Epoche: 3 [142/96 (148%)]\tLoss: 7.759431\n",
      "Train Epoche: 3 [143/96 (149%)]\tLoss: 18.107899\n",
      "Train Epoche: 3 [144/96 (150%)]\tLoss: 77.332466\n",
      "Train Epoche: 3 [145/96 (151%)]\tLoss: 12.213664\n",
      "Train Epoche: 3 [146/96 (152%)]\tLoss: 27.991758\n",
      "Train Epoche: 3 [147/96 (153%)]\tLoss: 2.400671\n",
      "Train Epoche: 3 [148/96 (154%)]\tLoss: 29.134438\n",
      "Train Epoche: 3 [149/96 (155%)]\tLoss: 0.209592\n",
      "Train Epoche: 3 [150/96 (156%)]\tLoss: 0.216530\n",
      "Train Epoche: 3 [151/96 (157%)]\tLoss: 1.506413\n",
      "Train Epoche: 3 [152/96 (158%)]\tLoss: 3.529189\n",
      "Train Epoche: 3 [153/96 (159%)]\tLoss: 7.928309\n",
      "Train Epoche: 3 [154/96 (160%)]\tLoss: 0.099040\n",
      "Train Epoche: 3 [155/96 (161%)]\tLoss: 0.528190\n",
      "Train Epoche: 3 [156/96 (162%)]\tLoss: 0.609693\n",
      "Train Epoche: 3 [157/96 (164%)]\tLoss: 0.003948\n",
      "Train Epoche: 3 [158/96 (165%)]\tLoss: 0.095859\n",
      "Train Epoche: 3 [159/96 (166%)]\tLoss: 45.828083\n",
      "Train Epoche: 3 [160/96 (167%)]\tLoss: 4.429816\n",
      "Train Epoche: 3 [161/96 (168%)]\tLoss: 17.620096\n",
      "Train Epoche: 3 [162/96 (169%)]\tLoss: 28.651409\n",
      "Train Epoche: 3 [163/96 (170%)]\tLoss: 22.827770\n",
      "Train Epoche: 3 [164/96 (171%)]\tLoss: 63.258144\n",
      "Train Epoche: 3 [165/96 (172%)]\tLoss: 9.776936\n",
      "Train Epoche: 3 [166/96 (173%)]\tLoss: 21.410620\n",
      "Train Epoche: 3 [167/96 (174%)]\tLoss: 2.346823\n",
      "Train Epoche: 3 [168/96 (175%)]\tLoss: 8.389708\n",
      "Train Epoche: 3 [169/96 (176%)]\tLoss: 0.544043\n",
      "Train Epoche: 3 [170/96 (177%)]\tLoss: 0.814155\n",
      "Train Epoche: 3 [171/96 (178%)]\tLoss: 1.564926\n",
      "Train Epoche: 3 [172/96 (179%)]\tLoss: 4.738349\n",
      "Train Epoche: 3 [173/96 (180%)]\tLoss: 3.016843\n",
      "Train Epoche: 3 [174/96 (181%)]\tLoss: 40.045795\n",
      "Train Epoche: 3 [175/96 (182%)]\tLoss: 0.006963\n",
      "Train Epoche: 3 [176/96 (183%)]\tLoss: 2.381197\n",
      "Train Epoche: 3 [177/96 (184%)]\tLoss: 0.033197\n",
      "Train Epoche: 3 [178/96 (185%)]\tLoss: 4.630958\n",
      "Train Epoche: 3 [179/96 (186%)]\tLoss: 0.523181\n",
      "Train Epoche: 3 [180/96 (188%)]\tLoss: 4.418640\n",
      "Train Epoche: 3 [181/96 (189%)]\tLoss: 25.842146\n",
      "Train Epoche: 3 [182/96 (190%)]\tLoss: 1.253684\n",
      "Train Epoche: 3 [183/96 (191%)]\tLoss: 6.431849\n",
      "Train Epoche: 3 [184/96 (192%)]\tLoss: 275.671783\n",
      "Train Epoche: 3 [185/96 (193%)]\tLoss: 13.995275\n",
      "Train Epoche: 3 [186/96 (194%)]\tLoss: 51.052856\n",
      "Train Epoche: 3 [187/96 (195%)]\tLoss: 71.220573\n",
      "Train Epoche: 3 [188/96 (196%)]\tLoss: 0.127214\n",
      "Train Epoche: 3 [189/96 (197%)]\tLoss: 87.105927\n",
      "Train Epoche: 3 [190/96 (198%)]\tLoss: 26.828827\n",
      "Train Epoche: 3 [191/96 (199%)]\tLoss: 25.434217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [192/96 (200%)]\tLoss: 8.002979\n",
      "Train Epoche: 3 [193/96 (201%)]\tLoss: 31.662086\n",
      "Train Epoche: 3 [194/96 (202%)]\tLoss: 8.671636\n",
      "Train Epoche: 3 [195/96 (203%)]\tLoss: 1.451234\n",
      "Train Epoche: 3 [196/96 (204%)]\tLoss: 73.550400\n",
      "Train Epoche: 3 [197/96 (205%)]\tLoss: 0.995810\n",
      "Train Epoche: 3 [198/96 (206%)]\tLoss: 2.783787\n",
      "Train Epoche: 3 [199/96 (207%)]\tLoss: 4.875871\n",
      "Train Epoche: 3 [200/96 (208%)]\tLoss: 21.835957\n",
      "Train Epoche: 3 [201/96 (209%)]\tLoss: 0.001640\n",
      "Train Epoche: 3 [202/96 (210%)]\tLoss: 7.254781\n",
      "Train Epoche: 3 [203/96 (211%)]\tLoss: 3.608252\n",
      "Train Epoche: 3 [204/96 (212%)]\tLoss: 1.992488\n",
      "Train Epoche: 3 [205/96 (214%)]\tLoss: 383.534302\n",
      "Train Epoche: 3 [206/96 (215%)]\tLoss: 0.420099\n",
      "Train Epoche: 3 [207/96 (216%)]\tLoss: 90.919044\n",
      "Train Epoche: 3 [208/96 (217%)]\tLoss: 8.721015\n",
      "Train Epoche: 3 [209/96 (218%)]\tLoss: 165.306183\n",
      "Train Epoche: 3 [210/96 (219%)]\tLoss: 153.508240\n",
      "Train Epoche: 3 [211/96 (220%)]\tLoss: 8.710235\n",
      "Train Epoche: 3 [212/96 (221%)]\tLoss: 23.551657\n",
      "Train Epoche: 3 [213/96 (222%)]\tLoss: 115.689247\n",
      "Train Epoche: 3 [214/96 (223%)]\tLoss: 8.900094\n",
      "Train Epoche: 3 [215/96 (224%)]\tLoss: 162.794296\n",
      "Train Epoche: 3 [216/96 (225%)]\tLoss: 0.190706\n",
      "Train Epoche: 3 [217/96 (226%)]\tLoss: 21.876123\n",
      "Train Epoche: 3 [218/96 (227%)]\tLoss: 7.320399\n",
      "Train Epoche: 3 [219/96 (228%)]\tLoss: 0.673175\n",
      "Train Epoche: 3 [220/96 (229%)]\tLoss: 3.335489\n",
      "Train Epoche: 3 [221/96 (230%)]\tLoss: 28.786394\n",
      "Train Epoche: 3 [222/96 (231%)]\tLoss: 1.901622\n",
      "Train Epoche: 3 [223/96 (232%)]\tLoss: 84.850700\n",
      "Train Epoche: 3 [224/96 (233%)]\tLoss: 19.645111\n",
      "Train Epoche: 3 [225/96 (234%)]\tLoss: 106.117844\n",
      "Train Epoche: 3 [226/96 (235%)]\tLoss: 92.108711\n",
      "Train Epoche: 3 [227/96 (236%)]\tLoss: 1.477230\n",
      "Train Epoche: 3 [228/96 (238%)]\tLoss: 52.743530\n",
      "Train Epoche: 3 [229/96 (239%)]\tLoss: 19.119518\n",
      "Train Epoche: 3 [230/96 (240%)]\tLoss: 22.911295\n",
      "Train Epoche: 3 [231/96 (241%)]\tLoss: 0.308090\n",
      "Train Epoche: 3 [232/96 (242%)]\tLoss: 4.863532\n",
      "Train Epoche: 3 [233/96 (243%)]\tLoss: 5.082204\n",
      "Train Epoche: 3 [234/96 (244%)]\tLoss: 9.187216\n",
      "Train Epoche: 3 [235/96 (245%)]\tLoss: 1.309858\n",
      "Train Epoche: 3 [236/96 (246%)]\tLoss: 3.671072\n",
      "Train Epoche: 3 [237/96 (247%)]\tLoss: 145.350189\n",
      "Train Epoche: 3 [238/96 (248%)]\tLoss: 8.933439\n",
      "Train Epoche: 3 [239/96 (249%)]\tLoss: 1.937191\n",
      "Train Epoche: 3 [240/96 (250%)]\tLoss: 13.899885\n",
      "Train Epoche: 3 [241/96 (251%)]\tLoss: 3.076973\n",
      "Train Epoche: 3 [242/96 (252%)]\tLoss: 20.859283\n",
      "Train Epoche: 3 [243/96 (253%)]\tLoss: 0.107646\n",
      "Train Epoche: 3 [244/96 (254%)]\tLoss: 0.125415\n",
      "Train Epoche: 3 [245/96 (255%)]\tLoss: 6.630005\n",
      "Train Epoche: 3 [246/96 (256%)]\tLoss: 5.927170\n",
      "Train Epoche: 3 [247/96 (257%)]\tLoss: 6.124017\n",
      "Train Epoche: 3 [248/96 (258%)]\tLoss: 2.335830\n",
      "Train Epoche: 3 [249/96 (259%)]\tLoss: 3.354074\n",
      "Train Epoche: 3 [250/96 (260%)]\tLoss: 0.255003\n",
      "Train Epoche: 3 [251/96 (261%)]\tLoss: 2.153066\n",
      "Train Epoche: 3 [252/96 (262%)]\tLoss: 0.223898\n",
      "Train Epoche: 3 [253/96 (264%)]\tLoss: 20.882282\n",
      "Train Epoche: 3 [254/96 (265%)]\tLoss: 5.274601\n",
      "Train Epoche: 3 [255/96 (266%)]\tLoss: 1.367615\n",
      "Train Epoche: 3 [256/96 (267%)]\tLoss: 0.403299\n",
      "Train Epoche: 3 [257/96 (268%)]\tLoss: 0.736311\n",
      "Train Epoche: 3 [258/96 (269%)]\tLoss: 62.254635\n",
      "Train Epoche: 3 [259/96 (270%)]\tLoss: 122.023148\n",
      "Train Epoche: 3 [260/96 (271%)]\tLoss: 0.820838\n",
      "Train Epoche: 3 [261/96 (272%)]\tLoss: 12.770091\n",
      "Train Epoche: 3 [262/96 (273%)]\tLoss: 10.920184\n",
      "Train Epoche: 3 [263/96 (274%)]\tLoss: 32.970715\n",
      "Train Epoche: 3 [264/96 (275%)]\tLoss: 3.865443\n",
      "Train Epoche: 3 [265/96 (276%)]\tLoss: 3.077490\n",
      "Train Epoche: 3 [266/96 (277%)]\tLoss: 1.774137\n",
      "Train Epoche: 3 [267/96 (278%)]\tLoss: 2.982548\n",
      "Train Epoche: 3 [268/96 (279%)]\tLoss: 4.384436\n",
      "Train Epoche: 3 [269/96 (280%)]\tLoss: 13.047560\n",
      "Train Epoche: 3 [270/96 (281%)]\tLoss: 14.697016\n",
      "Train Epoche: 3 [271/96 (282%)]\tLoss: 2.462734\n",
      "Train Epoche: 3 [272/96 (283%)]\tLoss: 31.346931\n",
      "Train Epoche: 3 [273/96 (284%)]\tLoss: 48.927517\n",
      "Train Epoche: 3 [274/96 (285%)]\tLoss: 1.412207\n",
      "Train Epoche: 3 [275/96 (286%)]\tLoss: 5.137140\n",
      "Train Epoche: 3 [276/96 (288%)]\tLoss: 6.116911\n",
      "Train Epoche: 3 [277/96 (289%)]\tLoss: 2.014396\n",
      "Train Epoche: 3 [278/96 (290%)]\tLoss: 2.151633\n",
      "Train Epoche: 3 [279/96 (291%)]\tLoss: 1.670747\n",
      "Train Epoche: 3 [280/96 (292%)]\tLoss: 61.970303\n",
      "Train Epoche: 3 [281/96 (293%)]\tLoss: 4.630966\n",
      "Train Epoche: 3 [282/96 (294%)]\tLoss: 2.092782\n",
      "Train Epoche: 3 [283/96 (295%)]\tLoss: 1.673645\n",
      "Train Epoche: 3 [284/96 (296%)]\tLoss: 6.605668\n",
      "Train Epoche: 3 [285/96 (297%)]\tLoss: 1.053547\n",
      "Train Epoche: 3 [286/96 (298%)]\tLoss: 0.075634\n",
      "Train Epoche: 3 [287/96 (299%)]\tLoss: 0.546872\n",
      "Train Epoche: 3 [288/96 (300%)]\tLoss: 0.000183\n",
      "Train Epoche: 3 [289/96 (301%)]\tLoss: 9.497852\n",
      "Train Epoche: 3 [290/96 (302%)]\tLoss: 19.291510\n",
      "Train Epoche: 3 [291/96 (303%)]\tLoss: 303.964508\n",
      "Train Epoche: 3 [292/96 (304%)]\tLoss: 15.463408\n",
      "Train Epoche: 3 [293/96 (305%)]\tLoss: 1.989611\n",
      "Train Epoche: 3 [294/96 (306%)]\tLoss: 0.656572\n",
      "Train Epoche: 3 [295/96 (307%)]\tLoss: 0.785628\n",
      "Train Epoche: 3 [296/96 (308%)]\tLoss: 0.606570\n",
      "Train Epoche: 3 [297/96 (309%)]\tLoss: 3.978141\n",
      "Train Epoche: 3 [298/96 (310%)]\tLoss: 0.324281\n",
      "Train Epoche: 3 [299/96 (311%)]\tLoss: 1.284920\n",
      "Train Epoche: 3 [300/96 (312%)]\tLoss: 28.388409\n",
      "Train Epoche: 3 [301/96 (314%)]\tLoss: 2.203120\n",
      "Train Epoche: 3 [302/96 (315%)]\tLoss: 2.441910\n",
      "Train Epoche: 3 [303/96 (316%)]\tLoss: 136.401932\n",
      "Train Epoche: 3 [304/96 (317%)]\tLoss: 1.288941\n",
      "Train Epoche: 3 [305/96 (318%)]\tLoss: 8.943460\n",
      "Train Epoche: 3 [306/96 (319%)]\tLoss: 0.355328\n",
      "Train Epoche: 3 [307/96 (320%)]\tLoss: 3.964123\n",
      "Train Epoche: 3 [308/96 (321%)]\tLoss: 2.858280\n",
      "Train Epoche: 3 [309/96 (322%)]\tLoss: 2.259173\n",
      "Train Epoche: 3 [310/96 (323%)]\tLoss: 16.671654\n",
      "Train Epoche: 3 [311/96 (324%)]\tLoss: 2.869517\n",
      "Train Epoche: 3 [312/96 (325%)]\tLoss: 0.002051\n",
      "Train Epoche: 3 [313/96 (326%)]\tLoss: 0.063817\n",
      "Train Epoche: 3 [314/96 (327%)]\tLoss: 2.915831\n",
      "Train Epoche: 3 [315/96 (328%)]\tLoss: 5.384014\n",
      "Train Epoche: 3 [316/96 (329%)]\tLoss: 58.559738\n",
      "Train Epoche: 3 [317/96 (330%)]\tLoss: 75.178032\n",
      "Train Epoche: 3 [318/96 (331%)]\tLoss: 25.486313\n",
      "Train Epoche: 3 [319/96 (332%)]\tLoss: 21.509527\n",
      "Train Epoche: 3 [320/96 (333%)]\tLoss: 0.012728\n",
      "Train Epoche: 3 [321/96 (334%)]\tLoss: 2.845108\n",
      "Train Epoche: 3 [322/96 (335%)]\tLoss: 0.513960\n",
      "Train Epoche: 3 [323/96 (336%)]\tLoss: 54.441319\n",
      "Train Epoche: 3 [324/96 (338%)]\tLoss: 26.594488\n",
      "Train Epoche: 3 [325/96 (339%)]\tLoss: 213.722336\n",
      "Train Epoche: 3 [326/96 (340%)]\tLoss: 4.062410\n",
      "Train Epoche: 3 [327/96 (341%)]\tLoss: 18.087402\n",
      "Train Epoche: 3 [328/96 (342%)]\tLoss: 0.365382\n",
      "Train Epoche: 3 [329/96 (343%)]\tLoss: 6.473135\n",
      "Train Epoche: 3 [330/96 (344%)]\tLoss: 0.859060\n",
      "Train Epoche: 3 [331/96 (345%)]\tLoss: 3.681068\n",
      "Train Epoche: 3 [332/96 (346%)]\tLoss: 12.876183\n",
      "Train Epoche: 3 [333/96 (347%)]\tLoss: 14.696204\n",
      "Train Epoche: 3 [334/96 (348%)]\tLoss: 0.126654\n",
      "Train Epoche: 3 [335/96 (349%)]\tLoss: 8.124603\n",
      "Train Epoche: 3 [336/96 (350%)]\tLoss: 14.854897\n",
      "Train Epoche: 3 [337/96 (351%)]\tLoss: 20.646200\n",
      "Train Epoche: 3 [338/96 (352%)]\tLoss: 4.085547\n",
      "Train Epoche: 3 [339/96 (353%)]\tLoss: 2.418223\n",
      "Train Epoche: 3 [340/96 (354%)]\tLoss: 6.242773\n",
      "Train Epoche: 3 [341/96 (355%)]\tLoss: 4.965528\n",
      "Train Epoche: 3 [342/96 (356%)]\tLoss: 106.558167\n",
      "Train Epoche: 3 [343/96 (357%)]\tLoss: 16.905645\n",
      "Train Epoche: 3 [344/96 (358%)]\tLoss: 4.785549\n",
      "Train Epoche: 3 [345/96 (359%)]\tLoss: 0.009865\n",
      "Train Epoche: 3 [346/96 (360%)]\tLoss: 0.017005\n",
      "Train Epoche: 3 [347/96 (361%)]\tLoss: 0.128336\n",
      "Train Epoche: 3 [348/96 (362%)]\tLoss: 0.135773\n",
      "Train Epoche: 3 [349/96 (364%)]\tLoss: 3.963021\n",
      "Train Epoche: 3 [350/96 (365%)]\tLoss: 0.300380\n",
      "Train Epoche: 3 [351/96 (366%)]\tLoss: 12.746525\n",
      "Train Epoche: 3 [352/96 (367%)]\tLoss: 16.230207\n",
      "Train Epoche: 3 [353/96 (368%)]\tLoss: 347.669037\n",
      "Train Epoche: 3 [354/96 (369%)]\tLoss: 1.867372\n",
      "Train Epoche: 3 [355/96 (370%)]\tLoss: 2.075505\n",
      "Train Epoche: 3 [356/96 (371%)]\tLoss: 9.545755\n",
      "Train Epoche: 3 [357/96 (372%)]\tLoss: 72.431854\n",
      "Train Epoche: 3 [358/96 (373%)]\tLoss: 139.134033\n",
      "Train Epoche: 3 [359/96 (374%)]\tLoss: 1.979157\n",
      "Train Epoche: 3 [360/96 (375%)]\tLoss: 0.567107\n",
      "Train Epoche: 3 [361/96 (376%)]\tLoss: 12.234103\n",
      "Train Epoche: 3 [362/96 (377%)]\tLoss: 0.001025\n",
      "Train Epoche: 3 [363/96 (378%)]\tLoss: 5.786042\n",
      "Train Epoche: 3 [364/96 (379%)]\tLoss: 22.940088\n",
      "Train Epoche: 3 [365/96 (380%)]\tLoss: 11.552606\n",
      "Train Epoche: 3 [366/96 (381%)]\tLoss: 9.403222\n",
      "Train Epoche: 3 [367/96 (382%)]\tLoss: 24.257704\n",
      "Train Epoche: 3 [368/96 (383%)]\tLoss: 19.787563\n",
      "Train Epoche: 3 [369/96 (384%)]\tLoss: 0.468941\n",
      "Train Epoche: 3 [370/96 (385%)]\tLoss: 0.705053\n",
      "Train Epoche: 3 [371/96 (386%)]\tLoss: 4.433159\n",
      "Train Epoche: 3 [372/96 (388%)]\tLoss: 12.562895\n",
      "Train Epoche: 3 [373/96 (389%)]\tLoss: 9.343239\n",
      "Train Epoche: 3 [374/96 (390%)]\tLoss: 12.781128\n",
      "Train Epoche: 3 [375/96 (391%)]\tLoss: 17.830004\n",
      "Train Epoche: 3 [376/96 (392%)]\tLoss: 22.772642\n",
      "Train Epoche: 3 [377/96 (393%)]\tLoss: 0.026212\n",
      "Train Epoche: 3 [378/96 (394%)]\tLoss: 9.875210\n",
      "Train Epoche: 3 [379/96 (395%)]\tLoss: 41.119621\n",
      "Train Epoche: 3 [380/96 (396%)]\tLoss: 0.039179\n",
      "Train Epoche: 3 [381/96 (397%)]\tLoss: 2.860559\n",
      "Train Epoche: 3 [382/96 (398%)]\tLoss: 1.465434\n",
      "Train Epoche: 3 [383/96 (399%)]\tLoss: 47.491550\n",
      "Train Epoche: 3 [384/96 (400%)]\tLoss: 52.150372\n",
      "Train Epoche: 3 [385/96 (401%)]\tLoss: 2.343068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [386/96 (402%)]\tLoss: 0.176752\n",
      "Train Epoche: 3 [387/96 (403%)]\tLoss: 84.981430\n",
      "Train Epoche: 3 [388/96 (404%)]\tLoss: 1.426860\n",
      "Train Epoche: 3 [389/96 (405%)]\tLoss: 1.879289\n",
      "Train Epoche: 3 [390/96 (406%)]\tLoss: 1.351959\n",
      "Train Epoche: 3 [391/96 (407%)]\tLoss: 4.690360\n",
      "Train Epoche: 3 [392/96 (408%)]\tLoss: 5.372217\n",
      "Train Epoche: 3 [393/96 (409%)]\tLoss: 72.796143\n",
      "Train Epoche: 3 [394/96 (410%)]\tLoss: 328.383362\n",
      "Train Epoche: 3 [395/96 (411%)]\tLoss: 93.028114\n",
      "Train Epoche: 3 [396/96 (412%)]\tLoss: 39.989639\n",
      "Train Epoche: 3 [397/96 (414%)]\tLoss: 74.633209\n",
      "Train Epoche: 3 [398/96 (415%)]\tLoss: 31.336210\n",
      "Train Epoche: 3 [399/96 (416%)]\tLoss: 2.598303\n",
      "Train Epoche: 3 [400/96 (417%)]\tLoss: 73.841919\n",
      "Train Epoche: 3 [401/96 (418%)]\tLoss: 10.277038\n",
      "Train Epoche: 3 [402/96 (419%)]\tLoss: 1.516937\n",
      "Train Epoche: 3 [403/96 (420%)]\tLoss: 8.551470\n",
      "Train Epoche: 3 [404/96 (421%)]\tLoss: 0.009424\n",
      "Train Epoche: 3 [405/96 (422%)]\tLoss: 11.420185\n",
      "Train Epoche: 3 [406/96 (423%)]\tLoss: 0.503465\n",
      "Train Epoche: 3 [407/96 (424%)]\tLoss: 5.119305\n",
      "Train Epoche: 3 [408/96 (425%)]\tLoss: 22.969358\n",
      "Train Epoche: 3 [409/96 (426%)]\tLoss: 102.960899\n",
      "Train Epoche: 3 [410/96 (427%)]\tLoss: 156.027466\n",
      "Train Epoche: 3 [411/96 (428%)]\tLoss: 9.203741\n",
      "Train Epoche: 3 [412/96 (429%)]\tLoss: 8.411941\n",
      "Train Epoche: 3 [413/96 (430%)]\tLoss: 2.894790\n",
      "Train Epoche: 3 [414/96 (431%)]\tLoss: 5.974760\n",
      "Train Epoche: 3 [415/96 (432%)]\tLoss: 14.251781\n",
      "Train Epoche: 3 [416/96 (433%)]\tLoss: 4.167425\n",
      "Train Epoche: 3 [417/96 (434%)]\tLoss: 0.091274\n",
      "Train Epoche: 3 [418/96 (435%)]\tLoss: 73.349464\n",
      "Train Epoche: 3 [419/96 (436%)]\tLoss: 0.813230\n",
      "Train Epoche: 3 [420/96 (438%)]\tLoss: 0.741058\n",
      "Train Epoche: 3 [421/96 (439%)]\tLoss: 14.572732\n",
      "Train Epoche: 3 [422/96 (440%)]\tLoss: 1.643242\n",
      "Train Epoche: 3 [423/96 (441%)]\tLoss: 13.222866\n",
      "Train Epoche: 3 [424/96 (442%)]\tLoss: 56.827686\n",
      "Train Epoche: 3 [425/96 (443%)]\tLoss: 0.002275\n",
      "Train Epoche: 3 [426/96 (444%)]\tLoss: 28.843916\n",
      "Train Epoche: 3 [427/96 (445%)]\tLoss: 44.502617\n",
      "Train Epoche: 3 [428/96 (446%)]\tLoss: 10.759312\n",
      "Train Epoche: 3 [429/96 (447%)]\tLoss: 10.954357\n",
      "Train Epoche: 3 [430/96 (448%)]\tLoss: 0.602844\n",
      "Train Epoche: 3 [431/96 (449%)]\tLoss: 2.355982\n",
      "Train Epoche: 3 [432/96 (450%)]\tLoss: 212.756134\n",
      "Train Epoche: 3 [433/96 (451%)]\tLoss: 3.936074\n",
      "Train Epoche: 3 [434/96 (452%)]\tLoss: 0.746637\n",
      "Train Epoche: 3 [435/96 (453%)]\tLoss: 17.444103\n",
      "Train Epoche: 3 [436/96 (454%)]\tLoss: 0.308746\n",
      "Train Epoche: 3 [437/96 (455%)]\tLoss: 0.525901\n",
      "Train Epoche: 3 [438/96 (456%)]\tLoss: 2.505505\n",
      "Train Epoche: 3 [439/96 (457%)]\tLoss: 23.987795\n",
      "Train Epoche: 3 [440/96 (458%)]\tLoss: 18.497425\n",
      "Train Epoche: 3 [441/96 (459%)]\tLoss: 164.742294\n",
      "Train Epoche: 3 [442/96 (460%)]\tLoss: 12.925632\n",
      "Train Epoche: 3 [443/96 (461%)]\tLoss: 7.706658\n",
      "Train Epoche: 3 [444/96 (462%)]\tLoss: 0.080836\n",
      "Train Epoche: 3 [445/96 (464%)]\tLoss: 17.050919\n",
      "Train Epoche: 3 [446/96 (465%)]\tLoss: 0.212227\n",
      "Train Epoche: 3 [447/96 (466%)]\tLoss: 4.285910\n",
      "Train Epoche: 3 [448/96 (467%)]\tLoss: 5.857109\n",
      "Train Epoche: 3 [449/96 (468%)]\tLoss: 2.847994\n",
      "Train Epoche: 3 [450/96 (469%)]\tLoss: 2.894615\n",
      "Train Epoche: 3 [451/96 (470%)]\tLoss: 0.970967\n",
      "Train Epoche: 3 [452/96 (471%)]\tLoss: 2.515337\n",
      "Train Epoche: 3 [453/96 (472%)]\tLoss: 1.327088\n",
      "Train Epoche: 3 [454/96 (473%)]\tLoss: 0.799826\n",
      "Train Epoche: 3 [455/96 (474%)]\tLoss: 0.713343\n",
      "Train Epoche: 3 [456/96 (475%)]\tLoss: 0.464819\n",
      "Train Epoche: 3 [457/96 (476%)]\tLoss: 1.211457\n",
      "Train Epoche: 3 [458/96 (477%)]\tLoss: 0.729519\n",
      "Train Epoche: 3 [459/96 (478%)]\tLoss: 11.477940\n",
      "Train Epoche: 3 [460/96 (479%)]\tLoss: 23.121069\n",
      "Train Epoche: 3 [461/96 (480%)]\tLoss: 25.428345\n",
      "Train Epoche: 3 [462/96 (481%)]\tLoss: 2.799686\n",
      "Train Epoche: 3 [463/96 (482%)]\tLoss: 7.174644\n",
      "Train Epoche: 3 [464/96 (483%)]\tLoss: 2.089882\n",
      "Train Epoche: 3 [465/96 (484%)]\tLoss: 4.461243\n",
      "Train Epoche: 3 [466/96 (485%)]\tLoss: 33.757092\n",
      "Train Epoche: 3 [467/96 (486%)]\tLoss: 112.843697\n",
      "Train Epoche: 3 [468/96 (488%)]\tLoss: 122.488907\n",
      "Train Epoche: 3 [469/96 (489%)]\tLoss: 0.796024\n",
      "Train Epoche: 3 [470/96 (490%)]\tLoss: 1.091529\n",
      "Train Epoche: 3 [471/96 (491%)]\tLoss: 19.062265\n",
      "Train Epoche: 3 [472/96 (492%)]\tLoss: 7.491798\n",
      "Train Epoche: 3 [473/96 (493%)]\tLoss: 0.024076\n",
      "Train Epoche: 3 [474/96 (494%)]\tLoss: 0.040582\n",
      "Train Epoche: 3 [475/96 (495%)]\tLoss: 3.488685\n",
      "Train Epoche: 3 [476/96 (496%)]\tLoss: 8.272171\n",
      "Train Epoche: 3 [477/96 (497%)]\tLoss: 20.292997\n",
      "Train Epoche: 3 [478/96 (498%)]\tLoss: 91.227531\n",
      "Train Epoche: 3 [479/96 (499%)]\tLoss: 61.092419\n",
      "Train Epoche: 3 [480/96 (500%)]\tLoss: 11.011497\n",
      "Train Epoche: 3 [481/96 (501%)]\tLoss: 19.901001\n",
      "Train Epoche: 3 [482/96 (502%)]\tLoss: 0.767004\n",
      "Train Epoche: 3 [483/96 (503%)]\tLoss: 25.171362\n",
      "Train Epoche: 3 [484/96 (504%)]\tLoss: 0.056831\n",
      "Train Epoche: 3 [485/96 (505%)]\tLoss: 9.360293\n",
      "Train Epoche: 3 [486/96 (506%)]\tLoss: 4.615038\n",
      "Train Epoche: 3 [487/96 (507%)]\tLoss: 16.046726\n",
      "Train Epoche: 3 [488/96 (508%)]\tLoss: 0.052557\n",
      "Train Epoche: 3 [489/96 (509%)]\tLoss: 9.269993\n",
      "Train Epoche: 3 [490/96 (510%)]\tLoss: 2.735909\n",
      "Train Epoche: 3 [491/96 (511%)]\tLoss: 1.333922\n",
      "Train Epoche: 3 [492/96 (512%)]\tLoss: 0.686205\n",
      "Train Epoche: 3 [493/96 (514%)]\tLoss: 77.779709\n",
      "Train Epoche: 3 [494/96 (515%)]\tLoss: 2.104795\n",
      "Train Epoche: 3 [495/96 (516%)]\tLoss: 0.030445\n",
      "Train Epoche: 3 [496/96 (517%)]\tLoss: 8.174684\n",
      "Train Epoche: 3 [497/96 (518%)]\tLoss: 0.239458\n",
      "Train Epoche: 3 [498/96 (519%)]\tLoss: 3.621247\n",
      "Train Epoche: 3 [499/96 (520%)]\tLoss: 342.046021\n",
      "Train Epoche: 3 [500/96 (521%)]\tLoss: 113.813942\n",
      "Train Epoche: 3 [501/96 (522%)]\tLoss: 141.420486\n",
      "Train Epoche: 3 [502/96 (523%)]\tLoss: 4.786435\n",
      "Train Epoche: 3 [503/96 (524%)]\tLoss: 2.909550\n",
      "Train Epoche: 3 [504/96 (525%)]\tLoss: 11.354347\n",
      "Train Epoche: 3 [505/96 (526%)]\tLoss: 13.285208\n",
      "Train Epoche: 3 [506/96 (527%)]\tLoss: 7.804638\n",
      "Train Epoche: 3 [507/96 (528%)]\tLoss: 35.379726\n",
      "Train Epoche: 3 [508/96 (529%)]\tLoss: 31.993208\n",
      "Train Epoche: 3 [509/96 (530%)]\tLoss: 0.128031\n",
      "Train Epoche: 3 [510/96 (531%)]\tLoss: 3.815991\n",
      "Train Epoche: 3 [511/96 (532%)]\tLoss: 380.452423\n",
      "Train Epoche: 3 [512/96 (533%)]\tLoss: 28.253256\n",
      "Train Epoche: 3 [513/96 (534%)]\tLoss: 51.222153\n",
      "Train Epoche: 3 [514/96 (535%)]\tLoss: 21.511703\n",
      "Train Epoche: 3 [515/96 (536%)]\tLoss: 63.962254\n",
      "Train Epoche: 3 [516/96 (538%)]\tLoss: 16.147869\n",
      "Train Epoche: 3 [517/96 (539%)]\tLoss: 0.004515\n",
      "Train Epoche: 3 [518/96 (540%)]\tLoss: 66.539085\n",
      "Train Epoche: 3 [519/96 (541%)]\tLoss: 35.053661\n",
      "Train Epoche: 3 [520/96 (542%)]\tLoss: 92.286926\n",
      "Train Epoche: 3 [521/96 (543%)]\tLoss: 299.878021\n",
      "Train Epoche: 3 [522/96 (544%)]\tLoss: 8.398253\n",
      "Train Epoche: 3 [523/96 (545%)]\tLoss: 21.838074\n",
      "Train Epoche: 3 [524/96 (546%)]\tLoss: 8.528444\n",
      "Train Epoche: 3 [525/96 (547%)]\tLoss: 5.293437\n",
      "Train Epoche: 3 [526/96 (548%)]\tLoss: 38.971096\n",
      "Train Epoche: 3 [527/96 (549%)]\tLoss: 29.920099\n",
      "Train Epoche: 3 [528/96 (550%)]\tLoss: 5.829282\n",
      "Train Epoche: 3 [529/96 (551%)]\tLoss: 2.921475\n",
      "Train Epoche: 3 [530/96 (552%)]\tLoss: 21.993073\n",
      "Train Epoche: 3 [531/96 (553%)]\tLoss: 26.500523\n",
      "Train Epoche: 3 [532/96 (554%)]\tLoss: 2.908823\n",
      "Train Epoche: 3 [533/96 (555%)]\tLoss: 1.910833\n",
      "Train Epoche: 3 [534/96 (556%)]\tLoss: 2.306979\n",
      "Train Epoche: 3 [535/96 (557%)]\tLoss: 0.090739\n",
      "Train Epoche: 3 [536/96 (558%)]\tLoss: 9.959813\n",
      "Train Epoche: 3 [537/96 (559%)]\tLoss: 23.744448\n",
      "Train Epoche: 3 [538/96 (560%)]\tLoss: 1.811014\n",
      "Train Epoche: 3 [539/96 (561%)]\tLoss: 4.612498\n",
      "Train Epoche: 3 [540/96 (562%)]\tLoss: 0.037124\n",
      "Train Epoche: 3 [541/96 (564%)]\tLoss: 1.115015\n",
      "Train Epoche: 3 [542/96 (565%)]\tLoss: 2.648361\n",
      "Train Epoche: 3 [543/96 (566%)]\tLoss: 51.347569\n",
      "Train Epoche: 3 [544/96 (567%)]\tLoss: 3.237862\n",
      "Train Epoche: 3 [545/96 (568%)]\tLoss: 0.166645\n",
      "Train Epoche: 3 [546/96 (569%)]\tLoss: 0.073968\n",
      "Train Epoche: 3 [547/96 (570%)]\tLoss: 203.348038\n",
      "Train Epoche: 3 [548/96 (571%)]\tLoss: 85.941345\n",
      "Train Epoche: 3 [549/96 (572%)]\tLoss: 11.920906\n",
      "Train Epoche: 3 [550/96 (573%)]\tLoss: 69.009674\n",
      "Train Epoche: 3 [551/96 (574%)]\tLoss: 66.431183\n",
      "Train Epoche: 3 [552/96 (575%)]\tLoss: 34.331059\n",
      "Train Epoche: 3 [553/96 (576%)]\tLoss: 17.128557\n",
      "Train Epoche: 3 [554/96 (577%)]\tLoss: 8.611541\n",
      "Train Epoche: 3 [555/96 (578%)]\tLoss: 0.401041\n",
      "Train Epoche: 3 [556/96 (579%)]\tLoss: 3.471604\n",
      "Train Epoche: 3 [557/96 (580%)]\tLoss: 0.015344\n",
      "Train Epoche: 3 [558/96 (581%)]\tLoss: 5.442658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [559/96 (582%)]\tLoss: 178.869659\n",
      "Train Epoche: 3 [560/96 (583%)]\tLoss: 12.354484\n",
      "Train Epoche: 3 [561/96 (584%)]\tLoss: 38.332531\n",
      "Train Epoche: 3 [562/96 (585%)]\tLoss: 20.496099\n",
      "Train Epoche: 3 [563/96 (586%)]\tLoss: 16.893766\n",
      "Train Epoche: 3 [564/96 (588%)]\tLoss: 16.500086\n",
      "Train Epoche: 3 [565/96 (589%)]\tLoss: 13.838573\n",
      "Train Epoche: 3 [566/96 (590%)]\tLoss: 4.684122\n",
      "Train Epoche: 3 [567/96 (591%)]\tLoss: 1.144359\n",
      "Train Epoche: 3 [568/96 (592%)]\tLoss: 8.270303\n",
      "Train Epoche: 3 [569/96 (593%)]\tLoss: 4.702742\n",
      "Train Epoche: 3 [570/96 (594%)]\tLoss: 0.053688\n",
      "Train Epoche: 3 [571/96 (595%)]\tLoss: 29.361185\n",
      "Train Epoche: 3 [572/96 (596%)]\tLoss: 1.914065\n",
      "Train Epoche: 3 [573/96 (597%)]\tLoss: 1.333212\n",
      "Train Epoche: 3 [574/96 (598%)]\tLoss: 0.667082\n",
      "Train Epoche: 3 [575/96 (599%)]\tLoss: 1.173449\n",
      "Train Epoche: 3 [576/96 (600%)]\tLoss: 0.252832\n",
      "Train Epoche: 3 [577/96 (601%)]\tLoss: 3.810459\n",
      "Train Epoche: 3 [578/96 (602%)]\tLoss: 12.646796\n",
      "Train Epoche: 3 [579/96 (603%)]\tLoss: 1.063620\n",
      "Train Epoche: 3 [580/96 (604%)]\tLoss: 22.031189\n",
      "Train Epoche: 3 [581/96 (605%)]\tLoss: 40.879345\n",
      "Train Epoche: 3 [582/96 (606%)]\tLoss: 1.541580\n",
      "Train Epoche: 3 [583/96 (607%)]\tLoss: 1.784403\n",
      "Train Epoche: 3 [584/96 (608%)]\tLoss: 0.025327\n",
      "Train Epoche: 3 [585/96 (609%)]\tLoss: 4.465964\n",
      "Train Epoche: 3 [586/96 (610%)]\tLoss: 5.358254\n",
      "Train Epoche: 3 [587/96 (611%)]\tLoss: 6.267961\n",
      "Train Epoche: 3 [588/96 (612%)]\tLoss: 11.015661\n",
      "Train Epoche: 3 [589/96 (614%)]\tLoss: 0.143090\n",
      "Train Epoche: 3 [590/96 (615%)]\tLoss: 0.038468\n",
      "Train Epoche: 3 [591/96 (616%)]\tLoss: 1.656972\n",
      "Train Epoche: 3 [592/96 (617%)]\tLoss: 0.140322\n",
      "Train Epoche: 3 [593/96 (618%)]\tLoss: 0.989381\n",
      "Train Epoche: 3 [594/96 (619%)]\tLoss: 2.382424\n",
      "Train Epoche: 3 [595/96 (620%)]\tLoss: 1.312651\n",
      "Train Epoche: 3 [596/96 (621%)]\tLoss: 9.527390\n",
      "Train Epoche: 3 [597/96 (622%)]\tLoss: 51.452003\n",
      "Train Epoche: 3 [598/96 (623%)]\tLoss: 2.172423\n",
      "Train Epoche: 3 [599/96 (624%)]\tLoss: 6.254166\n",
      "Train Epoche: 3 [600/96 (625%)]\tLoss: 6.783852\n",
      "Train Epoche: 3 [601/96 (626%)]\tLoss: 11.162311\n",
      "Train Epoche: 3 [602/96 (627%)]\tLoss: 1.242459\n",
      "Train Epoche: 3 [603/96 (628%)]\tLoss: 0.040965\n",
      "Train Epoche: 3 [604/96 (629%)]\tLoss: 2.989168\n",
      "Train Epoche: 3 [605/96 (630%)]\tLoss: 2.214802\n",
      "Train Epoche: 3 [606/96 (631%)]\tLoss: 0.954613\n",
      "Train Epoche: 3 [607/96 (632%)]\tLoss: 1.854588\n",
      "Train Epoche: 3 [608/96 (633%)]\tLoss: 48.418530\n",
      "Train Epoche: 3 [609/96 (634%)]\tLoss: 288.203552\n",
      "Train Epoche: 3 [610/96 (635%)]\tLoss: 0.887828\n",
      "Train Epoche: 3 [611/96 (636%)]\tLoss: 19.125357\n",
      "Train Epoche: 3 [612/96 (638%)]\tLoss: 124.378136\n",
      "Train Epoche: 3 [613/96 (639%)]\tLoss: 16.234392\n",
      "Train Epoche: 3 [614/96 (640%)]\tLoss: 30.538374\n",
      "Train Epoche: 3 [615/96 (641%)]\tLoss: 23.128334\n",
      "Train Epoche: 3 [616/96 (642%)]\tLoss: 125.638039\n",
      "Train Epoche: 3 [617/96 (643%)]\tLoss: 70.337692\n",
      "Train Epoche: 3 [618/96 (644%)]\tLoss: 57.947445\n",
      "Train Epoche: 3 [619/96 (645%)]\tLoss: 22.431486\n",
      "Train Epoche: 3 [620/96 (646%)]\tLoss: 4.896879\n",
      "Train Epoche: 3 [621/96 (647%)]\tLoss: 75.095596\n",
      "Train Epoche: 3 [622/96 (648%)]\tLoss: 155.991562\n",
      "Train Epoche: 3 [623/96 (649%)]\tLoss: 0.254422\n",
      "Train Epoche: 3 [624/96 (650%)]\tLoss: 7.745105\n",
      "Train Epoche: 3 [625/96 (651%)]\tLoss: 16.083666\n",
      "Train Epoche: 3 [626/96 (652%)]\tLoss: 63.154999\n",
      "Train Epoche: 3 [627/96 (653%)]\tLoss: 8.424664\n",
      "Train Epoche: 3 [628/96 (654%)]\tLoss: 5.406524\n",
      "Train Epoche: 3 [629/96 (655%)]\tLoss: 43.437969\n",
      "Train Epoche: 3 [630/96 (656%)]\tLoss: 15.060925\n",
      "Train Epoche: 3 [631/96 (657%)]\tLoss: 113.927246\n",
      "Train Epoche: 3 [632/96 (658%)]\tLoss: 0.173466\n",
      "Train Epoche: 3 [633/96 (659%)]\tLoss: 0.047334\n",
      "Train Epoche: 3 [634/96 (660%)]\tLoss: 31.901152\n",
      "Train Epoche: 3 [635/96 (661%)]\tLoss: 5.021496\n",
      "Train Epoche: 3 [636/96 (662%)]\tLoss: 27.956167\n",
      "Train Epoche: 3 [637/96 (664%)]\tLoss: 44.983143\n",
      "Train Epoche: 3 [638/96 (665%)]\tLoss: 3.004690\n",
      "Train Epoche: 3 [639/96 (666%)]\tLoss: 2.231681\n",
      "Train Epoche: 3 [640/96 (667%)]\tLoss: 8.769648\n",
      "Train Epoche: 3 [641/96 (668%)]\tLoss: 27.158163\n",
      "Train Epoche: 3 [642/96 (669%)]\tLoss: 13.796124\n",
      "Train Epoche: 3 [643/96 (670%)]\tLoss: 153.945633\n",
      "Train Epoche: 3 [644/96 (671%)]\tLoss: 0.253511\n",
      "Train Epoche: 3 [645/96 (672%)]\tLoss: 15.584370\n",
      "Train Epoche: 3 [646/96 (673%)]\tLoss: 1.511613\n",
      "Train Epoche: 3 [647/96 (674%)]\tLoss: 13.071871\n",
      "Train Epoche: 3 [648/96 (675%)]\tLoss: 14.205110\n",
      "Train Epoche: 3 [649/96 (676%)]\tLoss: 341.321259\n",
      "Train Epoche: 3 [650/96 (677%)]\tLoss: 0.046699\n",
      "Train Epoche: 3 [651/96 (678%)]\tLoss: 16.178381\n",
      "Train Epoche: 3 [652/96 (679%)]\tLoss: 2.194855\n",
      "Train Epoche: 3 [653/96 (680%)]\tLoss: 2.706428\n",
      "Train Epoche: 3 [654/96 (681%)]\tLoss: 8.149568\n",
      "Train Epoche: 3 [655/96 (682%)]\tLoss: 1.199707\n",
      "Train Epoche: 3 [656/96 (683%)]\tLoss: 2.254096\n",
      "Train Epoche: 3 [657/96 (684%)]\tLoss: 17.146339\n",
      "Train Epoche: 3 [658/96 (685%)]\tLoss: 13.297256\n",
      "Train Epoche: 3 [659/96 (686%)]\tLoss: 82.742744\n",
      "Train Epoche: 3 [660/96 (688%)]\tLoss: 47.167065\n",
      "Train Epoche: 3 [661/96 (689%)]\tLoss: 43.889072\n",
      "Train Epoche: 3 [662/96 (690%)]\tLoss: 101.949646\n",
      "Train Epoche: 3 [663/96 (691%)]\tLoss: 9.630098\n",
      "Train Epoche: 3 [664/96 (692%)]\tLoss: 2.132118\n",
      "Train Epoche: 3 [665/96 (693%)]\tLoss: 53.058147\n",
      "Train Epoche: 3 [666/96 (694%)]\tLoss: 5.237437\n",
      "Train Epoche: 3 [667/96 (695%)]\tLoss: 0.312745\n",
      "Train Epoche: 3 [668/96 (696%)]\tLoss: 3.558383\n",
      "Train Epoche: 3 [669/96 (697%)]\tLoss: 2.967242\n",
      "Train Epoche: 3 [670/96 (698%)]\tLoss: 2.676454\n",
      "Train Epoche: 3 [671/96 (699%)]\tLoss: 6.831826\n",
      "Train Epoche: 3 [672/96 (700%)]\tLoss: 0.316263\n",
      "Train Epoche: 3 [673/96 (701%)]\tLoss: 0.268054\n",
      "Train Epoche: 3 [674/96 (702%)]\tLoss: 0.588404\n",
      "Train Epoche: 3 [675/96 (703%)]\tLoss: 0.015537\n",
      "Train Epoche: 3 [676/96 (704%)]\tLoss: 7.230293\n",
      "Train Epoche: 3 [677/96 (705%)]\tLoss: 0.455248\n",
      "Train Epoche: 3 [678/96 (706%)]\tLoss: 4.316452\n",
      "Train Epoche: 3 [679/96 (707%)]\tLoss: 1.672520\n",
      "Train Epoche: 3 [680/96 (708%)]\tLoss: 175.219315\n",
      "Train Epoche: 3 [681/96 (709%)]\tLoss: 105.932579\n",
      "Train Epoche: 3 [682/96 (710%)]\tLoss: 74.130371\n",
      "Train Epoche: 3 [683/96 (711%)]\tLoss: 4.167341\n",
      "Train Epoche: 3 [684/96 (712%)]\tLoss: 46.808842\n",
      "Train Epoche: 3 [685/96 (714%)]\tLoss: 0.459930\n",
      "Train Epoche: 3 [686/96 (715%)]\tLoss: 78.738983\n",
      "Train Epoche: 3 [687/96 (716%)]\tLoss: 35.942894\n",
      "Train Epoche: 3 [688/96 (717%)]\tLoss: 0.950345\n",
      "Train Epoche: 3 [689/96 (718%)]\tLoss: 60.727032\n",
      "Train Epoche: 3 [690/96 (719%)]\tLoss: 16.744980\n",
      "Train Epoche: 3 [691/96 (720%)]\tLoss: 2.681200\n",
      "Train Epoche: 3 [692/96 (721%)]\tLoss: 3.261484\n",
      "Train Epoche: 3 [693/96 (722%)]\tLoss: 29.721025\n",
      "Train Epoche: 3 [694/96 (723%)]\tLoss: 3.092671\n",
      "Train Epoche: 3 [695/96 (724%)]\tLoss: 5.861883\n",
      "Train Epoche: 3 [696/96 (725%)]\tLoss: 2.820790\n",
      "Train Epoche: 3 [697/96 (726%)]\tLoss: 1.467629\n",
      "Train Epoche: 3 [698/96 (727%)]\tLoss: 65.095253\n",
      "Train Epoche: 3 [699/96 (728%)]\tLoss: 1.031921\n",
      "Train Epoche: 3 [700/96 (729%)]\tLoss: 7.964716\n",
      "Train Epoche: 3 [701/96 (730%)]\tLoss: 10.102688\n",
      "Train Epoche: 3 [702/96 (731%)]\tLoss: 0.435517\n",
      "Train Epoche: 3 [703/96 (732%)]\tLoss: 10.955285\n",
      "Train Epoche: 3 [704/96 (733%)]\tLoss: 128.878967\n",
      "Train Epoche: 3 [705/96 (734%)]\tLoss: 3.602208\n",
      "Train Epoche: 3 [706/96 (735%)]\tLoss: 20.745699\n",
      "Train Epoche: 3 [707/96 (736%)]\tLoss: 0.333904\n",
      "Train Epoche: 3 [708/96 (738%)]\tLoss: 5.431087\n",
      "Train Epoche: 3 [709/96 (739%)]\tLoss: 1.546619\n",
      "Train Epoche: 3 [710/96 (740%)]\tLoss: 6.009057\n",
      "Train Epoche: 3 [711/96 (741%)]\tLoss: 5.708456\n",
      "Train Epoche: 3 [712/96 (742%)]\tLoss: 0.038647\n",
      "Train Epoche: 3 [713/96 (743%)]\tLoss: 0.053406\n",
      "Train Epoche: 3 [714/96 (744%)]\tLoss: 10.913227\n",
      "Train Epoche: 3 [715/96 (745%)]\tLoss: 13.241654\n",
      "Train Epoche: 3 [716/96 (746%)]\tLoss: 11.277911\n",
      "Train Epoche: 3 [717/96 (747%)]\tLoss: 13.872758\n",
      "Train Epoche: 3 [718/96 (748%)]\tLoss: 5.884683\n",
      "Train Epoche: 3 [719/96 (749%)]\tLoss: 0.157664\n",
      "Train Epoche: 3 [720/96 (750%)]\tLoss: 0.674632\n",
      "Train Epoche: 3 [721/96 (751%)]\tLoss: 4.344324\n",
      "Train Epoche: 3 [722/96 (752%)]\tLoss: 9.671807\n",
      "Train Epoche: 3 [723/96 (753%)]\tLoss: 15.038590\n",
      "Train Epoche: 3 [724/96 (754%)]\tLoss: 11.769751\n",
      "Train Epoche: 3 [725/96 (755%)]\tLoss: 1.261936\n",
      "Train Epoche: 3 [726/96 (756%)]\tLoss: 0.488591\n",
      "Train Epoche: 3 [727/96 (757%)]\tLoss: 1.277131\n",
      "Train Epoche: 3 [728/96 (758%)]\tLoss: 118.843864\n",
      "Train Epoche: 3 [729/96 (759%)]\tLoss: 40.813957\n",
      "Train Epoche: 3 [730/96 (760%)]\tLoss: 3.086158\n",
      "Train Epoche: 3 [731/96 (761%)]\tLoss: 5.478806\n",
      "Train Epoche: 3 [732/96 (762%)]\tLoss: 0.199501\n",
      "Train Epoche: 3 [733/96 (764%)]\tLoss: 0.033089\n",
      "Train Epoche: 3 [734/96 (765%)]\tLoss: 3.598681\n",
      "Train Epoche: 3 [735/96 (766%)]\tLoss: 18.053543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [736/96 (767%)]\tLoss: 1.032356\n",
      "Train Epoche: 3 [737/96 (768%)]\tLoss: 0.458159\n",
      "Train Epoche: 3 [738/96 (769%)]\tLoss: 1.945309\n",
      "Train Epoche: 3 [739/96 (770%)]\tLoss: 85.337662\n",
      "Train Epoche: 3 [740/96 (771%)]\tLoss: 17.953127\n",
      "Train Epoche: 3 [741/96 (772%)]\tLoss: 2.574299\n",
      "Train Epoche: 3 [742/96 (773%)]\tLoss: 190.129379\n",
      "Train Epoche: 3 [743/96 (774%)]\tLoss: 1.122435\n",
      "Train Epoche: 3 [744/96 (775%)]\tLoss: 1.730927\n",
      "Train Epoche: 3 [745/96 (776%)]\tLoss: 5.053406\n",
      "Train Epoche: 3 [746/96 (777%)]\tLoss: 0.375419\n",
      "Train Epoche: 3 [747/96 (778%)]\tLoss: 0.089902\n",
      "Train Epoche: 3 [748/96 (779%)]\tLoss: 11.471660\n",
      "Train Epoche: 3 [749/96 (780%)]\tLoss: 16.874598\n",
      "Train Epoche: 3 [750/96 (781%)]\tLoss: 0.017498\n",
      "Train Epoche: 3 [751/96 (782%)]\tLoss: 0.774117\n",
      "Train Epoche: 3 [752/96 (783%)]\tLoss: 4.795710\n",
      "Train Epoche: 3 [753/96 (784%)]\tLoss: 86.068008\n",
      "Train Epoche: 3 [754/96 (785%)]\tLoss: 7.796775\n",
      "Train Epoche: 3 [755/96 (786%)]\tLoss: 4.630057\n",
      "Train Epoche: 3 [756/96 (788%)]\tLoss: 3.816120\n",
      "Train Epoche: 3 [757/96 (789%)]\tLoss: 19.626854\n",
      "Train Epoche: 3 [758/96 (790%)]\tLoss: 23.895811\n",
      "Train Epoche: 3 [759/96 (791%)]\tLoss: 0.377198\n",
      "Train Epoche: 3 [760/96 (792%)]\tLoss: 2.563167\n",
      "Train Epoche: 3 [761/96 (793%)]\tLoss: 18.641363\n",
      "Train Epoche: 3 [762/96 (794%)]\tLoss: 7.210858\n",
      "Train Epoche: 3 [763/96 (795%)]\tLoss: 6.583974\n",
      "Train Epoche: 3 [764/96 (796%)]\tLoss: 0.187770\n",
      "Train Epoche: 3 [765/96 (797%)]\tLoss: 4.607483\n",
      "Train Epoche: 3 [766/96 (798%)]\tLoss: 0.319819\n",
      "Train Epoche: 3 [767/96 (799%)]\tLoss: 3.561162\n",
      "Train Epoche: 3 [768/96 (800%)]\tLoss: 59.168850\n",
      "Train Epoche: 3 [769/96 (801%)]\tLoss: 1.096858\n",
      "Train Epoche: 3 [770/96 (802%)]\tLoss: 0.000000\n",
      "Train Epoche: 3 [771/96 (803%)]\tLoss: 4.318744\n",
      "Train Epoche: 3 [772/96 (804%)]\tLoss: 4.137829\n",
      "Train Epoche: 3 [773/96 (805%)]\tLoss: 0.030988\n",
      "Train Epoche: 3 [774/96 (806%)]\tLoss: 18.076109\n",
      "Train Epoche: 3 [775/96 (807%)]\tLoss: 10.040376\n",
      "Train Epoche: 3 [776/96 (808%)]\tLoss: 3.076333\n",
      "Train Epoche: 3 [777/96 (809%)]\tLoss: 3.482601\n",
      "Train Epoche: 3 [778/96 (810%)]\tLoss: 17.080708\n",
      "Train Epoche: 3 [779/96 (811%)]\tLoss: 0.013596\n",
      "Train Epoche: 3 [780/96 (812%)]\tLoss: 263.390228\n",
      "Train Epoche: 3 [781/96 (814%)]\tLoss: 19.615204\n",
      "Train Epoche: 3 [782/96 (815%)]\tLoss: 39.323238\n",
      "Train Epoche: 3 [783/96 (816%)]\tLoss: 17.162756\n",
      "Train Epoche: 3 [784/96 (817%)]\tLoss: 0.499756\n",
      "Train Epoche: 3 [785/96 (818%)]\tLoss: 5.854748\n",
      "Train Epoche: 3 [786/96 (819%)]\tLoss: 137.534637\n",
      "Train Epoche: 3 [787/96 (820%)]\tLoss: 5.045982\n",
      "Train Epoche: 3 [788/96 (821%)]\tLoss: 1.227909\n",
      "Train Epoche: 3 [789/96 (822%)]\tLoss: 0.102099\n",
      "Train Epoche: 3 [790/96 (823%)]\tLoss: 3.700108\n",
      "Train Epoche: 3 [791/96 (824%)]\tLoss: 1.618036\n",
      "Train Epoche: 3 [792/96 (825%)]\tLoss: 0.153155\n",
      "Train Epoche: 3 [793/96 (826%)]\tLoss: 3.830990\n",
      "Train Epoche: 3 [794/96 (827%)]\tLoss: 0.204803\n",
      "Train Epoche: 3 [795/96 (828%)]\tLoss: 39.019501\n",
      "Train Epoche: 3 [796/96 (829%)]\tLoss: 0.250916\n",
      "Train Epoche: 3 [797/96 (830%)]\tLoss: 5.960043\n",
      "Train Epoche: 3 [798/96 (831%)]\tLoss: 9.418060\n",
      "Train Epoche: 3 [799/96 (832%)]\tLoss: 0.018293\n",
      "Train Epoche: 3 [800/96 (833%)]\tLoss: 0.006030\n",
      "Train Epoche: 3 [801/96 (834%)]\tLoss: 234.985123\n",
      "Train Epoche: 3 [802/96 (835%)]\tLoss: 1.698648\n",
      "Train Epoche: 3 [803/96 (836%)]\tLoss: 2.805424\n",
      "Train Epoche: 3 [804/96 (838%)]\tLoss: 10.732226\n",
      "Train Epoche: 3 [805/96 (839%)]\tLoss: 2.164720\n",
      "Train Epoche: 3 [806/96 (840%)]\tLoss: 35.405472\n",
      "Train Epoche: 3 [807/96 (841%)]\tLoss: 10.062521\n",
      "Train Epoche: 3 [808/96 (842%)]\tLoss: 0.529960\n",
      "Train Epoche: 3 [809/96 (843%)]\tLoss: 1.015177\n",
      "Train Epoche: 3 [810/96 (844%)]\tLoss: 4.295238\n",
      "Train Epoche: 3 [811/96 (845%)]\tLoss: 0.757738\n",
      "Train Epoche: 3 [812/96 (846%)]\tLoss: 6.311543\n",
      "Train Epoche: 3 [813/96 (847%)]\tLoss: 112.287926\n",
      "Train Epoche: 3 [814/96 (848%)]\tLoss: 15.092237\n",
      "Train Epoche: 3 [815/96 (849%)]\tLoss: 0.642964\n",
      "Train Epoche: 3 [816/96 (850%)]\tLoss: 7.793564\n",
      "Train Epoche: 3 [817/96 (851%)]\tLoss: 0.174348\n",
      "Train Epoche: 3 [818/96 (852%)]\tLoss: 22.014851\n",
      "Train Epoche: 3 [819/96 (853%)]\tLoss: 16.411425\n",
      "Train Epoche: 3 [820/96 (854%)]\tLoss: 1.817918\n",
      "Train Epoche: 3 [821/96 (855%)]\tLoss: 12.799294\n",
      "Train Epoche: 3 [822/96 (856%)]\tLoss: 4.044235\n",
      "Train Epoche: 3 [823/96 (857%)]\tLoss: 107.582550\n",
      "Train Epoche: 3 [824/96 (858%)]\tLoss: 58.886219\n",
      "Train Epoche: 3 [825/96 (859%)]\tLoss: 66.116531\n",
      "Train Epoche: 3 [826/96 (860%)]\tLoss: 1.785950\n",
      "Train Epoche: 3 [827/96 (861%)]\tLoss: 41.025719\n",
      "Train Epoche: 3 [828/96 (862%)]\tLoss: 6.352277\n",
      "Train Epoche: 3 [829/96 (864%)]\tLoss: 4.786166\n",
      "Train Epoche: 3 [830/96 (865%)]\tLoss: 2.747694\n",
      "Train Epoche: 3 [831/96 (866%)]\tLoss: 0.039206\n",
      "Train Epoche: 3 [832/96 (867%)]\tLoss: 10.004703\n",
      "Train Epoche: 3 [833/96 (868%)]\tLoss: 9.410069\n",
      "Train Epoche: 3 [834/96 (869%)]\tLoss: 2.676368\n",
      "Train Epoche: 3 [835/96 (870%)]\tLoss: 2.117588\n",
      "Train Epoche: 3 [836/96 (871%)]\tLoss: 2.469426\n",
      "Train Epoche: 3 [837/96 (872%)]\tLoss: 6.153785\n",
      "Train Epoche: 3 [838/96 (873%)]\tLoss: 33.377876\n",
      "Train Epoche: 3 [839/96 (874%)]\tLoss: 23.680918\n",
      "Train Epoche: 3 [840/96 (875%)]\tLoss: 9.804705\n",
      "Train Epoche: 3 [841/96 (876%)]\tLoss: 15.872406\n",
      "Train Epoche: 3 [842/96 (877%)]\tLoss: 7.739323\n",
      "Train Epoche: 3 [843/96 (878%)]\tLoss: 29.606722\n",
      "Train Epoche: 3 [844/96 (879%)]\tLoss: 0.963284\n",
      "Train Epoche: 3 [845/96 (880%)]\tLoss: 11.129910\n",
      "Train Epoche: 3 [846/96 (881%)]\tLoss: 140.694504\n",
      "Train Epoche: 3 [847/96 (882%)]\tLoss: 0.323790\n",
      "Train Epoche: 3 [848/96 (883%)]\tLoss: 1.176135\n",
      "Train Epoche: 3 [849/96 (884%)]\tLoss: 67.096703\n",
      "Train Epoche: 3 [850/96 (885%)]\tLoss: 2.493267\n",
      "Train Epoche: 3 [851/96 (886%)]\tLoss: 224.519638\n",
      "Train Epoche: 3 [852/96 (888%)]\tLoss: 9.352907\n",
      "Train Epoche: 3 [853/96 (889%)]\tLoss: 1.029269\n",
      "Train Epoche: 3 [854/96 (890%)]\tLoss: 0.180940\n",
      "Train Epoche: 3 [855/96 (891%)]\tLoss: 5.456047\n",
      "Train Epoche: 3 [856/96 (892%)]\tLoss: 4.160964\n",
      "Train Epoche: 3 [857/96 (893%)]\tLoss: 4.404343\n",
      "Train Epoche: 3 [858/96 (894%)]\tLoss: 0.231991\n",
      "Train Epoche: 3 [859/96 (895%)]\tLoss: 7.760607\n",
      "Train Epoche: 3 [860/96 (896%)]\tLoss: 1.798493\n",
      "Train Epoche: 3 [861/96 (897%)]\tLoss: 333.014191\n",
      "Train Epoche: 3 [862/96 (898%)]\tLoss: 2.994938\n",
      "Train Epoche: 3 [863/96 (899%)]\tLoss: 3.581888\n",
      "Train Epoche: 3 [864/96 (900%)]\tLoss: 39.419529\n",
      "Train Epoche: 3 [865/96 (901%)]\tLoss: 15.371568\n",
      "Train Epoche: 3 [866/96 (902%)]\tLoss: 1.771267\n",
      "Train Epoche: 3 [867/96 (903%)]\tLoss: 8.003174\n",
      "Train Epoche: 3 [868/96 (904%)]\tLoss: 1.319906\n",
      "Train Epoche: 3 [869/96 (905%)]\tLoss: 2.568035\n",
      "Train Epoche: 3 [870/96 (906%)]\tLoss: 0.000149\n",
      "Train Epoche: 3 [871/96 (907%)]\tLoss: 86.835411\n",
      "Train Epoche: 3 [872/96 (908%)]\tLoss: 0.472212\n",
      "Train Epoche: 3 [873/96 (909%)]\tLoss: 0.117816\n",
      "Train Epoche: 3 [874/96 (910%)]\tLoss: 127.391968\n",
      "Train Epoche: 3 [875/96 (911%)]\tLoss: 0.242086\n",
      "Train Epoche: 3 [876/96 (912%)]\tLoss: 4.131239\n",
      "Train Epoche: 3 [877/96 (914%)]\tLoss: 242.974777\n",
      "Train Epoche: 3 [878/96 (915%)]\tLoss: 0.000004\n",
      "Train Epoche: 3 [879/96 (916%)]\tLoss: 11.802707\n",
      "Train Epoche: 3 [880/96 (917%)]\tLoss: 0.309292\n",
      "Train Epoche: 3 [881/96 (918%)]\tLoss: 6.095675\n",
      "Train Epoche: 3 [882/96 (919%)]\tLoss: 22.454771\n",
      "Train Epoche: 3 [883/96 (920%)]\tLoss: 18.001106\n",
      "Train Epoche: 3 [884/96 (921%)]\tLoss: 26.782631\n",
      "Train Epoche: 3 [885/96 (922%)]\tLoss: 43.119770\n",
      "Train Epoche: 3 [886/96 (923%)]\tLoss: 0.564261\n",
      "Train Epoche: 3 [887/96 (924%)]\tLoss: 6.423265\n",
      "Train Epoche: 3 [888/96 (925%)]\tLoss: 28.475609\n",
      "Train Epoche: 3 [889/96 (926%)]\tLoss: 13.284040\n",
      "Train Epoche: 3 [890/96 (927%)]\tLoss: 10.216057\n",
      "Train Epoche: 3 [891/96 (928%)]\tLoss: 107.243256\n",
      "Train Epoche: 3 [892/96 (929%)]\tLoss: 3.453312\n",
      "Train Epoche: 3 [893/96 (930%)]\tLoss: 0.241604\n",
      "Train Epoche: 3 [894/96 (931%)]\tLoss: 28.625759\n",
      "Train Epoche: 3 [895/96 (932%)]\tLoss: 4.856689\n",
      "Train Epoche: 3 [896/96 (933%)]\tLoss: 27.075020\n",
      "Train Epoche: 3 [897/96 (934%)]\tLoss: 7.694971\n",
      "Train Epoche: 3 [898/96 (935%)]\tLoss: 0.299107\n",
      "Train Epoche: 3 [899/96 (936%)]\tLoss: 0.179464\n",
      "Train Epoche: 3 [900/96 (938%)]\tLoss: 1.836874\n",
      "Train Epoche: 3 [901/96 (939%)]\tLoss: 0.045639\n",
      "Train Epoche: 3 [902/96 (940%)]\tLoss: 0.065103\n",
      "Train Epoche: 3 [903/96 (941%)]\tLoss: 39.447155\n",
      "Train Epoche: 3 [904/96 (942%)]\tLoss: 2.570755\n",
      "Train Epoche: 3 [905/96 (943%)]\tLoss: 46.219006\n",
      "Train Epoche: 3 [906/96 (944%)]\tLoss: 0.414570\n",
      "Train Epoche: 3 [907/96 (945%)]\tLoss: 0.325578\n",
      "Train Epoche: 3 [908/96 (946%)]\tLoss: 0.018959\n",
      "Train Epoche: 3 [909/96 (947%)]\tLoss: 80.954277\n",
      "Train Epoche: 3 [910/96 (948%)]\tLoss: 84.348312\n",
      "Train Epoche: 3 [911/96 (949%)]\tLoss: 3.556984\n",
      "Train Epoche: 3 [912/96 (950%)]\tLoss: 3.745260\n",
      "Train Epoche: 3 [913/96 (951%)]\tLoss: 7.043257\n",
      "Train Epoche: 3 [914/96 (952%)]\tLoss: 25.395206\n",
      "Train Epoche: 3 [915/96 (953%)]\tLoss: 34.676769\n",
      "Train Epoche: 3 [916/96 (954%)]\tLoss: 11.361843\n",
      "Train Epoche: 3 [917/96 (955%)]\tLoss: 10.757954\n",
      "Train Epoche: 3 [918/96 (956%)]\tLoss: 1.270106\n",
      "Train Epoche: 3 [919/96 (957%)]\tLoss: 13.099490\n",
      "Train Epoche: 3 [920/96 (958%)]\tLoss: 3.377726\n",
      "Train Epoche: 3 [921/96 (959%)]\tLoss: 54.630951\n",
      "Train Epoche: 3 [922/96 (960%)]\tLoss: 0.431351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [923/96 (961%)]\tLoss: 19.092449\n",
      "Train Epoche: 3 [924/96 (962%)]\tLoss: 18.235071\n",
      "Train Epoche: 3 [925/96 (964%)]\tLoss: 0.059796\n",
      "Train Epoche: 3 [926/96 (965%)]\tLoss: 87.319992\n",
      "Train Epoche: 3 [927/96 (966%)]\tLoss: 3.847924\n",
      "Train Epoche: 3 [928/96 (967%)]\tLoss: 0.048132\n",
      "Train Epoche: 3 [929/96 (968%)]\tLoss: 52.396511\n",
      "Train Epoche: 3 [930/96 (969%)]\tLoss: 7.372370\n",
      "Train Epoche: 3 [931/96 (970%)]\tLoss: 6.689085\n",
      "Train Epoche: 3 [932/96 (971%)]\tLoss: 22.774963\n",
      "Train Epoche: 3 [933/96 (972%)]\tLoss: 4.448598\n",
      "Train Epoche: 3 [934/96 (973%)]\tLoss: 3.344538\n",
      "Train Epoche: 3 [935/96 (974%)]\tLoss: 1.399984\n",
      "Train Epoche: 3 [936/96 (975%)]\tLoss: 0.109350\n",
      "Train Epoche: 3 [937/96 (976%)]\tLoss: 51.185108\n",
      "Train Epoche: 3 [938/96 (977%)]\tLoss: 19.985422\n",
      "Train Epoche: 3 [939/96 (978%)]\tLoss: 2.369148\n",
      "Train Epoche: 3 [940/96 (979%)]\tLoss: 15.111446\n",
      "Train Epoche: 3 [941/96 (980%)]\tLoss: 3.695336\n",
      "Train Epoche: 3 [942/96 (981%)]\tLoss: 15.508863\n",
      "Train Epoche: 3 [943/96 (982%)]\tLoss: 1.468164\n",
      "Train Epoche: 3 [944/96 (983%)]\tLoss: 0.000000\n",
      "Train Epoche: 3 [945/96 (984%)]\tLoss: 1.028389\n",
      "Train Epoche: 3 [946/96 (985%)]\tLoss: 4.182600\n",
      "Train Epoche: 3 [947/96 (986%)]\tLoss: 33.484123\n",
      "Train Epoche: 3 [948/96 (988%)]\tLoss: 11.723174\n",
      "Train Epoche: 3 [949/96 (989%)]\tLoss: 11.969673\n",
      "Train Epoche: 3 [950/96 (990%)]\tLoss: 0.070541\n",
      "Train Epoche: 3 [951/96 (991%)]\tLoss: 17.784866\n",
      "Train Epoche: 3 [952/96 (992%)]\tLoss: 0.003406\n",
      "Train Epoche: 3 [953/96 (993%)]\tLoss: 38.922413\n",
      "Train Epoche: 3 [954/96 (994%)]\tLoss: 100.054367\n",
      "Train Epoche: 3 [955/96 (995%)]\tLoss: 7.917727\n",
      "Train Epoche: 3 [956/96 (996%)]\tLoss: 6.338714\n",
      "Train Epoche: 3 [957/96 (997%)]\tLoss: 1.083054\n",
      "Train Epoche: 3 [958/96 (998%)]\tLoss: 0.770484\n",
      "Train Epoche: 3 [959/96 (999%)]\tLoss: 0.023797\n",
      "Train Epoche: 3 [960/96 (1000%)]\tLoss: 0.722015\n",
      "Train Epoche: 3 [961/96 (1001%)]\tLoss: 21.837967\n",
      "Train Epoche: 3 [962/96 (1002%)]\tLoss: 1.832126\n",
      "Train Epoche: 3 [963/96 (1003%)]\tLoss: 1.841341\n",
      "Train Epoche: 3 [964/96 (1004%)]\tLoss: 0.149449\n",
      "Train Epoche: 3 [965/96 (1005%)]\tLoss: 5.092077\n",
      "Train Epoche: 3 [966/96 (1006%)]\tLoss: 1.386914\n",
      "Train Epoche: 3 [967/96 (1007%)]\tLoss: 1.367379\n",
      "Train Epoche: 3 [968/96 (1008%)]\tLoss: 1.871690\n",
      "Train Epoche: 3 [969/96 (1009%)]\tLoss: 1.310006\n",
      "Train Epoche: 3 [970/96 (1010%)]\tLoss: 102.410309\n",
      "Train Epoche: 3 [971/96 (1011%)]\tLoss: 31.856028\n",
      "Train Epoche: 3 [972/96 (1012%)]\tLoss: 20.412312\n",
      "Train Epoche: 3 [973/96 (1014%)]\tLoss: 0.218720\n",
      "Train Epoche: 3 [974/96 (1015%)]\tLoss: 86.024857\n",
      "Train Epoche: 3 [975/96 (1016%)]\tLoss: 224.959259\n",
      "Train Epoche: 3 [976/96 (1017%)]\tLoss: 8.344337\n",
      "Train Epoche: 3 [977/96 (1018%)]\tLoss: 43.830093\n",
      "Train Epoche: 3 [978/96 (1019%)]\tLoss: 21.767626\n",
      "Train Epoche: 3 [979/96 (1020%)]\tLoss: 2.710493\n",
      "Train Epoche: 3 [980/96 (1021%)]\tLoss: 11.023981\n",
      "Train Epoche: 3 [981/96 (1022%)]\tLoss: 2.890098\n",
      "Train Epoche: 3 [982/96 (1023%)]\tLoss: 0.810937\n",
      "Train Epoche: 3 [983/96 (1024%)]\tLoss: 5.461462\n",
      "Train Epoche: 3 [984/96 (1025%)]\tLoss: 8.906147\n",
      "Train Epoche: 3 [985/96 (1026%)]\tLoss: 12.952346\n",
      "Train Epoche: 3 [986/96 (1027%)]\tLoss: 0.119491\n",
      "Train Epoche: 3 [987/96 (1028%)]\tLoss: 3.401771\n",
      "Train Epoche: 3 [988/96 (1029%)]\tLoss: 0.275803\n",
      "Train Epoche: 3 [989/96 (1030%)]\tLoss: 6.479688\n",
      "Train Epoche: 3 [990/96 (1031%)]\tLoss: 0.348213\n",
      "Train Epoche: 3 [991/96 (1032%)]\tLoss: 0.117042\n",
      "Train Epoche: 3 [992/96 (1033%)]\tLoss: 0.002368\n",
      "Train Epoche: 3 [993/96 (1034%)]\tLoss: 18.438753\n",
      "Train Epoche: 3 [994/96 (1035%)]\tLoss: 2.921880\n",
      "Train Epoche: 3 [995/96 (1036%)]\tLoss: 0.449732\n",
      "Train Epoche: 3 [996/96 (1038%)]\tLoss: 2.042013\n",
      "Train Epoche: 3 [997/96 (1039%)]\tLoss: 1.865136\n",
      "Train Epoche: 3 [998/96 (1040%)]\tLoss: 44.161507\n",
      "Train Epoche: 3 [999/96 (1041%)]\tLoss: 62.066074\n",
      "Train Epoche: 3 [1000/96 (1042%)]\tLoss: 6.019685\n",
      "Train Epoche: 3 [1001/96 (1043%)]\tLoss: 29.092974\n",
      "Train Epoche: 3 [1002/96 (1044%)]\tLoss: 14.858603\n",
      "Train Epoche: 3 [1003/96 (1045%)]\tLoss: 4.791742\n",
      "Train Epoche: 3 [1004/96 (1046%)]\tLoss: 243.346497\n",
      "Train Epoche: 3 [1005/96 (1047%)]\tLoss: 25.738638\n",
      "Train Epoche: 3 [1006/96 (1048%)]\tLoss: 0.693262\n",
      "Train Epoche: 3 [1007/96 (1049%)]\tLoss: 2.248084\n",
      "Train Epoche: 3 [1008/96 (1050%)]\tLoss: 1.434096\n",
      "Train Epoche: 3 [1009/96 (1051%)]\tLoss: 60.081127\n",
      "Train Epoche: 3 [1010/96 (1052%)]\tLoss: 37.285450\n",
      "Train Epoche: 3 [1011/96 (1053%)]\tLoss: 1.836047\n",
      "Train Epoche: 3 [1012/96 (1054%)]\tLoss: 0.453573\n",
      "Train Epoche: 3 [1013/96 (1055%)]\tLoss: 1.636113\n",
      "Train Epoche: 3 [1014/96 (1056%)]\tLoss: 4.475605\n",
      "Train Epoche: 3 [1015/96 (1057%)]\tLoss: 34.352360\n",
      "Train Epoche: 3 [1016/96 (1058%)]\tLoss: 2.215214\n",
      "Train Epoche: 3 [1017/96 (1059%)]\tLoss: 16.781128\n",
      "Train Epoche: 3 [1018/96 (1060%)]\tLoss: 5.086905\n",
      "Train Epoche: 3 [1019/96 (1061%)]\tLoss: 0.066873\n",
      "Train Epoche: 3 [1020/96 (1062%)]\tLoss: 48.942310\n",
      "Train Epoche: 3 [1021/96 (1064%)]\tLoss: 0.009971\n",
      "Train Epoche: 3 [1022/96 (1065%)]\tLoss: 91.874237\n",
      "Train Epoche: 3 [1023/96 (1066%)]\tLoss: 0.002448\n",
      "Train Epoche: 3 [1024/96 (1067%)]\tLoss: 9.298587\n",
      "Train Epoche: 3 [1025/96 (1068%)]\tLoss: 3.661210\n",
      "Train Epoche: 3 [1026/96 (1069%)]\tLoss: 1.537346\n",
      "Train Epoche: 3 [1027/96 (1070%)]\tLoss: 0.317927\n",
      "Train Epoche: 3 [1028/96 (1071%)]\tLoss: 16.293594\n",
      "Train Epoche: 3 [1029/96 (1072%)]\tLoss: 31.400606\n",
      "Train Epoche: 3 [1030/96 (1073%)]\tLoss: 0.897723\n",
      "Train Epoche: 3 [1031/96 (1074%)]\tLoss: 9.794000\n",
      "Train Epoche: 3 [1032/96 (1075%)]\tLoss: 3.443210\n",
      "Train Epoche: 3 [1033/96 (1076%)]\tLoss: 3.877359\n",
      "Train Epoche: 3 [1034/96 (1077%)]\tLoss: 1.565091\n",
      "Train Epoche: 3 [1035/96 (1078%)]\tLoss: 2.766120\n",
      "Train Epoche: 3 [1036/96 (1079%)]\tLoss: 0.595497\n",
      "Train Epoche: 3 [1037/96 (1080%)]\tLoss: 5.188684\n",
      "Train Epoche: 3 [1038/96 (1081%)]\tLoss: 0.752507\n",
      "Train Epoche: 3 [1039/96 (1082%)]\tLoss: 37.329765\n",
      "Train Epoche: 3 [1040/96 (1083%)]\tLoss: 15.424835\n",
      "Train Epoche: 3 [1041/96 (1084%)]\tLoss: 0.237638\n",
      "Train Epoche: 3 [1042/96 (1085%)]\tLoss: 6.255349\n",
      "Train Epoche: 3 [1043/96 (1086%)]\tLoss: 1.229577\n",
      "Train Epoche: 3 [1044/96 (1088%)]\tLoss: 1.203060\n",
      "Train Epoche: 3 [1045/96 (1089%)]\tLoss: 4.018626\n",
      "Train Epoche: 3 [1046/96 (1090%)]\tLoss: 5.805531\n",
      "Train Epoche: 3 [1047/96 (1091%)]\tLoss: 14.659908\n",
      "Train Epoche: 3 [1048/96 (1092%)]\tLoss: 5.503348\n",
      "Train Epoche: 3 [1049/96 (1093%)]\tLoss: 6.822526\n",
      "Train Epoche: 3 [1050/96 (1094%)]\tLoss: 0.287870\n",
      "Train Epoche: 3 [1051/96 (1095%)]\tLoss: 95.488243\n",
      "Train Epoche: 3 [1052/96 (1096%)]\tLoss: 3.577085\n",
      "Train Epoche: 3 [1053/96 (1097%)]\tLoss: 6.531984\n",
      "Train Epoche: 3 [1054/96 (1098%)]\tLoss: 3.504526\n",
      "Train Epoche: 3 [1055/96 (1099%)]\tLoss: 48.422791\n",
      "Train Epoche: 3 [1056/96 (1100%)]\tLoss: 65.303673\n",
      "Train Epoche: 3 [1057/96 (1101%)]\tLoss: 1.464095\n",
      "Train Epoche: 3 [1058/96 (1102%)]\tLoss: 19.955435\n",
      "Train Epoche: 3 [1059/96 (1103%)]\tLoss: 36.362980\n",
      "Train Epoche: 3 [1060/96 (1104%)]\tLoss: 3.802600\n",
      "Train Epoche: 3 [1061/96 (1105%)]\tLoss: 0.795046\n",
      "Train Epoche: 3 [1062/96 (1106%)]\tLoss: 11.684623\n",
      "Train Epoche: 3 [1063/96 (1107%)]\tLoss: 27.764582\n",
      "Train Epoche: 3 [1064/96 (1108%)]\tLoss: 0.124160\n",
      "Train Epoche: 3 [1065/96 (1109%)]\tLoss: 29.529839\n",
      "Train Epoche: 3 [1066/96 (1110%)]\tLoss: 2.954572\n",
      "Train Epoche: 3 [1067/96 (1111%)]\tLoss: 2.366580\n",
      "Train Epoche: 3 [1068/96 (1112%)]\tLoss: 0.977316\n",
      "Train Epoche: 3 [1069/96 (1114%)]\tLoss: 0.015253\n",
      "Train Epoche: 3 [1070/96 (1115%)]\tLoss: 0.055505\n",
      "Train Epoche: 3 [1071/96 (1116%)]\tLoss: 0.106772\n",
      "Train Epoche: 3 [1072/96 (1117%)]\tLoss: 7.755311\n",
      "Train Epoche: 3 [1073/96 (1118%)]\tLoss: 4.294125\n",
      "Train Epoche: 3 [1074/96 (1119%)]\tLoss: 1.098393\n",
      "Train Epoche: 3 [1075/96 (1120%)]\tLoss: 10.857019\n",
      "Train Epoche: 3 [1076/96 (1121%)]\tLoss: 8.750453\n",
      "Train Epoche: 3 [1077/96 (1122%)]\tLoss: 7.020456\n",
      "Train Epoche: 3 [1078/96 (1123%)]\tLoss: 0.802353\n",
      "Train Epoche: 3 [1079/96 (1124%)]\tLoss: 54.146420\n",
      "Train Epoche: 3 [1080/96 (1125%)]\tLoss: 29.305548\n",
      "Train Epoche: 3 [1081/96 (1126%)]\tLoss: 5.529563\n",
      "Train Epoche: 3 [1082/96 (1127%)]\tLoss: 1.180931\n",
      "Train Epoche: 3 [1083/96 (1128%)]\tLoss: 8.323411\n",
      "Train Epoche: 3 [1084/96 (1129%)]\tLoss: 0.003279\n",
      "Train Epoche: 3 [1085/96 (1130%)]\tLoss: 1.447845\n",
      "Train Epoche: 3 [1086/96 (1131%)]\tLoss: 0.632181\n",
      "Train Epoche: 3 [1087/96 (1132%)]\tLoss: 1.890893\n",
      "Train Epoche: 3 [1088/96 (1133%)]\tLoss: 9.718184\n",
      "Train Epoche: 3 [1089/96 (1134%)]\tLoss: 8.051117\n",
      "Train Epoche: 3 [1090/96 (1135%)]\tLoss: 4.057690\n",
      "Train Epoche: 3 [1091/96 (1136%)]\tLoss: 51.715103\n",
      "Train Epoche: 3 [1092/96 (1138%)]\tLoss: 4.752352\n",
      "Train Epoche: 3 [1093/96 (1139%)]\tLoss: 0.091574\n",
      "Train Epoche: 3 [1094/96 (1140%)]\tLoss: 2.211519\n",
      "Train Epoche: 3 [1095/96 (1141%)]\tLoss: 7.512774\n",
      "Train Epoche: 3 [1096/96 (1142%)]\tLoss: 5.298862\n",
      "Train Epoche: 3 [1097/96 (1143%)]\tLoss: 9.145247\n",
      "Train Epoche: 3 [1098/96 (1144%)]\tLoss: 415.708282\n",
      "Train Epoche: 3 [1099/96 (1145%)]\tLoss: 7.645789\n",
      "Train Epoche: 3 [1100/96 (1146%)]\tLoss: 16.859478\n",
      "Train Epoche: 3 [1101/96 (1147%)]\tLoss: 388.913269\n",
      "Train Epoche: 3 [1102/96 (1148%)]\tLoss: 24.161219\n",
      "Train Epoche: 3 [1103/96 (1149%)]\tLoss: 33.493637\n",
      "Train Epoche: 3 [1104/96 (1150%)]\tLoss: 1.174905\n",
      "Train Epoche: 3 [1105/96 (1151%)]\tLoss: 155.800430\n",
      "Train Epoche: 3 [1106/96 (1152%)]\tLoss: 0.190015\n",
      "Train Epoche: 3 [1107/96 (1153%)]\tLoss: 9.083012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1108/96 (1154%)]\tLoss: 0.156367\n",
      "Train Epoche: 3 [1109/96 (1155%)]\tLoss: 4.511023\n",
      "Train Epoche: 3 [1110/96 (1156%)]\tLoss: 0.925082\n",
      "Train Epoche: 3 [1111/96 (1157%)]\tLoss: 9.286353\n",
      "Train Epoche: 3 [1112/96 (1158%)]\tLoss: 0.305780\n",
      "Train Epoche: 3 [1113/96 (1159%)]\tLoss: 363.016571\n",
      "Train Epoche: 3 [1114/96 (1160%)]\tLoss: 9.384398\n",
      "Train Epoche: 3 [1115/96 (1161%)]\tLoss: 19.991093\n",
      "Train Epoche: 3 [1116/96 (1162%)]\tLoss: 2.468032\n",
      "Train Epoche: 3 [1117/96 (1164%)]\tLoss: 3.632204\n",
      "Train Epoche: 3 [1118/96 (1165%)]\tLoss: 11.455083\n",
      "Train Epoche: 3 [1119/96 (1166%)]\tLoss: 6.201006\n",
      "Train Epoche: 3 [1120/96 (1167%)]\tLoss: 88.473557\n",
      "Train Epoche: 3 [1121/96 (1168%)]\tLoss: 0.016317\n",
      "Train Epoche: 3 [1122/96 (1169%)]\tLoss: 107.791702\n",
      "Train Epoche: 3 [1123/96 (1170%)]\tLoss: 56.383762\n",
      "Train Epoche: 3 [1124/96 (1171%)]\tLoss: 3.657081\n",
      "Train Epoche: 3 [1125/96 (1172%)]\tLoss: 4.187640\n",
      "Train Epoche: 3 [1126/96 (1173%)]\tLoss: 22.485439\n",
      "Train Epoche: 3 [1127/96 (1174%)]\tLoss: 40.960373\n",
      "Train Epoche: 3 [1128/96 (1175%)]\tLoss: 5.387303\n",
      "Train Epoche: 3 [1129/96 (1176%)]\tLoss: 8.382027\n",
      "Train Epoche: 3 [1130/96 (1177%)]\tLoss: 20.250885\n",
      "Train Epoche: 3 [1131/96 (1178%)]\tLoss: 3.534397\n",
      "Train Epoche: 3 [1132/96 (1179%)]\tLoss: 4.669143\n",
      "Train Epoche: 3 [1133/96 (1180%)]\tLoss: 0.016120\n",
      "Train Epoche: 3 [1134/96 (1181%)]\tLoss: 9.300628\n",
      "Train Epoche: 3 [1135/96 (1182%)]\tLoss: 1.154411\n",
      "Train Epoche: 3 [1136/96 (1183%)]\tLoss: 181.062683\n",
      "Train Epoche: 3 [1137/96 (1184%)]\tLoss: 0.000854\n",
      "Train Epoche: 3 [1138/96 (1185%)]\tLoss: 8.840592\n",
      "Train Epoche: 3 [1139/96 (1186%)]\tLoss: 0.000904\n",
      "Train Epoche: 3 [1140/96 (1188%)]\tLoss: 0.513002\n",
      "Train Epoche: 3 [1141/96 (1189%)]\tLoss: 43.743702\n",
      "Train Epoche: 3 [1142/96 (1190%)]\tLoss: 4.706939\n",
      "Train Epoche: 3 [1143/96 (1191%)]\tLoss: 0.038255\n",
      "Train Epoche: 3 [1144/96 (1192%)]\tLoss: 30.239122\n",
      "Train Epoche: 3 [1145/96 (1193%)]\tLoss: 3.207699\n",
      "Train Epoche: 3 [1146/96 (1194%)]\tLoss: 28.946301\n",
      "Train Epoche: 3 [1147/96 (1195%)]\tLoss: 3.092920\n",
      "Train Epoche: 3 [1148/96 (1196%)]\tLoss: 8.177509\n",
      "Train Epoche: 3 [1149/96 (1197%)]\tLoss: 0.992305\n",
      "Train Epoche: 3 [1150/96 (1198%)]\tLoss: 5.949603\n",
      "Train Epoche: 3 [1151/96 (1199%)]\tLoss: 2.000487\n",
      "Train Epoche: 3 [1152/96 (1200%)]\tLoss: 1.020601\n",
      "Train Epoche: 3 [1153/96 (1201%)]\tLoss: 0.142032\n",
      "Train Epoche: 3 [1154/96 (1202%)]\tLoss: 1.499290\n",
      "Train Epoche: 3 [1155/96 (1203%)]\tLoss: 0.990235\n",
      "Train Epoche: 3 [1156/96 (1204%)]\tLoss: 29.004246\n",
      "Train Epoche: 3 [1157/96 (1205%)]\tLoss: 0.091408\n",
      "Train Epoche: 3 [1158/96 (1206%)]\tLoss: 0.084606\n",
      "Train Epoche: 3 [1159/96 (1207%)]\tLoss: 14.491007\n",
      "Train Epoche: 3 [1160/96 (1208%)]\tLoss: 4.328927\n",
      "Train Epoche: 3 [1161/96 (1209%)]\tLoss: 16.679808\n",
      "Train Epoche: 3 [1162/96 (1210%)]\tLoss: 25.991875\n",
      "Train Epoche: 3 [1163/96 (1211%)]\tLoss: 26.953482\n",
      "Train Epoche: 3 [1164/96 (1212%)]\tLoss: 2.443776\n",
      "Train Epoche: 3 [1165/96 (1214%)]\tLoss: 24.531672\n",
      "Train Epoche: 3 [1166/96 (1215%)]\tLoss: 87.429710\n",
      "Train Epoche: 3 [1167/96 (1216%)]\tLoss: 4.244702\n",
      "Train Epoche: 3 [1168/96 (1217%)]\tLoss: 0.445302\n",
      "Train Epoche: 3 [1169/96 (1218%)]\tLoss: 1.297163\n",
      "Train Epoche: 3 [1170/96 (1219%)]\tLoss: 1.748673\n",
      "Train Epoche: 3 [1171/96 (1220%)]\tLoss: 0.332053\n",
      "Train Epoche: 3 [1172/96 (1221%)]\tLoss: 262.765625\n",
      "Train Epoche: 3 [1173/96 (1222%)]\tLoss: 11.534072\n",
      "Train Epoche: 3 [1174/96 (1223%)]\tLoss: 10.943373\n",
      "Train Epoche: 3 [1175/96 (1224%)]\tLoss: 13.858386\n",
      "Train Epoche: 3 [1176/96 (1225%)]\tLoss: 1.836309\n",
      "Train Epoche: 3 [1177/96 (1226%)]\tLoss: 5.583724\n",
      "Train Epoche: 3 [1178/96 (1227%)]\tLoss: 15.649983\n",
      "Train Epoche: 3 [1179/96 (1228%)]\tLoss: 2.936490\n",
      "Train Epoche: 3 [1180/96 (1229%)]\tLoss: 3.115987\n",
      "Train Epoche: 3 [1181/96 (1230%)]\tLoss: 7.474084\n",
      "Train Epoche: 3 [1182/96 (1231%)]\tLoss: 6.300431\n",
      "Train Epoche: 3 [1183/96 (1232%)]\tLoss: 1.785705\n",
      "Train Epoche: 3 [1184/96 (1233%)]\tLoss: 5.369901\n",
      "Train Epoche: 3 [1185/96 (1234%)]\tLoss: 1.270434\n",
      "Train Epoche: 3 [1186/96 (1235%)]\tLoss: 3.196377\n",
      "Train Epoche: 3 [1187/96 (1236%)]\tLoss: 11.924576\n",
      "Train Epoche: 3 [1188/96 (1238%)]\tLoss: 2.493089\n",
      "Train Epoche: 3 [1189/96 (1239%)]\tLoss: 10.788749\n",
      "Train Epoche: 3 [1190/96 (1240%)]\tLoss: 2.612591\n",
      "Train Epoche: 3 [1191/96 (1241%)]\tLoss: 66.276588\n",
      "Train Epoche: 3 [1192/96 (1242%)]\tLoss: 2.540024\n",
      "Train Epoche: 3 [1193/96 (1243%)]\tLoss: 7.968031\n",
      "Train Epoche: 3 [1194/96 (1244%)]\tLoss: 66.178680\n",
      "Train Epoche: 3 [1195/96 (1245%)]\tLoss: 73.863800\n",
      "Train Epoche: 3 [1196/96 (1246%)]\tLoss: 19.580635\n",
      "Train Epoche: 3 [1197/96 (1247%)]\tLoss: 12.588706\n",
      "Train Epoche: 3 [1198/96 (1248%)]\tLoss: 0.136125\n",
      "Train Epoche: 3 [1199/96 (1249%)]\tLoss: 29.146633\n",
      "Train Epoche: 3 [1200/96 (1250%)]\tLoss: 26.379354\n",
      "Train Epoche: 3 [1201/96 (1251%)]\tLoss: 0.274964\n",
      "Train Epoche: 3 [1202/96 (1252%)]\tLoss: 4.733012\n",
      "Train Epoche: 3 [1203/96 (1253%)]\tLoss: 19.285612\n",
      "Train Epoche: 3 [1204/96 (1254%)]\tLoss: 0.457700\n",
      "Train Epoche: 3 [1205/96 (1255%)]\tLoss: 23.098469\n",
      "Train Epoche: 3 [1206/96 (1256%)]\tLoss: 1.189273\n",
      "Train Epoche: 3 [1207/96 (1257%)]\tLoss: 0.024937\n",
      "Train Epoche: 3 [1208/96 (1258%)]\tLoss: 41.968037\n",
      "Train Epoche: 3 [1209/96 (1259%)]\tLoss: 26.148922\n",
      "Train Epoche: 3 [1210/96 (1260%)]\tLoss: 1.277106\n",
      "Train Epoche: 3 [1211/96 (1261%)]\tLoss: 2.170610\n",
      "Train Epoche: 3 [1212/96 (1262%)]\tLoss: 52.735428\n",
      "Train Epoche: 3 [1213/96 (1264%)]\tLoss: 2.961122\n",
      "Train Epoche: 3 [1214/96 (1265%)]\tLoss: 1.891186\n",
      "Train Epoche: 3 [1215/96 (1266%)]\tLoss: 52.918293\n",
      "Train Epoche: 3 [1216/96 (1267%)]\tLoss: 0.794595\n",
      "Train Epoche: 3 [1217/96 (1268%)]\tLoss: 3.185167\n",
      "Train Epoche: 3 [1218/96 (1269%)]\tLoss: 0.041113\n",
      "Train Epoche: 3 [1219/96 (1270%)]\tLoss: 0.984228\n",
      "Train Epoche: 3 [1220/96 (1271%)]\tLoss: 8.698318\n",
      "Train Epoche: 3 [1221/96 (1272%)]\tLoss: 7.822910\n",
      "Train Epoche: 3 [1222/96 (1273%)]\tLoss: 1.215639\n",
      "Train Epoche: 3 [1223/96 (1274%)]\tLoss: 7.845294\n",
      "Train Epoche: 3 [1224/96 (1275%)]\tLoss: 0.077510\n",
      "Train Epoche: 3 [1225/96 (1276%)]\tLoss: 0.508503\n",
      "Train Epoche: 3 [1226/96 (1277%)]\tLoss: 33.025417\n",
      "Train Epoche: 3 [1227/96 (1278%)]\tLoss: 52.289001\n",
      "Train Epoche: 3 [1228/96 (1279%)]\tLoss: 17.206757\n",
      "Train Epoche: 3 [1229/96 (1280%)]\tLoss: 219.345978\n",
      "Train Epoche: 3 [1230/96 (1281%)]\tLoss: 17.746237\n",
      "Train Epoche: 3 [1231/96 (1282%)]\tLoss: 94.807411\n",
      "Train Epoche: 3 [1232/96 (1283%)]\tLoss: 2.016375\n",
      "Train Epoche: 3 [1233/96 (1284%)]\tLoss: 248.174271\n",
      "Train Epoche: 3 [1234/96 (1285%)]\tLoss: 3.717256\n",
      "Train Epoche: 3 [1235/96 (1286%)]\tLoss: 34.974060\n",
      "Train Epoche: 3 [1236/96 (1288%)]\tLoss: 2.958322\n",
      "Train Epoche: 3 [1237/96 (1289%)]\tLoss: 6.860481\n",
      "Train Epoche: 3 [1238/96 (1290%)]\tLoss: 7.252336\n",
      "Train Epoche: 3 [1239/96 (1291%)]\tLoss: 1.436717\n",
      "Train Epoche: 3 [1240/96 (1292%)]\tLoss: 146.571487\n",
      "Train Epoche: 3 [1241/96 (1293%)]\tLoss: 51.555717\n",
      "Train Epoche: 3 [1242/96 (1294%)]\tLoss: 43.685753\n",
      "Train Epoche: 3 [1243/96 (1295%)]\tLoss: 3.683281\n",
      "Train Epoche: 3 [1244/96 (1296%)]\tLoss: 5.337860\n",
      "Train Epoche: 3 [1245/96 (1297%)]\tLoss: 13.961588\n",
      "Train Epoche: 3 [1246/96 (1298%)]\tLoss: 22.020908\n",
      "Train Epoche: 3 [1247/96 (1299%)]\tLoss: 1.093969\n",
      "Train Epoche: 3 [1248/96 (1300%)]\tLoss: 0.124276\n",
      "Train Epoche: 3 [1249/96 (1301%)]\tLoss: 73.160637\n",
      "Train Epoche: 3 [1250/96 (1302%)]\tLoss: 0.036035\n",
      "Train Epoche: 3 [1251/96 (1303%)]\tLoss: 2.368308\n",
      "Train Epoche: 3 [1252/96 (1304%)]\tLoss: 5.118813\n",
      "Train Epoche: 3 [1253/96 (1305%)]\tLoss: 0.268370\n",
      "Train Epoche: 3 [1254/96 (1306%)]\tLoss: 1.339950\n",
      "Train Epoche: 3 [1255/96 (1307%)]\tLoss: 2.600044\n",
      "Train Epoche: 3 [1256/96 (1308%)]\tLoss: 2.688370\n",
      "Train Epoche: 3 [1257/96 (1309%)]\tLoss: 36.943222\n",
      "Train Epoche: 3 [1258/96 (1310%)]\tLoss: 3.356384\n",
      "Train Epoche: 3 [1259/96 (1311%)]\tLoss: 0.172027\n",
      "Train Epoche: 3 [1260/96 (1312%)]\tLoss: 61.732738\n",
      "Train Epoche: 3 [1261/96 (1314%)]\tLoss: 103.073143\n",
      "Train Epoche: 3 [1262/96 (1315%)]\tLoss: 35.915993\n",
      "Train Epoche: 3 [1263/96 (1316%)]\tLoss: 0.530942\n",
      "Train Epoche: 3 [1264/96 (1317%)]\tLoss: 1.572147\n",
      "Train Epoche: 3 [1265/96 (1318%)]\tLoss: 13.400646\n",
      "Train Epoche: 3 [1266/96 (1319%)]\tLoss: 9.436906\n",
      "Train Epoche: 3 [1267/96 (1320%)]\tLoss: 2.492463\n",
      "Train Epoche: 3 [1268/96 (1321%)]\tLoss: 0.947072\n",
      "Train Epoche: 3 [1269/96 (1322%)]\tLoss: 10.788824\n",
      "Train Epoche: 3 [1270/96 (1323%)]\tLoss: 21.829054\n",
      "Train Epoche: 3 [1271/96 (1324%)]\tLoss: 0.014538\n",
      "Train Epoche: 3 [1272/96 (1325%)]\tLoss: 26.221401\n",
      "Train Epoche: 3 [1273/96 (1326%)]\tLoss: 0.795289\n",
      "Train Epoche: 3 [1274/96 (1327%)]\tLoss: 1.942691\n",
      "Train Epoche: 3 [1275/96 (1328%)]\tLoss: 68.093956\n",
      "Train Epoche: 3 [1276/96 (1329%)]\tLoss: 2.751137\n",
      "Train Epoche: 3 [1277/96 (1330%)]\tLoss: 2.753016\n",
      "Train Epoche: 3 [1278/96 (1331%)]\tLoss: 0.539962\n",
      "Train Epoche: 3 [1279/96 (1332%)]\tLoss: 0.387506\n",
      "Train Epoche: 3 [1280/96 (1333%)]\tLoss: 29.888079\n",
      "Train Epoche: 3 [1281/96 (1334%)]\tLoss: 6.329031\n",
      "Train Epoche: 3 [1282/96 (1335%)]\tLoss: 76.077927\n",
      "Train Epoche: 3 [1283/96 (1336%)]\tLoss: 53.333744\n",
      "Train Epoche: 3 [1284/96 (1338%)]\tLoss: 29.584351\n",
      "Train Epoche: 3 [1285/96 (1339%)]\tLoss: 47.189980\n",
      "Train Epoche: 3 [1286/96 (1340%)]\tLoss: 1.940525\n",
      "Train Epoche: 3 [1287/96 (1341%)]\tLoss: 0.623418\n",
      "Train Epoche: 3 [1288/96 (1342%)]\tLoss: 8.139482\n",
      "Train Epoche: 3 [1289/96 (1343%)]\tLoss: 41.385715\n",
      "Train Epoche: 3 [1290/96 (1344%)]\tLoss: 2.984978\n",
      "Train Epoche: 3 [1291/96 (1345%)]\tLoss: 95.275291\n",
      "Train Epoche: 3 [1292/96 (1346%)]\tLoss: 0.797312\n",
      "Train Epoche: 3 [1293/96 (1347%)]\tLoss: 0.097423\n",
      "Train Epoche: 3 [1294/96 (1348%)]\tLoss: 24.227427\n",
      "Train Epoche: 3 [1295/96 (1349%)]\tLoss: 201.376953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1296/96 (1350%)]\tLoss: 67.481430\n",
      "Train Epoche: 3 [1297/96 (1351%)]\tLoss: 6.850684\n",
      "Train Epoche: 3 [1298/96 (1352%)]\tLoss: 261.607452\n",
      "Train Epoche: 3 [1299/96 (1353%)]\tLoss: 105.764702\n",
      "Train Epoche: 3 [1300/96 (1354%)]\tLoss: 113.526398\n",
      "Train Epoche: 3 [1301/96 (1355%)]\tLoss: 0.570864\n",
      "Train Epoche: 3 [1302/96 (1356%)]\tLoss: 1.721682\n",
      "Train Epoche: 3 [1303/96 (1357%)]\tLoss: 1.055709\n",
      "Train Epoche: 3 [1304/96 (1358%)]\tLoss: 14.067106\n",
      "Train Epoche: 3 [1305/96 (1359%)]\tLoss: 1.249730\n",
      "Train Epoche: 3 [1306/96 (1360%)]\tLoss: 3.549500\n",
      "Train Epoche: 3 [1307/96 (1361%)]\tLoss: 0.127219\n",
      "Train Epoche: 3 [1308/96 (1362%)]\tLoss: 16.475815\n",
      "Train Epoche: 3 [1309/96 (1364%)]\tLoss: 1.594725\n",
      "Train Epoche: 3 [1310/96 (1365%)]\tLoss: 9.205517\n",
      "Train Epoche: 3 [1311/96 (1366%)]\tLoss: 0.727072\n",
      "Train Epoche: 3 [1312/96 (1367%)]\tLoss: 40.976280\n",
      "Train Epoche: 3 [1313/96 (1368%)]\tLoss: 15.041556\n",
      "Train Epoche: 3 [1314/96 (1369%)]\tLoss: 1.591872\n",
      "Train Epoche: 3 [1315/96 (1370%)]\tLoss: 1.934648\n",
      "Train Epoche: 3 [1316/96 (1371%)]\tLoss: 0.937531\n",
      "Train Epoche: 3 [1317/96 (1372%)]\tLoss: 5.654305\n",
      "Train Epoche: 3 [1318/96 (1373%)]\tLoss: 5.303842\n",
      "Train Epoche: 3 [1319/96 (1374%)]\tLoss: 0.138668\n",
      "Train Epoche: 3 [1320/96 (1375%)]\tLoss: 3.780071\n",
      "Train Epoche: 3 [1321/96 (1376%)]\tLoss: 2.510391\n",
      "Train Epoche: 3 [1322/96 (1377%)]\tLoss: 4.483848\n",
      "Train Epoche: 3 [1323/96 (1378%)]\tLoss: 15.896758\n",
      "Train Epoche: 3 [1324/96 (1379%)]\tLoss: 1.240022\n",
      "Train Epoche: 3 [1325/96 (1380%)]\tLoss: 1.738613\n",
      "Train Epoche: 3 [1326/96 (1381%)]\tLoss: 0.702805\n",
      "Train Epoche: 3 [1327/96 (1382%)]\tLoss: 281.988159\n",
      "Train Epoche: 3 [1328/96 (1383%)]\tLoss: 0.000412\n",
      "Train Epoche: 3 [1329/96 (1384%)]\tLoss: 37.439575\n",
      "Train Epoche: 3 [1330/96 (1385%)]\tLoss: 0.054333\n",
      "Train Epoche: 3 [1331/96 (1386%)]\tLoss: 10.048115\n",
      "Train Epoche: 3 [1332/96 (1388%)]\tLoss: 1.788288\n",
      "Train Epoche: 3 [1333/96 (1389%)]\tLoss: 2.322495\n",
      "Train Epoche: 3 [1334/96 (1390%)]\tLoss: 199.606857\n",
      "Train Epoche: 3 [1335/96 (1391%)]\tLoss: 1.240901\n",
      "Train Epoche: 3 [1336/96 (1392%)]\tLoss: 56.501122\n",
      "Train Epoche: 3 [1337/96 (1393%)]\tLoss: 43.204479\n",
      "Train Epoche: 3 [1338/96 (1394%)]\tLoss: 12.476106\n",
      "Train Epoche: 3 [1339/96 (1395%)]\tLoss: 0.042000\n",
      "Train Epoche: 3 [1340/96 (1396%)]\tLoss: 5.509280\n",
      "Train Epoche: 3 [1341/96 (1397%)]\tLoss: 54.454731\n",
      "Train Epoche: 3 [1342/96 (1398%)]\tLoss: 1.027098\n",
      "Train Epoche: 3 [1343/96 (1399%)]\tLoss: 3.554612\n",
      "Train Epoche: 3 [1344/96 (1400%)]\tLoss: 0.965645\n",
      "Train Epoche: 3 [1345/96 (1401%)]\tLoss: 2.257770\n",
      "Train Epoche: 3 [1346/96 (1402%)]\tLoss: 175.390030\n",
      "Train Epoche: 3 [1347/96 (1403%)]\tLoss: 0.966521\n",
      "Train Epoche: 3 [1348/96 (1404%)]\tLoss: 8.127988\n",
      "Train Epoche: 3 [1349/96 (1405%)]\tLoss: 0.248280\n",
      "Train Epoche: 3 [1350/96 (1406%)]\tLoss: 10.977768\n",
      "Train Epoche: 3 [1351/96 (1407%)]\tLoss: 2.516771\n",
      "Train Epoche: 3 [1352/96 (1408%)]\tLoss: 0.026059\n",
      "Train Epoche: 3 [1353/96 (1409%)]\tLoss: 3.092707\n",
      "Train Epoche: 3 [1354/96 (1410%)]\tLoss: 2.052430\n",
      "Train Epoche: 3 [1355/96 (1411%)]\tLoss: 0.006467\n",
      "Train Epoche: 3 [1356/96 (1412%)]\tLoss: 20.129656\n",
      "Train Epoche: 3 [1357/96 (1414%)]\tLoss: 40.863396\n",
      "Train Epoche: 3 [1358/96 (1415%)]\tLoss: 9.278877\n",
      "Train Epoche: 3 [1359/96 (1416%)]\tLoss: 3.401968\n",
      "Train Epoche: 3 [1360/96 (1417%)]\tLoss: 1.479657\n",
      "Train Epoche: 3 [1361/96 (1418%)]\tLoss: 17.736542\n",
      "Train Epoche: 3 [1362/96 (1419%)]\tLoss: 5.349849\n",
      "Train Epoche: 3 [1363/96 (1420%)]\tLoss: 9.325686\n",
      "Train Epoche: 3 [1364/96 (1421%)]\tLoss: 1.853760\n",
      "Train Epoche: 3 [1365/96 (1422%)]\tLoss: 23.153723\n",
      "Train Epoche: 3 [1366/96 (1423%)]\tLoss: 2.612705\n",
      "Train Epoche: 3 [1367/96 (1424%)]\tLoss: 0.174237\n",
      "Train Epoche: 3 [1368/96 (1425%)]\tLoss: 0.847394\n",
      "Train Epoche: 3 [1369/96 (1426%)]\tLoss: 22.107151\n",
      "Train Epoche: 3 [1370/96 (1427%)]\tLoss: 2.654669\n",
      "Train Epoche: 3 [1371/96 (1428%)]\tLoss: 0.695398\n",
      "Train Epoche: 3 [1372/96 (1429%)]\tLoss: 57.619133\n",
      "Train Epoche: 3 [1373/96 (1430%)]\tLoss: 19.644907\n",
      "Train Epoche: 3 [1374/96 (1431%)]\tLoss: 111.189445\n",
      "Train Epoche: 3 [1375/96 (1432%)]\tLoss: 228.863831\n",
      "Train Epoche: 3 [1376/96 (1433%)]\tLoss: 7.230198\n",
      "Train Epoche: 3 [1377/96 (1434%)]\tLoss: 160.477219\n",
      "Train Epoche: 3 [1378/96 (1435%)]\tLoss: 36.239098\n",
      "Train Epoche: 3 [1379/96 (1436%)]\tLoss: 7.264298\n",
      "Train Epoche: 3 [1380/96 (1438%)]\tLoss: 74.688499\n",
      "Train Epoche: 3 [1381/96 (1439%)]\tLoss: 10.117255\n",
      "Train Epoche: 3 [1382/96 (1440%)]\tLoss: 39.984428\n",
      "Train Epoche: 3 [1383/96 (1441%)]\tLoss: 1.950237\n",
      "Train Epoche: 3 [1384/96 (1442%)]\tLoss: 25.513994\n",
      "Train Epoche: 3 [1385/96 (1443%)]\tLoss: 13.515890\n",
      "Train Epoche: 3 [1386/96 (1444%)]\tLoss: 2.480739\n",
      "Train Epoche: 3 [1387/96 (1445%)]\tLoss: 3.467355\n",
      "Train Epoche: 3 [1388/96 (1446%)]\tLoss: 0.761380\n",
      "Train Epoche: 3 [1389/96 (1447%)]\tLoss: 0.473738\n",
      "Train Epoche: 3 [1390/96 (1448%)]\tLoss: 0.075202\n",
      "Train Epoche: 3 [1391/96 (1449%)]\tLoss: 14.779275\n",
      "Train Epoche: 3 [1392/96 (1450%)]\tLoss: 7.018041\n",
      "Train Epoche: 3 [1393/96 (1451%)]\tLoss: 1.310561\n",
      "Train Epoche: 3 [1394/96 (1452%)]\tLoss: 7.477806\n",
      "Train Epoche: 3 [1395/96 (1453%)]\tLoss: 8.923420\n",
      "Train Epoche: 3 [1396/96 (1454%)]\tLoss: 14.735561\n",
      "Train Epoche: 3 [1397/96 (1455%)]\tLoss: 1.283111\n",
      "Train Epoche: 3 [1398/96 (1456%)]\tLoss: 19.908184\n",
      "Train Epoche: 3 [1399/96 (1457%)]\tLoss: 4.295562\n",
      "Train Epoche: 3 [1400/96 (1458%)]\tLoss: 92.989723\n",
      "Train Epoche: 3 [1401/96 (1459%)]\tLoss: 0.561897\n",
      "Train Epoche: 3 [1402/96 (1460%)]\tLoss: 3.333591\n",
      "Train Epoche: 3 [1403/96 (1461%)]\tLoss: 62.979382\n",
      "Train Epoche: 3 [1404/96 (1462%)]\tLoss: 0.797624\n",
      "Train Epoche: 3 [1405/96 (1464%)]\tLoss: 0.407138\n",
      "Train Epoche: 3 [1406/96 (1465%)]\tLoss: 21.695019\n",
      "Train Epoche: 3 [1407/96 (1466%)]\tLoss: 18.607191\n",
      "Train Epoche: 3 [1408/96 (1467%)]\tLoss: 5.105808\n",
      "Train Epoche: 3 [1409/96 (1468%)]\tLoss: 4.388734\n",
      "Train Epoche: 3 [1410/96 (1469%)]\tLoss: 0.005166\n",
      "Train Epoche: 3 [1411/96 (1470%)]\tLoss: 3.080471\n",
      "Train Epoche: 3 [1412/96 (1471%)]\tLoss: 5.126663\n",
      "Train Epoche: 3 [1413/96 (1472%)]\tLoss: 18.021368\n",
      "Train Epoche: 3 [1414/96 (1473%)]\tLoss: 26.543148\n",
      "Train Epoche: 3 [1415/96 (1474%)]\tLoss: 28.592016\n",
      "Train Epoche: 3 [1416/96 (1475%)]\tLoss: 9.253049\n",
      "Train Epoche: 3 [1417/96 (1476%)]\tLoss: 60.691605\n",
      "Train Epoche: 3 [1418/96 (1477%)]\tLoss: 8.510093\n",
      "Train Epoche: 3 [1419/96 (1478%)]\tLoss: 14.003510\n",
      "Train Epoche: 3 [1420/96 (1479%)]\tLoss: 42.032143\n",
      "Train Epoche: 3 [1421/96 (1480%)]\tLoss: 68.397354\n",
      "Train Epoche: 3 [1422/96 (1481%)]\tLoss: 15.003454\n",
      "Train Epoche: 3 [1423/96 (1482%)]\tLoss: 0.153748\n",
      "Train Epoche: 3 [1424/96 (1483%)]\tLoss: 3.211599\n",
      "Train Epoche: 3 [1425/96 (1484%)]\tLoss: 44.630329\n",
      "Train Epoche: 3 [1426/96 (1485%)]\tLoss: 47.582294\n",
      "Train Epoche: 3 [1427/96 (1486%)]\tLoss: 2.136769\n",
      "Train Epoche: 3 [1428/96 (1488%)]\tLoss: 0.404328\n",
      "Train Epoche: 3 [1429/96 (1489%)]\tLoss: 29.912233\n",
      "Train Epoche: 3 [1430/96 (1490%)]\tLoss: 1.847662\n",
      "Train Epoche: 3 [1431/96 (1491%)]\tLoss: 0.498164\n",
      "Train Epoche: 3 [1432/96 (1492%)]\tLoss: 0.010401\n",
      "Train Epoche: 3 [1433/96 (1493%)]\tLoss: 7.123651\n",
      "Train Epoche: 3 [1434/96 (1494%)]\tLoss: 0.428601\n",
      "Train Epoche: 3 [1435/96 (1495%)]\tLoss: 4.119482\n",
      "Train Epoche: 3 [1436/96 (1496%)]\tLoss: 100.643944\n",
      "Train Epoche: 3 [1437/96 (1497%)]\tLoss: 142.858383\n",
      "Train Epoche: 3 [1438/96 (1498%)]\tLoss: 8.548793\n",
      "Train Epoche: 3 [1439/96 (1499%)]\tLoss: 0.142597\n",
      "Train Epoche: 3 [1440/96 (1500%)]\tLoss: 5.038860\n",
      "Train Epoche: 3 [1441/96 (1501%)]\tLoss: 0.459027\n",
      "Train Epoche: 3 [1442/96 (1502%)]\tLoss: 2.374297\n",
      "Train Epoche: 3 [1443/96 (1503%)]\tLoss: 62.531517\n",
      "Train Epoche: 3 [1444/96 (1504%)]\tLoss: 9.327317\n",
      "Train Epoche: 3 [1445/96 (1505%)]\tLoss: 3.348387\n",
      "Train Epoche: 3 [1446/96 (1506%)]\tLoss: 28.926702\n",
      "Train Epoche: 3 [1447/96 (1507%)]\tLoss: 20.327898\n",
      "Train Epoche: 3 [1448/96 (1508%)]\tLoss: 3.451759\n",
      "Train Epoche: 3 [1449/96 (1509%)]\tLoss: 16.430237\n",
      "Train Epoche: 3 [1450/96 (1510%)]\tLoss: 1.546494\n",
      "Train Epoche: 3 [1451/96 (1511%)]\tLoss: 2.142131\n",
      "Train Epoche: 3 [1452/96 (1512%)]\tLoss: 3.328936\n",
      "Train Epoche: 3 [1453/96 (1514%)]\tLoss: 171.099838\n",
      "Train Epoche: 3 [1454/96 (1515%)]\tLoss: 5.505317\n",
      "Train Epoche: 3 [1455/96 (1516%)]\tLoss: 8.941314\n",
      "Train Epoche: 3 [1456/96 (1517%)]\tLoss: 6.490541\n",
      "Train Epoche: 3 [1457/96 (1518%)]\tLoss: 0.138218\n",
      "Train Epoche: 3 [1458/96 (1519%)]\tLoss: 38.191284\n",
      "Train Epoche: 3 [1459/96 (1520%)]\tLoss: 6.843613\n",
      "Train Epoche: 3 [1460/96 (1521%)]\tLoss: 0.031317\n",
      "Train Epoche: 3 [1461/96 (1522%)]\tLoss: 18.501034\n",
      "Train Epoche: 3 [1462/96 (1523%)]\tLoss: 38.966999\n",
      "Train Epoche: 3 [1463/96 (1524%)]\tLoss: 1.742666\n",
      "Train Epoche: 3 [1464/96 (1525%)]\tLoss: 341.326263\n",
      "Train Epoche: 3 [1465/96 (1526%)]\tLoss: 0.807708\n",
      "Train Epoche: 3 [1466/96 (1527%)]\tLoss: 5.780354\n",
      "Train Epoche: 3 [1467/96 (1528%)]\tLoss: 3.149486\n",
      "Train Epoche: 3 [1468/96 (1529%)]\tLoss: 0.737785\n",
      "Train Epoche: 3 [1469/96 (1530%)]\tLoss: 1.612024\n",
      "Train Epoche: 3 [1470/96 (1531%)]\tLoss: 73.961334\n",
      "Train Epoche: 3 [1471/96 (1532%)]\tLoss: 37.716694\n",
      "Train Epoche: 3 [1472/96 (1533%)]\tLoss: 30.412163\n",
      "Train Epoche: 3 [1473/96 (1534%)]\tLoss: 5.164551\n",
      "Train Epoche: 3 [1474/96 (1535%)]\tLoss: 5.834063\n",
      "Train Epoche: 3 [1475/96 (1536%)]\tLoss: 5.025891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1476/96 (1538%)]\tLoss: 0.105481\n",
      "Train Epoche: 3 [1477/96 (1539%)]\tLoss: 28.375528\n",
      "Train Epoche: 3 [1478/96 (1540%)]\tLoss: 0.326524\n",
      "Train Epoche: 3 [1479/96 (1541%)]\tLoss: 10.364625\n",
      "Train Epoche: 3 [1480/96 (1542%)]\tLoss: 0.478207\n",
      "Train Epoche: 3 [1481/96 (1543%)]\tLoss: 3.410111\n",
      "Train Epoche: 3 [1482/96 (1544%)]\tLoss: 0.043723\n",
      "Train Epoche: 3 [1483/96 (1545%)]\tLoss: 2.285294\n",
      "Train Epoche: 3 [1484/96 (1546%)]\tLoss: 43.940273\n",
      "Train Epoche: 3 [1485/96 (1547%)]\tLoss: 5.527273\n",
      "Train Epoche: 3 [1486/96 (1548%)]\tLoss: 324.147980\n",
      "Train Epoche: 3 [1487/96 (1549%)]\tLoss: 10.926582\n",
      "Train Epoche: 3 [1488/96 (1550%)]\tLoss: 0.542050\n",
      "Train Epoche: 3 [1489/96 (1551%)]\tLoss: 69.802017\n",
      "Train Epoche: 3 [1490/96 (1552%)]\tLoss: 3.166318\n",
      "Train Epoche: 3 [1491/96 (1553%)]\tLoss: 54.661140\n",
      "Train Epoche: 3 [1492/96 (1554%)]\tLoss: 9.037846\n",
      "Train Epoche: 3 [1493/96 (1555%)]\tLoss: 11.863358\n",
      "Train Epoche: 3 [1494/96 (1556%)]\tLoss: 0.511602\n",
      "Train Epoche: 3 [1495/96 (1557%)]\tLoss: 4.610978\n",
      "Train Epoche: 3 [1496/96 (1558%)]\tLoss: 83.350250\n",
      "Train Epoche: 3 [1497/96 (1559%)]\tLoss: 30.203377\n",
      "Train Epoche: 3 [1498/96 (1560%)]\tLoss: 79.503494\n",
      "Train Epoche: 3 [1499/96 (1561%)]\tLoss: 2.695607\n",
      "Train Epoche: 3 [1500/96 (1562%)]\tLoss: 1.652313\n",
      "Train Epoche: 3 [1501/96 (1564%)]\tLoss: 8.902006\n",
      "Train Epoche: 3 [1502/96 (1565%)]\tLoss: 89.198669\n",
      "Train Epoche: 3 [1503/96 (1566%)]\tLoss: 0.160477\n",
      "Train Epoche: 3 [1504/96 (1567%)]\tLoss: 28.390675\n",
      "Train Epoche: 3 [1505/96 (1568%)]\tLoss: 36.238327\n",
      "Train Epoche: 3 [1506/96 (1569%)]\tLoss: 332.042603\n",
      "Train Epoche: 3 [1507/96 (1570%)]\tLoss: 5.049124\n",
      "Train Epoche: 3 [1508/96 (1571%)]\tLoss: 2.962848\n",
      "Train Epoche: 3 [1509/96 (1572%)]\tLoss: 61.524742\n",
      "Train Epoche: 3 [1510/96 (1573%)]\tLoss: 32.259106\n",
      "Train Epoche: 3 [1511/96 (1574%)]\tLoss: 56.399933\n",
      "Train Epoche: 3 [1512/96 (1575%)]\tLoss: 18.042580\n",
      "Train Epoche: 3 [1513/96 (1576%)]\tLoss: 31.345062\n",
      "Train Epoche: 3 [1514/96 (1577%)]\tLoss: 110.858322\n",
      "Train Epoche: 3 [1515/96 (1578%)]\tLoss: 2.388293\n",
      "Train Epoche: 3 [1516/96 (1579%)]\tLoss: 11.987966\n",
      "Train Epoche: 3 [1517/96 (1580%)]\tLoss: 2.129440\n",
      "Train Epoche: 3 [1518/96 (1581%)]\tLoss: 16.521740\n",
      "Train Epoche: 3 [1519/96 (1582%)]\tLoss: 11.246598\n",
      "Train Epoche: 3 [1520/96 (1583%)]\tLoss: 76.256187\n",
      "Train Epoche: 3 [1521/96 (1584%)]\tLoss: 2.677568\n",
      "Train Epoche: 3 [1522/96 (1585%)]\tLoss: 0.084913\n",
      "Train Epoche: 3 [1523/96 (1586%)]\tLoss: 9.452701\n",
      "Train Epoche: 3 [1524/96 (1588%)]\tLoss: 28.622269\n",
      "Train Epoche: 3 [1525/96 (1589%)]\tLoss: 10.114088\n",
      "Train Epoche: 3 [1526/96 (1590%)]\tLoss: 0.011507\n",
      "Train Epoche: 3 [1527/96 (1591%)]\tLoss: 1.376405\n",
      "Train Epoche: 3 [1528/96 (1592%)]\tLoss: 47.004730\n",
      "Train Epoche: 3 [1529/96 (1593%)]\tLoss: 136.590546\n",
      "Train Epoche: 3 [1530/96 (1594%)]\tLoss: 6.550731\n",
      "Train Epoche: 3 [1531/96 (1595%)]\tLoss: 34.419994\n",
      "Train Epoche: 3 [1532/96 (1596%)]\tLoss: 11.569195\n",
      "Train Epoche: 3 [1533/96 (1597%)]\tLoss: 154.531158\n",
      "Train Epoche: 3 [1534/96 (1598%)]\tLoss: 80.702507\n",
      "Train Epoche: 3 [1535/96 (1599%)]\tLoss: 2.811024\n",
      "Train Epoche: 3 [1536/96 (1600%)]\tLoss: 46.723198\n",
      "Train Epoche: 3 [1537/96 (1601%)]\tLoss: 2.211403\n",
      "Train Epoche: 3 [1538/96 (1602%)]\tLoss: 102.417290\n",
      "Train Epoche: 3 [1539/96 (1603%)]\tLoss: 3.442771\n",
      "Train Epoche: 3 [1540/96 (1604%)]\tLoss: 3.010108\n",
      "Train Epoche: 3 [1541/96 (1605%)]\tLoss: 411.428436\n",
      "Train Epoche: 3 [1542/96 (1606%)]\tLoss: 32.894951\n",
      "Train Epoche: 3 [1543/96 (1607%)]\tLoss: 53.267937\n",
      "Train Epoche: 3 [1544/96 (1608%)]\tLoss: 2.797651\n",
      "Train Epoche: 3 [1545/96 (1609%)]\tLoss: 191.339020\n",
      "Train Epoche: 3 [1546/96 (1610%)]\tLoss: 50.782845\n",
      "Train Epoche: 3 [1547/96 (1611%)]\tLoss: 11.778729\n",
      "Train Epoche: 3 [1548/96 (1612%)]\tLoss: 0.364875\n",
      "Train Epoche: 3 [1549/96 (1614%)]\tLoss: 39.250683\n",
      "Train Epoche: 3 [1550/96 (1615%)]\tLoss: 32.179958\n",
      "Train Epoche: 3 [1551/96 (1616%)]\tLoss: 0.849623\n",
      "Train Epoche: 3 [1552/96 (1617%)]\tLoss: 12.899505\n",
      "Train Epoche: 3 [1553/96 (1618%)]\tLoss: 0.814612\n",
      "Train Epoche: 3 [1554/96 (1619%)]\tLoss: 76.177711\n",
      "Train Epoche: 3 [1555/96 (1620%)]\tLoss: 38.026783\n",
      "Train Epoche: 3 [1556/96 (1621%)]\tLoss: 27.150908\n",
      "Train Epoche: 3 [1557/96 (1622%)]\tLoss: 9.233582\n",
      "Train Epoche: 3 [1558/96 (1623%)]\tLoss: 9.434199\n",
      "Train Epoche: 3 [1559/96 (1624%)]\tLoss: 7.271991\n",
      "Train Epoche: 3 [1560/96 (1625%)]\tLoss: 32.412582\n",
      "Train Epoche: 3 [1561/96 (1626%)]\tLoss: 0.175572\n",
      "Train Epoche: 3 [1562/96 (1627%)]\tLoss: 13.278549\n",
      "Train Epoche: 3 [1563/96 (1628%)]\tLoss: 0.478094\n",
      "Train Epoche: 3 [1564/96 (1629%)]\tLoss: 4.542502\n",
      "Train Epoche: 3 [1565/96 (1630%)]\tLoss: 0.810224\n",
      "Train Epoche: 3 [1566/96 (1631%)]\tLoss: 0.853494\n",
      "Train Epoche: 3 [1567/96 (1632%)]\tLoss: 0.174844\n",
      "Train Epoche: 3 [1568/96 (1633%)]\tLoss: 1.119565\n",
      "Train Epoche: 3 [1569/96 (1634%)]\tLoss: 2.444396\n",
      "Train Epoche: 3 [1570/96 (1635%)]\tLoss: 0.105974\n",
      "Train Epoche: 3 [1571/96 (1636%)]\tLoss: 3.789118\n",
      "Train Epoche: 3 [1572/96 (1638%)]\tLoss: 153.881989\n",
      "Train Epoche: 3 [1573/96 (1639%)]\tLoss: 1.239139\n",
      "Train Epoche: 3 [1574/96 (1640%)]\tLoss: 339.673279\n",
      "Train Epoche: 3 [1575/96 (1641%)]\tLoss: 3.937691\n",
      "Train Epoche: 3 [1576/96 (1642%)]\tLoss: 128.557999\n",
      "Train Epoche: 3 [1577/96 (1643%)]\tLoss: 236.340073\n",
      "Train Epoche: 3 [1578/96 (1644%)]\tLoss: 0.176350\n",
      "Train Epoche: 3 [1579/96 (1645%)]\tLoss: 3.381692\n",
      "Train Epoche: 3 [1580/96 (1646%)]\tLoss: 4.285538\n",
      "Train Epoche: 3 [1581/96 (1647%)]\tLoss: 0.771563\n",
      "Train Epoche: 3 [1582/96 (1648%)]\tLoss: 3.075384\n",
      "Train Epoche: 3 [1583/96 (1649%)]\tLoss: 81.192787\n",
      "Train Epoche: 3 [1584/96 (1650%)]\tLoss: 3.440302\n",
      "Train Epoche: 3 [1585/96 (1651%)]\tLoss: 1.949399\n",
      "Train Epoche: 3 [1586/96 (1652%)]\tLoss: 13.834280\n",
      "Train Epoche: 3 [1587/96 (1653%)]\tLoss: 4.863026\n",
      "Train Epoche: 3 [1588/96 (1654%)]\tLoss: 2.058361\n",
      "Train Epoche: 3 [1589/96 (1655%)]\tLoss: 6.948635\n",
      "Train Epoche: 3 [1590/96 (1656%)]\tLoss: 3.551436\n",
      "Train Epoche: 3 [1591/96 (1657%)]\tLoss: 145.945709\n",
      "Train Epoche: 3 [1592/96 (1658%)]\tLoss: 0.190240\n",
      "Train Epoche: 3 [1593/96 (1659%)]\tLoss: 6.950606\n",
      "Train Epoche: 3 [1594/96 (1660%)]\tLoss: 108.860870\n",
      "Train Epoche: 3 [1595/96 (1661%)]\tLoss: 15.943657\n",
      "Train Epoche: 3 [1596/96 (1662%)]\tLoss: 0.399099\n",
      "Train Epoche: 3 [1597/96 (1664%)]\tLoss: 6.974956\n",
      "Train Epoche: 3 [1598/96 (1665%)]\tLoss: 6.706985\n",
      "Train Epoche: 3 [1599/96 (1666%)]\tLoss: 1.344967\n",
      "Train Epoche: 3 [1600/96 (1667%)]\tLoss: 27.026720\n",
      "Train Epoche: 3 [1601/96 (1668%)]\tLoss: 3.407718\n",
      "Train Epoche: 3 [1602/96 (1669%)]\tLoss: 21.488014\n",
      "Train Epoche: 3 [1603/96 (1670%)]\tLoss: 7.049454\n",
      "Train Epoche: 3 [1604/96 (1671%)]\tLoss: 0.042921\n",
      "Train Epoche: 3 [1605/96 (1672%)]\tLoss: 0.027213\n",
      "Train Epoche: 3 [1606/96 (1673%)]\tLoss: 1.313943\n",
      "Train Epoche: 3 [1607/96 (1674%)]\tLoss: 12.165721\n",
      "Train Epoche: 3 [1608/96 (1675%)]\tLoss: 6.902441\n",
      "Train Epoche: 3 [1609/96 (1676%)]\tLoss: 11.595510\n",
      "Train Epoche: 3 [1610/96 (1677%)]\tLoss: 1.234035\n",
      "Train Epoche: 3 [1611/96 (1678%)]\tLoss: 1.597802\n",
      "Train Epoche: 3 [1612/96 (1679%)]\tLoss: 189.020538\n",
      "Train Epoche: 3 [1613/96 (1680%)]\tLoss: 4.085297\n",
      "Train Epoche: 3 [1614/96 (1681%)]\tLoss: 0.589528\n",
      "Train Epoche: 3 [1615/96 (1682%)]\tLoss: 6.947217\n",
      "Train Epoche: 3 [1616/96 (1683%)]\tLoss: 0.033341\n",
      "Train Epoche: 3 [1617/96 (1684%)]\tLoss: 1.014468\n",
      "Train Epoche: 3 [1618/96 (1685%)]\tLoss: 1.269791\n",
      "Train Epoche: 3 [1619/96 (1686%)]\tLoss: 16.938467\n",
      "Train Epoche: 3 [1620/96 (1688%)]\tLoss: 16.128109\n",
      "Train Epoche: 3 [1621/96 (1689%)]\tLoss: 7.923643\n",
      "Train Epoche: 3 [1622/96 (1690%)]\tLoss: 0.176615\n",
      "Train Epoche: 3 [1623/96 (1691%)]\tLoss: 10.080366\n",
      "Train Epoche: 3 [1624/96 (1692%)]\tLoss: 8.692413\n",
      "Train Epoche: 3 [1625/96 (1693%)]\tLoss: 1.509811\n",
      "Train Epoche: 3 [1626/96 (1694%)]\tLoss: 24.637629\n",
      "Train Epoche: 3 [1627/96 (1695%)]\tLoss: 0.687103\n",
      "Train Epoche: 3 [1628/96 (1696%)]\tLoss: 1.845831\n",
      "Train Epoche: 3 [1629/96 (1697%)]\tLoss: 1.183507\n",
      "Train Epoche: 3 [1630/96 (1698%)]\tLoss: 4.924172\n",
      "Train Epoche: 3 [1631/96 (1699%)]\tLoss: 0.118700\n",
      "Train Epoche: 3 [1632/96 (1700%)]\tLoss: 1.100879\n",
      "Train Epoche: 3 [1633/96 (1701%)]\tLoss: 1.289386\n",
      "Train Epoche: 3 [1634/96 (1702%)]\tLoss: 5.674646\n",
      "Train Epoche: 3 [1635/96 (1703%)]\tLoss: 0.444159\n",
      "Train Epoche: 3 [1636/96 (1704%)]\tLoss: 2.170318\n",
      "Train Epoche: 3 [1637/96 (1705%)]\tLoss: 0.027614\n",
      "Train Epoche: 3 [1638/96 (1706%)]\tLoss: 1.499831\n",
      "Train Epoche: 3 [1639/96 (1707%)]\tLoss: 0.624746\n",
      "Train Epoche: 3 [1640/96 (1708%)]\tLoss: 276.961639\n",
      "Train Epoche: 3 [1641/96 (1709%)]\tLoss: 11.450842\n",
      "Train Epoche: 3 [1642/96 (1710%)]\tLoss: 35.413326\n",
      "Train Epoche: 3 [1643/96 (1711%)]\tLoss: 0.047195\n",
      "Train Epoche: 3 [1644/96 (1712%)]\tLoss: 12.762622\n",
      "Train Epoche: 3 [1645/96 (1714%)]\tLoss: 0.265971\n",
      "Train Epoche: 3 [1646/96 (1715%)]\tLoss: 0.891033\n",
      "Train Epoche: 3 [1647/96 (1716%)]\tLoss: 55.579399\n",
      "Train Epoche: 3 [1648/96 (1717%)]\tLoss: 5.595818\n",
      "Train Epoche: 3 [1649/96 (1718%)]\tLoss: 41.222893\n",
      "Train Epoche: 3 [1650/96 (1719%)]\tLoss: 75.281540\n",
      "Train Epoche: 3 [1651/96 (1720%)]\tLoss: 37.554291\n",
      "Train Epoche: 3 [1652/96 (1721%)]\tLoss: 23.280798\n",
      "Train Epoche: 3 [1653/96 (1722%)]\tLoss: 26.039389\n",
      "Train Epoche: 3 [1654/96 (1723%)]\tLoss: 11.436317\n",
      "Train Epoche: 3 [1655/96 (1724%)]\tLoss: 9.107183\n",
      "Train Epoche: 3 [1656/96 (1725%)]\tLoss: 9.771359\n",
      "Train Epoche: 3 [1657/96 (1726%)]\tLoss: 4.305209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1658/96 (1727%)]\tLoss: 13.955473\n",
      "Train Epoche: 3 [1659/96 (1728%)]\tLoss: 1.670072\n",
      "Train Epoche: 3 [1660/96 (1729%)]\tLoss: 0.759870\n",
      "Train Epoche: 3 [1661/96 (1730%)]\tLoss: 0.461058\n",
      "Train Epoche: 3 [1662/96 (1731%)]\tLoss: 25.650126\n",
      "Train Epoche: 3 [1663/96 (1732%)]\tLoss: 7.764050\n",
      "Train Epoche: 3 [1664/96 (1733%)]\tLoss: 32.347862\n",
      "Train Epoche: 3 [1665/96 (1734%)]\tLoss: 6.351508\n",
      "Train Epoche: 3 [1666/96 (1735%)]\tLoss: 0.261513\n",
      "Train Epoche: 3 [1667/96 (1736%)]\tLoss: 96.657661\n",
      "Train Epoche: 3 [1668/96 (1738%)]\tLoss: 5.933106\n",
      "Train Epoche: 3 [1669/96 (1739%)]\tLoss: 25.463131\n",
      "Train Epoche: 3 [1670/96 (1740%)]\tLoss: 40.992241\n",
      "Train Epoche: 3 [1671/96 (1741%)]\tLoss: 0.784401\n",
      "Train Epoche: 3 [1672/96 (1742%)]\tLoss: 26.844538\n",
      "Train Epoche: 3 [1673/96 (1743%)]\tLoss: 51.290810\n",
      "Train Epoche: 3 [1674/96 (1744%)]\tLoss: 5.185530\n",
      "Train Epoche: 3 [1675/96 (1745%)]\tLoss: 2.592956\n",
      "Train Epoche: 3 [1676/96 (1746%)]\tLoss: 1.412520\n",
      "Train Epoche: 3 [1677/96 (1747%)]\tLoss: 96.289200\n",
      "Train Epoche: 3 [1678/96 (1748%)]\tLoss: 2.616258\n",
      "Train Epoche: 3 [1679/96 (1749%)]\tLoss: 13.015571\n",
      "Train Epoche: 3 [1680/96 (1750%)]\tLoss: 10.827356\n",
      "Train Epoche: 3 [1681/96 (1751%)]\tLoss: 29.882385\n",
      "Train Epoche: 3 [1682/96 (1752%)]\tLoss: 0.239449\n",
      "Train Epoche: 3 [1683/96 (1753%)]\tLoss: 0.217248\n",
      "Train Epoche: 3 [1684/96 (1754%)]\tLoss: 1.549642\n",
      "Train Epoche: 3 [1685/96 (1755%)]\tLoss: 1.971260\n",
      "Train Epoche: 3 [1686/96 (1756%)]\tLoss: 0.543140\n",
      "Train Epoche: 3 [1687/96 (1757%)]\tLoss: 2.256355\n",
      "Train Epoche: 3 [1688/96 (1758%)]\tLoss: 14.398689\n",
      "Train Epoche: 3 [1689/96 (1759%)]\tLoss: 0.222422\n",
      "Train Epoche: 3 [1690/96 (1760%)]\tLoss: 2.490782\n",
      "Train Epoche: 3 [1691/96 (1761%)]\tLoss: 1.790979\n",
      "Train Epoche: 3 [1692/96 (1762%)]\tLoss: 0.173780\n",
      "Train Epoche: 3 [1693/96 (1764%)]\tLoss: 5.495337\n",
      "Train Epoche: 3 [1694/96 (1765%)]\tLoss: 0.530552\n",
      "Train Epoche: 3 [1695/96 (1766%)]\tLoss: 1.832689\n",
      "Train Epoche: 3 [1696/96 (1767%)]\tLoss: 6.241801\n",
      "Train Epoche: 3 [1697/96 (1768%)]\tLoss: 25.200520\n",
      "Train Epoche: 3 [1698/96 (1769%)]\tLoss: 19.829142\n",
      "Train Epoche: 3 [1699/96 (1770%)]\tLoss: 3.600445\n",
      "Train Epoche: 3 [1700/96 (1771%)]\tLoss: 3.325572\n",
      "Train Epoche: 3 [1701/96 (1772%)]\tLoss: 0.019274\n",
      "Train Epoche: 3 [1702/96 (1773%)]\tLoss: 2.693754\n",
      "Train Epoche: 3 [1703/96 (1774%)]\tLoss: 50.741459\n",
      "Train Epoche: 3 [1704/96 (1775%)]\tLoss: 0.361727\n",
      "Train Epoche: 3 [1705/96 (1776%)]\tLoss: 0.205594\n",
      "Train Epoche: 3 [1706/96 (1777%)]\tLoss: 1.958994\n",
      "Train Epoche: 3 [1707/96 (1778%)]\tLoss: 4.125056\n",
      "Train Epoche: 3 [1708/96 (1779%)]\tLoss: 1.791746\n",
      "Train Epoche: 3 [1709/96 (1780%)]\tLoss: 11.003789\n",
      "Train Epoche: 3 [1710/96 (1781%)]\tLoss: 10.734019\n",
      "Train Epoche: 3 [1711/96 (1782%)]\tLoss: 6.473868\n",
      "Train Epoche: 3 [1712/96 (1783%)]\tLoss: 5.630587\n",
      "Train Epoche: 3 [1713/96 (1784%)]\tLoss: 4.424505\n",
      "Train Epoche: 3 [1714/96 (1785%)]\tLoss: 34.103462\n",
      "Train Epoche: 3 [1715/96 (1786%)]\tLoss: 68.733826\n",
      "Train Epoche: 3 [1716/96 (1788%)]\tLoss: 3.166447\n",
      "Train Epoche: 3 [1717/96 (1789%)]\tLoss: 25.712652\n",
      "Train Epoche: 3 [1718/96 (1790%)]\tLoss: 7.585586\n",
      "Train Epoche: 3 [1719/96 (1791%)]\tLoss: 2.724668\n",
      "Train Epoche: 3 [1720/96 (1792%)]\tLoss: 0.310508\n",
      "Train Epoche: 3 [1721/96 (1793%)]\tLoss: 0.018385\n",
      "Train Epoche: 3 [1722/96 (1794%)]\tLoss: 6.943266\n",
      "Train Epoche: 3 [1723/96 (1795%)]\tLoss: 2.000029\n",
      "Train Epoche: 3 [1724/96 (1796%)]\tLoss: 8.639701\n",
      "Train Epoche: 3 [1725/96 (1797%)]\tLoss: 4.686795\n",
      "Train Epoche: 3 [1726/96 (1798%)]\tLoss: 3.786252\n",
      "Train Epoche: 3 [1727/96 (1799%)]\tLoss: 1.201512\n",
      "Train Epoche: 3 [1728/96 (1800%)]\tLoss: 0.693238\n",
      "Train Epoche: 3 [1729/96 (1801%)]\tLoss: 1.537079\n",
      "Train Epoche: 3 [1730/96 (1802%)]\tLoss: 10.765813\n",
      "Train Epoche: 3 [1731/96 (1803%)]\tLoss: 1.914394\n",
      "Train Epoche: 3 [1732/96 (1804%)]\tLoss: 1.113216\n",
      "Train Epoche: 3 [1733/96 (1805%)]\tLoss: 3.106016\n",
      "Train Epoche: 3 [1734/96 (1806%)]\tLoss: 1.154600\n",
      "Train Epoche: 3 [1735/96 (1807%)]\tLoss: 19.850542\n",
      "Train Epoche: 3 [1736/96 (1808%)]\tLoss: 4.961012\n",
      "Train Epoche: 3 [1737/96 (1809%)]\tLoss: 0.043201\n",
      "Train Epoche: 3 [1738/96 (1810%)]\tLoss: 0.744169\n",
      "Train Epoche: 3 [1739/96 (1811%)]\tLoss: 0.088019\n",
      "Train Epoche: 3 [1740/96 (1812%)]\tLoss: 3.316844\n",
      "Train Epoche: 3 [1741/96 (1814%)]\tLoss: 0.057710\n",
      "Train Epoche: 3 [1742/96 (1815%)]\tLoss: 1.216575\n",
      "Train Epoche: 3 [1743/96 (1816%)]\tLoss: 2.724784\n",
      "Train Epoche: 3 [1744/96 (1817%)]\tLoss: 3.654957\n",
      "Train Epoche: 3 [1745/96 (1818%)]\tLoss: 6.342188\n",
      "Train Epoche: 3 [1746/96 (1819%)]\tLoss: 12.774068\n",
      "Train Epoche: 3 [1747/96 (1820%)]\tLoss: 0.615031\n",
      "Train Epoche: 3 [1748/96 (1821%)]\tLoss: 3.019419\n",
      "Train Epoche: 3 [1749/96 (1822%)]\tLoss: 2.896439\n",
      "Train Epoche: 3 [1750/96 (1823%)]\tLoss: 1.212570\n",
      "Train Epoche: 3 [1751/96 (1824%)]\tLoss: 4.972135\n",
      "Train Epoche: 3 [1752/96 (1825%)]\tLoss: 2.783639\n",
      "Train Epoche: 3 [1753/96 (1826%)]\tLoss: 7.708109\n",
      "Train Epoche: 3 [1754/96 (1827%)]\tLoss: 20.691116\n",
      "Train Epoche: 3 [1755/96 (1828%)]\tLoss: 13.572854\n",
      "Train Epoche: 3 [1756/96 (1829%)]\tLoss: 3.567683\n",
      "Train Epoche: 3 [1757/96 (1830%)]\tLoss: 111.256828\n",
      "Train Epoche: 3 [1758/96 (1831%)]\tLoss: 43.365429\n",
      "Train Epoche: 3 [1759/96 (1832%)]\tLoss: 46.581287\n",
      "Train Epoche: 3 [1760/96 (1833%)]\tLoss: 3.563844\n",
      "Train Epoche: 3 [1761/96 (1834%)]\tLoss: 0.016139\n",
      "Train Epoche: 3 [1762/96 (1835%)]\tLoss: 14.805809\n",
      "Train Epoche: 3 [1763/96 (1836%)]\tLoss: 31.996725\n",
      "Train Epoche: 3 [1764/96 (1838%)]\tLoss: 6.489764\n",
      "Train Epoche: 3 [1765/96 (1839%)]\tLoss: 3.681831\n",
      "Train Epoche: 3 [1766/96 (1840%)]\tLoss: 11.853099\n",
      "Train Epoche: 3 [1767/96 (1841%)]\tLoss: 6.598189\n",
      "Train Epoche: 3 [1768/96 (1842%)]\tLoss: 8.814339\n",
      "Train Epoche: 3 [1769/96 (1843%)]\tLoss: 5.141132\n",
      "Train Epoche: 3 [1770/96 (1844%)]\tLoss: 29.503445\n",
      "Train Epoche: 3 [1771/96 (1845%)]\tLoss: 60.500458\n",
      "Train Epoche: 3 [1772/96 (1846%)]\tLoss: 2.324612\n",
      "Train Epoche: 3 [1773/96 (1847%)]\tLoss: 13.459956\n",
      "Train Epoche: 3 [1774/96 (1848%)]\tLoss: 71.607544\n",
      "Train Epoche: 3 [1775/96 (1849%)]\tLoss: 43.231964\n",
      "Train Epoche: 3 [1776/96 (1850%)]\tLoss: 0.346882\n",
      "Train Epoche: 3 [1777/96 (1851%)]\tLoss: 0.225926\n",
      "Train Epoche: 3 [1778/96 (1852%)]\tLoss: 25.251408\n",
      "Train Epoche: 3 [1779/96 (1853%)]\tLoss: 0.012793\n",
      "Train Epoche: 3 [1780/96 (1854%)]\tLoss: 0.035913\n",
      "Train Epoche: 3 [1781/96 (1855%)]\tLoss: 0.673988\n",
      "Train Epoche: 3 [1782/96 (1856%)]\tLoss: 37.258095\n",
      "Train Epoche: 3 [1783/96 (1857%)]\tLoss: 0.472616\n",
      "Train Epoche: 3 [1784/96 (1858%)]\tLoss: 3.043242\n",
      "Train Epoche: 3 [1785/96 (1859%)]\tLoss: 118.039268\n",
      "Train Epoche: 3 [1786/96 (1860%)]\tLoss: 4.584413\n",
      "Train Epoche: 3 [1787/96 (1861%)]\tLoss: 0.014381\n",
      "Train Epoche: 3 [1788/96 (1862%)]\tLoss: 0.368324\n",
      "Train Epoche: 3 [1789/96 (1864%)]\tLoss: 0.085120\n",
      "Train Epoche: 3 [1790/96 (1865%)]\tLoss: 4.827855\n",
      "Train Epoche: 3 [1791/96 (1866%)]\tLoss: 185.134216\n",
      "Train Epoche: 3 [1792/96 (1867%)]\tLoss: 53.951866\n",
      "Train Epoche: 3 [1793/96 (1868%)]\tLoss: 6.633782\n",
      "Train Epoche: 3 [1794/96 (1869%)]\tLoss: 2.805239\n",
      "Train Epoche: 3 [1795/96 (1870%)]\tLoss: 40.319366\n",
      "Train Epoche: 3 [1796/96 (1871%)]\tLoss: 3.023840\n",
      "Train Epoche: 3 [1797/96 (1872%)]\tLoss: 4.395196\n",
      "Train Epoche: 3 [1798/96 (1873%)]\tLoss: 3.718473\n",
      "Train Epoche: 3 [1799/96 (1874%)]\tLoss: 0.600395\n",
      "Train Epoche: 3 [1800/96 (1875%)]\tLoss: 2.178367\n",
      "Train Epoche: 3 [1801/96 (1876%)]\tLoss: 3.129215\n",
      "Train Epoche: 3 [1802/96 (1877%)]\tLoss: 16.588528\n",
      "Train Epoche: 3 [1803/96 (1878%)]\tLoss: 1.684028\n",
      "Train Epoche: 3 [1804/96 (1879%)]\tLoss: 1.470013\n",
      "Train Epoche: 3 [1805/96 (1880%)]\tLoss: 3.581126\n",
      "Train Epoche: 3 [1806/96 (1881%)]\tLoss: 1.503660\n",
      "Train Epoche: 3 [1807/96 (1882%)]\tLoss: 0.632392\n",
      "Train Epoche: 3 [1808/96 (1883%)]\tLoss: 112.356331\n",
      "Train Epoche: 3 [1809/96 (1884%)]\tLoss: 8.093063\n",
      "Train Epoche: 3 [1810/96 (1885%)]\tLoss: 33.062763\n",
      "Train Epoche: 3 [1811/96 (1886%)]\tLoss: 16.348064\n",
      "Train Epoche: 3 [1812/96 (1888%)]\tLoss: 45.094147\n",
      "Train Epoche: 3 [1813/96 (1889%)]\tLoss: 30.498207\n",
      "Train Epoche: 3 [1814/96 (1890%)]\tLoss: 30.585865\n",
      "Train Epoche: 3 [1815/96 (1891%)]\tLoss: 0.612004\n",
      "Train Epoche: 3 [1816/96 (1892%)]\tLoss: 18.330229\n",
      "Train Epoche: 3 [1817/96 (1893%)]\tLoss: 2.186087\n",
      "Train Epoche: 3 [1818/96 (1894%)]\tLoss: 7.266630\n",
      "Train Epoche: 3 [1819/96 (1895%)]\tLoss: 4.611804\n",
      "Train Epoche: 3 [1820/96 (1896%)]\tLoss: 68.896034\n",
      "Train Epoche: 3 [1821/96 (1897%)]\tLoss: 33.852760\n",
      "Train Epoche: 3 [1822/96 (1898%)]\tLoss: 11.531157\n",
      "Train Epoche: 3 [1823/96 (1899%)]\tLoss: 10.159135\n",
      "Train Epoche: 3 [1824/96 (1900%)]\tLoss: 0.168713\n",
      "Train Epoche: 3 [1825/96 (1901%)]\tLoss: 16.679590\n",
      "Train Epoche: 3 [1826/96 (1902%)]\tLoss: 2.151851\n",
      "Train Epoche: 3 [1827/96 (1903%)]\tLoss: 4.192184\n",
      "Train Epoche: 3 [1828/96 (1904%)]\tLoss: 0.000322\n",
      "Train Epoche: 3 [1829/96 (1905%)]\tLoss: 2.068279\n",
      "Train Epoche: 3 [1830/96 (1906%)]\tLoss: 1.433842\n",
      "Train Epoche: 3 [1831/96 (1907%)]\tLoss: 12.415216\n",
      "Train Epoche: 3 [1832/96 (1908%)]\tLoss: 21.058472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [1833/96 (1909%)]\tLoss: 27.824896\n",
      "Train Epoche: 3 [1834/96 (1910%)]\tLoss: 12.767221\n",
      "Train Epoche: 3 [1835/96 (1911%)]\tLoss: 10.611764\n",
      "Train Epoche: 3 [1836/96 (1912%)]\tLoss: 205.594528\n",
      "Train Epoche: 3 [1837/96 (1914%)]\tLoss: 2.225838\n",
      "Train Epoche: 3 [1838/96 (1915%)]\tLoss: 0.540620\n",
      "Train Epoche: 3 [1839/96 (1916%)]\tLoss: 7.024024\n",
      "Train Epoche: 3 [1840/96 (1917%)]\tLoss: 7.334809\n",
      "Train Epoche: 3 [1841/96 (1918%)]\tLoss: 1.930868\n",
      "Train Epoche: 3 [1842/96 (1919%)]\tLoss: 0.517762\n",
      "Train Epoche: 3 [1843/96 (1920%)]\tLoss: 5.633420\n",
      "Train Epoche: 3 [1844/96 (1921%)]\tLoss: 6.155370\n",
      "Train Epoche: 3 [1845/96 (1922%)]\tLoss: 42.632389\n",
      "Train Epoche: 3 [1846/96 (1923%)]\tLoss: 1.535329\n",
      "Train Epoche: 3 [1847/96 (1924%)]\tLoss: 0.540648\n",
      "Train Epoche: 3 [1848/96 (1925%)]\tLoss: 17.352539\n",
      "Train Epoche: 3 [1849/96 (1926%)]\tLoss: 15.847518\n",
      "Train Epoche: 3 [1850/96 (1927%)]\tLoss: 0.167587\n",
      "Train Epoche: 3 [1851/96 (1928%)]\tLoss: 81.129364\n",
      "Train Epoche: 3 [1852/96 (1929%)]\tLoss: 10.103645\n",
      "Train Epoche: 3 [1853/96 (1930%)]\tLoss: 10.821138\n",
      "Train Epoche: 3 [1854/96 (1931%)]\tLoss: 4.739748\n",
      "Train Epoche: 3 [1855/96 (1932%)]\tLoss: 30.816858\n",
      "Train Epoche: 3 [1856/96 (1933%)]\tLoss: 49.304646\n",
      "Train Epoche: 3 [1857/96 (1934%)]\tLoss: 48.658321\n",
      "Train Epoche: 3 [1858/96 (1935%)]\tLoss: 73.984612\n",
      "Train Epoche: 3 [1859/96 (1936%)]\tLoss: 0.777154\n",
      "Train Epoche: 3 [1860/96 (1938%)]\tLoss: 9.979290\n",
      "Train Epoche: 3 [1861/96 (1939%)]\tLoss: 108.464897\n",
      "Train Epoche: 3 [1862/96 (1940%)]\tLoss: 15.593030\n",
      "Train Epoche: 3 [1863/96 (1941%)]\tLoss: 163.682617\n",
      "Train Epoche: 3 [1864/96 (1942%)]\tLoss: 0.117719\n",
      "Train Epoche: 3 [1865/96 (1943%)]\tLoss: 4.886189\n",
      "Train Epoche: 3 [1866/96 (1944%)]\tLoss: 1.728112\n",
      "Train Epoche: 3 [1867/96 (1945%)]\tLoss: 21.971315\n",
      "Train Epoche: 3 [1868/96 (1946%)]\tLoss: 10.667739\n",
      "Train Epoche: 3 [1869/96 (1947%)]\tLoss: 6.125542\n",
      "Train Epoche: 3 [1870/96 (1948%)]\tLoss: 0.130171\n",
      "Train Epoche: 3 [1871/96 (1949%)]\tLoss: 9.328351\n",
      "Train Epoche: 3 [1872/96 (1950%)]\tLoss: 2.595298\n",
      "Train Epoche: 3 [1873/96 (1951%)]\tLoss: 1.204217\n",
      "Train Epoche: 3 [1874/96 (1952%)]\tLoss: 8.445646\n",
      "Train Epoche: 3 [1875/96 (1953%)]\tLoss: 1.903866\n",
      "Train Epoche: 3 [1876/96 (1954%)]\tLoss: 21.735792\n",
      "Train Epoche: 3 [1877/96 (1955%)]\tLoss: 2.467253\n",
      "Train Epoche: 3 [1878/96 (1956%)]\tLoss: 0.531764\n",
      "Train Epoche: 3 [1879/96 (1957%)]\tLoss: 0.136908\n",
      "Train Epoche: 3 [1880/96 (1958%)]\tLoss: 6.755748\n",
      "Train Epoche: 3 [1881/96 (1959%)]\tLoss: 0.953926\n",
      "Train Epoche: 3 [1882/96 (1960%)]\tLoss: 0.304667\n",
      "Train Epoche: 3 [1883/96 (1961%)]\tLoss: 0.106894\n",
      "Train Epoche: 3 [1884/96 (1962%)]\tLoss: 0.360319\n",
      "Train Epoche: 3 [1885/96 (1964%)]\tLoss: 0.730889\n",
      "Train Epoche: 3 [1886/96 (1965%)]\tLoss: 2.563530\n",
      "Train Epoche: 3 [1887/96 (1966%)]\tLoss: 22.684393\n",
      "Train Epoche: 3 [1888/96 (1967%)]\tLoss: 0.824254\n",
      "Train Epoche: 3 [1889/96 (1968%)]\tLoss: 36.627834\n",
      "Train Epoche: 3 [1890/96 (1969%)]\tLoss: 25.432274\n",
      "Train Epoche: 3 [1891/96 (1970%)]\tLoss: 4.231411\n",
      "Train Epoche: 3 [1892/96 (1971%)]\tLoss: 2.476916\n",
      "Train Epoche: 3 [1893/96 (1972%)]\tLoss: 6.090028\n",
      "Train Epoche: 3 [1894/96 (1973%)]\tLoss: 3.036773\n",
      "Train Epoche: 3 [1895/96 (1974%)]\tLoss: 3.851706\n",
      "Train Epoche: 3 [1896/96 (1975%)]\tLoss: 0.977200\n",
      "Train Epoche: 3 [1897/96 (1976%)]\tLoss: 0.413844\n",
      "Train Epoche: 3 [1898/96 (1977%)]\tLoss: 1.533222\n",
      "Train Epoche: 3 [1899/96 (1978%)]\tLoss: 24.242844\n",
      "Train Epoche: 3 [1900/96 (1979%)]\tLoss: 11.744859\n",
      "Train Epoche: 3 [1901/96 (1980%)]\tLoss: 1.008151\n",
      "Train Epoche: 3 [1902/96 (1981%)]\tLoss: 0.027152\n",
      "Train Epoche: 3 [1903/96 (1982%)]\tLoss: 18.905664\n",
      "Train Epoche: 3 [1904/96 (1983%)]\tLoss: 14.232829\n",
      "Train Epoche: 3 [1905/96 (1984%)]\tLoss: 9.344883\n",
      "Train Epoche: 3 [1906/96 (1985%)]\tLoss: 4.201848\n",
      "Train Epoche: 3 [1907/96 (1986%)]\tLoss: 276.156586\n",
      "Train Epoche: 3 [1908/96 (1988%)]\tLoss: 1.168775\n",
      "Train Epoche: 3 [1909/96 (1989%)]\tLoss: 28.024080\n",
      "Train Epoche: 3 [1910/96 (1990%)]\tLoss: 0.779964\n",
      "Train Epoche: 3 [1911/96 (1991%)]\tLoss: 2.240932\n",
      "Train Epoche: 3 [1912/96 (1992%)]\tLoss: 279.218201\n",
      "Train Epoche: 3 [1913/96 (1993%)]\tLoss: 6.721841\n",
      "Train Epoche: 3 [1914/96 (1994%)]\tLoss: 0.795516\n",
      "Train Epoche: 3 [1915/96 (1995%)]\tLoss: 6.093164\n",
      "Train Epoche: 3 [1916/96 (1996%)]\tLoss: 2.011702\n",
      "Train Epoche: 3 [1917/96 (1997%)]\tLoss: 1.597165\n",
      "Train Epoche: 3 [1918/96 (1998%)]\tLoss: 0.350931\n",
      "Train Epoche: 3 [1919/96 (1999%)]\tLoss: 1.179136\n",
      "Train Epoche: 3 [1920/96 (2000%)]\tLoss: 1.241192\n",
      "Train Epoche: 3 [1921/96 (2001%)]\tLoss: 0.249683\n",
      "Train Epoche: 3 [1922/96 (2002%)]\tLoss: 12.106361\n",
      "Train Epoche: 3 [1923/96 (2003%)]\tLoss: 215.425415\n",
      "Train Epoche: 3 [1924/96 (2004%)]\tLoss: 10.310512\n",
      "Train Epoche: 3 [1925/96 (2005%)]\tLoss: 2.635216\n",
      "Train Epoche: 3 [1926/96 (2006%)]\tLoss: 11.403806\n",
      "Train Epoche: 3 [1927/96 (2007%)]\tLoss: 0.826062\n",
      "Train Epoche: 3 [1928/96 (2008%)]\tLoss: 2.609940\n",
      "Train Epoche: 3 [1929/96 (2009%)]\tLoss: 2.258878\n",
      "Train Epoche: 3 [1930/96 (2010%)]\tLoss: 2.966303\n",
      "Train Epoche: 3 [1931/96 (2011%)]\tLoss: 2.904135\n",
      "Train Epoche: 3 [1932/96 (2012%)]\tLoss: 1.612835\n",
      "Train Epoche: 3 [1933/96 (2014%)]\tLoss: 14.184514\n",
      "Train Epoche: 3 [1934/96 (2015%)]\tLoss: 144.384689\n",
      "Train Epoche: 3 [1935/96 (2016%)]\tLoss: 1.508659\n",
      "Train Epoche: 3 [1936/96 (2017%)]\tLoss: 171.714874\n",
      "Train Epoche: 3 [1937/96 (2018%)]\tLoss: 47.966553\n",
      "Train Epoche: 3 [1938/96 (2019%)]\tLoss: 5.744728\n",
      "Train Epoche: 3 [1939/96 (2020%)]\tLoss: 0.559207\n",
      "Train Epoche: 3 [1940/96 (2021%)]\tLoss: 0.808280\n",
      "Train Epoche: 3 [1941/96 (2022%)]\tLoss: 19.846157\n",
      "Train Epoche: 3 [1942/96 (2023%)]\tLoss: 33.808388\n",
      "Train Epoche: 3 [1943/96 (2024%)]\tLoss: 2.737828\n",
      "Train Epoche: 3 [1944/96 (2025%)]\tLoss: 2.754941\n",
      "Train Epoche: 3 [1945/96 (2026%)]\tLoss: 0.671777\n",
      "Train Epoche: 3 [1946/96 (2027%)]\tLoss: 46.547577\n",
      "Train Epoche: 3 [1947/96 (2028%)]\tLoss: 0.321951\n",
      "Train Epoche: 3 [1948/96 (2029%)]\tLoss: 5.994291\n",
      "Train Epoche: 3 [1949/96 (2030%)]\tLoss: 1.494030\n",
      "Train Epoche: 3 [1950/96 (2031%)]\tLoss: 3.164574\n",
      "Train Epoche: 3 [1951/96 (2032%)]\tLoss: 9.968465\n",
      "Train Epoche: 3 [1952/96 (2033%)]\tLoss: 3.214491\n",
      "Train Epoche: 3 [1953/96 (2034%)]\tLoss: 0.095009\n",
      "Train Epoche: 3 [1954/96 (2035%)]\tLoss: 1.053017\n",
      "Train Epoche: 3 [1955/96 (2036%)]\tLoss: 7.357667\n",
      "Train Epoche: 3 [1956/96 (2038%)]\tLoss: 0.136373\n",
      "Train Epoche: 3 [1957/96 (2039%)]\tLoss: 1.970206\n",
      "Train Epoche: 3 [1958/96 (2040%)]\tLoss: 4.744779\n",
      "Train Epoche: 3 [1959/96 (2041%)]\tLoss: 0.006912\n",
      "Train Epoche: 3 [1960/96 (2042%)]\tLoss: 7.664578\n",
      "Train Epoche: 3 [1961/96 (2043%)]\tLoss: 3.973589\n",
      "Train Epoche: 3 [1962/96 (2044%)]\tLoss: 2.061393\n",
      "Train Epoche: 3 [1963/96 (2045%)]\tLoss: 1.899981\n",
      "Train Epoche: 3 [1964/96 (2046%)]\tLoss: 4.628455\n",
      "Train Epoche: 3 [1965/96 (2047%)]\tLoss: 5.556143\n",
      "Train Epoche: 3 [1966/96 (2048%)]\tLoss: 0.871902\n",
      "Train Epoche: 3 [1967/96 (2049%)]\tLoss: 4.199850\n",
      "Train Epoche: 3 [1968/96 (2050%)]\tLoss: 0.172806\n",
      "Train Epoche: 3 [1969/96 (2051%)]\tLoss: 14.837597\n",
      "Train Epoche: 3 [1970/96 (2052%)]\tLoss: 6.492136\n",
      "Train Epoche: 3 [1971/96 (2053%)]\tLoss: 127.071732\n",
      "Train Epoche: 3 [1972/96 (2054%)]\tLoss: 13.589421\n",
      "Train Epoche: 3 [1973/96 (2055%)]\tLoss: 1.556297\n",
      "Train Epoche: 3 [1974/96 (2056%)]\tLoss: 0.572603\n",
      "Train Epoche: 3 [1975/96 (2057%)]\tLoss: 3.425432\n",
      "Train Epoche: 3 [1976/96 (2058%)]\tLoss: 0.859808\n",
      "Train Epoche: 3 [1977/96 (2059%)]\tLoss: 0.014575\n",
      "Train Epoche: 3 [1978/96 (2060%)]\tLoss: 0.181191\n",
      "Train Epoche: 3 [1979/96 (2061%)]\tLoss: 2.290697\n",
      "Train Epoche: 3 [1980/96 (2062%)]\tLoss: 2.624415\n",
      "Train Epoche: 3 [1981/96 (2064%)]\tLoss: 0.828598\n",
      "Train Epoche: 3 [1982/96 (2065%)]\tLoss: 4.026971\n",
      "Train Epoche: 3 [1983/96 (2066%)]\tLoss: 24.321154\n",
      "Train Epoche: 3 [1984/96 (2067%)]\tLoss: 3.294151\n",
      "Train Epoche: 3 [1985/96 (2068%)]\tLoss: 30.987692\n",
      "Train Epoche: 3 [1986/96 (2069%)]\tLoss: 4.946112\n",
      "Train Epoche: 3 [1987/96 (2070%)]\tLoss: 4.059239\n",
      "Train Epoche: 3 [1988/96 (2071%)]\tLoss: 5.567289\n",
      "Train Epoche: 3 [1989/96 (2072%)]\tLoss: 27.723743\n",
      "Train Epoche: 3 [1990/96 (2073%)]\tLoss: 18.080574\n",
      "Train Epoche: 3 [1991/96 (2074%)]\tLoss: 6.179030\n",
      "Train Epoche: 3 [1992/96 (2075%)]\tLoss: 1.753520\n",
      "Train Epoche: 3 [1993/96 (2076%)]\tLoss: 15.218657\n",
      "Train Epoche: 3 [1994/96 (2077%)]\tLoss: 40.249565\n",
      "Train Epoche: 3 [1995/96 (2078%)]\tLoss: 7.237741\n",
      "Train Epoche: 3 [1996/96 (2079%)]\tLoss: 2.749990\n",
      "Train Epoche: 3 [1997/96 (2080%)]\tLoss: 43.678745\n",
      "Train Epoche: 3 [1998/96 (2081%)]\tLoss: 0.142041\n",
      "Train Epoche: 3 [1999/96 (2082%)]\tLoss: 7.510635\n",
      "Train Epoche: 3 [2000/96 (2083%)]\tLoss: 1.315809\n",
      "Train Epoche: 3 [2001/96 (2084%)]\tLoss: 52.631844\n",
      "Train Epoche: 3 [2002/96 (2085%)]\tLoss: 18.233694\n",
      "Train Epoche: 3 [2003/96 (2086%)]\tLoss: 12.047131\n",
      "Train Epoche: 3 [2004/96 (2088%)]\tLoss: 0.086041\n",
      "Train Epoche: 3 [2005/96 (2089%)]\tLoss: 0.128901\n",
      "Train Epoche: 3 [2006/96 (2090%)]\tLoss: 1.082265\n",
      "Train Epoche: 3 [2007/96 (2091%)]\tLoss: 32.629295\n",
      "Train Epoche: 3 [2008/96 (2092%)]\tLoss: 0.396325\n",
      "Train Epoche: 3 [2009/96 (2093%)]\tLoss: 0.164732\n",
      "Train Epoche: 3 [2010/96 (2094%)]\tLoss: 0.004753\n",
      "Train Epoche: 3 [2011/96 (2095%)]\tLoss: 375.533447\n",
      "Train Epoche: 3 [2012/96 (2096%)]\tLoss: 14.249571\n",
      "Train Epoche: 3 [2013/96 (2097%)]\tLoss: 31.742792\n",
      "Train Epoche: 3 [2014/96 (2098%)]\tLoss: 5.326202\n",
      "Train Epoche: 3 [2015/96 (2099%)]\tLoss: 0.226931\n",
      "Train Epoche: 3 [2016/96 (2100%)]\tLoss: 1.420087\n",
      "Train Epoche: 3 [2017/96 (2101%)]\tLoss: 69.889496\n",
      "Train Epoche: 3 [2018/96 (2102%)]\tLoss: 1.389899\n",
      "Train Epoche: 3 [2019/96 (2103%)]\tLoss: 19.072109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 3 [2020/96 (2104%)]\tLoss: 3.531357\n",
      "Train Epoche: 3 [2021/96 (2105%)]\tLoss: 0.884630\n",
      "Train Epoche: 3 [2022/96 (2106%)]\tLoss: 1.390425\n",
      "Train Epoche: 3 [2023/96 (2107%)]\tLoss: 3.775837\n",
      "Train Epoche: 3 [2024/96 (2108%)]\tLoss: 0.641073\n",
      "Train Epoche: 3 [2025/96 (2109%)]\tLoss: 221.239319\n",
      "Train Epoche: 3 [2026/96 (2110%)]\tLoss: 3.125722\n",
      "Train Epoche: 3 [2027/96 (2111%)]\tLoss: 40.537106\n",
      "Train Epoche: 3 [2028/96 (2112%)]\tLoss: 1.348318\n",
      "Train Epoche: 3 [2029/96 (2114%)]\tLoss: 6.753478\n",
      "Train Epoche: 3 [2030/96 (2115%)]\tLoss: 0.616860\n",
      "Train Epoche: 3 [2031/96 (2116%)]\tLoss: 0.010250\n",
      "Train Epoche: 3 [2032/96 (2117%)]\tLoss: 0.380774\n",
      "Train Epoche: 3 [2033/96 (2118%)]\tLoss: 9.734786\n",
      "Train Epoche: 1 [0/96 (0%)]\tLoss: 399.476044\n",
      "Train Epoche: 1 [1/96 (1%)]\tLoss: 143.696548\n",
      "Train Epoche: 1 [2/96 (2%)]\tLoss: 80.433601\n",
      "Train Epoche: 1 [3/96 (3%)]\tLoss: 120.594254\n",
      "Train Epoche: 1 [4/96 (4%)]\tLoss: 398.388397\n",
      "Train Epoche: 1 [5/96 (5%)]\tLoss: 48.705322\n",
      "Train Epoche: 1 [6/96 (6%)]\tLoss: 99.776009\n",
      "Train Epoche: 1 [7/96 (7%)]\tLoss: 168.168320\n",
      "Train Epoche: 1 [8/96 (8%)]\tLoss: 9.022539\n",
      "Train Epoche: 1 [9/96 (9%)]\tLoss: 3.942153\n",
      "Train Epoche: 1 [10/96 (10%)]\tLoss: 63.402927\n",
      "Train Epoche: 1 [11/96 (11%)]\tLoss: 35.681103\n",
      "Train Epoche: 1 [12/96 (12%)]\tLoss: 398.257202\n",
      "Train Epoche: 1 [13/96 (14%)]\tLoss: 15.916061\n",
      "Train Epoche: 1 [14/96 (15%)]\tLoss: 224.639313\n",
      "Train Epoche: 1 [15/96 (16%)]\tLoss: 398.394165\n",
      "Train Epoche: 1 [16/96 (17%)]\tLoss: 1.124541\n",
      "Train Epoche: 1 [17/96 (18%)]\tLoss: 24.751360\n",
      "Train Epoche: 1 [18/96 (19%)]\tLoss: 195.232864\n",
      "Train Epoche: 1 [19/96 (20%)]\tLoss: 257.986145\n",
      "Train Epoche: 1 [20/96 (21%)]\tLoss: 63.548073\n",
      "Train Epoche: 1 [21/96 (22%)]\tLoss: 322.263672\n",
      "Train Epoche: 1 [22/96 (23%)]\tLoss: 482.225128\n",
      "Train Epoche: 1 [23/96 (24%)]\tLoss: 574.689636\n",
      "Train Epoche: 1 [24/96 (25%)]\tLoss: 223.959763\n",
      "Train Epoche: 1 [25/96 (26%)]\tLoss: 195.650909\n",
      "Train Epoche: 1 [26/96 (27%)]\tLoss: 48.589092\n",
      "Train Epoche: 1 [27/96 (28%)]\tLoss: 80.382240\n",
      "Train Epoche: 1 [28/96 (29%)]\tLoss: 143.487488\n",
      "Train Epoche: 1 [29/96 (30%)]\tLoss: 35.621399\n",
      "Train Epoche: 1 [30/96 (31%)]\tLoss: 15.663337\n",
      "Train Epoche: 1 [31/96 (32%)]\tLoss: 0.927324\n",
      "Train Epoche: 1 [32/96 (33%)]\tLoss: 167.848679\n",
      "Train Epoche: 1 [33/96 (34%)]\tLoss: 119.917786\n",
      "Train Epoche: 1 [34/96 (35%)]\tLoss: 24.918041\n",
      "Train Epoche: 1 [35/96 (36%)]\tLoss: 100.389175\n",
      "Train Epoche: 1 [36/96 (38%)]\tLoss: 398.448303\n",
      "Train Epoche: 1 [37/96 (39%)]\tLoss: 443.232391\n",
      "Train Epoche: 1 [38/96 (40%)]\tLoss: 290.498230\n",
      "Train Epoche: 1 [39/96 (41%)]\tLoss: 255.665146\n",
      "Train Epoche: 1 [40/96 (42%)]\tLoss: 3.871895\n",
      "Train Epoche: 1 [41/96 (43%)]\tLoss: 9.188350\n",
      "Train Epoche: 1 [42/96 (44%)]\tLoss: 360.253418\n",
      "Train Epoche: 1 [43/96 (45%)]\tLoss: 574.250732\n",
      "Train Epoche: 1 [44/96 (46%)]\tLoss: 398.093719\n",
      "Train Epoche: 1 [45/96 (47%)]\tLoss: 195.038177\n",
      "Train Epoche: 1 [46/96 (48%)]\tLoss: 63.280090\n",
      "Train Epoche: 1 [47/96 (49%)]\tLoss: 120.103020\n",
      "Train Epoche: 1 [48/96 (50%)]\tLoss: 35.465057\n",
      "Train Epoche: 1 [49/96 (51%)]\tLoss: 168.045303\n",
      "Train Epoche: 1 [50/96 (52%)]\tLoss: 142.651855\n",
      "Train Epoche: 1 [51/96 (53%)]\tLoss: 98.801506\n",
      "Train Epoche: 1 [52/96 (54%)]\tLoss: 15.679142\n",
      "Train Epoche: 1 [53/96 (55%)]\tLoss: 3.755954\n",
      "Train Epoche: 1 [54/96 (56%)]\tLoss: 287.505707\n",
      "Train Epoche: 1 [55/96 (57%)]\tLoss: 80.210571\n",
      "Train Epoche: 1 [56/96 (58%)]\tLoss: 8.776279\n",
      "Train Epoche: 1 [57/96 (59%)]\tLoss: 398.482361\n",
      "Train Epoche: 1 [58/96 (60%)]\tLoss: 255.099060\n",
      "Train Epoche: 1 [59/96 (61%)]\tLoss: 397.979309\n",
      "Train Epoche: 1 [60/96 (62%)]\tLoss: 1.038958\n",
      "Train Epoche: 1 [61/96 (64%)]\tLoss: 24.920155\n",
      "Train Epoche: 1 [62/96 (65%)]\tLoss: 50.380428\n",
      "Train Epoche: 1 [63/96 (66%)]\tLoss: 223.291931\n",
      "Train Epoche: 1 [64/96 (67%)]\tLoss: 15.824333\n",
      "Train Epoche: 1 [65/96 (68%)]\tLoss: 8.821045\n",
      "Train Epoche: 1 [66/96 (69%)]\tLoss: 99.158691\n",
      "Train Epoche: 1 [67/96 (70%)]\tLoss: 167.837662\n",
      "Train Epoche: 1 [68/96 (71%)]\tLoss: 120.374069\n",
      "Train Epoche: 1 [69/96 (72%)]\tLoss: 35.724663\n",
      "Train Epoche: 1 [70/96 (73%)]\tLoss: 48.401520\n",
      "Train Epoche: 1 [71/96 (74%)]\tLoss: 63.429432\n",
      "Train Epoche: 1 [72/96 (75%)]\tLoss: 223.911179\n",
      "Train Epoche: 1 [73/96 (76%)]\tLoss: 24.638216\n",
      "Train Epoche: 1 [74/96 (77%)]\tLoss: 483.013397\n",
      "Train Epoche: 1 [75/96 (78%)]\tLoss: 254.797714\n",
      "Train Epoche: 1 [76/96 (79%)]\tLoss: 195.000000\n",
      "Train Epoche: 1 [77/96 (80%)]\tLoss: 3.898917\n",
      "Train Epoche: 1 [78/96 (81%)]\tLoss: 1.045950\n",
      "Train Epoche: 1 [79/96 (82%)]\tLoss: 288.248352\n",
      "Train Epoche: 1 [80/96 (83%)]\tLoss: 145.769379\n",
      "Train Epoche: 1 [81/96 (84%)]\tLoss: 121.060638\n",
      "Train Epoche: 1 [82/96 (85%)]\tLoss: 48.954262\n",
      "Train Epoche: 1 [83/96 (86%)]\tLoss: 15.741196\n",
      "Train Epoche: 1 [84/96 (88%)]\tLoss: 8.893019\n",
      "Train Epoche: 1 [85/96 (89%)]\tLoss: 484.360779\n",
      "Train Epoche: 1 [86/96 (90%)]\tLoss: 481.922455\n",
      "Train Epoche: 1 [87/96 (91%)]\tLoss: 24.781773\n",
      "Train Epoche: 1 [88/96 (92%)]\tLoss: 99.483963\n",
      "Train Epoche: 1 [89/96 (93%)]\tLoss: 80.535912\n",
      "Train Epoche: 1 [90/96 (94%)]\tLoss: 35.992470\n",
      "Train Epoche: 1 [91/96 (95%)]\tLoss: 481.760681\n",
      "Train Epoche: 1 [92/96 (96%)]\tLoss: 63.406975\n",
      "Train Epoche: 1 [93/96 (97%)]\tLoss: 168.651825\n",
      "Train Epoche: 1 [94/96 (98%)]\tLoss: 359.765930\n",
      "Train Epoche: 1 [95/96 (99%)]\tLoss: 3.850729\n",
      "Train Epoche: 1 [96/96 (100%)]\tLoss: 1.015306\n",
      "Train Epoche: 1 [97/96 (101%)]\tLoss: 255.112732\n",
      "Train Epoche: 1 [98/96 (102%)]\tLoss: 328.080719\n",
      "Train Epoche: 1 [99/96 (103%)]\tLoss: 196.969849\n",
      "Train Epoche: 1 [100/96 (104%)]\tLoss: 143.033661\n",
      "Train Epoche: 1 [101/96 (105%)]\tLoss: 290.198578\n",
      "Train Epoche: 1 [102/96 (106%)]\tLoss: 224.979996\n",
      "Train Epoche: 1 [103/96 (107%)]\tLoss: 143.143951\n",
      "Train Epoche: 1 [104/96 (108%)]\tLoss: 119.959602\n",
      "Train Epoche: 1 [105/96 (109%)]\tLoss: 255.097717\n",
      "Train Epoche: 1 [106/96 (110%)]\tLoss: 195.172180\n",
      "Train Epoche: 1 [107/96 (111%)]\tLoss: 224.611755\n",
      "Train Epoche: 1 [108/96 (112%)]\tLoss: 35.508324\n",
      "Train Epoche: 1 [109/96 (114%)]\tLoss: 3.795722\n",
      "Train Epoche: 1 [110/96 (115%)]\tLoss: 63.539391\n",
      "Train Epoche: 1 [111/96 (116%)]\tLoss: 99.230171\n",
      "Train Epoche: 1 [112/96 (117%)]\tLoss: 48.592823\n",
      "Train Epoche: 1 [113/96 (118%)]\tLoss: 15.584621\n",
      "Train Epoche: 1 [114/96 (119%)]\tLoss: 0.920684\n",
      "Train Epoche: 1 [115/96 (120%)]\tLoss: 482.569153\n",
      "Train Epoche: 1 [116/96 (121%)]\tLoss: 398.239624\n",
      "Train Epoche: 1 [117/96 (122%)]\tLoss: 8.886663\n",
      "Train Epoche: 1 [118/96 (123%)]\tLoss: 25.176760\n",
      "Train Epoche: 1 [119/96 (124%)]\tLoss: 323.260620\n",
      "Train Epoche: 1 [120/96 (125%)]\tLoss: 486.957672\n",
      "Train Epoche: 1 [121/96 (126%)]\tLoss: 80.737999\n",
      "Train Epoche: 1 [122/96 (127%)]\tLoss: 170.895126\n",
      "Train Epoche: 1 [123/96 (128%)]\tLoss: 365.904694\n",
      "Train Epoche: 1 [124/96 (129%)]\tLoss: 288.838165\n",
      "Train Epoche: 1 [125/96 (130%)]\tLoss: 80.477859\n",
      "Train Epoche: 1 [126/96 (131%)]\tLoss: 143.223190\n",
      "Train Epoche: 1 [127/96 (132%)]\tLoss: 99.053551\n",
      "Train Epoche: 1 [128/96 (133%)]\tLoss: 322.792572\n",
      "Train Epoche: 1 [129/96 (134%)]\tLoss: 35.574203\n",
      "Train Epoche: 1 [130/96 (135%)]\tLoss: 399.145569\n",
      "Train Epoche: 1 [131/96 (136%)]\tLoss: 254.858887\n",
      "Train Epoche: 1 [132/96 (138%)]\tLoss: 223.942093\n",
      "Train Epoche: 1 [133/96 (139%)]\tLoss: 15.691181\n",
      "Train Epoche: 1 [134/96 (140%)]\tLoss: 8.811404\n",
      "Train Epoche: 1 [135/96 (141%)]\tLoss: 48.497208\n",
      "Train Epoche: 1 [136/96 (142%)]\tLoss: 63.448975\n",
      "Train Epoche: 1 [137/96 (143%)]\tLoss: 398.544189\n",
      "Train Epoche: 1 [138/96 (144%)]\tLoss: 24.641531\n",
      "Train Epoche: 1 [139/96 (145%)]\tLoss: 288.157959\n",
      "Train Epoche: 1 [140/96 (146%)]\tLoss: 194.778336\n",
      "Train Epoche: 1 [141/96 (147%)]\tLoss: 3.949193\n",
      "Train Epoche: 1 [142/96 (148%)]\tLoss: 0.967154\n",
      "Train Epoche: 1 [143/96 (149%)]\tLoss: 123.037544\n",
      "Train Epoche: 1 [144/96 (150%)]\tLoss: 168.628540\n",
      "Train Epoche: 1 [145/96 (151%)]\tLoss: 15.958879\n",
      "Train Epoche: 1 [146/96 (152%)]\tLoss: 35.615829\n",
      "Train Epoche: 1 [147/96 (153%)]\tLoss: 255.030884\n",
      "Train Epoche: 1 [148/96 (154%)]\tLoss: 482.225555\n",
      "Train Epoche: 1 [149/96 (155%)]\tLoss: 99.244865\n",
      "Train Epoche: 1 [150/96 (156%)]\tLoss: 223.982773\n",
      "Train Epoche: 1 [151/96 (157%)]\tLoss: 8.738914\n",
      "Train Epoche: 1 [152/96 (158%)]\tLoss: 48.356266\n",
      "Train Epoche: 1 [153/96 (159%)]\tLoss: 168.397781\n",
      "Train Epoche: 1 [154/96 (160%)]\tLoss: 120.538902\n",
      "Train Epoche: 1 [155/96 (161%)]\tLoss: 3.848211\n",
      "Train Epoche: 1 [156/96 (162%)]\tLoss: 0.905653\n",
      "Train Epoche: 1 [157/96 (164%)]\tLoss: 63.432182\n",
      "Train Epoche: 1 [158/96 (165%)]\tLoss: 142.784698\n",
      "Train Epoche: 1 [159/96 (166%)]\tLoss: 80.157112\n",
      "Train Epoche: 1 [160/96 (167%)]\tLoss: 25.300369\n",
      "Train Epoche: 1 [161/96 (168%)]\tLoss: 483.727966\n",
      "Train Epoche: 1 [162/96 (169%)]\tLoss: 322.673035\n",
      "Train Epoche: 1 [163/96 (170%)]\tLoss: 197.533432\n",
      "Train Epoche: 1 [164/96 (171%)]\tLoss: 482.866455\n",
      "Train Epoche: 1 [165/96 (172%)]\tLoss: 360.095673\n",
      "Train Epoche: 1 [166/96 (173%)]\tLoss: 288.333130\n",
      "Train Epoche: 1 [167/96 (174%)]\tLoss: 24.673912\n",
      "Train Epoche: 1 [168/96 (175%)]\tLoss: 3.848299\n",
      "Train Epoche: 1 [169/96 (176%)]\tLoss: 167.738541\n",
      "Train Epoche: 1 [170/96 (177%)]\tLoss: 120.948578\n",
      "Train Epoche: 1 [171/96 (178%)]\tLoss: 287.357544\n",
      "Train Epoche: 1 [172/96 (179%)]\tLoss: 224.906631\n",
      "Train Epoche: 1 [173/96 (180%)]\tLoss: 143.510498\n",
      "Train Epoche: 1 [174/96 (181%)]\tLoss: 439.732635\n",
      "Train Epoche: 1 [175/96 (182%)]\tLoss: 15.680133\n",
      "Train Epoche: 1 [176/96 (183%)]\tLoss: 80.885025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [177/96 (184%)]\tLoss: 63.613438\n",
      "Train Epoche: 1 [178/96 (185%)]\tLoss: 35.485703\n",
      "Train Epoche: 1 [179/96 (186%)]\tLoss: 8.798529\n",
      "Train Epoche: 1 [180/96 (188%)]\tLoss: 0.920511\n",
      "Train Epoche: 1 [181/96 (189%)]\tLoss: 194.993683\n",
      "Train Epoche: 1 [182/96 (190%)]\tLoss: 98.950050\n",
      "Train Epoche: 1 [183/96 (191%)]\tLoss: 48.327328\n",
      "Train Epoche: 1 [184/96 (192%)]\tLoss: 574.708191\n",
      "Train Epoche: 1 [185/96 (193%)]\tLoss: 256.522736\n",
      "Train Epoche: 1 [186/96 (194%)]\tLoss: 572.268982\n",
      "Train Epoche: 1 [187/96 (195%)]\tLoss: 574.264526\n",
      "Train Epoche: 1 [188/96 (196%)]\tLoss: 323.127716\n",
      "Train Epoche: 1 [189/96 (197%)]\tLoss: 398.790741\n",
      "Train Epoche: 1 [190/96 (198%)]\tLoss: 360.274109\n",
      "Train Epoche: 1 [191/96 (199%)]\tLoss: 15.798792\n",
      "Train Epoche: 1 [192/96 (200%)]\tLoss: 80.370216\n",
      "Train Epoche: 1 [193/96 (201%)]\tLoss: 143.250702\n",
      "Train Epoche: 1 [194/96 (202%)]\tLoss: 48.392574\n",
      "Train Epoche: 1 [195/96 (203%)]\tLoss: 63.444130\n",
      "Train Epoche: 1 [196/96 (204%)]\tLoss: 482.159058\n",
      "Train Epoche: 1 [197/96 (205%)]\tLoss: 35.364372\n",
      "Train Epoche: 1 [198/96 (206%)]\tLoss: 99.247940\n",
      "Train Epoche: 1 [199/96 (207%)]\tLoss: 24.898693\n",
      "Train Epoche: 1 [200/96 (208%)]\tLoss: 120.221695\n",
      "Train Epoche: 1 [201/96 (209%)]\tLoss: 8.749033\n",
      "Train Epoche: 1 [202/96 (210%)]\tLoss: 0.909664\n",
      "Train Epoche: 1 [203/96 (211%)]\tLoss: 168.039230\n",
      "Train Epoche: 1 [204/96 (212%)]\tLoss: 195.031677\n",
      "Train Epoche: 1 [205/96 (214%)]\tLoss: 482.293091\n",
      "Train Epoche: 1 [206/96 (215%)]\tLoss: 4.086857\n",
      "Train Epoche: 1 [207/96 (216%)]\tLoss: 485.376221\n",
      "Train Epoche: 1 [208/96 (217%)]\tLoss: 485.017944\n",
      "Train Epoche: 1 [209/96 (218%)]\tLoss: 484.578247\n",
      "Train Epoche: 1 [210/96 (219%)]\tLoss: 485.420776\n",
      "Train Epoche: 1 [211/96 (220%)]\tLoss: 81.110344\n",
      "Train Epoche: 1 [212/96 (221%)]\tLoss: 143.512466\n",
      "Train Epoche: 1 [213/96 (222%)]\tLoss: 401.034058\n",
      "Train Epoche: 1 [214/96 (223%)]\tLoss: 9.123710\n",
      "Train Epoche: 1 [215/96 (224%)]\tLoss: 398.966492\n",
      "Train Epoche: 1 [216/96 (225%)]\tLoss: 399.307098\n",
      "Train Epoche: 1 [217/96 (226%)]\tLoss: 399.094604\n",
      "Train Epoche: 1 [218/96 (227%)]\tLoss: 63.723755\n",
      "Train Epoche: 1 [219/96 (228%)]\tLoss: 195.399460\n",
      "Train Epoche: 1 [220/96 (229%)]\tLoss: 15.978485\n",
      "Train Epoche: 1 [221/96 (230%)]\tLoss: 399.294739\n",
      "Train Epoche: 1 [222/96 (231%)]\tLoss: 35.992111\n",
      "Train Epoche: 1 [223/96 (232%)]\tLoss: 0.913593\n",
      "Train Epoche: 1 [224/96 (233%)]\tLoss: 398.940460\n",
      "Train Epoche: 1 [225/96 (234%)]\tLoss: 121.542503\n",
      "Train Epoche: 1 [226/96 (235%)]\tLoss: 99.699020\n",
      "Train Epoche: 1 [227/96 (236%)]\tLoss: 24.770782\n",
      "Train Epoche: 1 [228/96 (238%)]\tLoss: 3.993509\n",
      "Train Epoche: 1 [229/96 (239%)]\tLoss: 171.257507\n",
      "Train Epoche: 1 [230/96 (240%)]\tLoss: 48.924728\n",
      "Train Epoche: 1 [231/96 (241%)]\tLoss: 24.652060\n",
      "Train Epoche: 1 [232/96 (242%)]\tLoss: 0.950614\n",
      "Train Epoche: 1 [233/96 (243%)]\tLoss: 288.235931\n",
      "Train Epoche: 1 [234/96 (244%)]\tLoss: 194.940369\n",
      "Train Epoche: 1 [235/96 (245%)]\tLoss: 80.257591\n",
      "Train Epoche: 1 [236/96 (246%)]\tLoss: 255.393799\n",
      "Train Epoche: 1 [237/96 (247%)]\tLoss: 574.580444\n",
      "Train Epoche: 1 [238/96 (248%)]\tLoss: 224.268066\n",
      "Train Epoche: 1 [239/96 (249%)]\tLoss: 3.880487\n",
      "Train Epoche: 1 [240/96 (250%)]\tLoss: 48.471668\n",
      "Train Epoche: 1 [241/96 (251%)]\tLoss: 120.709000\n",
      "Train Epoche: 1 [242/96 (252%)]\tLoss: 143.362427\n",
      "Train Epoche: 1 [243/96 (253%)]\tLoss: 15.550031\n",
      "Train Epoche: 1 [244/96 (254%)]\tLoss: 8.723943\n",
      "Train Epoche: 1 [245/96 (255%)]\tLoss: 167.889160\n",
      "Train Epoche: 1 [246/96 (256%)]\tLoss: 63.499767\n",
      "Train Epoche: 1 [247/96 (257%)]\tLoss: 100.023613\n",
      "Train Epoche: 1 [248/96 (258%)]\tLoss: 35.953575\n",
      "Train Epoche: 1 [249/96 (259%)]\tLoss: 323.603516\n",
      "Train Epoche: 1 [250/96 (260%)]\tLoss: 361.893799\n",
      "Train Epoche: 1 [251/96 (261%)]\tLoss: 402.112701\n",
      "Train Epoche: 1 [252/96 (262%)]\tLoss: 441.285553\n",
      "Train Epoche: 1 [253/96 (264%)]\tLoss: 527.978638\n",
      "Train Epoche: 1 [254/96 (265%)]\tLoss: 487.241150\n",
      "Train Epoche: 1 [255/96 (266%)]\tLoss: 120.293770\n",
      "Train Epoche: 1 [256/96 (267%)]\tLoss: 194.999420\n",
      "Train Epoche: 1 [257/96 (268%)]\tLoss: 98.990898\n",
      "Train Epoche: 1 [258/96 (269%)]\tLoss: 399.377899\n",
      "Train Epoche: 1 [259/96 (270%)]\tLoss: 398.487213\n",
      "Train Epoche: 1 [260/96 (271%)]\tLoss: 142.811340\n",
      "Train Epoche: 1 [261/96 (272%)]\tLoss: 168.221375\n",
      "Train Epoche: 1 [262/96 (273%)]\tLoss: 24.696869\n",
      "Train Epoche: 1 [263/96 (274%)]\tLoss: 398.305176\n",
      "Train Epoche: 1 [264/96 (275%)]\tLoss: 48.359436\n",
      "Train Epoche: 1 [265/96 (276%)]\tLoss: 35.548668\n",
      "Train Epoche: 1 [266/96 (277%)]\tLoss: 8.780083\n",
      "Train Epoche: 1 [267/96 (278%)]\tLoss: 3.856227\n",
      "Train Epoche: 1 [268/96 (279%)]\tLoss: 397.794403\n",
      "Train Epoche: 1 [269/96 (280%)]\tLoss: 224.063309\n",
      "Train Epoche: 1 [270/96 (281%)]\tLoss: 0.925507\n",
      "Train Epoche: 1 [271/96 (282%)]\tLoss: 16.092567\n",
      "Train Epoche: 1 [272/96 (283%)]\tLoss: 80.360985\n",
      "Train Epoche: 1 [273/96 (284%)]\tLoss: 62.803646\n",
      "Train Epoche: 1 [274/96 (285%)]\tLoss: 15.747845\n",
      "Train Epoche: 1 [275/96 (286%)]\tLoss: 0.944395\n",
      "Train Epoche: 1 [276/96 (288%)]\tLoss: 168.148438\n",
      "Train Epoche: 1 [277/96 (289%)]\tLoss: 254.618744\n",
      "Train Epoche: 1 [278/96 (290%)]\tLoss: 574.454346\n",
      "Train Epoche: 1 [279/96 (291%)]\tLoss: 143.152176\n",
      "Train Epoche: 1 [280/96 (292%)]\tLoss: 63.882660\n",
      "Train Epoche: 1 [281/96 (293%)]\tLoss: 99.050858\n",
      "Train Epoche: 1 [282/96 (294%)]\tLoss: 8.739223\n",
      "Train Epoche: 1 [283/96 (295%)]\tLoss: 35.681320\n",
      "Train Epoche: 1 [284/96 (296%)]\tLoss: 194.586792\n",
      "Train Epoche: 1 [285/96 (297%)]\tLoss: 48.486752\n",
      "Train Epoche: 1 [286/96 (298%)]\tLoss: 24.613352\n",
      "Train Epoche: 1 [287/96 (299%)]\tLoss: 3.812687\n",
      "Train Epoche: 1 [288/96 (300%)]\tLoss: 119.724701\n",
      "Train Epoche: 1 [289/96 (301%)]\tLoss: 223.507309\n",
      "Train Epoche: 1 [290/96 (302%)]\tLoss: 81.221512\n",
      "Train Epoche: 1 [291/96 (303%)]\tLoss: 575.138550\n",
      "Train Epoche: 1 [292/96 (304%)]\tLoss: 577.541870\n",
      "Train Epoche: 1 [293/96 (305%)]\tLoss: 577.228271\n",
      "Train Epoche: 1 [294/96 (306%)]\tLoss: 288.387939\n",
      "Train Epoche: 1 [295/96 (307%)]\tLoss: 361.861786\n",
      "Train Epoche: 1 [296/96 (308%)]\tLoss: 397.309143\n",
      "Train Epoche: 1 [297/96 (309%)]\tLoss: 322.312927\n",
      "Train Epoche: 1 [298/96 (310%)]\tLoss: 99.506226\n",
      "Train Epoche: 1 [299/96 (311%)]\tLoss: 143.093613\n",
      "Train Epoche: 1 [300/96 (312%)]\tLoss: 195.122147\n",
      "Train Epoche: 1 [301/96 (314%)]\tLoss: 223.676208\n",
      "Train Epoche: 1 [302/96 (315%)]\tLoss: 48.503601\n",
      "Train Epoche: 1 [303/96 (316%)]\tLoss: 482.677277\n",
      "Train Epoche: 1 [304/96 (317%)]\tLoss: 3.817291\n",
      "Train Epoche: 1 [305/96 (318%)]\tLoss: 15.659234\n",
      "Train Epoche: 1 [306/96 (319%)]\tLoss: 255.013702\n",
      "Train Epoche: 1 [307/96 (320%)]\tLoss: 8.762681\n",
      "Train Epoche: 1 [308/96 (321%)]\tLoss: 0.907710\n",
      "Train Epoche: 1 [309/96 (322%)]\tLoss: 24.416798\n",
      "Train Epoche: 1 [310/96 (323%)]\tLoss: 168.399567\n",
      "Train Epoche: 1 [311/96 (324%)]\tLoss: 80.328232\n",
      "Train Epoche: 1 [312/96 (325%)]\tLoss: 35.305733\n",
      "Train Epoche: 1 [313/96 (326%)]\tLoss: 291.042145\n",
      "Train Epoche: 1 [314/96 (327%)]\tLoss: 325.231445\n",
      "Train Epoche: 1 [315/96 (328%)]\tLoss: 119.910851\n",
      "Train Epoche: 1 [316/96 (329%)]\tLoss: 63.580776\n",
      "Train Epoche: 1 [317/96 (330%)]\tLoss: 398.867676\n",
      "Train Epoche: 1 [318/96 (331%)]\tLoss: 362.467163\n",
      "Train Epoche: 1 [319/96 (332%)]\tLoss: 120.256371\n",
      "Train Epoche: 1 [320/96 (333%)]\tLoss: 48.593487\n",
      "Train Epoche: 1 [321/96 (334%)]\tLoss: 80.638336\n",
      "Train Epoche: 1 [322/96 (335%)]\tLoss: 63.234421\n",
      "Train Epoche: 1 [323/96 (336%)]\tLoss: 254.642120\n",
      "Train Epoche: 1 [324/96 (338%)]\tLoss: 223.785858\n",
      "Train Epoche: 1 [325/96 (339%)]\tLoss: 398.991943\n",
      "Train Epoche: 1 [326/96 (340%)]\tLoss: 195.363174\n",
      "Train Epoche: 1 [327/96 (341%)]\tLoss: 15.620369\n",
      "Train Epoche: 1 [328/96 (342%)]\tLoss: 35.424286\n",
      "Train Epoche: 1 [329/96 (343%)]\tLoss: 98.995567\n",
      "Train Epoche: 1 [330/96 (344%)]\tLoss: 8.794389\n",
      "Train Epoche: 1 [331/96 (345%)]\tLoss: 0.918730\n",
      "Train Epoche: 1 [332/96 (346%)]\tLoss: 322.413666\n",
      "Train Epoche: 1 [333/96 (347%)]\tLoss: 287.870178\n",
      "Train Epoche: 1 [334/96 (348%)]\tLoss: 4.030635\n",
      "Train Epoche: 1 [335/96 (349%)]\tLoss: 25.194315\n",
      "Train Epoche: 1 [336/96 (350%)]\tLoss: 169.992981\n",
      "Train Epoche: 1 [337/96 (351%)]\tLoss: 143.712158\n",
      "Train Epoche: 1 [338/96 (352%)]\tLoss: 48.630177\n",
      "Train Epoche: 1 [339/96 (353%)]\tLoss: 80.320396\n",
      "Train Epoche: 1 [340/96 (354%)]\tLoss: 168.250519\n",
      "Train Epoche: 1 [341/96 (355%)]\tLoss: 24.855642\n",
      "Train Epoche: 1 [342/96 (356%)]\tLoss: 482.373077\n",
      "Train Epoche: 1 [343/96 (357%)]\tLoss: 99.427109\n",
      "Train Epoche: 1 [344/96 (358%)]\tLoss: 195.019547\n",
      "Train Epoche: 1 [345/96 (359%)]\tLoss: 120.278976\n",
      "Train Epoche: 1 [346/96 (360%)]\tLoss: 15.723204\n",
      "Train Epoche: 1 [347/96 (361%)]\tLoss: 63.265236\n",
      "Train Epoche: 1 [348/96 (362%)]\tLoss: 35.412888\n",
      "Train Epoche: 1 [349/96 (364%)]\tLoss: 0.930094\n",
      "Train Epoche: 1 [350/96 (365%)]\tLoss: 3.857622\n",
      "Train Epoche: 1 [351/96 (366%)]\tLoss: 143.348923\n",
      "Train Epoche: 1 [352/96 (367%)]\tLoss: 481.935852\n",
      "Train Epoche: 1 [353/96 (368%)]\tLoss: 484.151489\n",
      "Train Epoche: 1 [354/96 (369%)]\tLoss: 8.858768\n",
      "Train Epoche: 1 [355/96 (370%)]\tLoss: 225.762543\n",
      "Train Epoche: 1 [356/96 (371%)]\tLoss: 256.203461\n",
      "Train Epoche: 1 [357/96 (372%)]\tLoss: 488.734558\n",
      "Train Epoche: 1 [358/96 (373%)]\tLoss: 487.156708\n",
      "Train Epoche: 1 [359/96 (374%)]\tLoss: 15.661848\n",
      "Train Epoche: 1 [360/96 (375%)]\tLoss: 24.880077\n",
      "Train Epoche: 1 [361/96 (376%)]\tLoss: 120.476547\n",
      "Train Epoche: 1 [362/96 (377%)]\tLoss: 8.931334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [363/96 (378%)]\tLoss: 167.777130\n",
      "Train Epoche: 1 [364/96 (379%)]\tLoss: 195.295837\n",
      "Train Epoche: 1 [365/96 (380%)]\tLoss: 36.040424\n",
      "Train Epoche: 1 [366/96 (381%)]\tLoss: 80.270134\n",
      "Train Epoche: 1 [367/96 (382%)]\tLoss: 143.044357\n",
      "Train Epoche: 1 [368/96 (383%)]\tLoss: 99.739494\n",
      "Train Epoche: 1 [369/96 (384%)]\tLoss: 63.309315\n",
      "Train Epoche: 1 [370/96 (385%)]\tLoss: 49.196381\n",
      "Train Epoche: 1 [371/96 (386%)]\tLoss: 256.326385\n",
      "Train Epoche: 1 [372/96 (388%)]\tLoss: 223.351273\n",
      "Train Epoche: 1 [373/96 (389%)]\tLoss: 0.929623\n",
      "Train Epoche: 1 [374/96 (390%)]\tLoss: 3.984025\n",
      "Train Epoche: 1 [375/96 (391%)]\tLoss: 485.369324\n",
      "Train Epoche: 1 [376/96 (392%)]\tLoss: 361.690613\n",
      "Train Epoche: 1 [377/96 (393%)]\tLoss: 288.042908\n",
      "Train Epoche: 1 [378/96 (394%)]\tLoss: 324.615112\n",
      "Train Epoche: 1 [379/96 (395%)]\tLoss: 483.812286\n",
      "Train Epoche: 1 [380/96 (396%)]\tLoss: 8.766146\n",
      "Train Epoche: 1 [381/96 (397%)]\tLoss: 0.947655\n",
      "Train Epoche: 1 [382/96 (398%)]\tLoss: 254.580002\n",
      "Train Epoche: 1 [383/96 (399%)]\tLoss: 167.999573\n",
      "Train Epoche: 1 [384/96 (400%)]\tLoss: 80.435753\n",
      "Train Epoche: 1 [385/96 (401%)]\tLoss: 120.139938\n",
      "Train Epoche: 1 [386/96 (402%)]\tLoss: 24.659845\n",
      "Train Epoche: 1 [387/96 (403%)]\tLoss: 573.513306\n",
      "Train Epoche: 1 [388/96 (404%)]\tLoss: 99.129707\n",
      "Train Epoche: 1 [389/96 (405%)]\tLoss: 15.695839\n",
      "Train Epoche: 1 [390/96 (406%)]\tLoss: 3.818451\n",
      "Train Epoche: 1 [391/96 (407%)]\tLoss: 35.475506\n",
      "Train Epoche: 1 [392/96 (408%)]\tLoss: 63.248032\n",
      "Train Epoche: 1 [393/96 (409%)]\tLoss: 143.529190\n",
      "Train Epoche: 1 [394/96 (410%)]\tLoss: 573.494568\n",
      "Train Epoche: 1 [395/96 (411%)]\tLoss: 577.843262\n",
      "Train Epoche: 1 [396/96 (412%)]\tLoss: 579.844177\n",
      "Train Epoche: 1 [397/96 (414%)]\tLoss: 48.735264\n",
      "Train Epoche: 1 [398/96 (415%)]\tLoss: 577.479004\n",
      "Train Epoche: 1 [399/96 (416%)]\tLoss: 196.480392\n",
      "Train Epoche: 1 [400/96 (417%)]\tLoss: 224.840149\n",
      "Train Epoche: 1 [401/96 (418%)]\tLoss: 99.276878\n",
      "Train Epoche: 1 [402/96 (419%)]\tLoss: 143.167007\n",
      "Train Epoche: 1 [403/96 (420%)]\tLoss: 24.562639\n",
      "Train Epoche: 1 [404/96 (421%)]\tLoss: 63.221817\n",
      "Train Epoche: 1 [405/96 (422%)]\tLoss: 80.396553\n",
      "Train Epoche: 1 [406/96 (423%)]\tLoss: 194.866928\n",
      "Train Epoche: 1 [407/96 (424%)]\tLoss: 3.890895\n",
      "Train Epoche: 1 [408/96 (425%)]\tLoss: 35.581493\n",
      "Train Epoche: 1 [409/96 (426%)]\tLoss: 481.970001\n",
      "Train Epoche: 1 [410/96 (427%)]\tLoss: 482.285553\n",
      "Train Epoche: 1 [411/96 (428%)]\tLoss: 48.598900\n",
      "Train Epoche: 1 [412/96 (429%)]\tLoss: 0.917492\n",
      "Train Epoche: 1 [413/96 (430%)]\tLoss: 120.349022\n",
      "Train Epoche: 1 [414/96 (431%)]\tLoss: 482.863098\n",
      "Train Epoche: 1 [415/96 (432%)]\tLoss: 8.815573\n",
      "Train Epoche: 1 [416/96 (433%)]\tLoss: 15.711733\n",
      "Train Epoche: 1 [417/96 (434%)]\tLoss: 484.900146\n",
      "Train Epoche: 1 [418/96 (435%)]\tLoss: 486.085815\n",
      "Train Epoche: 1 [419/96 (436%)]\tLoss: 483.507080\n",
      "Train Epoche: 1 [420/96 (438%)]\tLoss: 168.989609\n",
      "Train Epoche: 1 [421/96 (439%)]\tLoss: 255.462448\n",
      "Train Epoche: 1 [422/96 (440%)]\tLoss: 224.974503\n",
      "Train Epoche: 1 [423/96 (441%)]\tLoss: 35.443024\n",
      "Train Epoche: 1 [424/96 (442%)]\tLoss: 120.261345\n",
      "Train Epoche: 1 [425/96 (443%)]\tLoss: 288.026001\n",
      "Train Epoche: 1 [426/96 (444%)]\tLoss: 223.645294\n",
      "Train Epoche: 1 [427/96 (445%)]\tLoss: 99.063011\n",
      "Train Epoche: 1 [428/96 (446%)]\tLoss: 142.878311\n",
      "Train Epoche: 1 [429/96 (447%)]\tLoss: 3.797193\n",
      "Train Epoche: 1 [430/96 (448%)]\tLoss: 48.392494\n",
      "Train Epoche: 1 [431/96 (449%)]\tLoss: 80.140450\n",
      "Train Epoche: 1 [432/96 (450%)]\tLoss: 482.211975\n",
      "Train Epoche: 1 [433/96 (451%)]\tLoss: 24.511581\n",
      "Train Epoche: 1 [434/96 (452%)]\tLoss: 0.920739\n",
      "Train Epoche: 1 [435/96 (453%)]\tLoss: 168.051712\n",
      "Train Epoche: 1 [436/96 (454%)]\tLoss: 195.061584\n",
      "Train Epoche: 1 [437/96 (455%)]\tLoss: 8.743931\n",
      "Train Epoche: 1 [438/96 (456%)]\tLoss: 16.413082\n",
      "Train Epoche: 1 [439/96 (457%)]\tLoss: 487.549652\n",
      "Train Epoche: 1 [440/96 (458%)]\tLoss: 257.114136\n",
      "Train Epoche: 1 [441/96 (459%)]\tLoss: 483.624359\n",
      "Train Epoche: 1 [442/96 (460%)]\tLoss: 64.537498\n",
      "Train Epoche: 1 [443/96 (461%)]\tLoss: 360.631622\n",
      "Train Epoche: 1 [444/96 (462%)]\tLoss: 323.463074\n",
      "Train Epoche: 1 [445/96 (464%)]\tLoss: 142.834412\n",
      "Train Epoche: 1 [446/96 (465%)]\tLoss: 63.367569\n",
      "Train Epoche: 1 [447/96 (466%)]\tLoss: 482.693451\n",
      "Train Epoche: 1 [448/96 (467%)]\tLoss: 79.996658\n",
      "Train Epoche: 1 [449/96 (468%)]\tLoss: 255.381958\n",
      "Train Epoche: 1 [450/96 (469%)]\tLoss: 360.183472\n",
      "Train Epoche: 1 [451/96 (470%)]\tLoss: 223.822067\n",
      "Train Epoche: 1 [452/96 (471%)]\tLoss: 194.707794\n",
      "Train Epoche: 1 [453/96 (472%)]\tLoss: 35.450741\n",
      "Train Epoche: 1 [454/96 (473%)]\tLoss: 24.483553\n",
      "Train Epoche: 1 [455/96 (474%)]\tLoss: 48.433861\n",
      "Train Epoche: 1 [456/96 (475%)]\tLoss: 99.291191\n",
      "Train Epoche: 1 [457/96 (476%)]\tLoss: 3.863839\n",
      "Train Epoche: 1 [458/96 (477%)]\tLoss: 8.712743\n",
      "Train Epoche: 1 [459/96 (478%)]\tLoss: 322.196991\n",
      "Train Epoche: 1 [460/96 (479%)]\tLoss: 482.576111\n",
      "Train Epoche: 1 [461/96 (480%)]\tLoss: 0.891759\n",
      "Train Epoche: 1 [462/96 (481%)]\tLoss: 15.874315\n",
      "Train Epoche: 1 [463/96 (482%)]\tLoss: 290.178040\n",
      "Train Epoche: 1 [464/96 (483%)]\tLoss: 397.145996\n",
      "Train Epoche: 1 [465/96 (484%)]\tLoss: 171.953720\n",
      "Train Epoche: 1 [466/96 (485%)]\tLoss: 120.648888\n",
      "Train Epoche: 1 [467/96 (486%)]\tLoss: 287.978790\n",
      "Train Epoche: 1 [468/96 (488%)]\tLoss: 482.077881\n",
      "Train Epoche: 1 [469/96 (489%)]\tLoss: 48.573811\n",
      "Train Epoche: 1 [470/96 (490%)]\tLoss: 63.372746\n",
      "Train Epoche: 1 [471/96 (491%)]\tLoss: 482.079987\n",
      "Train Epoche: 1 [472/96 (492%)]\tLoss: 120.153984\n",
      "Train Epoche: 1 [473/96 (493%)]\tLoss: 80.298195\n",
      "Train Epoche: 1 [474/96 (494%)]\tLoss: 99.350937\n",
      "Train Epoche: 1 [475/96 (495%)]\tLoss: 24.597513\n",
      "Train Epoche: 1 [476/96 (496%)]\tLoss: 8.782721\n",
      "Train Epoche: 1 [477/96 (497%)]\tLoss: 35.624207\n",
      "Train Epoche: 1 [478/96 (498%)]\tLoss: 15.638445\n",
      "Train Epoche: 1 [479/96 (499%)]\tLoss: 482.192810\n",
      "Train Epoche: 1 [480/96 (500%)]\tLoss: 482.346100\n",
      "Train Epoche: 1 [481/96 (501%)]\tLoss: 0.952036\n",
      "Train Epoche: 1 [482/96 (502%)]\tLoss: 3.753057\n",
      "Train Epoche: 1 [483/96 (503%)]\tLoss: 226.652954\n",
      "Train Epoche: 1 [484/96 (504%)]\tLoss: 485.501038\n",
      "Train Epoche: 1 [485/96 (505%)]\tLoss: 143.305298\n",
      "Train Epoche: 1 [486/96 (506%)]\tLoss: 195.517700\n",
      "Train Epoche: 1 [487/96 (507%)]\tLoss: 168.630692\n",
      "Train Epoche: 1 [488/96 (508%)]\tLoss: 254.627762\n",
      "Train Epoche: 1 [489/96 (509%)]\tLoss: 35.423523\n",
      "Train Epoche: 1 [490/96 (510%)]\tLoss: 80.463318\n",
      "Train Epoche: 1 [491/96 (511%)]\tLoss: 48.391014\n",
      "Train Epoche: 1 [492/96 (512%)]\tLoss: 63.523010\n",
      "Train Epoche: 1 [493/96 (514%)]\tLoss: 482.277252\n",
      "Train Epoche: 1 [494/96 (515%)]\tLoss: 99.320137\n",
      "Train Epoche: 1 [495/96 (516%)]\tLoss: 15.719243\n",
      "Train Epoche: 1 [496/96 (517%)]\tLoss: 142.916550\n",
      "Train Epoche: 1 [497/96 (518%)]\tLoss: 24.554325\n",
      "Train Epoche: 1 [498/96 (519%)]\tLoss: 8.829757\n",
      "Train Epoche: 1 [499/96 (520%)]\tLoss: 482.009033\n",
      "Train Epoche: 1 [500/96 (521%)]\tLoss: 482.263855\n",
      "Train Epoche: 1 [501/96 (522%)]\tLoss: 482.398987\n",
      "Train Epoche: 1 [502/96 (523%)]\tLoss: 0.997154\n",
      "Train Epoche: 1 [503/96 (524%)]\tLoss: 3.786600\n",
      "Train Epoche: 1 [504/96 (525%)]\tLoss: 170.022446\n",
      "Train Epoche: 1 [505/96 (526%)]\tLoss: 194.981674\n",
      "Train Epoche: 1 [506/96 (527%)]\tLoss: 121.781685\n",
      "Train Epoche: 1 [507/96 (528%)]\tLoss: 483.860535\n",
      "Train Epoche: 1 [508/96 (529%)]\tLoss: 226.847015\n",
      "Train Epoche: 1 [509/96 (530%)]\tLoss: 483.643219\n",
      "Train Epoche: 1 [510/96 (531%)]\tLoss: 0.880495\n",
      "Train Epoche: 1 [511/96 (532%)]\tLoss: 574.064087\n",
      "Train Epoche: 1 [512/96 (533%)]\tLoss: 99.451210\n",
      "Train Epoche: 1 [513/96 (534%)]\tLoss: 120.213684\n",
      "Train Epoche: 1 [514/96 (535%)]\tLoss: 143.035736\n",
      "Train Epoche: 1 [515/96 (536%)]\tLoss: 573.382996\n",
      "Train Epoche: 1 [516/96 (538%)]\tLoss: 8.854295\n",
      "Train Epoche: 1 [517/96 (539%)]\tLoss: 15.661762\n",
      "Train Epoche: 1 [518/96 (540%)]\tLoss: 438.814636\n",
      "Train Epoche: 1 [519/96 (541%)]\tLoss: 63.261738\n",
      "Train Epoche: 1 [520/96 (542%)]\tLoss: 398.153381\n",
      "Train Epoche: 1 [521/96 (543%)]\tLoss: 482.432831\n",
      "Train Epoche: 1 [522/96 (544%)]\tLoss: 80.418533\n",
      "Train Epoche: 1 [523/96 (545%)]\tLoss: 3.829461\n",
      "Train Epoche: 1 [524/96 (546%)]\tLoss: 49.005894\n",
      "Train Epoche: 1 [525/96 (547%)]\tLoss: 36.345314\n",
      "Train Epoche: 1 [526/96 (548%)]\tLoss: 321.558044\n",
      "Train Epoche: 1 [527/96 (549%)]\tLoss: 358.700653\n",
      "Train Epoche: 1 [528/96 (550%)]\tLoss: 198.351257\n",
      "Train Epoche: 1 [529/96 (551%)]\tLoss: 224.850769\n",
      "Train Epoche: 1 [530/96 (552%)]\tLoss: 25.656960\n",
      "Train Epoche: 1 [531/96 (553%)]\tLoss: 168.402542\n",
      "Train Epoche: 1 [532/96 (554%)]\tLoss: 287.125183\n",
      "Train Epoche: 1 [533/96 (555%)]\tLoss: 259.114899\n",
      "Train Epoche: 1 [534/96 (556%)]\tLoss: 63.502041\n",
      "Train Epoche: 1 [535/96 (557%)]\tLoss: 99.064240\n",
      "Train Epoche: 1 [536/96 (558%)]\tLoss: 80.597374\n",
      "Train Epoche: 1 [537/96 (559%)]\tLoss: 254.569656\n",
      "Train Epoche: 1 [538/96 (560%)]\tLoss: 168.210052\n",
      "Train Epoche: 1 [539/96 (561%)]\tLoss: 194.940872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [540/96 (562%)]\tLoss: 3.851839\n",
      "Train Epoche: 1 [541/96 (564%)]\tLoss: 15.600881\n",
      "Train Epoche: 1 [542/96 (565%)]\tLoss: 142.793213\n",
      "Train Epoche: 1 [543/96 (566%)]\tLoss: 573.466858\n",
      "Train Epoche: 1 [544/96 (567%)]\tLoss: 0.938685\n",
      "Train Epoche: 1 [545/96 (568%)]\tLoss: 8.657161\n",
      "Train Epoche: 1 [546/96 (569%)]\tLoss: 120.021660\n",
      "Train Epoche: 1 [547/96 (570%)]\tLoss: 573.888794\n",
      "Train Epoche: 1 [548/96 (571%)]\tLoss: 224.420609\n",
      "Train Epoche: 1 [549/96 (572%)]\tLoss: 48.216763\n",
      "Train Epoche: 1 [550/96 (573%)]\tLoss: 401.594360\n",
      "Train Epoche: 1 [551/96 (574%)]\tLoss: 439.843842\n",
      "Train Epoche: 1 [552/96 (575%)]\tLoss: 288.326721\n",
      "Train Epoche: 1 [553/96 (576%)]\tLoss: 24.625229\n",
      "Train Epoche: 1 [554/96 (577%)]\tLoss: 36.752357\n",
      "Train Epoche: 1 [555/96 (578%)]\tLoss: 324.175262\n",
      "Train Epoche: 1 [556/96 (579%)]\tLoss: 360.183319\n",
      "Train Epoche: 1 [557/96 (580%)]\tLoss: 398.904633\n",
      "Train Epoche: 1 [558/96 (581%)]\tLoss: 397.889435\n",
      "Train Epoche: 1 [559/96 (582%)]\tLoss: 80.245872\n",
      "Train Epoche: 1 [560/96 (583%)]\tLoss: 223.757202\n",
      "Train Epoche: 1 [561/96 (584%)]\tLoss: 398.581268\n",
      "Train Epoche: 1 [562/96 (585%)]\tLoss: 120.025085\n",
      "Train Epoche: 1 [563/96 (586%)]\tLoss: 195.570450\n",
      "Train Epoche: 1 [564/96 (588%)]\tLoss: 35.601048\n",
      "Train Epoche: 1 [565/96 (589%)]\tLoss: 3.785231\n",
      "Train Epoche: 1 [566/96 (590%)]\tLoss: 0.950674\n",
      "Train Epoche: 1 [567/96 (591%)]\tLoss: 168.146530\n",
      "Train Epoche: 1 [568/96 (592%)]\tLoss: 143.128922\n",
      "Train Epoche: 1 [569/96 (593%)]\tLoss: 8.797196\n",
      "Train Epoche: 1 [570/96 (594%)]\tLoss: 24.636644\n",
      "Train Epoche: 1 [571/96 (595%)]\tLoss: 398.329468\n",
      "Train Epoche: 1 [572/96 (596%)]\tLoss: 398.673981\n",
      "Train Epoche: 1 [573/96 (597%)]\tLoss: 48.172543\n",
      "Train Epoche: 1 [574/96 (598%)]\tLoss: 16.000950\n",
      "Train Epoche: 1 [575/96 (599%)]\tLoss: 63.796284\n",
      "Train Epoche: 1 [576/96 (600%)]\tLoss: 99.770584\n",
      "Train Epoche: 1 [577/96 (601%)]\tLoss: 143.421738\n",
      "Train Epoche: 1 [578/96 (602%)]\tLoss: 168.328064\n",
      "Train Epoche: 1 [579/96 (603%)]\tLoss: 35.459690\n",
      "Train Epoche: 1 [580/96 (604%)]\tLoss: 99.087517\n",
      "Train Epoche: 1 [581/96 (605%)]\tLoss: 287.607330\n",
      "Train Epoche: 1 [582/96 (606%)]\tLoss: 482.964935\n",
      "Train Epoche: 1 [583/96 (607%)]\tLoss: 63.374454\n",
      "Train Epoche: 1 [584/96 (608%)]\tLoss: 80.023254\n",
      "Train Epoche: 1 [585/96 (609%)]\tLoss: 24.633057\n",
      "Train Epoche: 1 [586/96 (610%)]\tLoss: 3.888089\n",
      "Train Epoche: 1 [587/96 (611%)]\tLoss: 224.074463\n",
      "Train Epoche: 1 [588/96 (612%)]\tLoss: 120.120705\n",
      "Train Epoche: 1 [589/96 (614%)]\tLoss: 15.627290\n",
      "Train Epoche: 1 [590/96 (615%)]\tLoss: 8.864251\n",
      "Train Epoche: 1 [591/96 (616%)]\tLoss: 255.268463\n",
      "Train Epoche: 1 [592/96 (617%)]\tLoss: 398.980682\n",
      "Train Epoche: 1 [593/96 (618%)]\tLoss: 48.581821\n",
      "Train Epoche: 1 [594/96 (619%)]\tLoss: 1.023770\n",
      "Train Epoche: 1 [595/96 (620%)]\tLoss: 328.119904\n",
      "Train Epoche: 1 [596/96 (621%)]\tLoss: 441.324890\n",
      "Train Epoche: 1 [597/96 (622%)]\tLoss: 360.975708\n",
      "Train Epoche: 1 [598/96 (623%)]\tLoss: 196.025925\n",
      "Train Epoche: 1 [599/96 (624%)]\tLoss: 8.799135\n",
      "Train Epoche: 1 [600/96 (625%)]\tLoss: 3.878110\n",
      "Train Epoche: 1 [601/96 (626%)]\tLoss: 24.744623\n",
      "Train Epoche: 1 [602/96 (627%)]\tLoss: 63.707520\n",
      "Train Epoche: 1 [603/96 (628%)]\tLoss: 80.696152\n",
      "Train Epoche: 1 [604/96 (629%)]\tLoss: 15.629620\n",
      "Train Epoche: 1 [605/96 (630%)]\tLoss: 48.519184\n",
      "Train Epoche: 1 [606/96 (631%)]\tLoss: 35.657532\n",
      "Train Epoche: 1 [607/96 (632%)]\tLoss: 99.239792\n",
      "Train Epoche: 1 [608/96 (633%)]\tLoss: 481.944061\n",
      "Train Epoche: 1 [609/96 (634%)]\tLoss: 482.009369\n",
      "Train Epoche: 1 [610/96 (635%)]\tLoss: 120.442741\n",
      "Train Epoche: 1 [611/96 (636%)]\tLoss: 142.794632\n",
      "Train Epoche: 1 [612/96 (638%)]\tLoss: 482.437347\n",
      "Train Epoche: 1 [613/96 (639%)]\tLoss: 0.936022\n",
      "Train Epoche: 1 [614/96 (640%)]\tLoss: 485.140350\n",
      "Train Epoche: 1 [615/96 (641%)]\tLoss: 484.459595\n",
      "Train Epoche: 1 [616/96 (642%)]\tLoss: 485.808807\n",
      "Train Epoche: 1 [617/96 (643%)]\tLoss: 167.594986\n",
      "Train Epoche: 1 [618/96 (644%)]\tLoss: 483.652954\n",
      "Train Epoche: 1 [619/96 (645%)]\tLoss: 0.929137\n",
      "Train Epoche: 1 [620/96 (646%)]\tLoss: 15.793434\n",
      "Train Epoche: 1 [621/96 (647%)]\tLoss: 167.801315\n",
      "Train Epoche: 1 [622/96 (648%)]\tLoss: 322.834656\n",
      "Train Epoche: 1 [623/96 (649%)]\tLoss: 142.931717\n",
      "Train Epoche: 1 [624/96 (650%)]\tLoss: 80.748695\n",
      "Train Epoche: 1 [625/96 (651%)]\tLoss: 194.935822\n",
      "Train Epoche: 1 [626/96 (652%)]\tLoss: 573.479309\n",
      "Train Epoche: 1 [627/96 (653%)]\tLoss: 48.593182\n",
      "Train Epoche: 1 [628/96 (654%)]\tLoss: 35.445950\n",
      "Train Epoche: 1 [629/96 (655%)]\tLoss: 223.851883\n",
      "Train Epoche: 1 [630/96 (656%)]\tLoss: 120.209312\n",
      "Train Epoche: 1 [631/96 (657%)]\tLoss: 8.827339\n",
      "Train Epoche: 1 [632/96 (658%)]\tLoss: 3.817299\n",
      "Train Epoche: 1 [633/96 (659%)]\tLoss: 98.889435\n",
      "Train Epoche: 1 [634/96 (660%)]\tLoss: 287.674896\n",
      "Train Epoche: 1 [635/96 (661%)]\tLoss: 24.305645\n",
      "Train Epoche: 1 [636/96 (662%)]\tLoss: 62.965382\n",
      "Train Epoche: 1 [637/96 (664%)]\tLoss: 256.231384\n",
      "Train Epoche: 1 [638/96 (665%)]\tLoss: 361.240601\n",
      "Train Epoche: 1 [639/96 (666%)]\tLoss: 439.821259\n",
      "Train Epoche: 1 [640/96 (667%)]\tLoss: 400.612732\n",
      "Train Epoche: 1 [641/96 (668%)]\tLoss: 482.278595\n",
      "Train Epoche: 1 [642/96 (669%)]\tLoss: 526.271606\n",
      "Train Epoche: 1 [643/96 (670%)]\tLoss: 288.741943\n",
      "Train Epoche: 1 [644/96 (671%)]\tLoss: 80.186607\n",
      "Train Epoche: 1 [645/96 (672%)]\tLoss: 482.363861\n",
      "Train Epoche: 1 [646/96 (673%)]\tLoss: 120.271805\n",
      "Train Epoche: 1 [647/96 (674%)]\tLoss: 322.963379\n",
      "Train Epoche: 1 [648/96 (675%)]\tLoss: 99.218735\n",
      "Train Epoche: 1 [649/96 (676%)]\tLoss: 481.647888\n",
      "Train Epoche: 1 [650/96 (677%)]\tLoss: 24.687241\n",
      "Train Epoche: 1 [651/96 (678%)]\tLoss: 481.795166\n",
      "Train Epoche: 1 [652/96 (679%)]\tLoss: 482.060974\n",
      "Train Epoche: 1 [653/96 (680%)]\tLoss: 3.927304\n",
      "Train Epoche: 1 [654/96 (681%)]\tLoss: 0.933593\n",
      "Train Epoche: 1 [655/96 (682%)]\tLoss: 63.310856\n",
      "Train Epoche: 1 [656/96 (683%)]\tLoss: 143.308823\n",
      "Train Epoche: 1 [657/96 (684%)]\tLoss: 8.747025\n",
      "Train Epoche: 1 [658/96 (685%)]\tLoss: 16.312359\n",
      "Train Epoche: 1 [659/96 (686%)]\tLoss: 195.931961\n",
      "Train Epoche: 1 [660/96 (688%)]\tLoss: 226.482895\n",
      "Train Epoche: 1 [661/96 (689%)]\tLoss: 49.033585\n",
      "Train Epoche: 1 [662/96 (690%)]\tLoss: 36.037231\n",
      "Train Epoche: 1 [663/96 (691%)]\tLoss: 256.015930\n",
      "Train Epoche: 1 [664/96 (692%)]\tLoss: 167.754364\n",
      "Train Epoche: 1 [665/96 (693%)]\tLoss: 323.283752\n",
      "Train Epoche: 1 [666/96 (694%)]\tLoss: 195.131470\n",
      "Train Epoche: 1 [667/96 (695%)]\tLoss: 8.761334\n",
      "Train Epoche: 1 [668/96 (696%)]\tLoss: 15.718383\n",
      "Train Epoche: 1 [669/96 (697%)]\tLoss: 143.333633\n",
      "Train Epoche: 1 [670/96 (698%)]\tLoss: 120.065582\n",
      "Train Epoche: 1 [671/96 (699%)]\tLoss: 24.467346\n",
      "Train Epoche: 1 [672/96 (700%)]\tLoss: 3.824577\n",
      "Train Epoche: 1 [673/96 (701%)]\tLoss: 48.475922\n",
      "Train Epoche: 1 [674/96 (702%)]\tLoss: 35.548069\n",
      "Train Epoche: 1 [675/96 (703%)]\tLoss: 63.558952\n",
      "Train Epoche: 1 [676/96 (704%)]\tLoss: 99.024910\n",
      "Train Epoche: 1 [677/96 (705%)]\tLoss: 80.214828\n",
      "Train Epoche: 1 [678/96 (706%)]\tLoss: 167.801758\n",
      "Train Epoche: 1 [679/96 (707%)]\tLoss: 0.989800\n",
      "Train Epoche: 1 [680/96 (708%)]\tLoss: 290.672699\n",
      "Train Epoche: 1 [681/96 (709%)]\tLoss: 399.320648\n",
      "Train Epoche: 1 [682/96 (710%)]\tLoss: 402.724976\n",
      "Train Epoche: 1 [683/96 (711%)]\tLoss: 224.826859\n",
      "Train Epoche: 1 [684/96 (712%)]\tLoss: 254.664185\n",
      "Train Epoche: 1 [685/96 (714%)]\tLoss: 35.692032\n",
      "Train Epoche: 1 [686/96 (715%)]\tLoss: 254.798874\n",
      "Train Epoche: 1 [687/96 (716%)]\tLoss: 482.440613\n",
      "Train Epoche: 1 [688/96 (717%)]\tLoss: 143.791153\n",
      "Train Epoche: 1 [689/96 (718%)]\tLoss: 482.513519\n",
      "Train Epoche: 1 [690/96 (719%)]\tLoss: 63.305679\n",
      "Train Epoche: 1 [691/96 (720%)]\tLoss: 48.572224\n",
      "Train Epoche: 1 [692/96 (721%)]\tLoss: 483.180573\n",
      "Train Epoche: 1 [693/96 (722%)]\tLoss: 24.596903\n",
      "Train Epoche: 1 [694/96 (723%)]\tLoss: 80.511536\n",
      "Train Epoche: 1 [695/96 (724%)]\tLoss: 8.776707\n",
      "Train Epoche: 1 [696/96 (725%)]\tLoss: 3.846098\n",
      "Train Epoche: 1 [697/96 (726%)]\tLoss: 120.763618\n",
      "Train Epoche: 1 [698/96 (727%)]\tLoss: 168.499054\n",
      "Train Epoche: 1 [699/96 (728%)]\tLoss: 15.619630\n",
      "Train Epoche: 1 [700/96 (729%)]\tLoss: 0.992777\n",
      "Train Epoche: 1 [701/96 (730%)]\tLoss: 485.944855\n",
      "Train Epoche: 1 [702/96 (731%)]\tLoss: 225.820694\n",
      "Train Epoche: 1 [703/96 (732%)]\tLoss: 98.784744\n",
      "Train Epoche: 1 [704/96 (733%)]\tLoss: 485.861267\n",
      "Train Epoche: 1 [705/96 (734%)]\tLoss: 196.378830\n",
      "Train Epoche: 1 [706/96 (735%)]\tLoss: 480.999298\n",
      "Train Epoche: 1 [707/96 (736%)]\tLoss: 398.692322\n",
      "Train Epoche: 1 [708/96 (738%)]\tLoss: 120.335907\n",
      "Train Epoche: 1 [709/96 (739%)]\tLoss: 99.544891\n",
      "Train Epoche: 1 [710/96 (740%)]\tLoss: 254.978836\n",
      "Train Epoche: 1 [711/96 (741%)]\tLoss: 35.459164\n",
      "Train Epoche: 1 [712/96 (742%)]\tLoss: 224.358704\n",
      "Train Epoche: 1 [713/96 (743%)]\tLoss: 8.705314\n",
      "Train Epoche: 1 [714/96 (744%)]\tLoss: 48.367817\n",
      "Train Epoche: 1 [715/96 (745%)]\tLoss: 80.172417\n",
      "Train Epoche: 1 [716/96 (746%)]\tLoss: 63.319294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [717/96 (747%)]\tLoss: 24.587936\n",
      "Train Epoche: 1 [718/96 (748%)]\tLoss: 15.671391\n",
      "Train Epoche: 1 [719/96 (749%)]\tLoss: 195.218658\n",
      "Train Epoche: 1 [720/96 (750%)]\tLoss: 288.280365\n",
      "Train Epoche: 1 [721/96 (751%)]\tLoss: 1.027662\n",
      "Train Epoche: 1 [722/96 (752%)]\tLoss: 4.079352\n",
      "Train Epoche: 1 [723/96 (753%)]\tLoss: 167.685150\n",
      "Train Epoche: 1 [724/96 (754%)]\tLoss: 145.334915\n",
      "Train Epoche: 1 [725/96 (755%)]\tLoss: 3.879037\n",
      "Train Epoche: 1 [726/96 (756%)]\tLoss: 15.724270\n",
      "Train Epoche: 1 [727/96 (757%)]\tLoss: 143.176498\n",
      "Train Epoche: 1 [728/96 (758%)]\tLoss: 573.069946\n",
      "Train Epoche: 1 [729/96 (759%)]\tLoss: 574.592957\n",
      "Train Epoche: 1 [730/96 (760%)]\tLoss: 168.681458\n",
      "Train Epoche: 1 [731/96 (761%)]\tLoss: 80.927727\n",
      "Train Epoche: 1 [732/96 (762%)]\tLoss: 48.638538\n",
      "Train Epoche: 1 [733/96 (764%)]\tLoss: 24.665798\n",
      "Train Epoche: 1 [734/96 (765%)]\tLoss: 35.479740\n",
      "Train Epoche: 1 [735/96 (766%)]\tLoss: 120.555573\n",
      "Train Epoche: 1 [736/96 (767%)]\tLoss: 98.908195\n",
      "Train Epoche: 1 [737/96 (768%)]\tLoss: 8.837189\n",
      "Train Epoche: 1 [738/96 (769%)]\tLoss: 0.896135\n",
      "Train Epoche: 1 [739/96 (770%)]\tLoss: 223.866348\n",
      "Train Epoche: 1 [740/96 (771%)]\tLoss: 255.142563\n",
      "Train Epoche: 1 [741/96 (772%)]\tLoss: 63.700287\n",
      "Train Epoche: 1 [742/96 (773%)]\tLoss: 573.855713\n",
      "Train Epoche: 1 [743/96 (774%)]\tLoss: 197.987381\n",
      "Train Epoche: 1 [744/96 (775%)]\tLoss: 290.512146\n",
      "Train Epoche: 1 [745/96 (776%)]\tLoss: 324.954254\n",
      "Train Epoche: 1 [746/96 (777%)]\tLoss: 401.652252\n",
      "Train Epoche: 1 [747/96 (778%)]\tLoss: 438.852570\n",
      "Train Epoche: 1 [748/96 (779%)]\tLoss: 361.671021\n",
      "Train Epoche: 1 [749/96 (780%)]\tLoss: 99.581848\n",
      "Train Epoche: 1 [750/96 (781%)]\tLoss: 15.721327\n",
      "Train Epoche: 1 [751/96 (782%)]\tLoss: 24.727224\n",
      "Train Epoche: 1 [752/96 (783%)]\tLoss: 398.170197\n",
      "Train Epoche: 1 [753/96 (784%)]\tLoss: 398.265747\n",
      "Train Epoche: 1 [754/96 (785%)]\tLoss: 63.396603\n",
      "Train Epoche: 1 [755/96 (786%)]\tLoss: 8.761271\n",
      "Train Epoche: 1 [756/96 (788%)]\tLoss: 48.437977\n",
      "Train Epoche: 1 [757/96 (789%)]\tLoss: 80.423256\n",
      "Train Epoche: 1 [758/96 (790%)]\tLoss: 397.860321\n",
      "Train Epoche: 1 [759/96 (791%)]\tLoss: 35.578911\n",
      "Train Epoche: 1 [760/96 (792%)]\tLoss: 119.971390\n",
      "Train Epoche: 1 [761/96 (793%)]\tLoss: 0.935465\n",
      "Train Epoche: 1 [762/96 (794%)]\tLoss: 3.824382\n",
      "Train Epoche: 1 [763/96 (795%)]\tLoss: 169.689178\n",
      "Train Epoche: 1 [764/96 (796%)]\tLoss: 143.104202\n",
      "Train Epoche: 1 [765/96 (797%)]\tLoss: 0.951620\n",
      "Train Epoche: 1 [766/96 (798%)]\tLoss: 254.528732\n",
      "Train Epoche: 1 [767/96 (799%)]\tLoss: 287.670624\n",
      "Train Epoche: 1 [768/96 (800%)]\tLoss: 167.880875\n",
      "Train Epoche: 1 [769/96 (801%)]\tLoss: 195.232666\n",
      "Train Epoche: 1 [770/96 (802%)]\tLoss: 224.132111\n",
      "Train Epoche: 1 [771/96 (803%)]\tLoss: 24.778217\n",
      "Train Epoche: 1 [772/96 (804%)]\tLoss: 99.483566\n",
      "Train Epoche: 1 [773/96 (805%)]\tLoss: 142.866318\n",
      "Train Epoche: 1 [774/96 (806%)]\tLoss: 119.959663\n",
      "Train Epoche: 1 [775/96 (807%)]\tLoss: 48.416161\n",
      "Train Epoche: 1 [776/96 (808%)]\tLoss: 15.669784\n",
      "Train Epoche: 1 [777/96 (809%)]\tLoss: 80.360352\n",
      "Train Epoche: 1 [778/96 (810%)]\tLoss: 8.818252\n",
      "Train Epoche: 1 [779/96 (811%)]\tLoss: 36.223751\n",
      "Train Epoche: 1 [780/96 (812%)]\tLoss: 575.835938\n",
      "Train Epoche: 1 [781/96 (814%)]\tLoss: 575.240906\n",
      "Train Epoche: 1 [782/96 (815%)]\tLoss: 577.968994\n",
      "Train Epoche: 1 [783/96 (816%)]\tLoss: 323.222229\n",
      "Train Epoche: 1 [784/96 (817%)]\tLoss: 361.349884\n",
      "Train Epoche: 1 [785/96 (818%)]\tLoss: 64.827393\n",
      "Train Epoche: 1 [786/96 (819%)]\tLoss: 3.896531\n",
      "Train Epoche: 1 [787/96 (820%)]\tLoss: 441.918671\n",
      "Train Epoche: 1 [788/96 (821%)]\tLoss: 400.873352\n",
      "Train Epoche: 1 [789/96 (822%)]\tLoss: 194.863846\n",
      "Train Epoche: 1 [790/96 (823%)]\tLoss: 35.226765\n",
      "Train Epoche: 1 [791/96 (824%)]\tLoss: 398.372772\n",
      "Train Epoche: 1 [792/96 (825%)]\tLoss: 80.377846\n",
      "Train Epoche: 1 [793/96 (826%)]\tLoss: 168.271210\n",
      "Train Epoche: 1 [794/96 (827%)]\tLoss: 143.342422\n",
      "Train Epoche: 1 [795/96 (828%)]\tLoss: 398.085022\n",
      "Train Epoche: 1 [796/96 (829%)]\tLoss: 15.678485\n",
      "Train Epoche: 1 [797/96 (830%)]\tLoss: 0.918371\n",
      "Train Epoche: 1 [798/96 (831%)]\tLoss: 48.474461\n",
      "Train Epoche: 1 [799/96 (832%)]\tLoss: 99.204643\n",
      "Train Epoche: 1 [800/96 (833%)]\tLoss: 24.618538\n",
      "Train Epoche: 1 [801/96 (834%)]\tLoss: 397.959747\n",
      "Train Epoche: 1 [802/96 (835%)]\tLoss: 398.862122\n",
      "Train Epoche: 1 [803/96 (836%)]\tLoss: 120.374969\n",
      "Train Epoche: 1 [804/96 (838%)]\tLoss: 3.889438\n",
      "Train Epoche: 1 [805/96 (839%)]\tLoss: 8.995000\n",
      "Train Epoche: 1 [806/96 (840%)]\tLoss: 63.467155\n",
      "Train Epoche: 1 [807/96 (841%)]\tLoss: 398.101318\n",
      "Train Epoche: 1 [808/96 (842%)]\tLoss: 24.504093\n",
      "Train Epoche: 1 [809/96 (843%)]\tLoss: 120.351639\n",
      "Train Epoche: 1 [810/96 (844%)]\tLoss: 3.874882\n",
      "Train Epoche: 1 [811/96 (845%)]\tLoss: 8.841549\n",
      "Train Epoche: 1 [812/96 (846%)]\tLoss: 142.684998\n",
      "Train Epoche: 1 [813/96 (847%)]\tLoss: 482.376343\n",
      "Train Epoche: 1 [814/96 (848%)]\tLoss: 80.266785\n",
      "Train Epoche: 1 [815/96 (849%)]\tLoss: 99.247597\n",
      "Train Epoche: 1 [816/96 (850%)]\tLoss: 35.652081\n",
      "Train Epoche: 1 [817/96 (851%)]\tLoss: 48.599731\n",
      "Train Epoche: 1 [818/96 (852%)]\tLoss: 63.640079\n",
      "Train Epoche: 1 [819/96 (853%)]\tLoss: 15.863699\n",
      "Train Epoche: 1 [820/96 (854%)]\tLoss: 254.528610\n",
      "Train Epoche: 1 [821/96 (855%)]\tLoss: 223.922836\n",
      "Train Epoche: 1 [822/96 (856%)]\tLoss: 0.978381\n",
      "Train Epoche: 1 [823/96 (857%)]\tLoss: 194.596481\n",
      "Train Epoche: 1 [824/96 (858%)]\tLoss: 485.674377\n",
      "Train Epoche: 1 [825/96 (859%)]\tLoss: 291.313263\n",
      "Train Epoche: 1 [826/96 (860%)]\tLoss: 170.115128\n",
      "Train Epoche: 1 [827/96 (861%)]\tLoss: 486.936890\n",
      "Train Epoche: 1 [828/96 (862%)]\tLoss: 120.848984\n",
      "Train Epoche: 1 [829/96 (864%)]\tLoss: 99.144104\n",
      "Train Epoche: 1 [830/96 (865%)]\tLoss: 15.711098\n",
      "Train Epoche: 1 [831/96 (866%)]\tLoss: 397.343292\n",
      "Train Epoche: 1 [832/96 (867%)]\tLoss: 398.593536\n",
      "Train Epoche: 1 [833/96 (868%)]\tLoss: 3.820382\n",
      "Train Epoche: 1 [834/96 (869%)]\tLoss: 24.760185\n",
      "Train Epoche: 1 [835/96 (870%)]\tLoss: 168.268082\n",
      "Train Epoche: 1 [836/96 (871%)]\tLoss: 63.271198\n",
      "Train Epoche: 1 [837/96 (872%)]\tLoss: 35.535301\n",
      "Train Epoche: 1 [838/96 (873%)]\tLoss: 80.218925\n",
      "Train Epoche: 1 [839/96 (874%)]\tLoss: 194.841049\n",
      "Train Epoche: 1 [840/96 (875%)]\tLoss: 142.742874\n",
      "Train Epoche: 1 [841/96 (876%)]\tLoss: 0.927288\n",
      "Train Epoche: 1 [842/96 (877%)]\tLoss: 8.982316\n",
      "Train Epoche: 1 [843/96 (878%)]\tLoss: 49.077938\n",
      "Train Epoche: 1 [844/96 (879%)]\tLoss: 225.642471\n",
      "Train Epoche: 1 [845/96 (880%)]\tLoss: 258.220551\n",
      "Train Epoche: 1 [846/96 (881%)]\tLoss: 290.017609\n",
      "Train Epoche: 1 [847/96 (882%)]\tLoss: 80.376167\n",
      "Train Epoche: 1 [848/96 (883%)]\tLoss: 120.415428\n",
      "Train Epoche: 1 [849/96 (884%)]\tLoss: 482.542999\n",
      "Train Epoche: 1 [850/96 (885%)]\tLoss: 195.627487\n",
      "Train Epoche: 1 [851/96 (886%)]\tLoss: 482.172607\n",
      "Train Epoche: 1 [852/96 (888%)]\tLoss: 143.059753\n",
      "Train Epoche: 1 [853/96 (889%)]\tLoss: 3.823313\n",
      "Train Epoche: 1 [854/96 (890%)]\tLoss: 15.754998\n",
      "Train Epoche: 1 [855/96 (891%)]\tLoss: 48.609638\n",
      "Train Epoche: 1 [856/96 (892%)]\tLoss: 63.513935\n",
      "Train Epoche: 1 [857/96 (893%)]\tLoss: 35.377785\n",
      "Train Epoche: 1 [858/96 (894%)]\tLoss: 8.785741\n",
      "Train Epoche: 1 [859/96 (895%)]\tLoss: 167.555237\n",
      "Train Epoche: 1 [860/96 (896%)]\tLoss: 24.868095\n",
      "Train Epoche: 1 [861/96 (897%)]\tLoss: 483.707489\n",
      "Train Epoche: 1 [862/96 (898%)]\tLoss: 258.129669\n",
      "Train Epoche: 1 [863/96 (899%)]\tLoss: 322.561646\n",
      "Train Epoche: 1 [864/96 (900%)]\tLoss: 1.056667\n",
      "Train Epoche: 1 [865/96 (901%)]\tLoss: 101.207443\n",
      "Train Epoche: 1 [866/96 (902%)]\tLoss: 287.551331\n",
      "Train Epoche: 1 [867/96 (903%)]\tLoss: 227.384079\n",
      "Train Epoche: 1 [868/96 (904%)]\tLoss: 48.408386\n",
      "Train Epoche: 1 [869/96 (905%)]\tLoss: 3.949053\n",
      "Train Epoche: 1 [870/96 (906%)]\tLoss: 224.678329\n",
      "Train Epoche: 1 [871/96 (907%)]\tLoss: 575.470825\n",
      "Train Epoche: 1 [872/96 (908%)]\tLoss: 120.743942\n",
      "Train Epoche: 1 [873/96 (909%)]\tLoss: 143.239136\n",
      "Train Epoche: 1 [874/96 (910%)]\tLoss: 574.514343\n",
      "Train Epoche: 1 [875/96 (911%)]\tLoss: 63.486568\n",
      "Train Epoche: 1 [876/96 (912%)]\tLoss: 8.912988\n",
      "Train Epoche: 1 [877/96 (914%)]\tLoss: 574.938599\n",
      "Train Epoche: 1 [878/96 (915%)]\tLoss: 80.535912\n",
      "Train Epoche: 1 [879/96 (916%)]\tLoss: 169.115921\n",
      "Train Epoche: 1 [880/96 (917%)]\tLoss: 15.655774\n",
      "Train Epoche: 1 [881/96 (918%)]\tLoss: 0.933440\n",
      "Train Epoche: 1 [882/96 (919%)]\tLoss: 99.223694\n",
      "Train Epoche: 1 [883/96 (920%)]\tLoss: 35.067345\n",
      "Train Epoche: 1 [884/96 (921%)]\tLoss: 24.932825\n",
      "Train Epoche: 1 [885/96 (922%)]\tLoss: 196.264389\n",
      "Train Epoche: 1 [886/96 (923%)]\tLoss: 364.179901\n",
      "Train Epoche: 1 [887/96 (924%)]\tLoss: 573.304382\n",
      "Train Epoche: 1 [888/96 (925%)]\tLoss: 256.173309\n",
      "Train Epoche: 1 [889/96 (926%)]\tLoss: 291.867554\n",
      "Train Epoche: 1 [890/96 (927%)]\tLoss: 324.540955\n",
      "Train Epoche: 1 [891/96 (928%)]\tLoss: 398.367615\n",
      "Train Epoche: 1 [892/96 (929%)]\tLoss: 168.323532\n",
      "Train Epoche: 1 [893/96 (930%)]\tLoss: 35.543282\n",
      "Train Epoche: 1 [894/96 (931%)]\tLoss: 398.962463\n",
      "Train Epoche: 1 [895/96 (932%)]\tLoss: 120.117653\n",
      "Train Epoche: 1 [896/96 (933%)]\tLoss: 398.079254\n",
      "Train Epoche: 1 [897/96 (934%)]\tLoss: 80.265877\n",
      "Train Epoche: 1 [898/96 (935%)]\tLoss: 63.348057\n",
      "Train Epoche: 1 [899/96 (936%)]\tLoss: 15.708533\n",
      "Train Epoche: 1 [900/96 (938%)]\tLoss: 0.898378\n",
      "Train Epoche: 1 [901/96 (939%)]\tLoss: 48.511326\n",
      "Train Epoche: 1 [902/96 (940%)]\tLoss: 99.306496\n",
      "Train Epoche: 1 [903/96 (941%)]\tLoss: 398.975952\n",
      "Train Epoche: 1 [904/96 (942%)]\tLoss: 24.509245\n",
      "Train Epoche: 1 [905/96 (943%)]\tLoss: 398.286743\n",
      "Train Epoche: 1 [906/96 (944%)]\tLoss: 143.531174\n",
      "Train Epoche: 1 [907/96 (945%)]\tLoss: 3.899615\n",
      "Train Epoche: 1 [908/96 (946%)]\tLoss: 9.156016\n",
      "Train Epoche: 1 [909/96 (947%)]\tLoss: 400.246765\n",
      "Train Epoche: 1 [910/96 (948%)]\tLoss: 397.836487\n",
      "Train Epoche: 1 [911/96 (949%)]\tLoss: 24.560673\n",
      "Train Epoche: 1 [912/96 (950%)]\tLoss: 120.256935\n",
      "Train Epoche: 1 [913/96 (951%)]\tLoss: 195.126556\n",
      "Train Epoche: 1 [914/96 (952%)]\tLoss: 167.547516\n",
      "Train Epoche: 1 [915/96 (953%)]\tLoss: 48.661945\n",
      "Train Epoche: 1 [916/96 (954%)]\tLoss: 143.039276\n",
      "Train Epoche: 1 [917/96 (955%)]\tLoss: 0.915147\n",
      "Train Epoche: 1 [918/96 (956%)]\tLoss: 35.446903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [919/96 (957%)]\tLoss: 481.934174\n",
      "Train Epoche: 1 [920/96 (958%)]\tLoss: 63.452301\n",
      "Train Epoche: 1 [921/96 (959%)]\tLoss: 482.356323\n",
      "Train Epoche: 1 [922/96 (960%)]\tLoss: 15.704094\n",
      "Train Epoche: 1 [923/96 (961%)]\tLoss: 99.364624\n",
      "Train Epoche: 1 [924/96 (962%)]\tLoss: 482.006622\n",
      "Train Epoche: 1 [925/96 (964%)]\tLoss: 8.878308\n",
      "Train Epoche: 1 [926/96 (965%)]\tLoss: 485.269928\n",
      "Train Epoche: 1 [927/96 (966%)]\tLoss: 256.677704\n",
      "Train Epoche: 1 [928/96 (967%)]\tLoss: 322.318329\n",
      "Train Epoche: 1 [929/96 (968%)]\tLoss: 4.189995\n",
      "Train Epoche: 1 [930/96 (969%)]\tLoss: 79.501762\n",
      "Train Epoche: 1 [931/96 (970%)]\tLoss: 290.584961\n",
      "Train Epoche: 1 [932/96 (971%)]\tLoss: 228.751724\n",
      "Train Epoche: 1 [933/96 (972%)]\tLoss: 99.583694\n",
      "Train Epoche: 1 [934/96 (973%)]\tLoss: 143.018600\n",
      "Train Epoche: 1 [935/96 (974%)]\tLoss: 120.313873\n",
      "Train Epoche: 1 [936/96 (975%)]\tLoss: 35.741154\n",
      "Train Epoche: 1 [937/96 (976%)]\tLoss: 398.352905\n",
      "Train Epoche: 1 [938/96 (977%)]\tLoss: 398.897156\n",
      "Train Epoche: 1 [939/96 (978%)]\tLoss: 167.833359\n",
      "Train Epoche: 1 [940/96 (979%)]\tLoss: 398.967499\n",
      "Train Epoche: 1 [941/96 (980%)]\tLoss: 8.785498\n",
      "Train Epoche: 1 [942/96 (981%)]\tLoss: 15.723434\n",
      "Train Epoche: 1 [943/96 (982%)]\tLoss: 48.501644\n",
      "Train Epoche: 1 [944/96 (983%)]\tLoss: 24.866920\n",
      "Train Epoche: 1 [945/96 (984%)]\tLoss: 398.809631\n",
      "Train Epoche: 1 [946/96 (985%)]\tLoss: 0.948288\n",
      "Train Epoche: 1 [947/96 (986%)]\tLoss: 398.276550\n",
      "Train Epoche: 1 [948/96 (988%)]\tLoss: 195.446869\n",
      "Train Epoche: 1 [949/96 (989%)]\tLoss: 80.827141\n",
      "Train Epoche: 1 [950/96 (990%)]\tLoss: 3.933589\n",
      "Train Epoche: 1 [951/96 (991%)]\tLoss: 225.678772\n",
      "Train Epoche: 1 [952/96 (992%)]\tLoss: 63.532017\n",
      "Train Epoche: 1 [953/96 (993%)]\tLoss: 398.693909\n",
      "Train Epoche: 1 [954/96 (994%)]\tLoss: 255.281052\n",
      "Train Epoche: 1 [955/96 (995%)]\tLoss: 35.653503\n",
      "Train Epoche: 1 [956/96 (996%)]\tLoss: 15.729245\n",
      "Train Epoche: 1 [957/96 (997%)]\tLoss: 120.134209\n",
      "Train Epoche: 1 [958/96 (998%)]\tLoss: 80.566689\n",
      "Train Epoche: 1 [959/96 (999%)]\tLoss: 24.702614\n",
      "Train Epoche: 1 [960/96 (1000%)]\tLoss: 8.808512\n",
      "Train Epoche: 1 [961/96 (1001%)]\tLoss: 223.694305\n",
      "Train Epoche: 1 [962/96 (1002%)]\tLoss: 168.200058\n",
      "Train Epoche: 1 [963/96 (1003%)]\tLoss: 48.418060\n",
      "Train Epoche: 1 [964/96 (1004%)]\tLoss: 99.047859\n",
      "Train Epoche: 1 [965/96 (1005%)]\tLoss: 195.400986\n",
      "Train Epoche: 1 [966/96 (1006%)]\tLoss: 143.229111\n",
      "Train Epoche: 1 [967/96 (1007%)]\tLoss: 3.985683\n",
      "Train Epoche: 1 [968/96 (1008%)]\tLoss: 0.961832\n",
      "Train Epoche: 1 [969/96 (1009%)]\tLoss: 64.398918\n",
      "Train Epoche: 1 [970/96 (1010%)]\tLoss: 403.474854\n",
      "Train Epoche: 1 [971/96 (1011%)]\tLoss: 288.762695\n",
      "Train Epoche: 1 [972/96 (1012%)]\tLoss: 325.549896\n",
      "Train Epoche: 1 [973/96 (1014%)]\tLoss: 120.122147\n",
      "Train Epoche: 1 [974/96 (1015%)]\tLoss: 481.866180\n",
      "Train Epoche: 1 [975/96 (1016%)]\tLoss: 482.486877\n",
      "Train Epoche: 1 [976/96 (1017%)]\tLoss: 8.770911\n",
      "Train Epoche: 1 [977/96 (1018%)]\tLoss: 254.815643\n",
      "Train Epoche: 1 [978/96 (1019%)]\tLoss: 482.197571\n",
      "Train Epoche: 1 [979/96 (1020%)]\tLoss: 142.742813\n",
      "Train Epoche: 1 [980/96 (1021%)]\tLoss: 80.519699\n",
      "Train Epoche: 1 [981/96 (1022%)]\tLoss: 35.446072\n",
      "Train Epoche: 1 [982/96 (1023%)]\tLoss: 3.747946\n",
      "Train Epoche: 1 [983/96 (1024%)]\tLoss: 63.544994\n",
      "Train Epoche: 1 [984/96 (1025%)]\tLoss: 99.098434\n",
      "Train Epoche: 1 [985/96 (1026%)]\tLoss: 48.247150\n",
      "Train Epoche: 1 [986/96 (1027%)]\tLoss: 15.799767\n",
      "Train Epoche: 1 [987/96 (1028%)]\tLoss: 223.945755\n",
      "Train Epoche: 1 [988/96 (1029%)]\tLoss: 322.636932\n",
      "Train Epoche: 1 [989/96 (1030%)]\tLoss: 0.969621\n",
      "Train Epoche: 1 [990/96 (1031%)]\tLoss: 24.831532\n",
      "Train Epoche: 1 [991/96 (1032%)]\tLoss: 292.637390\n",
      "Train Epoche: 1 [992/96 (1033%)]\tLoss: 362.101349\n",
      "Train Epoche: 1 [993/96 (1034%)]\tLoss: 195.694397\n",
      "Train Epoche: 1 [994/96 (1035%)]\tLoss: 171.143890\n",
      "Train Epoche: 1 [995/96 (1036%)]\tLoss: 3.885006\n",
      "Train Epoche: 1 [996/96 (1038%)]\tLoss: 8.803312\n",
      "Train Epoche: 1 [997/96 (1039%)]\tLoss: 287.969666\n",
      "Train Epoche: 1 [998/96 (1040%)]\tLoss: 223.830963\n",
      "Train Epoche: 1 [999/96 (1041%)]\tLoss: 63.201366\n",
      "Train Epoche: 1 [1000/96 (1042%)]\tLoss: 120.235497\n",
      "Train Epoche: 1 [1001/96 (1043%)]\tLoss: 194.629425\n",
      "Train Epoche: 1 [1002/96 (1044%)]\tLoss: 254.769577\n",
      "Train Epoche: 1 [1003/96 (1045%)]\tLoss: 24.682501\n",
      "Train Epoche: 1 [1004/96 (1046%)]\tLoss: 574.438049\n",
      "Train Epoche: 1 [1005/96 (1047%)]\tLoss: 168.502197\n",
      "Train Epoche: 1 [1006/96 (1048%)]\tLoss: 142.949005\n",
      "Train Epoche: 1 [1007/96 (1049%)]\tLoss: 15.727702\n",
      "Train Epoche: 1 [1008/96 (1050%)]\tLoss: 0.924145\n",
      "Train Epoche: 1 [1009/96 (1051%)]\tLoss: 99.291191\n",
      "Train Epoche: 1 [1010/96 (1052%)]\tLoss: 80.587067\n",
      "Train Epoche: 1 [1011/96 (1053%)]\tLoss: 48.659439\n",
      "Train Epoche: 1 [1012/96 (1054%)]\tLoss: 36.861137\n",
      "Train Epoche: 1 [1013/96 (1055%)]\tLoss: 579.269287\n",
      "Train Epoche: 1 [1014/96 (1056%)]\tLoss: 323.659729\n",
      "Train Epoche: 1 [1015/96 (1057%)]\tLoss: 362.695465\n",
      "Train Epoche: 1 [1016/96 (1058%)]\tLoss: 400.553711\n",
      "Train Epoche: 1 [1017/96 (1059%)]\tLoss: 574.295105\n",
      "Train Epoche: 1 [1018/96 (1060%)]\tLoss: 439.748962\n",
      "Train Epoche: 1 [1019/96 (1061%)]\tLoss: 48.773510\n",
      "Train Epoche: 1 [1020/96 (1062%)]\tLoss: 482.472534\n",
      "Train Epoche: 1 [1021/96 (1064%)]\tLoss: 143.655197\n",
      "Train Epoche: 1 [1022/96 (1065%)]\tLoss: 482.149506\n",
      "Train Epoche: 1 [1023/96 (1066%)]\tLoss: 99.377419\n",
      "Train Epoche: 1 [1024/96 (1067%)]\tLoss: 223.598633\n",
      "Train Epoche: 1 [1025/96 (1068%)]\tLoss: 80.061462\n",
      "Train Epoche: 1 [1026/96 (1069%)]\tLoss: 194.813553\n",
      "Train Epoche: 1 [1027/96 (1070%)]\tLoss: 15.691070\n",
      "Train Epoche: 1 [1028/96 (1071%)]\tLoss: 24.537519\n",
      "Train Epoche: 1 [1029/96 (1072%)]\tLoss: 63.455433\n",
      "Train Epoche: 1 [1030/96 (1073%)]\tLoss: 3.927080\n",
      "Train Epoche: 1 [1031/96 (1074%)]\tLoss: 35.721851\n",
      "Train Epoche: 1 [1032/96 (1075%)]\tLoss: 287.406952\n",
      "Train Epoche: 1 [1033/96 (1076%)]\tLoss: 167.876724\n",
      "Train Epoche: 1 [1034/96 (1077%)]\tLoss: 8.830441\n",
      "Train Epoche: 1 [1035/96 (1078%)]\tLoss: 0.945340\n",
      "Train Epoche: 1 [1036/96 (1079%)]\tLoss: 255.025497\n",
      "Train Epoche: 1 [1037/96 (1080%)]\tLoss: 324.158020\n",
      "Train Epoche: 1 [1038/96 (1081%)]\tLoss: 120.603218\n",
      "Train Epoche: 1 [1039/96 (1082%)]\tLoss: 398.430664\n",
      "Train Epoche: 1 [1040/96 (1083%)]\tLoss: 398.267426\n",
      "Train Epoche: 1 [1041/96 (1084%)]\tLoss: 35.606579\n",
      "Train Epoche: 1 [1042/96 (1085%)]\tLoss: 24.529716\n",
      "Train Epoche: 1 [1043/96 (1086%)]\tLoss: 48.412373\n",
      "Train Epoche: 1 [1044/96 (1088%)]\tLoss: 63.261009\n",
      "Train Epoche: 1 [1045/96 (1089%)]\tLoss: 15.797063\n",
      "Train Epoche: 1 [1046/96 (1090%)]\tLoss: 0.941792\n",
      "Train Epoche: 1 [1047/96 (1091%)]\tLoss: 194.995453\n",
      "Train Epoche: 1 [1048/96 (1092%)]\tLoss: 168.178925\n",
      "Train Epoche: 1 [1049/96 (1093%)]\tLoss: 99.171188\n",
      "Train Epoche: 1 [1050/96 (1094%)]\tLoss: 80.456367\n",
      "Train Epoche: 1 [1051/96 (1095%)]\tLoss: 397.785583\n",
      "Train Epoche: 1 [1052/96 (1096%)]\tLoss: 143.410782\n",
      "Train Epoche: 1 [1053/96 (1097%)]\tLoss: 3.745673\n",
      "Train Epoche: 1 [1054/96 (1098%)]\tLoss: 8.979798\n",
      "Train Epoche: 1 [1055/96 (1099%)]\tLoss: 121.113152\n",
      "Train Epoche: 1 [1056/96 (1100%)]\tLoss: 399.886719\n",
      "Train Epoche: 1 [1057/96 (1101%)]\tLoss: 225.005585\n",
      "Train Epoche: 1 [1058/96 (1102%)]\tLoss: 398.102905\n",
      "Train Epoche: 1 [1059/96 (1103%)]\tLoss: 398.676544\n",
      "Train Epoche: 1 [1060/96 (1104%)]\tLoss: 35.698383\n",
      "Train Epoche: 1 [1061/96 (1105%)]\tLoss: 8.761330\n",
      "Train Epoche: 1 [1062/96 (1106%)]\tLoss: 223.819611\n",
      "Train Epoche: 1 [1063/96 (1107%)]\tLoss: 142.832108\n",
      "Train Epoche: 1 [1064/96 (1108%)]\tLoss: 15.693497\n",
      "Train Epoche: 1 [1065/96 (1109%)]\tLoss: 24.824091\n",
      "Train Epoche: 1 [1066/96 (1110%)]\tLoss: 63.431423\n",
      "Train Epoche: 1 [1067/96 (1111%)]\tLoss: 120.467484\n",
      "Train Epoche: 1 [1068/96 (1112%)]\tLoss: 167.885620\n",
      "Train Epoche: 1 [1069/96 (1114%)]\tLoss: 80.696976\n",
      "Train Epoche: 1 [1070/96 (1115%)]\tLoss: 194.951340\n",
      "Train Epoche: 1 [1071/96 (1116%)]\tLoss: 255.007523\n",
      "Train Epoche: 1 [1072/96 (1117%)]\tLoss: 0.976120\n",
      "Train Epoche: 1 [1073/96 (1118%)]\tLoss: 3.797485\n",
      "Train Epoche: 1 [1074/96 (1119%)]\tLoss: 100.847145\n",
      "Train Epoche: 1 [1075/96 (1120%)]\tLoss: 48.642975\n",
      "Train Epoche: 1 [1076/96 (1121%)]\tLoss: 290.148773\n",
      "Train Epoche: 1 [1077/96 (1122%)]\tLoss: 401.378052\n",
      "Train Epoche: 1 [1078/96 (1123%)]\tLoss: 120.361473\n",
      "Train Epoche: 1 [1079/96 (1124%)]\tLoss: 255.286087\n",
      "Train Epoche: 1 [1080/96 (1125%)]\tLoss: 286.879883\n",
      "Train Epoche: 1 [1081/96 (1126%)]\tLoss: 24.489882\n",
      "Train Epoche: 1 [1082/96 (1127%)]\tLoss: 80.662094\n",
      "Train Epoche: 1 [1083/96 (1128%)]\tLoss: 99.162506\n",
      "Train Epoche: 1 [1084/96 (1129%)]\tLoss: 15.656490\n",
      "Train Epoche: 1 [1085/96 (1130%)]\tLoss: 8.784277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1086/96 (1131%)]\tLoss: 35.462700\n",
      "Train Epoche: 1 [1087/96 (1132%)]\tLoss: 142.985779\n",
      "Train Epoche: 1 [1088/96 (1133%)]\tLoss: 223.379776\n",
      "Train Epoche: 1 [1089/96 (1134%)]\tLoss: 168.489716\n",
      "Train Epoche: 1 [1090/96 (1135%)]\tLoss: 194.783340\n",
      "Train Epoche: 1 [1091/96 (1136%)]\tLoss: 399.583405\n",
      "Train Epoche: 1 [1092/96 (1138%)]\tLoss: 0.932129\n",
      "Train Epoche: 1 [1093/96 (1139%)]\tLoss: 4.048241\n",
      "Train Epoche: 1 [1094/96 (1140%)]\tLoss: 49.859051\n",
      "Train Epoche: 1 [1095/96 (1141%)]\tLoss: 63.094990\n",
      "Train Epoche: 1 [1096/96 (1142%)]\tLoss: 362.033417\n",
      "Train Epoche: 1 [1097/96 (1143%)]\tLoss: 324.374939\n",
      "Train Epoche: 1 [1098/96 (1144%)]\tLoss: 573.972351\n",
      "Train Epoche: 1 [1099/96 (1145%)]\tLoss: 3.858724\n",
      "Train Epoche: 1 [1100/96 (1146%)]\tLoss: 323.532562\n",
      "Train Epoche: 1 [1101/96 (1147%)]\tLoss: 573.533447\n",
      "Train Epoche: 1 [1102/96 (1148%)]\tLoss: 80.605949\n",
      "Train Epoche: 1 [1103/96 (1149%)]\tLoss: 573.928589\n",
      "Train Epoche: 1 [1104/96 (1150%)]\tLoss: 8.648109\n",
      "Train Epoche: 1 [1105/96 (1151%)]\tLoss: 63.170506\n",
      "Train Epoche: 1 [1106/96 (1152%)]\tLoss: 195.072479\n",
      "Train Epoche: 1 [1107/96 (1153%)]\tLoss: 15.843309\n",
      "Train Epoche: 1 [1108/96 (1154%)]\tLoss: 120.068283\n",
      "Train Epoche: 1 [1109/96 (1155%)]\tLoss: 0.954260\n",
      "Train Epoche: 1 [1110/96 (1156%)]\tLoss: 167.699585\n",
      "Train Epoche: 1 [1111/96 (1157%)]\tLoss: 99.142662\n",
      "Train Epoche: 1 [1112/96 (1158%)]\tLoss: 25.200319\n",
      "Train Epoche: 1 [1113/96 (1159%)]\tLoss: 575.023010\n",
      "Train Epoche: 1 [1114/96 (1160%)]\tLoss: 288.709625\n",
      "Train Epoche: 1 [1115/96 (1161%)]\tLoss: 576.364990\n",
      "Train Epoche: 1 [1116/96 (1162%)]\tLoss: 225.852310\n",
      "Train Epoche: 1 [1117/96 (1164%)]\tLoss: 362.733612\n",
      "Train Epoche: 1 [1118/96 (1165%)]\tLoss: 36.395218\n",
      "Train Epoche: 1 [1119/96 (1166%)]\tLoss: 50.069008\n",
      "Train Epoche: 1 [1120/96 (1167%)]\tLoss: 142.732559\n",
      "Train Epoche: 1 [1121/96 (1168%)]\tLoss: 256.265564\n",
      "Train Epoche: 1 [1122/96 (1169%)]\tLoss: 399.480530\n",
      "Train Epoche: 1 [1123/96 (1170%)]\tLoss: 398.654480\n",
      "Train Epoche: 1 [1124/96 (1171%)]\tLoss: 398.095367\n",
      "Train Epoche: 1 [1125/96 (1172%)]\tLoss: 24.510138\n",
      "Train Epoche: 1 [1126/96 (1173%)]\tLoss: 63.290955\n",
      "Train Epoche: 1 [1127/96 (1174%)]\tLoss: 80.429306\n",
      "Train Epoche: 1 [1128/96 (1175%)]\tLoss: 8.781284\n",
      "Train Epoche: 1 [1129/96 (1176%)]\tLoss: 0.958128\n",
      "Train Epoche: 1 [1130/96 (1177%)]\tLoss: 398.591797\n",
      "Train Epoche: 1 [1131/96 (1178%)]\tLoss: 48.459366\n",
      "Train Epoche: 1 [1132/96 (1179%)]\tLoss: 3.842698\n",
      "Train Epoche: 1 [1133/96 (1180%)]\tLoss: 35.852482\n",
      "Train Epoche: 1 [1134/96 (1181%)]\tLoss: 120.226944\n",
      "Train Epoche: 1 [1135/96 (1182%)]\tLoss: 99.717796\n",
      "Train Epoche: 1 [1136/96 (1183%)]\tLoss: 398.959503\n",
      "Train Epoche: 1 [1137/96 (1184%)]\tLoss: 15.960876\n",
      "Train Epoche: 1 [1138/96 (1185%)]\tLoss: 169.623260\n",
      "Train Epoche: 1 [1139/96 (1186%)]\tLoss: 144.894272\n",
      "Train Epoche: 1 [1140/96 (1188%)]\tLoss: 224.393417\n",
      "Train Epoche: 1 [1141/96 (1189%)]\tLoss: 196.189468\n",
      "Train Epoche: 1 [1142/96 (1190%)]\tLoss: 35.572216\n",
      "Train Epoche: 1 [1143/96 (1191%)]\tLoss: 99.188301\n",
      "Train Epoche: 1 [1144/96 (1192%)]\tLoss: 398.574951\n",
      "Train Epoche: 1 [1145/96 (1193%)]\tLoss: 195.678955\n",
      "Train Epoche: 1 [1146/96 (1194%)]\tLoss: 287.420807\n",
      "Train Epoche: 1 [1147/96 (1195%)]\tLoss: 143.542984\n",
      "Train Epoche: 1 [1148/96 (1196%)]\tLoss: 120.139099\n",
      "Train Epoche: 1 [1149/96 (1197%)]\tLoss: 48.437630\n",
      "Train Epoche: 1 [1150/96 (1198%)]\tLoss: 3.862961\n",
      "Train Epoche: 1 [1151/96 (1199%)]\tLoss: 0.920222\n",
      "Train Epoche: 1 [1152/96 (1200%)]\tLoss: 63.617500\n",
      "Train Epoche: 1 [1153/96 (1201%)]\tLoss: 80.305290\n",
      "Train Epoche: 1 [1154/96 (1202%)]\tLoss: 24.459332\n",
      "Train Epoche: 1 [1155/96 (1203%)]\tLoss: 254.944565\n",
      "Train Epoche: 1 [1156/96 (1204%)]\tLoss: 223.627411\n",
      "Train Epoche: 1 [1157/96 (1205%)]\tLoss: 16.154400\n",
      "Train Epoche: 1 [1158/96 (1206%)]\tLoss: 9.226764\n",
      "Train Epoche: 1 [1159/96 (1207%)]\tLoss: 401.430176\n",
      "Train Epoche: 1 [1160/96 (1208%)]\tLoss: 168.093597\n",
      "Train Epoche: 1 [1161/96 (1209%)]\tLoss: 255.026627\n",
      "Train Epoche: 1 [1162/96 (1210%)]\tLoss: 195.443527\n",
      "Train Epoche: 1 [1163/96 (1211%)]\tLoss: 80.111580\n",
      "Train Epoche: 1 [1164/96 (1212%)]\tLoss: 63.413937\n",
      "Train Epoche: 1 [1165/96 (1214%)]\tLoss: 120.501068\n",
      "Train Epoche: 1 [1166/96 (1215%)]\tLoss: 398.604431\n",
      "Train Epoche: 1 [1167/96 (1216%)]\tLoss: 48.464920\n",
      "Train Epoche: 1 [1168/96 (1217%)]\tLoss: 15.664143\n",
      "Train Epoche: 1 [1169/96 (1218%)]\tLoss: 24.463888\n",
      "Train Epoche: 1 [1170/96 (1219%)]\tLoss: 35.470158\n",
      "Train Epoche: 1 [1171/96 (1220%)]\tLoss: 8.751721\n",
      "Train Epoche: 1 [1172/96 (1221%)]\tLoss: 398.197998\n",
      "Train Epoche: 1 [1173/96 (1222%)]\tLoss: 167.947510\n",
      "Train Epoche: 1 [1174/96 (1223%)]\tLoss: 224.305847\n",
      "Train Epoche: 1 [1175/96 (1224%)]\tLoss: 1.021456\n",
      "Train Epoche: 1 [1176/96 (1225%)]\tLoss: 4.047862\n",
      "Train Epoche: 1 [1177/96 (1226%)]\tLoss: 100.180420\n",
      "Train Epoche: 1 [1178/96 (1227%)]\tLoss: 143.294037\n",
      "Train Epoche: 1 [1179/96 (1228%)]\tLoss: 0.905096\n",
      "Train Epoche: 1 [1180/96 (1229%)]\tLoss: 35.500271\n",
      "Train Epoche: 1 [1181/96 (1230%)]\tLoss: 48.944847\n",
      "Train Epoche: 1 [1182/96 (1231%)]\tLoss: 167.830536\n",
      "Train Epoche: 1 [1183/96 (1232%)]\tLoss: 223.771164\n",
      "Train Epoche: 1 [1184/96 (1233%)]\tLoss: 254.937866\n",
      "Train Epoche: 1 [1185/96 (1234%)]\tLoss: 24.642117\n",
      "Train Epoche: 1 [1186/96 (1235%)]\tLoss: 80.664680\n",
      "Train Epoche: 1 [1187/96 (1236%)]\tLoss: 120.183357\n",
      "Train Epoche: 1 [1188/96 (1238%)]\tLoss: 142.677170\n",
      "Train Epoche: 1 [1189/96 (1239%)]\tLoss: 63.447910\n",
      "Train Epoche: 1 [1190/96 (1240%)]\tLoss: 15.671481\n",
      "Train Epoche: 1 [1191/96 (1241%)]\tLoss: 322.434692\n",
      "Train Epoche: 1 [1192/96 (1242%)]\tLoss: 194.887054\n",
      "Train Epoche: 1 [1193/96 (1243%)]\tLoss: 100.770714\n",
      "Train Epoche: 1 [1194/96 (1244%)]\tLoss: 574.255798\n",
      "Train Epoche: 1 [1195/96 (1245%)]\tLoss: 485.281433\n",
      "Train Epoche: 1 [1196/96 (1246%)]\tLoss: 577.689636\n",
      "Train Epoche: 1 [1197/96 (1247%)]\tLoss: 290.282867\n",
      "Train Epoche: 1 [1198/96 (1248%)]\tLoss: 360.777740\n",
      "Train Epoche: 1 [1199/96 (1249%)]\tLoss: 4.365136\n",
      "Train Epoche: 1 [1200/96 (1250%)]\tLoss: 8.873693\n",
      "Train Epoche: 1 [1201/96 (1251%)]\tLoss: 443.167267\n",
      "Train Epoche: 1 [1202/96 (1252%)]\tLoss: 400.945312\n",
      "Train Epoche: 1 [1203/96 (1253%)]\tLoss: 143.271744\n",
      "Train Epoche: 1 [1204/96 (1254%)]\tLoss: 80.351784\n",
      "Train Epoche: 1 [1205/96 (1255%)]\tLoss: 120.446198\n",
      "Train Epoche: 1 [1206/96 (1256%)]\tLoss: 224.153442\n",
      "Train Epoche: 1 [1207/96 (1257%)]\tLoss: 255.637482\n",
      "Train Epoche: 1 [1208/96 (1258%)]\tLoss: 288.529785\n",
      "Train Epoche: 1 [1209/96 (1259%)]\tLoss: 24.733053\n",
      "Train Epoche: 1 [1210/96 (1260%)]\tLoss: 63.563438\n",
      "Train Epoche: 1 [1211/96 (1261%)]\tLoss: 99.952934\n",
      "Train Epoche: 1 [1212/96 (1262%)]\tLoss: 35.588257\n",
      "Train Epoche: 1 [1213/96 (1264%)]\tLoss: 3.797363\n",
      "Train Epoche: 1 [1214/96 (1265%)]\tLoss: 0.920179\n",
      "Train Epoche: 1 [1215/96 (1266%)]\tLoss: 195.019150\n",
      "Train Epoche: 1 [1216/96 (1267%)]\tLoss: 168.247086\n",
      "Train Epoche: 1 [1217/96 (1268%)]\tLoss: 48.348343\n",
      "Train Epoche: 1 [1218/96 (1269%)]\tLoss: 8.704721\n",
      "Train Epoche: 1 [1219/96 (1270%)]\tLoss: 359.224121\n",
      "Train Epoche: 1 [1220/96 (1271%)]\tLoss: 323.602295\n",
      "Train Epoche: 1 [1221/96 (1272%)]\tLoss: 16.314793\n",
      "Train Epoche: 1 [1222/96 (1273%)]\tLoss: 441.015472\n",
      "Train Epoche: 1 [1223/96 (1274%)]\tLoss: 401.995117\n",
      "Train Epoche: 1 [1224/96 (1275%)]\tLoss: 35.723980\n",
      "Train Epoche: 1 [1225/96 (1276%)]\tLoss: 8.694892\n",
      "Train Epoche: 1 [1226/96 (1277%)]\tLoss: 80.619324\n",
      "Train Epoche: 1 [1227/96 (1278%)]\tLoss: 323.078278\n",
      "Train Epoche: 1 [1228/96 (1279%)]\tLoss: 63.550846\n",
      "Train Epoche: 1 [1229/96 (1280%)]\tLoss: 574.217957\n",
      "Train Epoche: 1 [1230/96 (1281%)]\tLoss: 99.312103\n",
      "Train Epoche: 1 [1231/96 (1282%)]\tLoss: 574.786072\n",
      "Train Epoche: 1 [1232/96 (1283%)]\tLoss: 3.797489\n",
      "Train Epoche: 1 [1233/96 (1284%)]\tLoss: 574.103027\n",
      "Train Epoche: 1 [1234/96 (1285%)]\tLoss: 48.845238\n",
      "Train Epoche: 1 [1235/96 (1286%)]\tLoss: 143.593857\n",
      "Train Epoche: 1 [1236/96 (1288%)]\tLoss: 15.651580\n",
      "Train Epoche: 1 [1237/96 (1289%)]\tLoss: 0.949061\n",
      "Train Epoche: 1 [1238/96 (1290%)]\tLoss: 24.683842\n",
      "Train Epoche: 1 [1239/96 (1291%)]\tLoss: 120.983109\n",
      "Train Epoche: 1 [1240/96 (1292%)]\tLoss: 574.823120\n",
      "Train Epoche: 1 [1241/96 (1293%)]\tLoss: 196.723114\n",
      "Train Epoche: 1 [1242/96 (1294%)]\tLoss: 170.103760\n",
      "Train Epoche: 1 [1243/96 (1295%)]\tLoss: 576.367615\n",
      "Train Epoche: 1 [1244/96 (1296%)]\tLoss: 224.529373\n",
      "Train Epoche: 1 [1245/96 (1297%)]\tLoss: 256.513977\n",
      "Train Epoche: 1 [1246/96 (1298%)]\tLoss: 290.408569\n",
      "Train Epoche: 1 [1247/96 (1299%)]\tLoss: 0.924076\n",
      "Train Epoche: 1 [1248/96 (1300%)]\tLoss: 143.104065\n",
      "Train Epoche: 1 [1249/96 (1301%)]\tLoss: 574.103027\n",
      "Train Epoche: 1 [1250/96 (1302%)]\tLoss: 80.588181\n",
      "Train Epoche: 1 [1251/96 (1303%)]\tLoss: 63.790920\n",
      "Train Epoche: 1 [1252/96 (1304%)]\tLoss: 24.754971\n",
      "Train Epoche: 1 [1253/96 (1305%)]\tLoss: 15.699563\n",
      "Train Epoche: 1 [1254/96 (1306%)]\tLoss: 99.580803\n",
      "Train Epoche: 1 [1255/96 (1307%)]\tLoss: 35.465694\n",
      "Train Epoche: 1 [1256/96 (1308%)]\tLoss: 3.875415\n",
      "Train Epoche: 1 [1257/96 (1309%)]\tLoss: 167.880905\n",
      "Train Epoche: 1 [1258/96 (1310%)]\tLoss: 120.879517\n",
      "Train Epoche: 1 [1259/96 (1311%)]\tLoss: 49.449207\n",
      "Train Epoche: 1 [1260/96 (1312%)]\tLoss: 324.309265\n",
      "Train Epoche: 1 [1261/96 (1314%)]\tLoss: 576.180115\n",
      "Train Epoche: 1 [1262/96 (1315%)]\tLoss: 287.678070\n",
      "Train Epoche: 1 [1263/96 (1316%)]\tLoss: 195.971619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1264/96 (1317%)]\tLoss: 9.177383\n",
      "Train Epoche: 1 [1265/96 (1318%)]\tLoss: 226.404892\n",
      "Train Epoche: 1 [1266/96 (1319%)]\tLoss: 257.405731\n",
      "Train Epoche: 1 [1267/96 (1320%)]\tLoss: 48.454517\n",
      "Train Epoche: 1 [1268/96 (1321%)]\tLoss: 80.220703\n",
      "Train Epoche: 1 [1269/96 (1322%)]\tLoss: 98.959854\n",
      "Train Epoche: 1 [1270/96 (1323%)]\tLoss: 482.691254\n",
      "Train Epoche: 1 [1271/96 (1324%)]\tLoss: 168.119644\n",
      "Train Epoche: 1 [1272/96 (1325%)]\tLoss: 143.671738\n",
      "Train Epoche: 1 [1273/96 (1326%)]\tLoss: 24.680948\n",
      "Train Epoche: 1 [1274/96 (1327%)]\tLoss: 63.454369\n",
      "Train Epoche: 1 [1275/96 (1328%)]\tLoss: 481.906952\n",
      "Train Epoche: 1 [1276/96 (1329%)]\tLoss: 322.325806\n",
      "Train Epoche: 1 [1277/96 (1330%)]\tLoss: 15.793553\n",
      "Train Epoche: 1 [1278/96 (1331%)]\tLoss: 8.755818\n",
      "Train Epoche: 1 [1279/96 (1332%)]\tLoss: 120.291512\n",
      "Train Epoche: 1 [1280/96 (1333%)]\tLoss: 482.688324\n",
      "Train Epoche: 1 [1281/96 (1334%)]\tLoss: 0.938763\n",
      "Train Epoche: 1 [1282/96 (1335%)]\tLoss: 360.090302\n",
      "Train Epoche: 1 [1283/96 (1336%)]\tLoss: 226.991425\n",
      "Train Epoche: 1 [1284/96 (1338%)]\tLoss: 195.483246\n",
      "Train Epoche: 1 [1285/96 (1339%)]\tLoss: 4.136127\n",
      "Train Epoche: 1 [1286/96 (1340%)]\tLoss: 35.924206\n",
      "Train Epoche: 1 [1287/96 (1341%)]\tLoss: 290.836670\n",
      "Train Epoche: 1 [1288/96 (1342%)]\tLoss: 253.663895\n",
      "Train Epoche: 1 [1289/96 (1343%)]\tLoss: 323.631653\n",
      "Train Epoche: 1 [1290/96 (1344%)]\tLoss: 35.431892\n",
      "Train Epoche: 1 [1291/96 (1345%)]\tLoss: 397.972473\n",
      "Train Epoche: 1 [1292/96 (1346%)]\tLoss: 80.340607\n",
      "Train Epoche: 1 [1293/96 (1347%)]\tLoss: 195.164612\n",
      "Train Epoche: 1 [1294/96 (1348%)]\tLoss: 143.638428\n",
      "Train Epoche: 1 [1295/96 (1349%)]\tLoss: 483.141174\n",
      "Train Epoche: 1 [1296/96 (1350%)]\tLoss: 63.909065\n",
      "Train Epoche: 1 [1297/96 (1351%)]\tLoss: 8.724360\n",
      "Train Epoche: 1 [1298/96 (1352%)]\tLoss: 482.792603\n",
      "Train Epoche: 1 [1299/96 (1353%)]\tLoss: 361.236694\n",
      "Train Epoche: 1 [1300/96 (1354%)]\tLoss: 288.338715\n",
      "Train Epoche: 1 [1301/96 (1355%)]\tLoss: 24.823858\n",
      "Train Epoche: 1 [1302/96 (1356%)]\tLoss: 3.828323\n",
      "Train Epoche: 1 [1303/96 (1357%)]\tLoss: 223.410172\n",
      "Train Epoche: 1 [1304/96 (1358%)]\tLoss: 168.732681\n",
      "Train Epoche: 1 [1305/96 (1359%)]\tLoss: 0.893375\n",
      "Train Epoche: 1 [1306/96 (1360%)]\tLoss: 15.796276\n",
      "Train Epoche: 1 [1307/96 (1361%)]\tLoss: 102.112213\n",
      "Train Epoche: 1 [1308/96 (1362%)]\tLoss: 257.504822\n",
      "Train Epoche: 1 [1309/96 (1364%)]\tLoss: 49.562862\n",
      "Train Epoche: 1 [1310/96 (1365%)]\tLoss: 124.102219\n",
      "Train Epoche: 1 [1311/96 (1366%)]\tLoss: 143.142563\n",
      "Train Epoche: 1 [1312/96 (1367%)]\tLoss: 399.026611\n",
      "Train Epoche: 1 [1313/96 (1368%)]\tLoss: 168.595825\n",
      "Train Epoche: 1 [1314/96 (1369%)]\tLoss: 255.410034\n",
      "Train Epoche: 1 [1315/96 (1370%)]\tLoss: 35.613079\n",
      "Train Epoche: 1 [1316/96 (1371%)]\tLoss: 223.793274\n",
      "Train Epoche: 1 [1317/96 (1372%)]\tLoss: 80.141624\n",
      "Train Epoche: 1 [1318/96 (1373%)]\tLoss: 48.409607\n",
      "Train Epoche: 1 [1319/96 (1374%)]\tLoss: 3.870806\n",
      "Train Epoche: 1 [1320/96 (1375%)]\tLoss: 15.601419\n",
      "Train Epoche: 1 [1321/96 (1376%)]\tLoss: 24.704340\n",
      "Train Epoche: 1 [1322/96 (1377%)]\tLoss: 8.743223\n",
      "Train Epoche: 1 [1323/96 (1378%)]\tLoss: 397.696503\n",
      "Train Epoche: 1 [1324/96 (1379%)]\tLoss: 119.896751\n",
      "Train Epoche: 1 [1325/96 (1380%)]\tLoss: 63.425533\n",
      "Train Epoche: 1 [1326/96 (1381%)]\tLoss: 0.909806\n",
      "Train Epoche: 1 [1327/96 (1382%)]\tLoss: 398.125214\n",
      "Train Epoche: 1 [1328/96 (1383%)]\tLoss: 100.375626\n",
      "Train Epoche: 1 [1329/96 (1384%)]\tLoss: 195.065201\n",
      "Train Epoche: 1 [1330/96 (1385%)]\tLoss: 398.448456\n",
      "Train Epoche: 1 [1331/96 (1386%)]\tLoss: 48.755672\n",
      "Train Epoche: 1 [1332/96 (1388%)]\tLoss: 120.503242\n",
      "Train Epoche: 1 [1333/96 (1389%)]\tLoss: 63.835548\n",
      "Train Epoche: 1 [1334/96 (1390%)]\tLoss: 399.556335\n",
      "Train Epoche: 1 [1335/96 (1391%)]\tLoss: 35.459206\n",
      "Train Epoche: 1 [1336/96 (1392%)]\tLoss: 398.960785\n",
      "Train Epoche: 1 [1337/96 (1393%)]\tLoss: 15.787492\n",
      "Train Epoche: 1 [1338/96 (1394%)]\tLoss: 24.639740\n",
      "Train Epoche: 1 [1339/96 (1395%)]\tLoss: 99.519508\n",
      "Train Epoche: 1 [1340/96 (1396%)]\tLoss: 3.969626\n",
      "Train Epoche: 1 [1341/96 (1397%)]\tLoss: 398.715851\n",
      "Train Epoche: 1 [1342/96 (1398%)]\tLoss: 143.749435\n",
      "Train Epoche: 1 [1343/96 (1399%)]\tLoss: 0.964846\n",
      "Train Epoche: 1 [1344/96 (1400%)]\tLoss: 8.985853\n",
      "Train Epoche: 1 [1345/96 (1401%)]\tLoss: 81.538055\n",
      "Train Epoche: 1 [1346/96 (1402%)]\tLoss: 401.114349\n",
      "Train Epoche: 1 [1347/96 (1403%)]\tLoss: 120.365349\n",
      "Train Epoche: 1 [1348/96 (1404%)]\tLoss: 143.269791\n",
      "Train Epoche: 1 [1349/96 (1405%)]\tLoss: 167.823944\n",
      "Train Epoche: 1 [1350/96 (1406%)]\tLoss: 24.646990\n",
      "Train Epoche: 1 [1351/96 (1407%)]\tLoss: 482.623291\n",
      "Train Epoche: 1 [1352/96 (1408%)]\tLoss: 195.016113\n",
      "Train Epoche: 1 [1353/96 (1409%)]\tLoss: 35.389004\n",
      "Train Epoche: 1 [1354/96 (1410%)]\tLoss: 48.306873\n",
      "Train Epoche: 1 [1355/96 (1411%)]\tLoss: 99.071930\n",
      "Train Epoche: 1 [1356/96 (1412%)]\tLoss: 80.230667\n",
      "Train Epoche: 1 [1357/96 (1414%)]\tLoss: 15.715258\n",
      "Train Epoche: 1 [1358/96 (1415%)]\tLoss: 8.782171\n",
      "Train Epoche: 1 [1359/96 (1416%)]\tLoss: 288.122162\n",
      "Train Epoche: 1 [1360/96 (1417%)]\tLoss: 254.776062\n",
      "Train Epoche: 1 [1361/96 (1418%)]\tLoss: 0.985508\n",
      "Train Epoche: 1 [1362/96 (1419%)]\tLoss: 4.017049\n",
      "Train Epoche: 1 [1363/96 (1420%)]\tLoss: 487.129181\n",
      "Train Epoche: 1 [1364/96 (1421%)]\tLoss: 400.486816\n",
      "Train Epoche: 1 [1365/96 (1422%)]\tLoss: 63.738983\n",
      "Train Epoche: 1 [1366/96 (1423%)]\tLoss: 227.711899\n",
      "Train Epoche: 1 [1367/96 (1424%)]\tLoss: 360.036743\n",
      "Train Epoche: 1 [1368/96 (1425%)]\tLoss: 324.632507\n",
      "Train Epoche: 1 [1369/96 (1426%)]\tLoss: 24.893255\n",
      "Train Epoche: 1 [1370/96 (1427%)]\tLoss: 80.416168\n",
      "Train Epoche: 1 [1371/96 (1428%)]\tLoss: 142.980865\n",
      "Train Epoche: 1 [1372/96 (1429%)]\tLoss: 167.989487\n",
      "Train Epoche: 1 [1373/96 (1430%)]\tLoss: 15.724123\n",
      "Train Epoche: 1 [1374/96 (1431%)]\tLoss: 399.322021\n",
      "Train Epoche: 1 [1375/96 (1432%)]\tLoss: 397.893799\n",
      "Train Epoche: 1 [1376/96 (1433%)]\tLoss: 0.907955\n",
      "Train Epoche: 1 [1377/96 (1434%)]\tLoss: 398.522095\n",
      "Train Epoche: 1 [1378/96 (1435%)]\tLoss: 398.292236\n",
      "Train Epoche: 1 [1379/96 (1436%)]\tLoss: 8.705530\n",
      "Train Epoche: 1 [1380/96 (1438%)]\tLoss: 3.843259\n",
      "Train Epoche: 1 [1381/96 (1439%)]\tLoss: 99.453987\n",
      "Train Epoche: 1 [1382/96 (1440%)]\tLoss: 120.063057\n",
      "Train Epoche: 1 [1383/96 (1441%)]\tLoss: 35.994678\n",
      "Train Epoche: 1 [1384/96 (1442%)]\tLoss: 63.559956\n",
      "Train Epoche: 1 [1385/96 (1443%)]\tLoss: 49.450668\n",
      "Train Epoche: 1 [1386/96 (1444%)]\tLoss: 197.899445\n",
      "Train Epoche: 1 [1387/96 (1445%)]\tLoss: 257.007751\n",
      "Train Epoche: 1 [1388/96 (1446%)]\tLoss: 226.252228\n",
      "Train Epoche: 1 [1389/96 (1447%)]\tLoss: 15.709109\n",
      "Train Epoche: 1 [1390/96 (1448%)]\tLoss: 35.359875\n",
      "Train Epoche: 1 [1391/96 (1449%)]\tLoss: 223.889374\n",
      "Train Epoche: 1 [1392/96 (1450%)]\tLoss: 287.813232\n",
      "Train Epoche: 1 [1393/96 (1451%)]\tLoss: 48.393677\n",
      "Train Epoche: 1 [1394/96 (1452%)]\tLoss: 63.175259\n",
      "Train Epoche: 1 [1395/96 (1453%)]\tLoss: 80.396927\n",
      "Train Epoche: 1 [1396/96 (1454%)]\tLoss: 254.712463\n",
      "Train Epoche: 1 [1397/96 (1455%)]\tLoss: 8.814777\n",
      "Train Epoche: 1 [1398/96 (1456%)]\tLoss: 120.040298\n",
      "Train Epoche: 1 [1399/96 (1457%)]\tLoss: 168.379318\n",
      "Train Epoche: 1 [1400/96 (1458%)]\tLoss: 574.195984\n",
      "Train Epoche: 1 [1401/96 (1459%)]\tLoss: 3.805239\n",
      "Train Epoche: 1 [1402/96 (1460%)]\tLoss: 0.938274\n",
      "Train Epoche: 1 [1403/96 (1461%)]\tLoss: 99.568932\n",
      "Train Epoche: 1 [1404/96 (1462%)]\tLoss: 195.110733\n",
      "Train Epoche: 1 [1405/96 (1464%)]\tLoss: 25.000950\n",
      "Train Epoche: 1 [1406/96 (1465%)]\tLoss: 145.647858\n",
      "Train Epoche: 1 [1407/96 (1466%)]\tLoss: 360.435760\n",
      "Train Epoche: 1 [1408/96 (1467%)]\tLoss: 321.841644\n",
      "Train Epoche: 1 [1409/96 (1468%)]\tLoss: 398.702911\n",
      "Train Epoche: 1 [1410/96 (1469%)]\tLoss: 482.062225\n",
      "Train Epoche: 1 [1411/96 (1470%)]\tLoss: 443.506470\n",
      "Train Epoche: 1 [1412/96 (1471%)]\tLoss: 120.927185\n",
      "Train Epoche: 1 [1413/96 (1472%)]\tLoss: 81.089836\n",
      "Train Epoche: 1 [1414/96 (1473%)]\tLoss: 15.968242\n",
      "Train Epoche: 1 [1415/96 (1474%)]\tLoss: 143.788757\n",
      "Train Epoche: 1 [1416/96 (1475%)]\tLoss: 99.196625\n",
      "Train Epoche: 1 [1417/96 (1476%)]\tLoss: 398.381470\n",
      "Train Epoche: 1 [1418/96 (1477%)]\tLoss: 63.959442\n",
      "Train Epoche: 1 [1419/96 (1478%)]\tLoss: 3.904869\n",
      "Train Epoche: 1 [1420/96 (1479%)]\tLoss: 8.852186\n",
      "Train Epoche: 1 [1421/96 (1480%)]\tLoss: 223.525620\n",
      "Train Epoche: 1 [1422/96 (1481%)]\tLoss: 24.797831\n",
      "Train Epoche: 1 [1423/96 (1482%)]\tLoss: 35.618713\n",
      "Train Epoche: 1 [1424/96 (1483%)]\tLoss: 1.196083\n",
      "Train Epoche: 1 [1425/96 (1484%)]\tLoss: 401.593140\n",
      "Train Epoche: 1 [1426/96 (1485%)]\tLoss: 398.521729\n",
      "Train Epoche: 1 [1427/96 (1486%)]\tLoss: 50.717060\n",
      "Train Epoche: 1 [1428/96 (1488%)]\tLoss: 197.186142\n",
      "Train Epoche: 1 [1429/96 (1489%)]\tLoss: 170.612167\n",
      "Train Epoche: 1 [1430/96 (1490%)]\tLoss: 398.528198\n",
      "Train Epoche: 1 [1431/96 (1491%)]\tLoss: 195.428818\n",
      "Train Epoche: 1 [1432/96 (1492%)]\tLoss: 35.597015\n",
      "Train Epoche: 1 [1433/96 (1493%)]\tLoss: 8.846254\n",
      "Train Epoche: 1 [1434/96 (1494%)]\tLoss: 80.447235\n",
      "Train Epoche: 1 [1435/96 (1495%)]\tLoss: 168.284958\n",
      "Train Epoche: 1 [1436/96 (1496%)]\tLoss: 399.066315\n",
      "Train Epoche: 1 [1437/96 (1497%)]\tLoss: 398.049042\n",
      "Train Epoche: 1 [1438/96 (1498%)]\tLoss: 48.633987\n",
      "Train Epoche: 1 [1439/96 (1499%)]\tLoss: 63.625793\n",
      "Train Epoche: 1 [1440/96 (1500%)]\tLoss: 24.686207\n",
      "Train Epoche: 1 [1441/96 (1501%)]\tLoss: 15.697726\n",
      "Train Epoche: 1 [1442/96 (1502%)]\tLoss: 143.206146\n",
      "Train Epoche: 1 [1443/96 (1503%)]\tLoss: 398.137177\n",
      "Train Epoche: 1 [1444/96 (1504%)]\tLoss: 3.960461\n",
      "Train Epoche: 1 [1445/96 (1505%)]\tLoss: 1.057397\n",
      "Train Epoche: 1 [1446/96 (1506%)]\tLoss: 101.309341\n",
      "Train Epoche: 1 [1447/96 (1507%)]\tLoss: 119.555153\n",
      "Train Epoche: 1 [1448/96 (1508%)]\tLoss: 258.629028\n",
      "Train Epoche: 1 [1449/96 (1509%)]\tLoss: 224.924225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1450/96 (1510%)]\tLoss: 24.706648\n",
      "Train Epoche: 1 [1451/96 (1511%)]\tLoss: 254.440308\n",
      "Train Epoche: 1 [1452/96 (1512%)]\tLoss: 99.155975\n",
      "Train Epoche: 1 [1453/96 (1514%)]\tLoss: 574.570740\n",
      "Train Epoche: 1 [1454/96 (1515%)]\tLoss: 143.229675\n",
      "Train Epoche: 1 [1455/96 (1516%)]\tLoss: 8.882803\n",
      "Train Epoche: 1 [1456/96 (1517%)]\tLoss: 35.693768\n",
      "Train Epoche: 1 [1457/96 (1518%)]\tLoss: 63.284042\n",
      "Train Epoche: 1 [1458/96 (1519%)]\tLoss: 48.693047\n",
      "Train Epoche: 1 [1459/96 (1520%)]\tLoss: 0.921531\n",
      "Train Epoche: 1 [1460/96 (1521%)]\tLoss: 15.773168\n",
      "Train Epoche: 1 [1461/96 (1522%)]\tLoss: 573.325439\n",
      "Train Epoche: 1 [1462/96 (1523%)]\tLoss: 120.687714\n",
      "Train Epoche: 1 [1463/96 (1524%)]\tLoss: 3.849910\n",
      "Train Epoche: 1 [1464/96 (1525%)]\tLoss: 575.450806\n",
      "Train Epoche: 1 [1465/96 (1526%)]\tLoss: 227.869766\n",
      "Train Epoche: 1 [1466/96 (1527%)]\tLoss: 169.447479\n",
      "Train Epoche: 1 [1467/96 (1528%)]\tLoss: 574.543152\n",
      "Train Epoche: 1 [1468/96 (1529%)]\tLoss: 80.229652\n",
      "Train Epoche: 1 [1469/96 (1530%)]\tLoss: 199.843857\n",
      "Train Epoche: 1 [1470/96 (1531%)]\tLoss: 575.357910\n",
      "Train Epoche: 1 [1471/96 (1532%)]\tLoss: 8.873116\n",
      "Train Epoche: 1 [1472/96 (1533%)]\tLoss: 3.841569\n",
      "Train Epoche: 1 [1473/96 (1534%)]\tLoss: 48.536858\n",
      "Train Epoche: 1 [1474/96 (1535%)]\tLoss: 63.319210\n",
      "Train Epoche: 1 [1475/96 (1536%)]\tLoss: 287.403778\n",
      "Train Epoche: 1 [1476/96 (1538%)]\tLoss: 254.975143\n",
      "Train Epoche: 1 [1477/96 (1539%)]\tLoss: 80.409912\n",
      "Train Epoche: 1 [1478/96 (1540%)]\tLoss: 168.271667\n",
      "Train Epoche: 1 [1479/96 (1541%)]\tLoss: 224.115982\n",
      "Train Epoche: 1 [1480/96 (1542%)]\tLoss: 143.013794\n",
      "Train Epoche: 1 [1481/96 (1543%)]\tLoss: 15.575684\n",
      "Train Epoche: 1 [1482/96 (1544%)]\tLoss: 24.621977\n",
      "Train Epoche: 1 [1483/96 (1545%)]\tLoss: 99.406479\n",
      "Train Epoche: 1 [1484/96 (1546%)]\tLoss: 119.814331\n",
      "Train Epoche: 1 [1485/96 (1547%)]\tLoss: 0.936357\n",
      "Train Epoche: 1 [1486/96 (1548%)]\tLoss: 575.237976\n",
      "Train Epoche: 1 [1487/96 (1549%)]\tLoss: 437.742401\n",
      "Train Epoche: 1 [1488/96 (1550%)]\tLoss: 481.740143\n",
      "Train Epoche: 1 [1489/96 (1551%)]\tLoss: 529.384277\n",
      "Train Epoche: 1 [1490/96 (1552%)]\tLoss: 326.922607\n",
      "Train Epoche: 1 [1491/96 (1553%)]\tLoss: 196.639099\n",
      "Train Epoche: 1 [1492/96 (1554%)]\tLoss: 35.316277\n",
      "Train Epoche: 1 [1493/96 (1555%)]\tLoss: 362.695251\n",
      "Train Epoche: 1 [1494/96 (1556%)]\tLoss: 400.132233\n",
      "Train Epoche: 1 [1495/96 (1557%)]\tLoss: 168.404221\n",
      "Train Epoche: 1 [1496/96 (1558%)]\tLoss: 398.469727\n",
      "Train Epoche: 1 [1497/96 (1559%)]\tLoss: 120.231689\n",
      "Train Epoche: 1 [1498/96 (1560%)]\tLoss: 143.038727\n",
      "Train Epoche: 1 [1499/96 (1561%)]\tLoss: 63.431862\n",
      "Train Epoche: 1 [1500/96 (1562%)]\tLoss: 482.822601\n",
      "Train Epoche: 1 [1501/96 (1564%)]\tLoss: 8.676422\n",
      "Train Epoche: 1 [1502/96 (1565%)]\tLoss: 35.677902\n",
      "Train Epoche: 1 [1503/96 (1566%)]\tLoss: 48.594486\n",
      "Train Epoche: 1 [1504/96 (1567%)]\tLoss: 80.421043\n",
      "Train Epoche: 1 [1505/96 (1568%)]\tLoss: 3.838974\n",
      "Train Epoche: 1 [1506/96 (1569%)]\tLoss: 481.543243\n",
      "Train Epoche: 1 [1507/96 (1570%)]\tLoss: 99.296326\n",
      "Train Epoche: 1 [1508/96 (1571%)]\tLoss: 194.493393\n",
      "Train Epoche: 1 [1509/96 (1572%)]\tLoss: 15.896132\n",
      "Train Epoche: 1 [1510/96 (1573%)]\tLoss: 1.014009\n",
      "Train Epoche: 1 [1511/96 (1574%)]\tLoss: 225.917664\n",
      "Train Epoche: 1 [1512/96 (1575%)]\tLoss: 324.630371\n",
      "Train Epoche: 1 [1513/96 (1576%)]\tLoss: 25.215904\n",
      "Train Epoche: 1 [1514/96 (1577%)]\tLoss: 365.068634\n",
      "Train Epoche: 1 [1515/96 (1578%)]\tLoss: 289.770935\n",
      "Train Epoche: 1 [1516/96 (1579%)]\tLoss: 257.441772\n",
      "Train Epoche: 1 [1517/96 (1580%)]\tLoss: 8.943106\n",
      "Train Epoche: 1 [1518/96 (1581%)]\tLoss: 195.836182\n",
      "Train Epoche: 1 [1519/96 (1582%)]\tLoss: 35.729652\n",
      "Train Epoche: 1 [1520/96 (1583%)]\tLoss: 362.315308\n",
      "Train Epoche: 1 [1521/96 (1584%)]\tLoss: 143.948441\n",
      "Train Epoche: 1 [1522/96 (1585%)]\tLoss: 63.947140\n",
      "Train Epoche: 1 [1523/96 (1586%)]\tLoss: 0.945802\n",
      "Train Epoche: 1 [1524/96 (1588%)]\tLoss: 224.423279\n",
      "Train Epoche: 1 [1525/96 (1589%)]\tLoss: 80.562836\n",
      "Train Epoche: 1 [1526/96 (1590%)]\tLoss: 48.897541\n",
      "Train Epoche: 1 [1527/96 (1591%)]\tLoss: 16.033384\n",
      "Train Epoche: 1 [1528/96 (1592%)]\tLoss: 120.786613\n",
      "Train Epoche: 1 [1529/96 (1593%)]\tLoss: 577.098511\n",
      "Train Epoche: 1 [1530/96 (1594%)]\tLoss: 3.969443\n",
      "Train Epoche: 1 [1531/96 (1595%)]\tLoss: 169.871994\n",
      "Train Epoche: 1 [1532/96 (1596%)]\tLoss: 100.277618\n",
      "Train Epoche: 1 [1533/96 (1597%)]\tLoss: 483.546234\n",
      "Train Epoche: 1 [1534/96 (1598%)]\tLoss: 443.433441\n",
      "Train Epoche: 1 [1535/96 (1599%)]\tLoss: 323.389984\n",
      "Train Epoche: 1 [1536/96 (1600%)]\tLoss: 255.039963\n",
      "Train Epoche: 1 [1537/96 (1601%)]\tLoss: 25.652491\n",
      "Train Epoche: 1 [1538/96 (1602%)]\tLoss: 576.316101\n",
      "Train Epoche: 1 [1539/96 (1603%)]\tLoss: 290.008575\n",
      "Train Epoche: 1 [1540/96 (1604%)]\tLoss: 401.675415\n",
      "Train Epoche: 1 [1541/96 (1605%)]\tLoss: 573.956787\n",
      "Train Epoche: 1 [1542/96 (1606%)]\tLoss: 0.961392\n",
      "Train Epoche: 1 [1543/96 (1607%)]\tLoss: 574.008545\n",
      "Train Epoche: 1 [1544/96 (1608%)]\tLoss: 168.659439\n",
      "Train Epoche: 1 [1545/96 (1609%)]\tLoss: 63.459877\n",
      "Train Epoche: 1 [1546/96 (1610%)]\tLoss: 3.835815\n",
      "Train Epoche: 1 [1547/96 (1611%)]\tLoss: 8.726171\n",
      "Train Epoche: 1 [1548/96 (1612%)]\tLoss: 24.611971\n",
      "Train Epoche: 1 [1549/96 (1614%)]\tLoss: 359.609222\n",
      "Train Epoche: 1 [1550/96 (1615%)]\tLoss: 15.683321\n",
      "Train Epoche: 1 [1551/96 (1616%)]\tLoss: 35.558727\n",
      "Train Epoche: 1 [1552/96 (1617%)]\tLoss: 80.702148\n",
      "Train Epoche: 1 [1553/96 (1618%)]\tLoss: 225.602356\n",
      "Train Epoche: 1 [1554/96 (1619%)]\tLoss: 49.176430\n",
      "Train Epoche: 1 [1555/96 (1620%)]\tLoss: 287.397522\n",
      "Train Epoche: 1 [1556/96 (1621%)]\tLoss: 322.960968\n",
      "Train Epoche: 1 [1557/96 (1622%)]\tLoss: 198.584946\n",
      "Train Epoche: 1 [1558/96 (1623%)]\tLoss: 121.414749\n",
      "Train Epoche: 1 [1559/96 (1624%)]\tLoss: 101.631012\n",
      "Train Epoche: 1 [1560/96 (1625%)]\tLoss: 577.901855\n",
      "Train Epoche: 1 [1561/96 (1626%)]\tLoss: 257.478027\n",
      "Train Epoche: 1 [1562/96 (1627%)]\tLoss: 143.636124\n",
      "Train Epoche: 1 [1563/96 (1628%)]\tLoss: 15.686208\n",
      "Train Epoche: 1 [1564/96 (1629%)]\tLoss: 3.794332\n",
      "Train Epoche: 1 [1565/96 (1630%)]\tLoss: 142.954773\n",
      "Train Epoche: 1 [1566/96 (1631%)]\tLoss: 120.367546\n",
      "Train Epoche: 1 [1567/96 (1632%)]\tLoss: 80.511696\n",
      "Train Epoche: 1 [1568/96 (1633%)]\tLoss: 99.335365\n",
      "Train Epoche: 1 [1569/96 (1634%)]\tLoss: 48.724621\n",
      "Train Epoche: 1 [1570/96 (1635%)]\tLoss: 8.814958\n",
      "Train Epoche: 1 [1571/96 (1636%)]\tLoss: 35.520794\n",
      "Train Epoche: 1 [1572/96 (1638%)]\tLoss: 573.210999\n",
      "Train Epoche: 1 [1573/96 (1639%)]\tLoss: 63.338951\n",
      "Train Epoche: 1 [1574/96 (1640%)]\tLoss: 573.293762\n",
      "Train Epoche: 1 [1575/96 (1641%)]\tLoss: 0.910366\n",
      "Train Epoche: 1 [1576/96 (1642%)]\tLoss: 573.822021\n",
      "Train Epoche: 1 [1577/96 (1643%)]\tLoss: 574.112976\n",
      "Train Epoche: 1 [1578/96 (1644%)]\tLoss: 25.578228\n",
      "Train Epoche: 1 [1579/96 (1645%)]\tLoss: 169.596802\n",
      "Train Epoche: 1 [1580/96 (1646%)]\tLoss: 195.773041\n",
      "Train Epoche: 1 [1581/96 (1647%)]\tLoss: 225.256592\n",
      "Train Epoche: 1 [1582/96 (1648%)]\tLoss: 579.883789\n",
      "Train Epoche: 1 [1583/96 (1649%)]\tLoss: 577.373535\n",
      "Train Epoche: 1 [1584/96 (1650%)]\tLoss: 35.719757\n",
      "Train Epoche: 1 [1585/96 (1651%)]\tLoss: 63.731438\n",
      "Train Epoche: 1 [1586/96 (1652%)]\tLoss: 224.226273\n",
      "Train Epoche: 1 [1587/96 (1653%)]\tLoss: 255.301285\n",
      "Train Epoche: 1 [1588/96 (1654%)]\tLoss: 143.255615\n",
      "Train Epoche: 1 [1589/96 (1655%)]\tLoss: 482.824280\n",
      "Train Epoche: 1 [1590/96 (1656%)]\tLoss: 15.685622\n",
      "Train Epoche: 1 [1591/96 (1657%)]\tLoss: 482.322479\n",
      "Train Epoche: 1 [1592/96 (1658%)]\tLoss: 167.958939\n",
      "Train Epoche: 1 [1593/96 (1659%)]\tLoss: 120.287888\n",
      "Train Epoche: 1 [1594/96 (1660%)]\tLoss: 48.660828\n",
      "Train Epoche: 1 [1595/96 (1661%)]\tLoss: 0.966884\n",
      "Train Epoche: 1 [1596/96 (1662%)]\tLoss: 99.175575\n",
      "Train Epoche: 1 [1597/96 (1664%)]\tLoss: 194.806274\n",
      "Train Epoche: 1 [1598/96 (1665%)]\tLoss: 24.612860\n",
      "Train Epoche: 1 [1599/96 (1666%)]\tLoss: 80.609512\n",
      "Train Epoche: 1 [1600/96 (1667%)]\tLoss: 289.833221\n",
      "Train Epoche: 1 [1601/96 (1668%)]\tLoss: 324.438904\n",
      "Train Epoche: 1 [1602/96 (1669%)]\tLoss: 4.239035\n",
      "Train Epoche: 1 [1603/96 (1670%)]\tLoss: 9.068766\n",
      "Train Epoche: 1 [1604/96 (1671%)]\tLoss: 362.196106\n",
      "Train Epoche: 1 [1605/96 (1672%)]\tLoss: 484.384460\n",
      "Train Epoche: 1 [1606/96 (1673%)]\tLoss: 63.558464\n",
      "Train Epoche: 1 [1607/96 (1674%)]\tLoss: 99.232857\n",
      "Train Epoche: 1 [1608/96 (1675%)]\tLoss: 8.813788\n",
      "Train Epoche: 1 [1609/96 (1676%)]\tLoss: 15.674226\n",
      "Train Epoche: 1 [1610/96 (1677%)]\tLoss: 168.041061\n",
      "Train Epoche: 1 [1611/96 (1678%)]\tLoss: 119.956322\n",
      "Train Epoche: 1 [1612/96 (1679%)]\tLoss: 481.511444\n",
      "Train Epoche: 1 [1613/96 (1680%)]\tLoss: 80.217438\n",
      "Train Epoche: 1 [1614/96 (1681%)]\tLoss: 142.661972\n",
      "Train Epoche: 1 [1615/96 (1682%)]\tLoss: 48.770432\n",
      "Train Epoche: 1 [1616/96 (1683%)]\tLoss: 35.581944\n",
      "Train Epoche: 1 [1617/96 (1684%)]\tLoss: 24.540981\n",
      "Train Epoche: 1 [1618/96 (1685%)]\tLoss: 223.449661\n",
      "Train Epoche: 1 [1619/96 (1686%)]\tLoss: 359.172302\n",
      "Train Epoche: 1 [1620/96 (1688%)]\tLoss: 0.976705\n",
      "Train Epoche: 1 [1621/96 (1689%)]\tLoss: 4.003165\n",
      "Train Epoche: 1 [1622/96 (1690%)]\tLoss: 289.664642\n",
      "Train Epoche: 1 [1623/96 (1691%)]\tLoss: 401.320526\n",
      "Train Epoche: 1 [1624/96 (1692%)]\tLoss: 256.179657\n",
      "Train Epoche: 1 [1625/96 (1693%)]\tLoss: 195.774475\n",
      "Train Epoche: 1 [1626/96 (1694%)]\tLoss: 484.381012\n",
      "Train Epoche: 1 [1627/96 (1695%)]\tLoss: 323.722992\n",
      "Train Epoche: 1 [1628/96 (1696%)]\tLoss: 24.580910\n",
      "Train Epoche: 1 [1629/96 (1697%)]\tLoss: 15.612965\n",
      "Train Epoche: 1 [1630/96 (1698%)]\tLoss: 194.663940\n",
      "Train Epoche: 1 [1631/96 (1699%)]\tLoss: 63.465847\n",
      "Train Epoche: 1 [1632/96 (1700%)]\tLoss: 99.523865\n",
      "Train Epoche: 1 [1633/96 (1701%)]\tLoss: 167.845459\n",
      "Train Epoche: 1 [1634/96 (1702%)]\tLoss: 3.796674\n",
      "Train Epoche: 1 [1635/96 (1703%)]\tLoss: 48.432964\n",
      "Train Epoche: 1 [1636/96 (1704%)]\tLoss: 142.928635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1637/96 (1705%)]\tLoss: 80.562340\n",
      "Train Epoche: 1 [1638/96 (1706%)]\tLoss: 0.898898\n",
      "Train Epoche: 1 [1639/96 (1707%)]\tLoss: 8.850053\n",
      "Train Epoche: 1 [1640/96 (1708%)]\tLoss: 574.161560\n",
      "Train Epoche: 1 [1641/96 (1709%)]\tLoss: 120.475670\n",
      "Train Epoche: 1 [1642/96 (1710%)]\tLoss: 324.100891\n",
      "Train Epoche: 1 [1643/96 (1711%)]\tLoss: 574.373230\n",
      "Train Epoche: 1 [1644/96 (1712%)]\tLoss: 225.615997\n",
      "Train Epoche: 1 [1645/96 (1714%)]\tLoss: 291.038818\n",
      "Train Epoche: 1 [1646/96 (1715%)]\tLoss: 36.267273\n",
      "Train Epoche: 1 [1647/96 (1716%)]\tLoss: 361.214355\n",
      "Train Epoche: 1 [1648/96 (1717%)]\tLoss: 256.759705\n",
      "Train Epoche: 1 [1649/96 (1718%)]\tLoss: 579.252319\n",
      "Train Epoche: 1 [1650/96 (1719%)]\tLoss: 574.340820\n",
      "Train Epoche: 1 [1651/96 (1720%)]\tLoss: 3.838881\n",
      "Train Epoche: 1 [1652/96 (1721%)]\tLoss: 288.271301\n",
      "Train Epoche: 1 [1653/96 (1722%)]\tLoss: 224.149078\n",
      "Train Epoche: 1 [1654/96 (1723%)]\tLoss: 168.227493\n",
      "Train Epoche: 1 [1655/96 (1724%)]\tLoss: 194.950989\n",
      "Train Epoche: 1 [1656/96 (1725%)]\tLoss: 0.940853\n",
      "Train Epoche: 1 [1657/96 (1726%)]\tLoss: 143.590393\n",
      "Train Epoche: 1 [1658/96 (1727%)]\tLoss: 80.319542\n",
      "Train Epoche: 1 [1659/96 (1728%)]\tLoss: 120.090309\n",
      "Train Epoche: 1 [1660/96 (1729%)]\tLoss: 63.329243\n",
      "Train Epoche: 1 [1661/96 (1730%)]\tLoss: 24.566864\n",
      "Train Epoche: 1 [1662/96 (1731%)]\tLoss: 15.628803\n",
      "Train Epoche: 1 [1663/96 (1732%)]\tLoss: 35.519714\n",
      "Train Epoche: 1 [1664/96 (1733%)]\tLoss: 99.836151\n",
      "Train Epoche: 1 [1665/96 (1734%)]\tLoss: 49.496174\n",
      "Train Epoche: 1 [1666/96 (1735%)]\tLoss: 439.049927\n",
      "Train Epoche: 1 [1667/96 (1736%)]\tLoss: 528.605408\n",
      "Train Epoche: 1 [1668/96 (1738%)]\tLoss: 360.881134\n",
      "Train Epoche: 1 [1669/96 (1739%)]\tLoss: 256.243042\n",
      "Train Epoche: 1 [1670/96 (1740%)]\tLoss: 9.220668\n",
      "Train Epoche: 1 [1671/96 (1741%)]\tLoss: 324.773499\n",
      "Train Epoche: 1 [1672/96 (1742%)]\tLoss: 485.197479\n",
      "Train Epoche: 1 [1673/96 (1743%)]\tLoss: 398.506042\n",
      "Train Epoche: 1 [1674/96 (1744%)]\tLoss: 99.551437\n",
      "Train Epoche: 1 [1675/96 (1745%)]\tLoss: 482.767365\n",
      "Train Epoche: 1 [1676/96 (1746%)]\tLoss: 80.098824\n",
      "Train Epoche: 1 [1677/96 (1747%)]\tLoss: 481.962128\n",
      "Train Epoche: 1 [1678/96 (1748%)]\tLoss: 481.742828\n",
      "Train Epoche: 1 [1679/96 (1749%)]\tLoss: 287.959503\n",
      "Train Epoche: 1 [1680/96 (1750%)]\tLoss: 481.974701\n",
      "Train Epoche: 1 [1681/96 (1751%)]\tLoss: 484.166931\n",
      "Train Epoche: 1 [1682/96 (1752%)]\tLoss: 35.874413\n",
      "Train Epoche: 1 [1683/96 (1753%)]\tLoss: 8.798506\n",
      "Train Epoche: 1 [1684/96 (1754%)]\tLoss: 48.435608\n",
      "Train Epoche: 1 [1685/96 (1755%)]\tLoss: 63.567200\n",
      "Train Epoche: 1 [1686/96 (1756%)]\tLoss: 24.710175\n",
      "Train Epoche: 1 [1687/96 (1757%)]\tLoss: 15.682395\n",
      "Train Epoche: 1 [1688/96 (1758%)]\tLoss: 224.243301\n",
      "Train Epoche: 1 [1689/96 (1759%)]\tLoss: 254.842194\n",
      "Train Epoche: 1 [1690/96 (1760%)]\tLoss: 0.967795\n",
      "Train Epoche: 1 [1691/96 (1761%)]\tLoss: 3.893750\n",
      "Train Epoche: 1 [1692/96 (1762%)]\tLoss: 196.585495\n",
      "Train Epoche: 1 [1693/96 (1764%)]\tLoss: 168.256699\n",
      "Train Epoche: 1 [1694/96 (1765%)]\tLoss: 120.827873\n",
      "Train Epoche: 1 [1695/96 (1766%)]\tLoss: 143.892242\n",
      "Train Epoche: 1 [1696/96 (1767%)]\tLoss: 99.040268\n",
      "Train Epoche: 1 [1697/96 (1768%)]\tLoss: 35.527832\n",
      "Train Epoche: 1 [1698/96 (1769%)]\tLoss: 120.644722\n",
      "Train Epoche: 1 [1699/96 (1770%)]\tLoss: 194.952164\n",
      "Train Epoche: 1 [1700/96 (1771%)]\tLoss: 255.233978\n",
      "Train Epoche: 1 [1701/96 (1772%)]\tLoss: 481.447327\n",
      "Train Epoche: 1 [1702/96 (1773%)]\tLoss: 63.186493\n",
      "Train Epoche: 1 [1703/96 (1774%)]\tLoss: 223.844635\n",
      "Train Epoche: 1 [1704/96 (1775%)]\tLoss: 168.458298\n",
      "Train Epoche: 1 [1705/96 (1776%)]\tLoss: 15.819509\n",
      "Train Epoche: 1 [1706/96 (1777%)]\tLoss: 48.509163\n",
      "Train Epoche: 1 [1707/96 (1778%)]\tLoss: 0.934058\n",
      "Train Epoche: 1 [1708/96 (1779%)]\tLoss: 142.777786\n",
      "Train Epoche: 1 [1709/96 (1780%)]\tLoss: 322.816895\n",
      "Train Epoche: 1 [1710/96 (1781%)]\tLoss: 25.101688\n",
      "Train Epoche: 1 [1711/96 (1782%)]\tLoss: 81.360718\n",
      "Train Epoche: 1 [1712/96 (1783%)]\tLoss: 288.019409\n",
      "Train Epoche: 1 [1713/96 (1784%)]\tLoss: 439.118500\n",
      "Train Epoche: 1 [1714/96 (1785%)]\tLoss: 4.201270\n",
      "Train Epoche: 1 [1715/96 (1786%)]\tLoss: 9.428619\n",
      "Train Epoche: 1 [1716/96 (1788%)]\tLoss: 399.968048\n",
      "Train Epoche: 1 [1717/96 (1789%)]\tLoss: 360.492737\n",
      "Train Epoche: 1 [1718/96 (1790%)]\tLoss: 287.705353\n",
      "Train Epoche: 1 [1719/96 (1791%)]\tLoss: 143.028961\n",
      "Train Epoche: 1 [1720/96 (1792%)]\tLoss: 63.443871\n",
      "Train Epoche: 1 [1721/96 (1793%)]\tLoss: 168.861694\n",
      "Train Epoche: 1 [1722/96 (1794%)]\tLoss: 254.615585\n",
      "Train Epoche: 1 [1723/96 (1795%)]\tLoss: 120.252373\n",
      "Train Epoche: 1 [1724/96 (1796%)]\tLoss: 8.725692\n",
      "Train Epoche: 1 [1725/96 (1797%)]\tLoss: 15.715598\n",
      "Train Epoche: 1 [1726/96 (1798%)]\tLoss: 48.637432\n",
      "Train Epoche: 1 [1727/96 (1799%)]\tLoss: 24.381340\n",
      "Train Epoche: 1 [1728/96 (1800%)]\tLoss: 35.551395\n",
      "Train Epoche: 1 [1729/96 (1801%)]\tLoss: 99.375824\n",
      "Train Epoche: 1 [1730/96 (1802%)]\tLoss: 194.917496\n",
      "Train Epoche: 1 [1731/96 (1803%)]\tLoss: 223.700943\n",
      "Train Epoche: 1 [1732/96 (1804%)]\tLoss: 3.737252\n",
      "Train Epoche: 1 [1733/96 (1805%)]\tLoss: 0.988859\n",
      "Train Epoche: 1 [1734/96 (1806%)]\tLoss: 80.807747\n",
      "Train Epoche: 1 [1735/96 (1807%)]\tLoss: 323.006500\n",
      "Train Epoche: 1 [1736/96 (1808%)]\tLoss: 363.395203\n",
      "Train Epoche: 1 [1737/96 (1809%)]\tLoss: 63.804119\n",
      "Train Epoche: 1 [1738/96 (1810%)]\tLoss: 48.545872\n",
      "Train Epoche: 1 [1739/96 (1811%)]\tLoss: 254.628952\n",
      "Train Epoche: 1 [1740/96 (1812%)]\tLoss: 99.017448\n",
      "Train Epoche: 1 [1741/96 (1814%)]\tLoss: 119.991669\n",
      "Train Epoche: 1 [1742/96 (1815%)]\tLoss: 143.059799\n",
      "Train Epoche: 1 [1743/96 (1816%)]\tLoss: 398.236267\n",
      "Train Epoche: 1 [1744/96 (1817%)]\tLoss: 8.774852\n",
      "Train Epoche: 1 [1745/96 (1818%)]\tLoss: 0.935706\n",
      "Train Epoche: 1 [1746/96 (1819%)]\tLoss: 80.296982\n",
      "Train Epoche: 1 [1747/96 (1820%)]\tLoss: 35.472736\n",
      "Train Epoche: 1 [1748/96 (1821%)]\tLoss: 24.689699\n",
      "Train Epoche: 1 [1749/96 (1822%)]\tLoss: 167.633804\n",
      "Train Epoche: 1 [1750/96 (1823%)]\tLoss: 194.361450\n",
      "Train Epoche: 1 [1751/96 (1824%)]\tLoss: 15.798873\n",
      "Train Epoche: 1 [1752/96 (1825%)]\tLoss: 4.206642\n",
      "Train Epoche: 1 [1753/96 (1826%)]\tLoss: 225.618317\n",
      "Train Epoche: 1 [1754/96 (1827%)]\tLoss: 35.780388\n",
      "Train Epoche: 1 [1755/96 (1828%)]\tLoss: 99.232475\n",
      "Train Epoche: 1 [1756/96 (1829%)]\tLoss: 48.418682\n",
      "Train Epoche: 1 [1757/96 (1830%)]\tLoss: 483.493317\n",
      "Train Epoche: 1 [1758/96 (1831%)]\tLoss: 483.324219\n",
      "Train Epoche: 1 [1759/96 (1832%)]\tLoss: 481.870270\n",
      "Train Epoche: 1 [1760/96 (1833%)]\tLoss: 15.745088\n",
      "Train Epoche: 1 [1761/96 (1834%)]\tLoss: 143.743393\n",
      "Train Epoche: 1 [1762/96 (1835%)]\tLoss: 24.595854\n",
      "Train Epoche: 1 [1763/96 (1836%)]\tLoss: 481.801270\n",
      "Train Epoche: 1 [1764/96 (1838%)]\tLoss: 8.927402\n",
      "Train Epoche: 1 [1765/96 (1839%)]\tLoss: 481.941284\n",
      "Train Epoche: 1 [1766/96 (1840%)]\tLoss: 482.494507\n",
      "Train Epoche: 1 [1767/96 (1841%)]\tLoss: 3.826933\n",
      "Train Epoche: 1 [1768/96 (1842%)]\tLoss: 0.978951\n",
      "Train Epoche: 1 [1769/96 (1843%)]\tLoss: 169.648163\n",
      "Train Epoche: 1 [1770/96 (1844%)]\tLoss: 120.881927\n",
      "Train Epoche: 1 [1771/96 (1845%)]\tLoss: 65.583328\n",
      "Train Epoche: 1 [1772/96 (1846%)]\tLoss: 195.793320\n",
      "Train Epoche: 1 [1773/96 (1847%)]\tLoss: 81.856575\n",
      "Train Epoche: 1 [1774/96 (1848%)]\tLoss: 143.047028\n",
      "Train Epoche: 1 [1775/96 (1849%)]\tLoss: 195.151230\n",
      "Train Epoche: 1 [1776/96 (1850%)]\tLoss: 24.550526\n",
      "Train Epoche: 1 [1777/96 (1851%)]\tLoss: 35.600285\n",
      "Train Epoche: 1 [1778/96 (1852%)]\tLoss: 287.630371\n",
      "Train Epoche: 1 [1779/96 (1853%)]\tLoss: 168.053421\n",
      "Train Epoche: 1 [1780/96 (1854%)]\tLoss: 15.713693\n",
      "Train Epoche: 1 [1781/96 (1855%)]\tLoss: 8.737958\n",
      "Train Epoche: 1 [1782/96 (1856%)]\tLoss: 398.335236\n",
      "Train Epoche: 1 [1783/96 (1857%)]\tLoss: 120.277473\n",
      "Train Epoche: 1 [1784/96 (1858%)]\tLoss: 81.010231\n",
      "Train Epoche: 1 [1785/96 (1859%)]\tLoss: 398.414825\n",
      "Train Epoche: 1 [1786/96 (1860%)]\tLoss: 99.146553\n",
      "Train Epoche: 1 [1787/96 (1861%)]\tLoss: 63.249313\n",
      "Train Epoche: 1 [1788/96 (1862%)]\tLoss: 0.903503\n",
      "Train Epoche: 1 [1789/96 (1864%)]\tLoss: 3.897053\n",
      "Train Epoche: 1 [1790/96 (1865%)]\tLoss: 48.674026\n",
      "Train Epoche: 1 [1791/96 (1866%)]\tLoss: 402.633179\n",
      "Train Epoche: 1 [1792/96 (1867%)]\tLoss: 225.843109\n",
      "Train Epoche: 1 [1793/96 (1868%)]\tLoss: 254.904312\n",
      "Train Epoche: 1 [1794/96 (1869%)]\tLoss: 35.430149\n",
      "Train Epoche: 1 [1795/96 (1870%)]\tLoss: 143.133987\n",
      "Train Epoche: 1 [1796/96 (1871%)]\tLoss: 168.088043\n",
      "Train Epoche: 1 [1797/96 (1872%)]\tLoss: 8.794951\n",
      "Train Epoche: 1 [1798/96 (1873%)]\tLoss: 119.982712\n",
      "Train Epoche: 1 [1799/96 (1874%)]\tLoss: 80.308945\n",
      "Train Epoche: 1 [1800/96 (1875%)]\tLoss: 48.382862\n",
      "Train Epoche: 1 [1801/96 (1876%)]\tLoss: 15.727600\n",
      "Train Epoche: 1 [1802/96 (1877%)]\tLoss: 99.371262\n",
      "Train Epoche: 1 [1803/96 (1878%)]\tLoss: 63.450039\n",
      "Train Epoche: 1 [1804/96 (1879%)]\tLoss: 24.537279\n",
      "Train Epoche: 1 [1805/96 (1880%)]\tLoss: 0.944093\n",
      "Train Epoche: 1 [1806/96 (1881%)]\tLoss: 195.143112\n",
      "Train Epoche: 1 [1807/96 (1882%)]\tLoss: 223.043381\n",
      "Train Epoche: 1 [1808/96 (1883%)]\tLoss: 483.654877\n",
      "Train Epoche: 1 [1809/96 (1884%)]\tLoss: 3.922230\n",
      "Train Epoche: 1 [1810/96 (1885%)]\tLoss: 489.058197\n",
      "Train Epoche: 1 [1811/96 (1886%)]\tLoss: 290.391083\n",
      "Train Epoche: 1 [1812/96 (1888%)]\tLoss: 487.412872\n",
      "Train Epoche: 1 [1813/96 (1889%)]\tLoss: 482.652618\n",
      "Train Epoche: 1 [1814/96 (1890%)]\tLoss: 256.134430\n",
      "Train Epoche: 1 [1815/96 (1891%)]\tLoss: 321.662628\n",
      "Train Epoche: 1 [1816/96 (1892%)]\tLoss: 482.542938\n",
      "Train Epoche: 1 [1817/96 (1893%)]\tLoss: 99.307083\n",
      "Train Epoche: 1 [1818/96 (1894%)]\tLoss: 63.445057\n",
      "Train Epoche: 1 [1819/96 (1895%)]\tLoss: 80.333305\n",
      "Train Epoche: 1 [1820/96 (1896%)]\tLoss: 120.390709\n",
      "Train Epoche: 1 [1821/96 (1897%)]\tLoss: 35.502285\n",
      "Train Epoche: 1 [1822/96 (1898%)]\tLoss: 482.686493\n",
      "Train Epoche: 1 [1823/96 (1899%)]\tLoss: 3.865616\n",
      "Train Epoche: 1 [1824/96 (1900%)]\tLoss: 223.813950\n",
      "Train Epoche: 1 [1825/96 (1901%)]\tLoss: 254.704010\n",
      "Train Epoche: 1 [1826/96 (1902%)]\tLoss: 15.758992\n",
      "Train Epoche: 1 [1827/96 (1903%)]\tLoss: 48.536976\n",
      "Train Epoche: 1 [1828/96 (1904%)]\tLoss: 143.272659\n",
      "Train Epoche: 1 [1829/96 (1905%)]\tLoss: 195.075363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [1830/96 (1906%)]\tLoss: 8.772148\n",
      "Train Epoche: 1 [1831/96 (1907%)]\tLoss: 0.959422\n",
      "Train Epoche: 1 [1832/96 (1908%)]\tLoss: 169.091751\n",
      "Train Epoche: 1 [1833/96 (1909%)]\tLoss: 288.798615\n",
      "Train Epoche: 1 [1834/96 (1910%)]\tLoss: 24.300163\n",
      "Train Epoche: 1 [1835/96 (1911%)]\tLoss: 486.091766\n",
      "Train Epoche: 1 [1836/96 (1912%)]\tLoss: 482.606537\n",
      "Train Epoche: 1 [1837/96 (1914%)]\tLoss: 99.147598\n",
      "Train Epoche: 1 [1838/96 (1915%)]\tLoss: 24.830706\n",
      "Train Epoche: 1 [1839/96 (1916%)]\tLoss: 120.122917\n",
      "Train Epoche: 1 [1840/96 (1917%)]\tLoss: 35.700699\n",
      "Train Epoche: 1 [1841/96 (1918%)]\tLoss: 194.825775\n",
      "Train Epoche: 1 [1842/96 (1919%)]\tLoss: 15.648092\n",
      "Train Epoche: 1 [1843/96 (1920%)]\tLoss: 63.491714\n",
      "Train Epoche: 1 [1844/96 (1921%)]\tLoss: 80.063545\n",
      "Train Epoche: 1 [1845/96 (1922%)]\tLoss: 48.654388\n",
      "Train Epoche: 1 [1846/96 (1923%)]\tLoss: 3.842545\n",
      "Train Epoche: 1 [1847/96 (1924%)]\tLoss: 8.846057\n",
      "Train Epoche: 1 [1848/96 (1925%)]\tLoss: 481.724091\n",
      "Train Epoche: 1 [1849/96 (1926%)]\tLoss: 482.093964\n",
      "Train Epoche: 1 [1850/96 (1927%)]\tLoss: 0.921688\n",
      "Train Epoche: 1 [1851/96 (1928%)]\tLoss: 482.368469\n",
      "Train Epoche: 1 [1852/96 (1929%)]\tLoss: 226.531235\n",
      "Train Epoche: 1 [1853/96 (1930%)]\tLoss: 171.433060\n",
      "Train Epoche: 1 [1854/96 (1931%)]\tLoss: 144.133774\n",
      "Train Epoche: 1 [1855/96 (1932%)]\tLoss: 289.062439\n",
      "Train Epoche: 1 [1856/96 (1933%)]\tLoss: 257.210297\n",
      "Train Epoche: 1 [1857/96 (1934%)]\tLoss: 35.575691\n",
      "Train Epoche: 1 [1858/96 (1935%)]\tLoss: 99.332443\n",
      "Train Epoche: 1 [1859/96 (1936%)]\tLoss: 24.531799\n",
      "Train Epoche: 1 [1860/96 (1938%)]\tLoss: 15.967990\n",
      "Train Epoche: 1 [1861/96 (1939%)]\tLoss: 49.006336\n",
      "Train Epoche: 1 [1862/96 (1940%)]\tLoss: 169.166840\n",
      "Train Epoche: 1 [1863/96 (1941%)]\tLoss: 482.837433\n",
      "Train Epoche: 1 [1864/96 (1942%)]\tLoss: 143.109207\n",
      "Train Epoche: 1 [1865/96 (1943%)]\tLoss: 8.964022\n",
      "Train Epoche: 1 [1866/96 (1944%)]\tLoss: 80.540154\n",
      "Train Epoche: 1 [1867/96 (1945%)]\tLoss: 121.088661\n",
      "Train Epoche: 1 [1868/96 (1946%)]\tLoss: 224.556793\n",
      "Train Epoche: 1 [1869/96 (1947%)]\tLoss: 196.126266\n",
      "Train Epoche: 1 [1870/96 (1948%)]\tLoss: 255.663986\n",
      "Train Epoche: 1 [1871/96 (1949%)]\tLoss: 3.947826\n",
      "Train Epoche: 1 [1872/96 (1950%)]\tLoss: 1.073005\n",
      "Train Epoche: 1 [1873/96 (1951%)]\tLoss: 326.533905\n",
      "Train Epoche: 1 [1874/96 (1952%)]\tLoss: 65.064232\n",
      "Train Epoche: 1 [1875/96 (1953%)]\tLoss: 289.158325\n",
      "Train Epoche: 1 [1876/96 (1954%)]\tLoss: 99.061111\n",
      "Train Epoche: 1 [1877/96 (1955%)]\tLoss: 224.413361\n",
      "Train Epoche: 1 [1878/96 (1956%)]\tLoss: 195.550201\n",
      "Train Epoche: 1 [1879/96 (1957%)]\tLoss: 80.418449\n",
      "Train Epoche: 1 [1880/96 (1958%)]\tLoss: 63.703156\n",
      "Train Epoche: 1 [1881/96 (1959%)]\tLoss: 8.730867\n",
      "Train Epoche: 1 [1882/96 (1960%)]\tLoss: 15.443780\n",
      "Train Epoche: 1 [1883/96 (1961%)]\tLoss: 35.577507\n",
      "Train Epoche: 1 [1884/96 (1962%)]\tLoss: 143.697281\n",
      "Train Epoche: 1 [1885/96 (1964%)]\tLoss: 3.833507\n",
      "Train Epoche: 1 [1886/96 (1965%)]\tLoss: 0.922560\n",
      "Train Epoche: 1 [1887/96 (1966%)]\tLoss: 573.997681\n",
      "Train Epoche: 1 [1888/96 (1967%)]\tLoss: 120.314377\n",
      "Train Epoche: 1 [1889/96 (1968%)]\tLoss: 575.713745\n",
      "Train Epoche: 1 [1890/96 (1969%)]\tLoss: 167.404083\n",
      "Train Epoche: 1 [1891/96 (1970%)]\tLoss: 576.717163\n",
      "Train Epoche: 1 [1892/96 (1971%)]\tLoss: 399.928986\n",
      "Train Epoche: 1 [1893/96 (1972%)]\tLoss: 288.924835\n",
      "Train Epoche: 1 [1894/96 (1973%)]\tLoss: 256.147980\n",
      "Train Epoche: 1 [1895/96 (1974%)]\tLoss: 24.624586\n",
      "Train Epoche: 1 [1896/96 (1975%)]\tLoss: 49.151062\n",
      "Train Epoche: 1 [1897/96 (1976%)]\tLoss: 323.030579\n",
      "Train Epoche: 1 [1898/96 (1977%)]\tLoss: 363.774750\n",
      "Train Epoche: 1 [1899/96 (1978%)]\tLoss: 24.902506\n",
      "Train Epoche: 1 [1900/96 (1979%)]\tLoss: 81.006676\n",
      "Train Epoche: 1 [1901/96 (1980%)]\tLoss: 48.832363\n",
      "Train Epoche: 1 [1902/96 (1981%)]\tLoss: 255.513687\n",
      "Train Epoche: 1 [1903/96 (1982%)]\tLoss: 144.273743\n",
      "Train Epoche: 1 [1904/96 (1983%)]\tLoss: 168.618011\n",
      "Train Epoche: 1 [1905/96 (1984%)]\tLoss: 120.343018\n",
      "Train Epoche: 1 [1906/96 (1985%)]\tLoss: 36.068676\n",
      "Train Epoche: 1 [1907/96 (1986%)]\tLoss: 482.216766\n",
      "Train Epoche: 1 [1908/96 (1988%)]\tLoss: 15.573075\n",
      "Train Epoche: 1 [1909/96 (1989%)]\tLoss: 483.195343\n",
      "Train Epoche: 1 [1910/96 (1990%)]\tLoss: 63.588997\n",
      "Train Epoche: 1 [1911/96 (1991%)]\tLoss: 8.969691\n",
      "Train Epoche: 1 [1912/96 (1992%)]\tLoss: 484.735291\n",
      "Train Epoche: 1 [1913/96 (1993%)]\tLoss: 195.307327\n",
      "Train Epoche: 1 [1914/96 (1994%)]\tLoss: 224.606323\n",
      "Train Epoche: 1 [1915/96 (1995%)]\tLoss: 0.903179\n",
      "Train Epoche: 1 [1916/96 (1996%)]\tLoss: 3.888194\n",
      "Train Epoche: 1 [1917/96 (1997%)]\tLoss: 290.615662\n",
      "Train Epoche: 1 [1918/96 (1998%)]\tLoss: 324.131287\n",
      "Train Epoche: 1 [1919/96 (1999%)]\tLoss: 102.652458\n",
      "Train Epoche: 1 [1920/96 (2000%)]\tLoss: 490.102295\n",
      "Train Epoche: 1 [1921/96 (2001%)]\tLoss: 63.536964\n",
      "Train Epoche: 1 [1922/96 (2002%)]\tLoss: 80.340126\n",
      "Train Epoche: 1 [1923/96 (2003%)]\tLoss: 573.476379\n",
      "Train Epoche: 1 [1924/96 (2004%)]\tLoss: 0.910885\n",
      "Train Epoche: 1 [1925/96 (2005%)]\tLoss: 168.148605\n",
      "Train Epoche: 1 [1926/96 (2006%)]\tLoss: 143.140320\n",
      "Train Epoche: 1 [1927/96 (2007%)]\tLoss: 3.800430\n",
      "Train Epoche: 1 [1928/96 (2008%)]\tLoss: 224.058762\n",
      "Train Epoche: 1 [1929/96 (2009%)]\tLoss: 99.436775\n",
      "Train Epoche: 1 [1930/96 (2010%)]\tLoss: 194.958450\n",
      "Train Epoche: 1 [1931/96 (2011%)]\tLoss: 119.989876\n",
      "Train Epoche: 1 [1932/96 (2012%)]\tLoss: 35.505398\n",
      "Train Epoche: 1 [1933/96 (2014%)]\tLoss: 24.744598\n",
      "Train Epoche: 1 [1934/96 (2015%)]\tLoss: 574.258240\n",
      "Train Epoche: 1 [1935/96 (2016%)]\tLoss: 48.668926\n",
      "Train Epoche: 1 [1936/96 (2017%)]\tLoss: 575.932800\n",
      "Train Epoche: 1 [1937/96 (2018%)]\tLoss: 361.376404\n",
      "Train Epoche: 1 [1938/96 (2019%)]\tLoss: 575.557312\n",
      "Train Epoche: 1 [1939/96 (2020%)]\tLoss: 256.756470\n",
      "Train Epoche: 1 [1940/96 (2021%)]\tLoss: 288.091919\n",
      "Train Epoche: 1 [1941/96 (2022%)]\tLoss: 8.820839\n",
      "Train Epoche: 1 [1942/96 (2023%)]\tLoss: 15.895992\n",
      "Train Epoche: 1 [1943/96 (2024%)]\tLoss: 324.938141\n",
      "Train Epoche: 1 [1944/96 (2025%)]\tLoss: 576.162964\n",
      "Train Epoche: 1 [1945/96 (2026%)]\tLoss: 288.001862\n",
      "Train Epoche: 1 [1946/96 (2027%)]\tLoss: 398.365021\n",
      "Train Epoche: 1 [1947/96 (2028%)]\tLoss: 63.280075\n",
      "Train Epoche: 1 [1948/96 (2029%)]\tLoss: 48.375965\n",
      "Train Epoche: 1 [1949/96 (2030%)]\tLoss: 167.911697\n",
      "Train Epoche: 1 [1950/96 (2031%)]\tLoss: 399.292084\n",
      "Train Epoche: 1 [1951/96 (2032%)]\tLoss: 143.114532\n",
      "Train Epoche: 1 [1952/96 (2033%)]\tLoss: 195.261215\n",
      "Train Epoche: 1 [1953/96 (2034%)]\tLoss: 24.685146\n",
      "Train Epoche: 1 [1954/96 (2035%)]\tLoss: 8.746202\n",
      "Train Epoche: 1 [1955/96 (2036%)]\tLoss: 80.130241\n",
      "Train Epoche: 1 [1956/96 (2038%)]\tLoss: 35.577915\n",
      "Train Epoche: 1 [1957/96 (2039%)]\tLoss: 15.695384\n",
      "Train Epoche: 1 [1958/96 (2040%)]\tLoss: 99.061050\n",
      "Train Epoche: 1 [1959/96 (2041%)]\tLoss: 322.208679\n",
      "Train Epoche: 1 [1960/96 (2042%)]\tLoss: 254.667480\n",
      "Train Epoche: 1 [1961/96 (2043%)]\tLoss: 0.921221\n",
      "Train Epoche: 1 [1962/96 (2044%)]\tLoss: 4.067694\n",
      "Train Epoche: 1 [1963/96 (2045%)]\tLoss: 225.002777\n",
      "Train Epoche: 1 [1964/96 (2046%)]\tLoss: 121.505974\n",
      "Train Epoche: 1 [1965/96 (2047%)]\tLoss: 0.912617\n",
      "Train Epoche: 1 [1966/96 (2048%)]\tLoss: 8.691526\n",
      "Train Epoche: 1 [1967/96 (2049%)]\tLoss: 143.142517\n",
      "Train Epoche: 1 [1968/96 (2050%)]\tLoss: 194.959625\n",
      "Train Epoche: 1 [1969/96 (2051%)]\tLoss: 167.847534\n",
      "Train Epoche: 1 [1970/96 (2052%)]\tLoss: 255.078552\n",
      "Train Epoche: 1 [1971/96 (2053%)]\tLoss: 574.197266\n",
      "Train Epoche: 1 [1972/96 (2054%)]\tLoss: 223.642578\n",
      "Train Epoche: 1 [1973/96 (2055%)]\tLoss: 3.861223\n",
      "Train Epoche: 1 [1974/96 (2056%)]\tLoss: 24.701998\n",
      "Train Epoche: 1 [1975/96 (2057%)]\tLoss: 63.403778\n",
      "Train Epoche: 1 [1976/96 (2058%)]\tLoss: 80.728729\n",
      "Train Epoche: 1 [1977/96 (2059%)]\tLoss: 15.818377\n",
      "Train Epoche: 1 [1978/96 (2060%)]\tLoss: 573.604370\n",
      "Train Epoche: 1 [1979/96 (2061%)]\tLoss: 99.316093\n",
      "Train Epoche: 1 [1980/96 (2062%)]\tLoss: 120.251747\n",
      "Train Epoche: 1 [1981/96 (2064%)]\tLoss: 36.549187\n",
      "Train Epoche: 1 [1982/96 (2065%)]\tLoss: 48.626480\n",
      "Train Epoche: 1 [1983/96 (2066%)]\tLoss: 289.029968\n",
      "Train Epoche: 1 [1984/96 (2067%)]\tLoss: 324.268738\n",
      "Train Epoche: 1 [1985/96 (2068%)]\tLoss: 359.454742\n",
      "Train Epoche: 1 [1986/96 (2069%)]\tLoss: 579.389954\n",
      "Train Epoche: 1 [1987/96 (2070%)]\tLoss: 402.372040\n",
      "Train Epoche: 1 [1988/96 (2071%)]\tLoss: 576.062378\n",
      "Train Epoche: 1 [1989/96 (2072%)]\tLoss: 63.646030\n",
      "Train Epoche: 1 [1990/96 (2073%)]\tLoss: 80.525536\n",
      "Train Epoche: 1 [1991/96 (2074%)]\tLoss: 194.791077\n",
      "Train Epoche: 1 [1992/96 (2075%)]\tLoss: 254.933304\n",
      "Train Epoche: 1 [1993/96 (2076%)]\tLoss: 99.414352\n",
      "Train Epoche: 1 [1994/96 (2077%)]\tLoss: 398.581726\n",
      "Train Epoche: 1 [1995/96 (2078%)]\tLoss: 0.932268\n",
      "Train Epoche: 1 [1996/96 (2079%)]\tLoss: 8.773099\n",
      "Train Epoche: 1 [1997/96 (2080%)]\tLoss: 168.213562\n",
      "Train Epoche: 1 [1998/96 (2081%)]\tLoss: 48.696625\n",
      "Train Epoche: 1 [1999/96 (2082%)]\tLoss: 24.678867\n",
      "Train Epoche: 1 [2000/96 (2083%)]\tLoss: 15.577685\n",
      "Train Epoche: 1 [2001/96 (2084%)]\tLoss: 223.881653\n",
      "Train Epoche: 1 [2002/96 (2085%)]\tLoss: 120.485214\n",
      "Train Epoche: 1 [2003/96 (2086%)]\tLoss: 143.460510\n",
      "Train Epoche: 1 [2004/96 (2088%)]\tLoss: 35.229279\n",
      "Train Epoche: 1 [2005/96 (2089%)]\tLoss: 289.833282\n",
      "Train Epoche: 1 [2006/96 (2090%)]\tLoss: 440.018738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 1 [2007/96 (2091%)]\tLoss: 4.081321\n",
      "Train Epoche: 1 [2008/96 (2092%)]\tLoss: 485.300018\n",
      "Train Epoche: 1 [2009/96 (2093%)]\tLoss: 360.895569\n",
      "Train Epoche: 1 [2010/96 (2094%)]\tLoss: 323.293549\n",
      "Train Epoche: 1 [2011/96 (2095%)]\tLoss: 573.301086\n",
      "Train Epoche: 1 [2012/96 (2096%)]\tLoss: 8.772166\n",
      "Train Epoche: 1 [2013/96 (2097%)]\tLoss: 254.765717\n",
      "Train Epoche: 1 [2014/96 (2098%)]\tLoss: 99.543655\n",
      "Train Epoche: 1 [2015/96 (2099%)]\tLoss: 80.232643\n",
      "Train Epoche: 1 [2016/96 (2100%)]\tLoss: 167.640350\n",
      "Train Epoche: 1 [2017/96 (2101%)]\tLoss: 574.456543\n",
      "Train Epoche: 1 [2018/96 (2102%)]\tLoss: 15.657699\n",
      "Train Epoche: 1 [2019/96 (2103%)]\tLoss: 63.452629\n",
      "Train Epoche: 1 [2020/96 (2104%)]\tLoss: 48.572155\n",
      "Train Epoche: 1 [2021/96 (2105%)]\tLoss: 120.224434\n",
      "Train Epoche: 1 [2022/96 (2106%)]\tLoss: 3.912652\n",
      "Train Epoche: 1 [2023/96 (2107%)]\tLoss: 0.909120\n",
      "Train Epoche: 1 [2024/96 (2108%)]\tLoss: 142.727066\n",
      "Train Epoche: 1 [2025/96 (2109%)]\tLoss: 574.130066\n",
      "Train Epoche: 1 [2026/96 (2110%)]\tLoss: 36.156715\n",
      "Train Epoche: 1 [2027/96 (2111%)]\tLoss: 24.807648\n",
      "Train Epoche: 1 [2028/96 (2112%)]\tLoss: 224.801025\n",
      "Train Epoche: 1 [2029/96 (2114%)]\tLoss: 197.272629\n",
      "Train Epoche: 1 [2030/96 (2115%)]\tLoss: 325.169312\n",
      "Train Epoche: 1 [2031/96 (2116%)]\tLoss: 288.580688\n",
      "Train Epoche: 1 [2032/96 (2117%)]\tLoss: 364.119781\n",
      "Train Epoche: 1 [2033/96 (2118%)]\tLoss: 576.618958\n",
      "Train Epoche: 2 [0/96 (0%)]\tLoss: 399.136139\n",
      "Train Epoche: 2 [1/96 (1%)]\tLoss: 143.415268\n",
      "Train Epoche: 2 [2/96 (2%)]\tLoss: 81.054596\n",
      "Train Epoche: 2 [3/96 (3%)]\tLoss: 120.821732\n",
      "Train Epoche: 2 [4/96 (4%)]\tLoss: 398.190308\n",
      "Train Epoche: 2 [5/96 (5%)]\tLoss: 48.657589\n",
      "Train Epoche: 2 [6/96 (6%)]\tLoss: 100.131805\n",
      "Train Epoche: 2 [7/96 (7%)]\tLoss: 167.723694\n",
      "Train Epoche: 2 [8/96 (8%)]\tLoss: 8.728880\n",
      "Train Epoche: 2 [9/96 (9%)]\tLoss: 3.920014\n",
      "Train Epoche: 2 [10/96 (10%)]\tLoss: 63.632767\n",
      "Train Epoche: 2 [11/96 (11%)]\tLoss: 35.515572\n",
      "Train Epoche: 2 [12/96 (12%)]\tLoss: 398.713043\n",
      "Train Epoche: 2 [13/96 (14%)]\tLoss: 15.804997\n",
      "Train Epoche: 2 [14/96 (15%)]\tLoss: 223.973633\n",
      "Train Epoche: 2 [15/96 (16%)]\tLoss: 397.992950\n",
      "Train Epoche: 2 [16/96 (17%)]\tLoss: 1.106990\n",
      "Train Epoche: 2 [17/96 (18%)]\tLoss: 25.332783\n",
      "Train Epoche: 2 [18/96 (19%)]\tLoss: 197.397202\n",
      "Train Epoche: 2 [19/96 (20%)]\tLoss: 258.512482\n",
      "Train Epoche: 2 [20/96 (21%)]\tLoss: 63.270821\n",
      "Train Epoche: 2 [21/96 (22%)]\tLoss: 322.848206\n",
      "Train Epoche: 2 [22/96 (23%)]\tLoss: 481.955872\n",
      "Train Epoche: 2 [23/96 (24%)]\tLoss: 574.635864\n",
      "Train Epoche: 2 [24/96 (25%)]\tLoss: 223.793442\n",
      "Train Epoche: 2 [25/96 (26%)]\tLoss: 195.007385\n",
      "Train Epoche: 2 [26/96 (27%)]\tLoss: 48.365120\n",
      "Train Epoche: 2 [27/96 (28%)]\tLoss: 80.655121\n",
      "Train Epoche: 2 [28/96 (29%)]\tLoss: 143.319839\n",
      "Train Epoche: 2 [29/96 (30%)]\tLoss: 35.410969\n",
      "Train Epoche: 2 [30/96 (31%)]\tLoss: 15.650842\n",
      "Train Epoche: 2 [31/96 (32%)]\tLoss: 0.913132\n",
      "Train Epoche: 2 [32/96 (33%)]\tLoss: 168.024338\n",
      "Train Epoche: 2 [33/96 (34%)]\tLoss: 120.379745\n",
      "Train Epoche: 2 [34/96 (35%)]\tLoss: 24.702505\n",
      "Train Epoche: 2 [35/96 (36%)]\tLoss: 99.021378\n",
      "Train Epoche: 2 [36/96 (38%)]\tLoss: 397.749817\n",
      "Train Epoche: 2 [37/96 (39%)]\tLoss: 440.880737\n",
      "Train Epoche: 2 [38/96 (40%)]\tLoss: 294.067596\n",
      "Train Epoche: 2 [39/96 (41%)]\tLoss: 257.565125\n",
      "Train Epoche: 2 [40/96 (42%)]\tLoss: 4.185451\n",
      "Train Epoche: 2 [41/96 (43%)]\tLoss: 8.911420\n",
      "Train Epoche: 2 [42/96 (44%)]\tLoss: 362.567749\n",
      "Train Epoche: 2 [43/96 (45%)]\tLoss: 574.965088\n",
      "Train Epoche: 2 [44/96 (46%)]\tLoss: 397.937622\n",
      "Train Epoche: 2 [45/96 (47%)]\tLoss: 194.913742\n",
      "Train Epoche: 2 [46/96 (48%)]\tLoss: 63.472111\n",
      "Train Epoche: 2 [47/96 (49%)]\tLoss: 120.576935\n",
      "Train Epoche: 2 [48/96 (50%)]\tLoss: 35.369289\n",
      "Train Epoche: 2 [49/96 (51%)]\tLoss: 167.781403\n",
      "Train Epoche: 2 [50/96 (52%)]\tLoss: 143.105667\n",
      "Train Epoche: 2 [51/96 (53%)]\tLoss: 99.382652\n",
      "Train Epoche: 2 [52/96 (54%)]\tLoss: 15.739103\n",
      "Train Epoche: 2 [53/96 (55%)]\tLoss: 3.819346\n",
      "Train Epoche: 2 [54/96 (56%)]\tLoss: 287.401642\n",
      "Train Epoche: 2 [55/96 (57%)]\tLoss: 80.045166\n",
      "Train Epoche: 2 [56/96 (58%)]\tLoss: 8.786283\n",
      "Train Epoche: 2 [57/96 (59%)]\tLoss: 398.071564\n",
      "Train Epoche: 2 [58/96 (60%)]\tLoss: 254.841125\n",
      "Train Epoche: 2 [59/96 (61%)]\tLoss: 398.700012\n",
      "Train Epoche: 2 [60/96 (62%)]\tLoss: 0.894254\n",
      "Train Epoche: 2 [61/96 (64%)]\tLoss: 24.562355\n",
      "Train Epoche: 2 [62/96 (65%)]\tLoss: 48.850517\n",
      "Train Epoche: 2 [63/96 (66%)]\tLoss: 224.384079\n",
      "Train Epoche: 2 [64/96 (67%)]\tLoss: 15.732448\n",
      "Train Epoche: 2 [65/96 (68%)]\tLoss: 8.837481\n",
      "Train Epoche: 2 [66/96 (69%)]\tLoss: 99.051903\n",
      "Train Epoche: 2 [67/96 (70%)]\tLoss: 167.642487\n",
      "Train Epoche: 2 [68/96 (71%)]\tLoss: 119.960060\n",
      "Train Epoche: 2 [69/96 (72%)]\tLoss: 35.511200\n",
      "Train Epoche: 2 [70/96 (73%)]\tLoss: 48.529434\n",
      "Train Epoche: 2 [71/96 (74%)]\tLoss: 63.591434\n",
      "Train Epoche: 2 [72/96 (75%)]\tLoss: 223.873917\n",
      "Train Epoche: 2 [73/96 (76%)]\tLoss: 24.522953\n",
      "Train Epoche: 2 [74/96 (77%)]\tLoss: 482.770050\n",
      "Train Epoche: 2 [75/96 (78%)]\tLoss: 255.662460\n",
      "Train Epoche: 2 [76/96 (79%)]\tLoss: 195.008179\n",
      "Train Epoche: 2 [77/96 (80%)]\tLoss: 4.106668\n",
      "Train Epoche: 2 [78/96 (81%)]\tLoss: 0.998146\n",
      "Train Epoche: 2 [79/96 (82%)]\tLoss: 287.572998\n",
      "Train Epoche: 2 [80/96 (83%)]\tLoss: 143.541687\n",
      "Train Epoche: 2 [81/96 (84%)]\tLoss: 120.529541\n",
      "Train Epoche: 2 [82/96 (85%)]\tLoss: 49.000881\n",
      "Train Epoche: 2 [83/96 (86%)]\tLoss: 15.683100\n",
      "Train Epoche: 2 [84/96 (88%)]\tLoss: 8.849119\n",
      "Train Epoche: 2 [85/96 (89%)]\tLoss: 482.989349\n",
      "Train Epoche: 2 [86/96 (90%)]\tLoss: 483.484924\n",
      "Train Epoche: 2 [87/96 (91%)]\tLoss: 24.895777\n",
      "Train Epoche: 2 [88/96 (92%)]\tLoss: 100.023979\n",
      "Train Epoche: 2 [89/96 (93%)]\tLoss: 80.333900\n",
      "Train Epoche: 2 [90/96 (94%)]\tLoss: 35.903370\n",
      "Train Epoche: 2 [91/96 (95%)]\tLoss: 482.806580\n",
      "Train Epoche: 2 [92/96 (96%)]\tLoss: 63.732655\n",
      "Train Epoche: 2 [93/96 (97%)]\tLoss: 167.937256\n",
      "Train Epoche: 2 [94/96 (98%)]\tLoss: 359.994049\n",
      "Train Epoche: 2 [95/96 (99%)]\tLoss: 3.812162\n",
      "Train Epoche: 2 [96/96 (100%)]\tLoss: 0.973403\n",
      "Train Epoche: 2 [97/96 (101%)]\tLoss: 255.797409\n",
      "Train Epoche: 2 [98/96 (102%)]\tLoss: 324.466949\n",
      "Train Epoche: 2 [99/96 (103%)]\tLoss: 193.997040\n",
      "Train Epoche: 2 [100/96 (104%)]\tLoss: 143.658707\n",
      "Train Epoche: 2 [101/96 (105%)]\tLoss: 288.846710\n",
      "Train Epoche: 2 [102/96 (106%)]\tLoss: 227.166779\n",
      "Train Epoche: 2 [103/96 (107%)]\tLoss: 142.643219\n",
      "Train Epoche: 2 [104/96 (108%)]\tLoss: 120.321175\n",
      "Train Epoche: 2 [105/96 (109%)]\tLoss: 254.558456\n",
      "Train Epoche: 2 [106/96 (110%)]\tLoss: 194.647491\n",
      "Train Epoche: 2 [107/96 (111%)]\tLoss: 223.540253\n",
      "Train Epoche: 2 [108/96 (112%)]\tLoss: 35.510246\n",
      "Train Epoche: 2 [109/96 (114%)]\tLoss: 3.834926\n",
      "Train Epoche: 2 [110/96 (115%)]\tLoss: 63.446301\n",
      "Train Epoche: 2 [111/96 (116%)]\tLoss: 99.151306\n",
      "Train Epoche: 2 [112/96 (117%)]\tLoss: 48.466003\n",
      "Train Epoche: 2 [113/96 (118%)]\tLoss: 15.674435\n",
      "Train Epoche: 2 [114/96 (119%)]\tLoss: 0.915681\n",
      "Train Epoche: 2 [115/96 (120%)]\tLoss: 482.778778\n",
      "Train Epoche: 2 [116/96 (121%)]\tLoss: 398.438965\n",
      "Train Epoche: 2 [117/96 (122%)]\tLoss: 8.952907\n",
      "Train Epoche: 2 [118/96 (123%)]\tLoss: 24.880753\n",
      "Train Epoche: 2 [119/96 (124%)]\tLoss: 325.500885\n",
      "Train Epoche: 2 [120/96 (125%)]\tLoss: 480.608398\n",
      "Train Epoche: 2 [121/96 (126%)]\tLoss: 80.749222\n",
      "Train Epoche: 2 [122/96 (127%)]\tLoss: 170.171951\n",
      "Train Epoche: 2 [123/96 (128%)]\tLoss: 361.591156\n",
      "Train Epoche: 2 [124/96 (129%)]\tLoss: 288.832764\n",
      "Train Epoche: 2 [125/96 (130%)]\tLoss: 80.391991\n",
      "Train Epoche: 2 [126/96 (131%)]\tLoss: 143.371857\n",
      "Train Epoche: 2 [127/96 (132%)]\tLoss: 99.124451\n",
      "Train Epoche: 2 [128/96 (133%)]\tLoss: 323.326752\n",
      "Train Epoche: 2 [129/96 (134%)]\tLoss: 35.676395\n",
      "Train Epoche: 2 [130/96 (135%)]\tLoss: 399.016113\n",
      "Train Epoche: 2 [131/96 (136%)]\tLoss: 255.303146\n",
      "Train Epoche: 2 [132/96 (138%)]\tLoss: 224.098053\n",
      "Train Epoche: 2 [133/96 (139%)]\tLoss: 15.754145\n",
      "Train Epoche: 2 [134/96 (140%)]\tLoss: 8.850109\n",
      "Train Epoche: 2 [135/96 (141%)]\tLoss: 48.418106\n",
      "Train Epoche: 2 [136/96 (142%)]\tLoss: 63.466843\n",
      "Train Epoche: 2 [137/96 (143%)]\tLoss: 398.306610\n",
      "Train Epoche: 2 [138/96 (144%)]\tLoss: 24.812864\n",
      "Train Epoche: 2 [139/96 (145%)]\tLoss: 288.017059\n",
      "Train Epoche: 2 [140/96 (146%)]\tLoss: 195.257507\n",
      "Train Epoche: 2 [141/96 (147%)]\tLoss: 3.944568\n",
      "Train Epoche: 2 [142/96 (148%)]\tLoss: 0.940725\n",
      "Train Epoche: 2 [143/96 (149%)]\tLoss: 120.705772\n",
      "Train Epoche: 2 [144/96 (150%)]\tLoss: 169.540878\n",
      "Train Epoche: 2 [145/96 (151%)]\tLoss: 15.785320\n",
      "Train Epoche: 2 [146/96 (152%)]\tLoss: 35.582985\n",
      "Train Epoche: 2 [147/96 (153%)]\tLoss: 255.055893\n",
      "Train Epoche: 2 [148/96 (154%)]\tLoss: 482.423950\n",
      "Train Epoche: 2 [149/96 (155%)]\tLoss: 99.463837\n",
      "Train Epoche: 2 [150/96 (156%)]\tLoss: 224.288940\n",
      "Train Epoche: 2 [151/96 (157%)]\tLoss: 8.759625\n",
      "Train Epoche: 2 [152/96 (158%)]\tLoss: 48.348644\n",
      "Train Epoche: 2 [153/96 (159%)]\tLoss: 168.322540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [154/96 (160%)]\tLoss: 119.821579\n",
      "Train Epoche: 2 [155/96 (161%)]\tLoss: 3.862595\n",
      "Train Epoche: 2 [156/96 (162%)]\tLoss: 0.908967\n",
      "Train Epoche: 2 [157/96 (164%)]\tLoss: 63.547768\n",
      "Train Epoche: 2 [158/96 (165%)]\tLoss: 143.393524\n",
      "Train Epoche: 2 [159/96 (166%)]\tLoss: 80.283409\n",
      "Train Epoche: 2 [160/96 (167%)]\tLoss: 24.970417\n",
      "Train Epoche: 2 [161/96 (168%)]\tLoss: 485.893494\n",
      "Train Epoche: 2 [162/96 (169%)]\tLoss: 324.546051\n",
      "Train Epoche: 2 [163/96 (170%)]\tLoss: 198.051361\n",
      "Train Epoche: 2 [164/96 (171%)]\tLoss: 482.949432\n",
      "Train Epoche: 2 [165/96 (172%)]\tLoss: 364.041595\n",
      "Train Epoche: 2 [166/96 (173%)]\tLoss: 291.748474\n",
      "Train Epoche: 2 [167/96 (174%)]\tLoss: 24.691851\n",
      "Train Epoche: 2 [168/96 (175%)]\tLoss: 3.879364\n",
      "Train Epoche: 2 [169/96 (176%)]\tLoss: 168.256851\n",
      "Train Epoche: 2 [170/96 (177%)]\tLoss: 120.469620\n",
      "Train Epoche: 2 [171/96 (178%)]\tLoss: 288.815735\n",
      "Train Epoche: 2 [172/96 (179%)]\tLoss: 224.160187\n",
      "Train Epoche: 2 [173/96 (180%)]\tLoss: 143.290604\n",
      "Train Epoche: 2 [174/96 (181%)]\tLoss: 440.128693\n",
      "Train Epoche: 2 [175/96 (182%)]\tLoss: 15.688886\n",
      "Train Epoche: 2 [176/96 (183%)]\tLoss: 80.568916\n",
      "Train Epoche: 2 [177/96 (184%)]\tLoss: 63.715176\n",
      "Train Epoche: 2 [178/96 (185%)]\tLoss: 35.789978\n",
      "Train Epoche: 2 [179/96 (186%)]\tLoss: 8.931535\n",
      "Train Epoche: 2 [180/96 (188%)]\tLoss: 0.924707\n",
      "Train Epoche: 2 [181/96 (189%)]\tLoss: 195.211777\n",
      "Train Epoche: 2 [182/96 (190%)]\tLoss: 99.246078\n",
      "Train Epoche: 2 [183/96 (191%)]\tLoss: 48.508984\n",
      "Train Epoche: 2 [184/96 (192%)]\tLoss: 574.741211\n",
      "Train Epoche: 2 [185/96 (193%)]\tLoss: 256.338867\n",
      "Train Epoche: 2 [186/96 (194%)]\tLoss: 577.587891\n",
      "Train Epoche: 2 [187/96 (195%)]\tLoss: 574.471191\n",
      "Train Epoche: 2 [188/96 (196%)]\tLoss: 324.216400\n",
      "Train Epoche: 2 [189/96 (197%)]\tLoss: 400.598297\n",
      "Train Epoche: 2 [190/96 (198%)]\tLoss: 361.097992\n",
      "Train Epoche: 2 [191/96 (199%)]\tLoss: 15.708257\n",
      "Train Epoche: 2 [192/96 (200%)]\tLoss: 80.734215\n",
      "Train Epoche: 2 [193/96 (201%)]\tLoss: 143.140121\n",
      "Train Epoche: 2 [194/96 (202%)]\tLoss: 48.557861\n",
      "Train Epoche: 2 [195/96 (203%)]\tLoss: 63.321213\n",
      "Train Epoche: 2 [196/96 (204%)]\tLoss: 482.833740\n",
      "Train Epoche: 2 [197/96 (205%)]\tLoss: 35.482147\n",
      "Train Epoche: 2 [198/96 (206%)]\tLoss: 99.006142\n",
      "Train Epoche: 2 [199/96 (207%)]\tLoss: 24.617043\n",
      "Train Epoche: 2 [200/96 (208%)]\tLoss: 120.250259\n",
      "Train Epoche: 2 [201/96 (209%)]\tLoss: 8.751689\n",
      "Train Epoche: 2 [202/96 (210%)]\tLoss: 0.901460\n",
      "Train Epoche: 2 [203/96 (211%)]\tLoss: 168.663422\n",
      "Train Epoche: 2 [204/96 (212%)]\tLoss: 194.792206\n",
      "Train Epoche: 2 [205/96 (214%)]\tLoss: 484.426270\n",
      "Train Epoche: 2 [206/96 (215%)]\tLoss: 3.965903\n",
      "Train Epoche: 2 [207/96 (216%)]\tLoss: 482.526337\n",
      "Train Epoche: 2 [208/96 (217%)]\tLoss: 482.150757\n",
      "Train Epoche: 2 [209/96 (218%)]\tLoss: 487.042450\n",
      "Train Epoche: 2 [210/96 (219%)]\tLoss: 484.586548\n",
      "Train Epoche: 2 [211/96 (220%)]\tLoss: 80.392105\n",
      "Train Epoche: 2 [212/96 (221%)]\tLoss: 142.873032\n",
      "Train Epoche: 2 [213/96 (222%)]\tLoss: 398.834686\n",
      "Train Epoche: 2 [214/96 (223%)]\tLoss: 9.043054\n",
      "Train Epoche: 2 [215/96 (224%)]\tLoss: 398.695526\n",
      "Train Epoche: 2 [216/96 (225%)]\tLoss: 398.628876\n",
      "Train Epoche: 2 [217/96 (226%)]\tLoss: 399.612976\n",
      "Train Epoche: 2 [218/96 (227%)]\tLoss: 64.236732\n",
      "Train Epoche: 2 [219/96 (228%)]\tLoss: 196.026779\n",
      "Train Epoche: 2 [220/96 (229%)]\tLoss: 16.045069\n",
      "Train Epoche: 2 [221/96 (230%)]\tLoss: 399.605652\n",
      "Train Epoche: 2 [222/96 (231%)]\tLoss: 35.460991\n",
      "Train Epoche: 2 [223/96 (232%)]\tLoss: 0.936773\n",
      "Train Epoche: 2 [224/96 (233%)]\tLoss: 397.842804\n",
      "Train Epoche: 2 [225/96 (234%)]\tLoss: 120.881615\n",
      "Train Epoche: 2 [226/96 (235%)]\tLoss: 100.097473\n",
      "Train Epoche: 2 [227/96 (236%)]\tLoss: 24.428675\n",
      "Train Epoche: 2 [228/96 (238%)]\tLoss: 3.973847\n",
      "Train Epoche: 2 [229/96 (239%)]\tLoss: 168.578964\n",
      "Train Epoche: 2 [230/96 (240%)]\tLoss: 49.796291\n",
      "Train Epoche: 2 [231/96 (241%)]\tLoss: 24.507725\n",
      "Train Epoche: 2 [232/96 (242%)]\tLoss: 0.910205\n",
      "Train Epoche: 2 [233/96 (243%)]\tLoss: 287.185944\n",
      "Train Epoche: 2 [234/96 (244%)]\tLoss: 194.662949\n",
      "Train Epoche: 2 [235/96 (245%)]\tLoss: 80.401260\n",
      "Train Epoche: 2 [236/96 (246%)]\tLoss: 254.886536\n",
      "Train Epoche: 2 [237/96 (247%)]\tLoss: 573.818787\n",
      "Train Epoche: 2 [238/96 (248%)]\tLoss: 223.870407\n",
      "Train Epoche: 2 [239/96 (249%)]\tLoss: 3.880106\n",
      "Train Epoche: 2 [240/96 (250%)]\tLoss: 48.482445\n",
      "Train Epoche: 2 [241/96 (251%)]\tLoss: 119.960251\n",
      "Train Epoche: 2 [242/96 (252%)]\tLoss: 143.055374\n",
      "Train Epoche: 2 [243/96 (253%)]\tLoss: 15.628701\n",
      "Train Epoche: 2 [244/96 (254%)]\tLoss: 8.755118\n",
      "Train Epoche: 2 [245/96 (255%)]\tLoss: 167.907791\n",
      "Train Epoche: 2 [246/96 (256%)]\tLoss: 63.223354\n",
      "Train Epoche: 2 [247/96 (257%)]\tLoss: 99.762314\n",
      "Train Epoche: 2 [248/96 (258%)]\tLoss: 35.764759\n",
      "Train Epoche: 2 [249/96 (259%)]\tLoss: 324.648712\n",
      "Train Epoche: 2 [250/96 (260%)]\tLoss: 361.350891\n",
      "Train Epoche: 2 [251/96 (261%)]\tLoss: 399.924774\n",
      "Train Epoche: 2 [252/96 (262%)]\tLoss: 439.565552\n",
      "Train Epoche: 2 [253/96 (264%)]\tLoss: 527.875610\n",
      "Train Epoche: 2 [254/96 (265%)]\tLoss: 483.023621\n",
      "Train Epoche: 2 [255/96 (266%)]\tLoss: 120.251411\n",
      "Train Epoche: 2 [256/96 (267%)]\tLoss: 194.993179\n",
      "Train Epoche: 2 [257/96 (268%)]\tLoss: 99.607986\n",
      "Train Epoche: 2 [258/96 (269%)]\tLoss: 398.893280\n",
      "Train Epoche: 2 [259/96 (270%)]\tLoss: 398.335693\n",
      "Train Epoche: 2 [260/96 (271%)]\tLoss: 144.079132\n",
      "Train Epoche: 2 [261/96 (272%)]\tLoss: 168.180191\n",
      "Train Epoche: 2 [262/96 (273%)]\tLoss: 24.554968\n",
      "Train Epoche: 2 [263/96 (274%)]\tLoss: 398.299774\n",
      "Train Epoche: 2 [264/96 (275%)]\tLoss: 48.373707\n",
      "Train Epoche: 2 [265/96 (276%)]\tLoss: 35.634510\n",
      "Train Epoche: 2 [266/96 (277%)]\tLoss: 8.921285\n",
      "Train Epoche: 2 [267/96 (278%)]\tLoss: 3.916138\n",
      "Train Epoche: 2 [268/96 (279%)]\tLoss: 398.052765\n",
      "Train Epoche: 2 [269/96 (280%)]\tLoss: 223.730072\n",
      "Train Epoche: 2 [270/96 (281%)]\tLoss: 0.905068\n",
      "Train Epoche: 2 [271/96 (282%)]\tLoss: 15.818640\n",
      "Train Epoche: 2 [272/96 (283%)]\tLoss: 81.208511\n",
      "Train Epoche: 2 [273/96 (284%)]\tLoss: 64.827255\n",
      "Train Epoche: 2 [274/96 (285%)]\tLoss: 15.689120\n",
      "Train Epoche: 2 [275/96 (286%)]\tLoss: 0.896282\n",
      "Train Epoche: 2 [276/96 (288%)]\tLoss: 168.362793\n",
      "Train Epoche: 2 [277/96 (289%)]\tLoss: 254.456375\n",
      "Train Epoche: 2 [278/96 (290%)]\tLoss: 574.532959\n",
      "Train Epoche: 2 [279/96 (291%)]\tLoss: 143.432571\n",
      "Train Epoche: 2 [280/96 (292%)]\tLoss: 63.443542\n",
      "Train Epoche: 2 [281/96 (293%)]\tLoss: 99.085449\n",
      "Train Epoche: 2 [282/96 (294%)]\tLoss: 8.771957\n",
      "Train Epoche: 2 [283/96 (295%)]\tLoss: 35.481754\n",
      "Train Epoche: 2 [284/96 (296%)]\tLoss: 194.626678\n",
      "Train Epoche: 2 [285/96 (297%)]\tLoss: 48.553879\n",
      "Train Epoche: 2 [286/96 (298%)]\tLoss: 24.470554\n",
      "Train Epoche: 2 [287/96 (299%)]\tLoss: 3.832319\n",
      "Train Epoche: 2 [288/96 (300%)]\tLoss: 120.190300\n",
      "Train Epoche: 2 [289/96 (301%)]\tLoss: 223.528671\n",
      "Train Epoche: 2 [290/96 (302%)]\tLoss: 80.841583\n",
      "Train Epoche: 2 [291/96 (303%)]\tLoss: 575.960083\n",
      "Train Epoche: 2 [292/96 (304%)]\tLoss: 574.609863\n",
      "Train Epoche: 2 [293/96 (305%)]\tLoss: 575.252258\n",
      "Train Epoche: 2 [294/96 (306%)]\tLoss: 289.373779\n",
      "Train Epoche: 2 [295/96 (307%)]\tLoss: 361.544739\n",
      "Train Epoche: 2 [296/96 (308%)]\tLoss: 398.705811\n",
      "Train Epoche: 2 [297/96 (309%)]\tLoss: 324.240295\n",
      "Train Epoche: 2 [298/96 (310%)]\tLoss: 98.985718\n",
      "Train Epoche: 2 [299/96 (311%)]\tLoss: 143.283234\n",
      "Train Epoche: 2 [300/96 (312%)]\tLoss: 194.571045\n",
      "Train Epoche: 2 [301/96 (314%)]\tLoss: 224.202347\n",
      "Train Epoche: 2 [302/96 (315%)]\tLoss: 48.432434\n",
      "Train Epoche: 2 [303/96 (316%)]\tLoss: 482.077118\n",
      "Train Epoche: 2 [304/96 (317%)]\tLoss: 3.826592\n",
      "Train Epoche: 2 [305/96 (318%)]\tLoss: 15.693049\n",
      "Train Epoche: 2 [306/96 (319%)]\tLoss: 254.624466\n",
      "Train Epoche: 2 [307/96 (320%)]\tLoss: 8.825425\n",
      "Train Epoche: 2 [308/96 (321%)]\tLoss: 0.937396\n",
      "Train Epoche: 2 [309/96 (322%)]\tLoss: 24.810114\n",
      "Train Epoche: 2 [310/96 (323%)]\tLoss: 167.972778\n",
      "Train Epoche: 2 [311/96 (324%)]\tLoss: 80.327011\n",
      "Train Epoche: 2 [312/96 (325%)]\tLoss: 36.064884\n",
      "Train Epoche: 2 [313/96 (326%)]\tLoss: 289.679504\n",
      "Train Epoche: 2 [314/96 (327%)]\tLoss: 323.271545\n",
      "Train Epoche: 2 [315/96 (328%)]\tLoss: 121.229324\n",
      "Train Epoche: 2 [316/96 (329%)]\tLoss: 64.357857\n",
      "Train Epoche: 2 [317/96 (330%)]\tLoss: 397.698486\n",
      "Train Epoche: 2 [318/96 (331%)]\tLoss: 360.462830\n",
      "Train Epoche: 2 [319/96 (332%)]\tLoss: 120.245949\n",
      "Train Epoche: 2 [320/96 (333%)]\tLoss: 48.468925\n",
      "Train Epoche: 2 [321/96 (334%)]\tLoss: 80.491074\n",
      "Train Epoche: 2 [322/96 (335%)]\tLoss: 63.431801\n",
      "Train Epoche: 2 [323/96 (336%)]\tLoss: 254.695969\n",
      "Train Epoche: 2 [324/96 (338%)]\tLoss: 224.086945\n",
      "Train Epoche: 2 [325/96 (339%)]\tLoss: 398.269012\n",
      "Train Epoche: 2 [326/96 (340%)]\tLoss: 194.908417\n",
      "Train Epoche: 2 [327/96 (341%)]\tLoss: 15.907553\n",
      "Train Epoche: 2 [328/96 (342%)]\tLoss: 35.503700\n",
      "Train Epoche: 2 [329/96 (343%)]\tLoss: 99.583389\n",
      "Train Epoche: 2 [330/96 (344%)]\tLoss: 8.745663\n",
      "Train Epoche: 2 [331/96 (345%)]\tLoss: 0.949997\n",
      "Train Epoche: 2 [332/96 (346%)]\tLoss: 322.635010\n",
      "Train Epoche: 2 [333/96 (347%)]\tLoss: 287.691528\n",
      "Train Epoche: 2 [334/96 (348%)]\tLoss: 3.820165\n",
      "Train Epoche: 2 [335/96 (349%)]\tLoss: 25.217297\n",
      "Train Epoche: 2 [336/96 (350%)]\tLoss: 168.175369\n",
      "Train Epoche: 2 [337/96 (351%)]\tLoss: 144.836975\n",
      "Train Epoche: 2 [338/96 (352%)]\tLoss: 48.429516\n",
      "Train Epoche: 2 [339/96 (353%)]\tLoss: 80.340027\n",
      "Train Epoche: 2 [340/96 (354%)]\tLoss: 167.747604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [341/96 (355%)]\tLoss: 24.655502\n",
      "Train Epoche: 2 [342/96 (356%)]\tLoss: 482.321228\n",
      "Train Epoche: 2 [343/96 (357%)]\tLoss: 99.438049\n",
      "Train Epoche: 2 [344/96 (358%)]\tLoss: 195.003143\n",
      "Train Epoche: 2 [345/96 (359%)]\tLoss: 120.009018\n",
      "Train Epoche: 2 [346/96 (360%)]\tLoss: 15.711614\n",
      "Train Epoche: 2 [347/96 (361%)]\tLoss: 63.574318\n",
      "Train Epoche: 2 [348/96 (362%)]\tLoss: 35.640255\n",
      "Train Epoche: 2 [349/96 (364%)]\tLoss: 0.911375\n",
      "Train Epoche: 2 [350/96 (365%)]\tLoss: 3.831638\n",
      "Train Epoche: 2 [351/96 (366%)]\tLoss: 143.116867\n",
      "Train Epoche: 2 [352/96 (367%)]\tLoss: 482.416504\n",
      "Train Epoche: 2 [353/96 (368%)]\tLoss: 484.728058\n",
      "Train Epoche: 2 [354/96 (369%)]\tLoss: 9.044634\n",
      "Train Epoche: 2 [355/96 (370%)]\tLoss: 225.408829\n",
      "Train Epoche: 2 [356/96 (371%)]\tLoss: 255.118652\n",
      "Train Epoche: 2 [357/96 (372%)]\tLoss: 484.412811\n",
      "Train Epoche: 2 [358/96 (373%)]\tLoss: 487.639862\n",
      "Train Epoche: 2 [359/96 (374%)]\tLoss: 15.892019\n",
      "Train Epoche: 2 [360/96 (375%)]\tLoss: 24.757475\n",
      "Train Epoche: 2 [361/96 (376%)]\tLoss: 120.855865\n",
      "Train Epoche: 2 [362/96 (377%)]\tLoss: 8.829941\n",
      "Train Epoche: 2 [363/96 (378%)]\tLoss: 168.717651\n",
      "Train Epoche: 2 [364/96 (379%)]\tLoss: 194.920258\n",
      "Train Epoche: 2 [365/96 (380%)]\tLoss: 35.921532\n",
      "Train Epoche: 2 [366/96 (381%)]\tLoss: 80.842476\n",
      "Train Epoche: 2 [367/96 (382%)]\tLoss: 143.240845\n",
      "Train Epoche: 2 [368/96 (383%)]\tLoss: 99.197975\n",
      "Train Epoche: 2 [369/96 (384%)]\tLoss: 63.333553\n",
      "Train Epoche: 2 [370/96 (385%)]\tLoss: 48.567562\n",
      "Train Epoche: 2 [371/96 (386%)]\tLoss: 254.135727\n",
      "Train Epoche: 2 [372/96 (388%)]\tLoss: 223.525742\n",
      "Train Epoche: 2 [373/96 (389%)]\tLoss: 0.920975\n",
      "Train Epoche: 2 [374/96 (390%)]\tLoss: 3.769124\n",
      "Train Epoche: 2 [375/96 (391%)]\tLoss: 486.858032\n",
      "Train Epoche: 2 [376/96 (392%)]\tLoss: 363.512146\n",
      "Train Epoche: 2 [377/96 (393%)]\tLoss: 290.584686\n",
      "Train Epoche: 2 [378/96 (394%)]\tLoss: 323.512726\n",
      "Train Epoche: 2 [379/96 (395%)]\tLoss: 485.752563\n",
      "Train Epoche: 2 [380/96 (396%)]\tLoss: 8.747909\n",
      "Train Epoche: 2 [381/96 (397%)]\tLoss: 0.913392\n",
      "Train Epoche: 2 [382/96 (398%)]\tLoss: 254.570755\n",
      "Train Epoche: 2 [383/96 (399%)]\tLoss: 167.976807\n",
      "Train Epoche: 2 [384/96 (400%)]\tLoss: 80.680511\n",
      "Train Epoche: 2 [385/96 (401%)]\tLoss: 120.007973\n",
      "Train Epoche: 2 [386/96 (402%)]\tLoss: 24.567327\n",
      "Train Epoche: 2 [387/96 (403%)]\tLoss: 573.550537\n",
      "Train Epoche: 2 [388/96 (404%)]\tLoss: 99.600349\n",
      "Train Epoche: 2 [389/96 (405%)]\tLoss: 15.650348\n",
      "Train Epoche: 2 [390/96 (406%)]\tLoss: 3.852483\n",
      "Train Epoche: 2 [391/96 (407%)]\tLoss: 35.341129\n",
      "Train Epoche: 2 [392/96 (408%)]\tLoss: 63.531517\n",
      "Train Epoche: 2 [393/96 (409%)]\tLoss: 143.604996\n",
      "Train Epoche: 2 [394/96 (410%)]\tLoss: 575.115112\n",
      "Train Epoche: 2 [395/96 (411%)]\tLoss: 581.113892\n",
      "Train Epoche: 2 [396/96 (412%)]\tLoss: 577.864502\n",
      "Train Epoche: 2 [397/96 (414%)]\tLoss: 49.380089\n",
      "Train Epoche: 2 [398/96 (415%)]\tLoss: 575.512695\n",
      "Train Epoche: 2 [399/96 (416%)]\tLoss: 199.726624\n",
      "Train Epoche: 2 [400/96 (417%)]\tLoss: 224.987610\n",
      "Train Epoche: 2 [401/96 (418%)]\tLoss: 99.518173\n",
      "Train Epoche: 2 [402/96 (419%)]\tLoss: 143.862701\n",
      "Train Epoche: 2 [403/96 (420%)]\tLoss: 24.651724\n",
      "Train Epoche: 2 [404/96 (421%)]\tLoss: 63.243225\n",
      "Train Epoche: 2 [405/96 (422%)]\tLoss: 80.269058\n",
      "Train Epoche: 2 [406/96 (423%)]\tLoss: 195.275208\n",
      "Train Epoche: 2 [407/96 (424%)]\tLoss: 3.780318\n",
      "Train Epoche: 2 [408/96 (425%)]\tLoss: 35.950806\n",
      "Train Epoche: 2 [409/96 (426%)]\tLoss: 481.905792\n",
      "Train Epoche: 2 [410/96 (427%)]\tLoss: 482.808685\n",
      "Train Epoche: 2 [411/96 (428%)]\tLoss: 48.781582\n",
      "Train Epoche: 2 [412/96 (429%)]\tLoss: 0.938371\n",
      "Train Epoche: 2 [413/96 (430%)]\tLoss: 120.500458\n",
      "Train Epoche: 2 [414/96 (431%)]\tLoss: 482.296692\n",
      "Train Epoche: 2 [415/96 (432%)]\tLoss: 9.061153\n",
      "Train Epoche: 2 [416/96 (433%)]\tLoss: 15.726267\n",
      "Train Epoche: 2 [417/96 (434%)]\tLoss: 486.254456\n",
      "Train Epoche: 2 [418/96 (435%)]\tLoss: 487.261871\n",
      "Train Epoche: 2 [419/96 (436%)]\tLoss: 485.130341\n",
      "Train Epoche: 2 [420/96 (438%)]\tLoss: 168.073715\n",
      "Train Epoche: 2 [421/96 (439%)]\tLoss: 255.529846\n",
      "Train Epoche: 2 [422/96 (440%)]\tLoss: 225.186523\n",
      "Train Epoche: 2 [423/96 (441%)]\tLoss: 35.539417\n",
      "Train Epoche: 2 [424/96 (442%)]\tLoss: 120.510803\n",
      "Train Epoche: 2 [425/96 (443%)]\tLoss: 288.075073\n",
      "Train Epoche: 2 [426/96 (444%)]\tLoss: 223.789932\n",
      "Train Epoche: 2 [427/96 (445%)]\tLoss: 99.322937\n",
      "Train Epoche: 2 [428/96 (446%)]\tLoss: 143.674393\n",
      "Train Epoche: 2 [429/96 (447%)]\tLoss: 3.845065\n",
      "Train Epoche: 2 [430/96 (448%)]\tLoss: 48.353996\n",
      "Train Epoche: 2 [431/96 (449%)]\tLoss: 80.323715\n",
      "Train Epoche: 2 [432/96 (450%)]\tLoss: 482.807526\n",
      "Train Epoche: 2 [433/96 (451%)]\tLoss: 24.505005\n",
      "Train Epoche: 2 [434/96 (452%)]\tLoss: 0.914802\n",
      "Train Epoche: 2 [435/96 (453%)]\tLoss: 167.858261\n",
      "Train Epoche: 2 [436/96 (454%)]\tLoss: 195.217667\n",
      "Train Epoche: 2 [437/96 (455%)]\tLoss: 8.670378\n",
      "Train Epoche: 2 [438/96 (456%)]\tLoss: 15.808339\n",
      "Train Epoche: 2 [439/96 (457%)]\tLoss: 483.467163\n",
      "Train Epoche: 2 [440/96 (458%)]\tLoss: 259.748108\n",
      "Train Epoche: 2 [441/96 (459%)]\tLoss: 484.952148\n",
      "Train Epoche: 2 [442/96 (460%)]\tLoss: 64.834953\n",
      "Train Epoche: 2 [443/96 (461%)]\tLoss: 361.868896\n",
      "Train Epoche: 2 [444/96 (462%)]\tLoss: 324.500336\n",
      "Train Epoche: 2 [445/96 (464%)]\tLoss: 143.039932\n",
      "Train Epoche: 2 [446/96 (465%)]\tLoss: 63.394688\n",
      "Train Epoche: 2 [447/96 (466%)]\tLoss: 482.147919\n",
      "Train Epoche: 2 [448/96 (467%)]\tLoss: 80.377846\n",
      "Train Epoche: 2 [449/96 (468%)]\tLoss: 254.910278\n",
      "Train Epoche: 2 [450/96 (469%)]\tLoss: 359.426178\n",
      "Train Epoche: 2 [451/96 (470%)]\tLoss: 223.581604\n",
      "Train Epoche: 2 [452/96 (471%)]\tLoss: 195.114380\n",
      "Train Epoche: 2 [453/96 (472%)]\tLoss: 35.543839\n",
      "Train Epoche: 2 [454/96 (473%)]\tLoss: 24.706179\n",
      "Train Epoche: 2 [455/96 (474%)]\tLoss: 48.456223\n",
      "Train Epoche: 2 [456/96 (475%)]\tLoss: 99.346260\n",
      "Train Epoche: 2 [457/96 (476%)]\tLoss: 3.815566\n",
      "Train Epoche: 2 [458/96 (477%)]\tLoss: 8.778219\n",
      "Train Epoche: 2 [459/96 (478%)]\tLoss: 322.367493\n",
      "Train Epoche: 2 [460/96 (479%)]\tLoss: 482.061890\n",
      "Train Epoche: 2 [461/96 (480%)]\tLoss: 0.890423\n",
      "Train Epoche: 2 [462/96 (481%)]\tLoss: 16.140049\n",
      "Train Epoche: 2 [463/96 (482%)]\tLoss: 289.318695\n",
      "Train Epoche: 2 [464/96 (483%)]\tLoss: 399.569427\n",
      "Train Epoche: 2 [465/96 (484%)]\tLoss: 170.746078\n",
      "Train Epoche: 2 [466/96 (485%)]\tLoss: 122.217293\n",
      "Train Epoche: 2 [467/96 (486%)]\tLoss: 287.546997\n",
      "Train Epoche: 2 [468/96 (488%)]\tLoss: 482.552307\n",
      "Train Epoche: 2 [469/96 (489%)]\tLoss: 48.594498\n",
      "Train Epoche: 2 [470/96 (490%)]\tLoss: 63.552490\n",
      "Train Epoche: 2 [471/96 (491%)]\tLoss: 482.036163\n",
      "Train Epoche: 2 [472/96 (492%)]\tLoss: 120.370850\n",
      "Train Epoche: 2 [473/96 (493%)]\tLoss: 80.392296\n",
      "Train Epoche: 2 [474/96 (494%)]\tLoss: 99.419907\n",
      "Train Epoche: 2 [475/96 (495%)]\tLoss: 24.701254\n",
      "Train Epoche: 2 [476/96 (496%)]\tLoss: 8.815808\n",
      "Train Epoche: 2 [477/96 (497%)]\tLoss: 35.495152\n",
      "Train Epoche: 2 [478/96 (498%)]\tLoss: 15.765311\n",
      "Train Epoche: 2 [479/96 (499%)]\tLoss: 482.560181\n",
      "Train Epoche: 2 [480/96 (500%)]\tLoss: 482.536316\n",
      "Train Epoche: 2 [481/96 (501%)]\tLoss: 0.991019\n",
      "Train Epoche: 2 [482/96 (502%)]\tLoss: 3.846033\n",
      "Train Epoche: 2 [483/96 (503%)]\tLoss: 225.322495\n",
      "Train Epoche: 2 [484/96 (504%)]\tLoss: 485.237396\n",
      "Train Epoche: 2 [485/96 (505%)]\tLoss: 144.817169\n",
      "Train Epoche: 2 [486/96 (506%)]\tLoss: 196.901352\n",
      "Train Epoche: 2 [487/96 (507%)]\tLoss: 171.172531\n",
      "Train Epoche: 2 [488/96 (508%)]\tLoss: 258.222931\n",
      "Train Epoche: 2 [489/96 (509%)]\tLoss: 35.552521\n",
      "Train Epoche: 2 [490/96 (510%)]\tLoss: 80.553917\n",
      "Train Epoche: 2 [491/96 (511%)]\tLoss: 48.479183\n",
      "Train Epoche: 2 [492/96 (512%)]\tLoss: 63.512672\n",
      "Train Epoche: 2 [493/96 (514%)]\tLoss: 482.101074\n",
      "Train Epoche: 2 [494/96 (515%)]\tLoss: 99.258965\n",
      "Train Epoche: 2 [495/96 (516%)]\tLoss: 15.706655\n",
      "Train Epoche: 2 [496/96 (517%)]\tLoss: 143.159134\n",
      "Train Epoche: 2 [497/96 (518%)]\tLoss: 24.558239\n",
      "Train Epoche: 2 [498/96 (519%)]\tLoss: 8.680444\n",
      "Train Epoche: 2 [499/96 (520%)]\tLoss: 482.433411\n",
      "Train Epoche: 2 [500/96 (521%)]\tLoss: 482.032562\n",
      "Train Epoche: 2 [501/96 (522%)]\tLoss: 481.674103\n",
      "Train Epoche: 2 [502/96 (523%)]\tLoss: 0.868975\n",
      "Train Epoche: 2 [503/96 (524%)]\tLoss: 4.036607\n",
      "Train Epoche: 2 [504/96 (525%)]\tLoss: 168.242905\n",
      "Train Epoche: 2 [505/96 (526%)]\tLoss: 195.588272\n",
      "Train Epoche: 2 [506/96 (527%)]\tLoss: 122.911377\n",
      "Train Epoche: 2 [507/96 (528%)]\tLoss: 489.001495\n",
      "Train Epoche: 2 [508/96 (529%)]\tLoss: 225.358398\n",
      "Train Epoche: 2 [509/96 (530%)]\tLoss: 483.050385\n",
      "Train Epoche: 2 [510/96 (531%)]\tLoss: 0.938942\n",
      "Train Epoche: 2 [511/96 (532%)]\tLoss: 574.289856\n",
      "Train Epoche: 2 [512/96 (533%)]\tLoss: 99.327805\n",
      "Train Epoche: 2 [513/96 (534%)]\tLoss: 120.088097\n",
      "Train Epoche: 2 [514/96 (535%)]\tLoss: 142.934219\n",
      "Train Epoche: 2 [515/96 (536%)]\tLoss: 573.858643\n",
      "Train Epoche: 2 [516/96 (538%)]\tLoss: 8.814631\n",
      "Train Epoche: 2 [517/96 (539%)]\tLoss: 15.647516\n",
      "Train Epoche: 2 [518/96 (540%)]\tLoss: 439.995850\n",
      "Train Epoche: 2 [519/96 (541%)]\tLoss: 63.539520\n",
      "Train Epoche: 2 [520/96 (542%)]\tLoss: 398.483276\n",
      "Train Epoche: 2 [521/96 (543%)]\tLoss: 481.932922\n",
      "Train Epoche: 2 [522/96 (544%)]\tLoss: 80.047760\n",
      "Train Epoche: 2 [523/96 (545%)]\tLoss: 3.796762\n",
      "Train Epoche: 2 [524/96 (546%)]\tLoss: 48.901104\n",
      "Train Epoche: 2 [525/96 (547%)]\tLoss: 36.088001\n",
      "Train Epoche: 2 [526/96 (548%)]\tLoss: 323.607697\n",
      "Train Epoche: 2 [527/96 (549%)]\tLoss: 361.198181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [528/96 (550%)]\tLoss: 198.881500\n",
      "Train Epoche: 2 [529/96 (551%)]\tLoss: 226.054459\n",
      "Train Epoche: 2 [530/96 (552%)]\tLoss: 25.619040\n",
      "Train Epoche: 2 [531/96 (553%)]\tLoss: 169.234604\n",
      "Train Epoche: 2 [532/96 (554%)]\tLoss: 288.637604\n",
      "Train Epoche: 2 [533/96 (555%)]\tLoss: 256.949066\n",
      "Train Epoche: 2 [534/96 (556%)]\tLoss: 63.251240\n",
      "Train Epoche: 2 [535/96 (557%)]\tLoss: 99.577415\n",
      "Train Epoche: 2 [536/96 (558%)]\tLoss: 80.283997\n",
      "Train Epoche: 2 [537/96 (559%)]\tLoss: 254.989853\n",
      "Train Epoche: 2 [538/96 (560%)]\tLoss: 167.881226\n",
      "Train Epoche: 2 [539/96 (561%)]\tLoss: 194.899384\n",
      "Train Epoche: 2 [540/96 (562%)]\tLoss: 3.902902\n",
      "Train Epoche: 2 [541/96 (564%)]\tLoss: 15.728211\n",
      "Train Epoche: 2 [542/96 (565%)]\tLoss: 143.213882\n",
      "Train Epoche: 2 [543/96 (566%)]\tLoss: 573.011658\n",
      "Train Epoche: 2 [544/96 (567%)]\tLoss: 0.912077\n",
      "Train Epoche: 2 [545/96 (568%)]\tLoss: 8.641345\n",
      "Train Epoche: 2 [546/96 (569%)]\tLoss: 119.928146\n",
      "Train Epoche: 2 [547/96 (570%)]\tLoss: 573.677002\n",
      "Train Epoche: 2 [548/96 (571%)]\tLoss: 223.323853\n",
      "Train Epoche: 2 [549/96 (572%)]\tLoss: 49.256268\n",
      "Train Epoche: 2 [550/96 (573%)]\tLoss: 399.538544\n",
      "Train Epoche: 2 [551/96 (574%)]\tLoss: 440.284027\n",
      "Train Epoche: 2 [552/96 (575%)]\tLoss: 290.237671\n",
      "Train Epoche: 2 [553/96 (576%)]\tLoss: 25.247023\n",
      "Train Epoche: 2 [554/96 (577%)]\tLoss: 35.948284\n",
      "Train Epoche: 2 [555/96 (578%)]\tLoss: 325.813293\n",
      "Train Epoche: 2 [556/96 (579%)]\tLoss: 362.315521\n",
      "Train Epoche: 2 [557/96 (580%)]\tLoss: 398.175140\n",
      "Train Epoche: 2 [558/96 (581%)]\tLoss: 398.745636\n",
      "Train Epoche: 2 [559/96 (582%)]\tLoss: 80.723282\n",
      "Train Epoche: 2 [560/96 (583%)]\tLoss: 223.609604\n",
      "Train Epoche: 2 [561/96 (584%)]\tLoss: 397.938293\n",
      "Train Epoche: 2 [562/96 (585%)]\tLoss: 120.460220\n",
      "Train Epoche: 2 [563/96 (586%)]\tLoss: 195.175720\n",
      "Train Epoche: 2 [564/96 (588%)]\tLoss: 35.555882\n",
      "Train Epoche: 2 [565/96 (589%)]\tLoss: 3.872453\n",
      "Train Epoche: 2 [566/96 (590%)]\tLoss: 0.929107\n",
      "Train Epoche: 2 [567/96 (591%)]\tLoss: 167.353989\n",
      "Train Epoche: 2 [568/96 (592%)]\tLoss: 143.111526\n",
      "Train Epoche: 2 [569/96 (593%)]\tLoss: 8.774235\n",
      "Train Epoche: 2 [570/96 (594%)]\tLoss: 24.726376\n",
      "Train Epoche: 2 [571/96 (595%)]\tLoss: 398.224243\n",
      "Train Epoche: 2 [572/96 (596%)]\tLoss: 398.577759\n",
      "Train Epoche: 2 [573/96 (597%)]\tLoss: 49.357582\n",
      "Train Epoche: 2 [574/96 (598%)]\tLoss: 16.005005\n",
      "Train Epoche: 2 [575/96 (599%)]\tLoss: 65.365021\n",
      "Train Epoche: 2 [576/96 (600%)]\tLoss: 102.478821\n",
      "Train Epoche: 2 [577/96 (601%)]\tLoss: 143.112488\n",
      "Train Epoche: 2 [578/96 (602%)]\tLoss: 168.233032\n",
      "Train Epoche: 2 [579/96 (603%)]\tLoss: 35.669258\n",
      "Train Epoche: 2 [580/96 (604%)]\tLoss: 99.562187\n",
      "Train Epoche: 2 [581/96 (605%)]\tLoss: 287.611237\n",
      "Train Epoche: 2 [582/96 (606%)]\tLoss: 482.807251\n",
      "Train Epoche: 2 [583/96 (607%)]\tLoss: 63.340088\n",
      "Train Epoche: 2 [584/96 (608%)]\tLoss: 80.113541\n",
      "Train Epoche: 2 [585/96 (609%)]\tLoss: 24.640867\n",
      "Train Epoche: 2 [586/96 (610%)]\tLoss: 3.863079\n",
      "Train Epoche: 2 [587/96 (611%)]\tLoss: 223.617279\n",
      "Train Epoche: 2 [588/96 (612%)]\tLoss: 120.324547\n",
      "Train Epoche: 2 [589/96 (614%)]\tLoss: 15.647429\n",
      "Train Epoche: 2 [590/96 (615%)]\tLoss: 8.770784\n",
      "Train Epoche: 2 [591/96 (616%)]\tLoss: 254.648514\n",
      "Train Epoche: 2 [592/96 (617%)]\tLoss: 398.837067\n",
      "Train Epoche: 2 [593/96 (618%)]\tLoss: 48.614651\n",
      "Train Epoche: 2 [594/96 (619%)]\tLoss: 0.927954\n",
      "Train Epoche: 2 [595/96 (620%)]\tLoss: 325.204956\n",
      "Train Epoche: 2 [596/96 (621%)]\tLoss: 445.688263\n",
      "Train Epoche: 2 [597/96 (622%)]\tLoss: 361.131714\n",
      "Train Epoche: 2 [598/96 (623%)]\tLoss: 198.110672\n",
      "Train Epoche: 2 [599/96 (624%)]\tLoss: 8.817886\n",
      "Train Epoche: 2 [600/96 (625%)]\tLoss: 3.837168\n",
      "Train Epoche: 2 [601/96 (626%)]\tLoss: 24.750387\n",
      "Train Epoche: 2 [602/96 (627%)]\tLoss: 63.546345\n",
      "Train Epoche: 2 [603/96 (628%)]\tLoss: 80.674911\n",
      "Train Epoche: 2 [604/96 (629%)]\tLoss: 15.800740\n",
      "Train Epoche: 2 [605/96 (630%)]\tLoss: 48.338638\n",
      "Train Epoche: 2 [606/96 (631%)]\tLoss: 35.692612\n",
      "Train Epoche: 2 [607/96 (632%)]\tLoss: 99.433388\n",
      "Train Epoche: 2 [608/96 (633%)]\tLoss: 481.669586\n",
      "Train Epoche: 2 [609/96 (634%)]\tLoss: 482.387573\n",
      "Train Epoche: 2 [610/96 (635%)]\tLoss: 119.967247\n",
      "Train Epoche: 2 [611/96 (636%)]\tLoss: 142.852737\n",
      "Train Epoche: 2 [612/96 (638%)]\tLoss: 484.391418\n",
      "Train Epoche: 2 [613/96 (639%)]\tLoss: 1.025988\n",
      "Train Epoche: 2 [614/96 (640%)]\tLoss: 481.903778\n",
      "Train Epoche: 2 [615/96 (641%)]\tLoss: 484.079651\n",
      "Train Epoche: 2 [616/96 (642%)]\tLoss: 486.012482\n",
      "Train Epoche: 2 [617/96 (643%)]\tLoss: 167.961655\n",
      "Train Epoche: 2 [618/96 (644%)]\tLoss: 486.970062\n",
      "Train Epoche: 2 [619/96 (645%)]\tLoss: 0.948713\n",
      "Train Epoche: 2 [620/96 (646%)]\tLoss: 15.691202\n",
      "Train Epoche: 2 [621/96 (647%)]\tLoss: 168.014374\n",
      "Train Epoche: 2 [622/96 (648%)]\tLoss: 323.097687\n",
      "Train Epoche: 2 [623/96 (649%)]\tLoss: 143.249741\n",
      "Train Epoche: 2 [624/96 (650%)]\tLoss: 80.251198\n",
      "Train Epoche: 2 [625/96 (651%)]\tLoss: 194.813782\n",
      "Train Epoche: 2 [626/96 (652%)]\tLoss: 574.029480\n",
      "Train Epoche: 2 [627/96 (653%)]\tLoss: 48.536427\n",
      "Train Epoche: 2 [628/96 (654%)]\tLoss: 35.607395\n",
      "Train Epoche: 2 [629/96 (655%)]\tLoss: 224.132034\n",
      "Train Epoche: 2 [630/96 (656%)]\tLoss: 120.009537\n",
      "Train Epoche: 2 [631/96 (657%)]\tLoss: 8.796321\n",
      "Train Epoche: 2 [632/96 (658%)]\tLoss: 3.910099\n",
      "Train Epoche: 2 [633/96 (659%)]\tLoss: 99.311241\n",
      "Train Epoche: 2 [634/96 (660%)]\tLoss: 287.476044\n",
      "Train Epoche: 2 [635/96 (661%)]\tLoss: 24.320219\n",
      "Train Epoche: 2 [636/96 (662%)]\tLoss: 63.028172\n",
      "Train Epoche: 2 [637/96 (664%)]\tLoss: 257.885834\n",
      "Train Epoche: 2 [638/96 (665%)]\tLoss: 360.444305\n",
      "Train Epoche: 2 [639/96 (666%)]\tLoss: 439.351227\n",
      "Train Epoche: 2 [640/96 (667%)]\tLoss: 397.526886\n",
      "Train Epoche: 2 [641/96 (668%)]\tLoss: 483.772675\n",
      "Train Epoche: 2 [642/96 (669%)]\tLoss: 526.117554\n",
      "Train Epoche: 2 [643/96 (670%)]\tLoss: 287.957245\n",
      "Train Epoche: 2 [644/96 (671%)]\tLoss: 80.238762\n",
      "Train Epoche: 2 [645/96 (672%)]\tLoss: 482.522980\n",
      "Train Epoche: 2 [646/96 (673%)]\tLoss: 120.137993\n",
      "Train Epoche: 2 [647/96 (674%)]\tLoss: 322.448669\n",
      "Train Epoche: 2 [648/96 (675%)]\tLoss: 99.555130\n",
      "Train Epoche: 2 [649/96 (676%)]\tLoss: 481.767212\n",
      "Train Epoche: 2 [650/96 (677%)]\tLoss: 24.593399\n",
      "Train Epoche: 2 [651/96 (678%)]\tLoss: 482.455292\n",
      "Train Epoche: 2 [652/96 (679%)]\tLoss: 481.871368\n",
      "Train Epoche: 2 [653/96 (680%)]\tLoss: 3.786927\n",
      "Train Epoche: 2 [654/96 (681%)]\tLoss: 0.952268\n",
      "Train Epoche: 2 [655/96 (682%)]\tLoss: 63.090336\n",
      "Train Epoche: 2 [656/96 (683%)]\tLoss: 143.468613\n",
      "Train Epoche: 2 [657/96 (684%)]\tLoss: 8.966866\n",
      "Train Epoche: 2 [658/96 (685%)]\tLoss: 15.629093\n",
      "Train Epoche: 2 [659/96 (686%)]\tLoss: 197.294388\n",
      "Train Epoche: 2 [660/96 (688%)]\tLoss: 226.381104\n",
      "Train Epoche: 2 [661/96 (689%)]\tLoss: 49.333405\n",
      "Train Epoche: 2 [662/96 (690%)]\tLoss: 36.421017\n",
      "Train Epoche: 2 [663/96 (691%)]\tLoss: 255.617508\n",
      "Train Epoche: 2 [664/96 (692%)]\tLoss: 169.376511\n",
      "Train Epoche: 2 [665/96 (693%)]\tLoss: 323.299805\n",
      "Train Epoche: 2 [666/96 (694%)]\tLoss: 194.856918\n",
      "Train Epoche: 2 [667/96 (695%)]\tLoss: 8.692275\n",
      "Train Epoche: 2 [668/96 (696%)]\tLoss: 15.624387\n",
      "Train Epoche: 2 [669/96 (697%)]\tLoss: 143.576675\n",
      "Train Epoche: 2 [670/96 (698%)]\tLoss: 120.263184\n",
      "Train Epoche: 2 [671/96 (699%)]\tLoss: 24.555058\n",
      "Train Epoche: 2 [672/96 (700%)]\tLoss: 3.839925\n",
      "Train Epoche: 2 [673/96 (701%)]\tLoss: 48.528458\n",
      "Train Epoche: 2 [674/96 (702%)]\tLoss: 35.638809\n",
      "Train Epoche: 2 [675/96 (703%)]\tLoss: 63.492496\n",
      "Train Epoche: 2 [676/96 (704%)]\tLoss: 98.916168\n",
      "Train Epoche: 2 [677/96 (705%)]\tLoss: 80.381813\n",
      "Train Epoche: 2 [678/96 (706%)]\tLoss: 167.912491\n",
      "Train Epoche: 2 [679/96 (707%)]\tLoss: 1.021643\n",
      "Train Epoche: 2 [680/96 (708%)]\tLoss: 287.022430\n",
      "Train Epoche: 2 [681/96 (709%)]\tLoss: 399.156403\n",
      "Train Epoche: 2 [682/96 (710%)]\tLoss: 400.381622\n",
      "Train Epoche: 2 [683/96 (711%)]\tLoss: 225.388519\n",
      "Train Epoche: 2 [684/96 (712%)]\tLoss: 255.648285\n",
      "Train Epoche: 2 [685/96 (714%)]\tLoss: 35.732941\n",
      "Train Epoche: 2 [686/96 (715%)]\tLoss: 254.892807\n",
      "Train Epoche: 2 [687/96 (716%)]\tLoss: 483.070740\n",
      "Train Epoche: 2 [688/96 (717%)]\tLoss: 143.397324\n",
      "Train Epoche: 2 [689/96 (718%)]\tLoss: 482.601929\n",
      "Train Epoche: 2 [690/96 (719%)]\tLoss: 63.415661\n",
      "Train Epoche: 2 [691/96 (720%)]\tLoss: 48.645786\n",
      "Train Epoche: 2 [692/96 (721%)]\tLoss: 482.862000\n",
      "Train Epoche: 2 [693/96 (722%)]\tLoss: 24.649715\n",
      "Train Epoche: 2 [694/96 (723%)]\tLoss: 80.231964\n",
      "Train Epoche: 2 [695/96 (724%)]\tLoss: 8.771789\n",
      "Train Epoche: 2 [696/96 (725%)]\tLoss: 3.930478\n",
      "Train Epoche: 2 [697/96 (726%)]\tLoss: 119.993408\n",
      "Train Epoche: 2 [698/96 (727%)]\tLoss: 167.895187\n",
      "Train Epoche: 2 [699/96 (728%)]\tLoss: 15.723723\n",
      "Train Epoche: 2 [700/96 (729%)]\tLoss: 0.977349\n",
      "Train Epoche: 2 [701/96 (730%)]\tLoss: 486.552429\n",
      "Train Epoche: 2 [702/96 (731%)]\tLoss: 224.358734\n",
      "Train Epoche: 2 [703/96 (732%)]\tLoss: 98.931419\n",
      "Train Epoche: 2 [704/96 (733%)]\tLoss: 489.365570\n",
      "Train Epoche: 2 [705/96 (734%)]\tLoss: 196.774353\n",
      "Train Epoche: 2 [706/96 (735%)]\tLoss: 482.990021\n",
      "Train Epoche: 2 [707/96 (736%)]\tLoss: 398.664978\n",
      "Train Epoche: 2 [708/96 (738%)]\tLoss: 119.935791\n",
      "Train Epoche: 2 [709/96 (739%)]\tLoss: 99.320808\n",
      "Train Epoche: 2 [710/96 (740%)]\tLoss: 255.170258\n",
      "Train Epoche: 2 [711/96 (741%)]\tLoss: 35.406815\n",
      "Train Epoche: 2 [712/96 (742%)]\tLoss: 224.277405\n",
      "Train Epoche: 2 [713/96 (743%)]\tLoss: 8.736382\n",
      "Train Epoche: 2 [714/96 (744%)]\tLoss: 48.356976\n",
      "Train Epoche: 2 [715/96 (745%)]\tLoss: 80.241905\n",
      "Train Epoche: 2 [716/96 (746%)]\tLoss: 63.424114\n",
      "Train Epoche: 2 [717/96 (747%)]\tLoss: 24.718746\n",
      "Train Epoche: 2 [718/96 (748%)]\tLoss: 15.705203\n",
      "Train Epoche: 2 [719/96 (749%)]\tLoss: 195.029114\n",
      "Train Epoche: 2 [720/96 (750%)]\tLoss: 287.759857\n",
      "Train Epoche: 2 [721/96 (751%)]\tLoss: 0.937142\n",
      "Train Epoche: 2 [722/96 (752%)]\tLoss: 3.963467\n",
      "Train Epoche: 2 [723/96 (753%)]\tLoss: 172.189102\n",
      "Train Epoche: 2 [724/96 (754%)]\tLoss: 144.349808\n",
      "Train Epoche: 2 [725/96 (755%)]\tLoss: 3.901670\n",
      "Train Epoche: 2 [726/96 (756%)]\tLoss: 15.596372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [727/96 (757%)]\tLoss: 143.023254\n",
      "Train Epoche: 2 [728/96 (758%)]\tLoss: 573.725159\n",
      "Train Epoche: 2 [729/96 (759%)]\tLoss: 574.756287\n",
      "Train Epoche: 2 [730/96 (760%)]\tLoss: 168.227936\n",
      "Train Epoche: 2 [731/96 (761%)]\tLoss: 80.492523\n",
      "Train Epoche: 2 [732/96 (762%)]\tLoss: 48.729652\n",
      "Train Epoche: 2 [733/96 (764%)]\tLoss: 24.475508\n",
      "Train Epoche: 2 [734/96 (765%)]\tLoss: 35.774826\n",
      "Train Epoche: 2 [735/96 (766%)]\tLoss: 119.867790\n",
      "Train Epoche: 2 [736/96 (767%)]\tLoss: 99.272186\n",
      "Train Epoche: 2 [737/96 (768%)]\tLoss: 8.801802\n",
      "Train Epoche: 2 [738/96 (769%)]\tLoss: 0.916775\n",
      "Train Epoche: 2 [739/96 (770%)]\tLoss: 224.325302\n",
      "Train Epoche: 2 [740/96 (771%)]\tLoss: 254.271545\n",
      "Train Epoche: 2 [741/96 (772%)]\tLoss: 64.147789\n",
      "Train Epoche: 2 [742/96 (773%)]\tLoss: 576.070984\n",
      "Train Epoche: 2 [743/96 (774%)]\tLoss: 198.209534\n",
      "Train Epoche: 2 [744/96 (775%)]\tLoss: 289.547516\n",
      "Train Epoche: 2 [745/96 (776%)]\tLoss: 322.735474\n",
      "Train Epoche: 2 [746/96 (777%)]\tLoss: 401.260590\n",
      "Train Epoche: 2 [747/96 (778%)]\tLoss: 439.958649\n",
      "Train Epoche: 2 [748/96 (779%)]\tLoss: 360.801147\n",
      "Train Epoche: 2 [749/96 (780%)]\tLoss: 99.249710\n",
      "Train Epoche: 2 [750/96 (781%)]\tLoss: 15.716481\n",
      "Train Epoche: 2 [751/96 (782%)]\tLoss: 24.735369\n",
      "Train Epoche: 2 [752/96 (783%)]\tLoss: 397.945312\n",
      "Train Epoche: 2 [753/96 (784%)]\tLoss: 398.730255\n",
      "Train Epoche: 2 [754/96 (785%)]\tLoss: 63.302048\n",
      "Train Epoche: 2 [755/96 (786%)]\tLoss: 8.700890\n",
      "Train Epoche: 2 [756/96 (788%)]\tLoss: 48.421616\n",
      "Train Epoche: 2 [757/96 (789%)]\tLoss: 80.525063\n",
      "Train Epoche: 2 [758/96 (790%)]\tLoss: 397.959534\n",
      "Train Epoche: 2 [759/96 (791%)]\tLoss: 35.484024\n",
      "Train Epoche: 2 [760/96 (792%)]\tLoss: 120.073502\n",
      "Train Epoche: 2 [761/96 (793%)]\tLoss: 0.948334\n",
      "Train Epoche: 2 [762/96 (794%)]\tLoss: 4.031987\n",
      "Train Epoche: 2 [763/96 (795%)]\tLoss: 171.135956\n",
      "Train Epoche: 2 [764/96 (796%)]\tLoss: 145.438553\n",
      "Train Epoche: 2 [765/96 (797%)]\tLoss: 0.930524\n",
      "Train Epoche: 2 [766/96 (798%)]\tLoss: 254.778564\n",
      "Train Epoche: 2 [767/96 (799%)]\tLoss: 287.732727\n",
      "Train Epoche: 2 [768/96 (800%)]\tLoss: 168.564926\n",
      "Train Epoche: 2 [769/96 (801%)]\tLoss: 195.084946\n",
      "Train Epoche: 2 [770/96 (802%)]\tLoss: 223.494278\n",
      "Train Epoche: 2 [771/96 (803%)]\tLoss: 24.639509\n",
      "Train Epoche: 2 [772/96 (804%)]\tLoss: 99.040970\n",
      "Train Epoche: 2 [773/96 (805%)]\tLoss: 143.040100\n",
      "Train Epoche: 2 [774/96 (806%)]\tLoss: 120.113930\n",
      "Train Epoche: 2 [775/96 (807%)]\tLoss: 48.519791\n",
      "Train Epoche: 2 [776/96 (808%)]\tLoss: 15.902976\n",
      "Train Epoche: 2 [777/96 (809%)]\tLoss: 80.132507\n",
      "Train Epoche: 2 [778/96 (810%)]\tLoss: 8.806475\n",
      "Train Epoche: 2 [779/96 (811%)]\tLoss: 36.450634\n",
      "Train Epoche: 2 [780/96 (812%)]\tLoss: 577.024658\n",
      "Train Epoche: 2 [781/96 (814%)]\tLoss: 574.635742\n",
      "Train Epoche: 2 [782/96 (815%)]\tLoss: 578.738770\n",
      "Train Epoche: 2 [783/96 (816%)]\tLoss: 322.190887\n",
      "Train Epoche: 2 [784/96 (817%)]\tLoss: 362.134552\n",
      "Train Epoche: 2 [785/96 (818%)]\tLoss: 65.436348\n",
      "Train Epoche: 2 [786/96 (819%)]\tLoss: 4.187418\n",
      "Train Epoche: 2 [787/96 (820%)]\tLoss: 446.029388\n",
      "Train Epoche: 2 [788/96 (821%)]\tLoss: 397.036621\n",
      "Train Epoche: 2 [789/96 (822%)]\tLoss: 195.138580\n",
      "Train Epoche: 2 [790/96 (823%)]\tLoss: 35.665943\n",
      "Train Epoche: 2 [791/96 (824%)]\tLoss: 398.273438\n",
      "Train Epoche: 2 [792/96 (825%)]\tLoss: 80.528366\n",
      "Train Epoche: 2 [793/96 (826%)]\tLoss: 168.387711\n",
      "Train Epoche: 2 [794/96 (827%)]\tLoss: 143.273972\n",
      "Train Epoche: 2 [795/96 (828%)]\tLoss: 398.725922\n",
      "Train Epoche: 2 [796/96 (829%)]\tLoss: 15.691419\n",
      "Train Epoche: 2 [797/96 (830%)]\tLoss: 0.897626\n",
      "Train Epoche: 2 [798/96 (831%)]\tLoss: 48.602802\n",
      "Train Epoche: 2 [799/96 (832%)]\tLoss: 99.212219\n",
      "Train Epoche: 2 [800/96 (833%)]\tLoss: 24.448349\n",
      "Train Epoche: 2 [801/96 (834%)]\tLoss: 397.992340\n",
      "Train Epoche: 2 [802/96 (835%)]\tLoss: 398.607941\n",
      "Train Epoche: 2 [803/96 (836%)]\tLoss: 120.191475\n",
      "Train Epoche: 2 [804/96 (838%)]\tLoss: 4.056650\n",
      "Train Epoche: 2 [805/96 (839%)]\tLoss: 8.875976\n",
      "Train Epoche: 2 [806/96 (840%)]\tLoss: 64.747040\n",
      "Train Epoche: 2 [807/96 (841%)]\tLoss: 404.371857\n",
      "Train Epoche: 2 [808/96 (842%)]\tLoss: 24.850687\n",
      "Train Epoche: 2 [809/96 (843%)]\tLoss: 120.260406\n",
      "Train Epoche: 2 [810/96 (844%)]\tLoss: 3.816574\n",
      "Train Epoche: 2 [811/96 (845%)]\tLoss: 8.794322\n",
      "Train Epoche: 2 [812/96 (846%)]\tLoss: 143.586868\n",
      "Train Epoche: 2 [813/96 (847%)]\tLoss: 482.633942\n",
      "Train Epoche: 2 [814/96 (848%)]\tLoss: 80.415405\n",
      "Train Epoche: 2 [815/96 (849%)]\tLoss: 99.472061\n",
      "Train Epoche: 2 [816/96 (850%)]\tLoss: 35.688927\n",
      "Train Epoche: 2 [817/96 (851%)]\tLoss: 48.487289\n",
      "Train Epoche: 2 [818/96 (852%)]\tLoss: 63.281803\n",
      "Train Epoche: 2 [819/96 (853%)]\tLoss: 15.835166\n",
      "Train Epoche: 2 [820/96 (854%)]\tLoss: 254.344574\n",
      "Train Epoche: 2 [821/96 (855%)]\tLoss: 224.132202\n",
      "Train Epoche: 2 [822/96 (856%)]\tLoss: 0.965668\n",
      "Train Epoche: 2 [823/96 (857%)]\tLoss: 195.768051\n",
      "Train Epoche: 2 [824/96 (858%)]\tLoss: 484.342712\n",
      "Train Epoche: 2 [825/96 (859%)]\tLoss: 292.512360\n",
      "Train Epoche: 2 [826/96 (860%)]\tLoss: 170.432877\n",
      "Train Epoche: 2 [827/96 (861%)]\tLoss: 483.228729\n",
      "Train Epoche: 2 [828/96 (862%)]\tLoss: 120.291824\n",
      "Train Epoche: 2 [829/96 (864%)]\tLoss: 99.253700\n",
      "Train Epoche: 2 [830/96 (865%)]\tLoss: 15.610019\n",
      "Train Epoche: 2 [831/96 (866%)]\tLoss: 398.845428\n",
      "Train Epoche: 2 [832/96 (867%)]\tLoss: 398.150482\n",
      "Train Epoche: 2 [833/96 (868%)]\tLoss: 3.812929\n",
      "Train Epoche: 2 [834/96 (869%)]\tLoss: 24.657696\n",
      "Train Epoche: 2 [835/96 (870%)]\tLoss: 168.076172\n",
      "Train Epoche: 2 [836/96 (871%)]\tLoss: 63.584942\n",
      "Train Epoche: 2 [837/96 (872%)]\tLoss: 35.719238\n",
      "Train Epoche: 2 [838/96 (873%)]\tLoss: 80.098145\n",
      "Train Epoche: 2 [839/96 (874%)]\tLoss: 194.866318\n",
      "Train Epoche: 2 [840/96 (875%)]\tLoss: 143.024673\n",
      "Train Epoche: 2 [841/96 (876%)]\tLoss: 0.988089\n",
      "Train Epoche: 2 [842/96 (877%)]\tLoss: 8.900019\n",
      "Train Epoche: 2 [843/96 (878%)]\tLoss: 49.190903\n",
      "Train Epoche: 2 [844/96 (879%)]\tLoss: 225.793289\n",
      "Train Epoche: 2 [845/96 (880%)]\tLoss: 255.797867\n",
      "Train Epoche: 2 [846/96 (881%)]\tLoss: 291.458923\n",
      "Train Epoche: 2 [847/96 (882%)]\tLoss: 80.478752\n",
      "Train Epoche: 2 [848/96 (883%)]\tLoss: 120.101349\n",
      "Train Epoche: 2 [849/96 (884%)]\tLoss: 482.544189\n",
      "Train Epoche: 2 [850/96 (885%)]\tLoss: 195.265472\n",
      "Train Epoche: 2 [851/96 (886%)]\tLoss: 482.995117\n",
      "Train Epoche: 2 [852/96 (888%)]\tLoss: 143.078690\n",
      "Train Epoche: 2 [853/96 (889%)]\tLoss: 3.896823\n",
      "Train Epoche: 2 [854/96 (890%)]\tLoss: 15.605546\n",
      "Train Epoche: 2 [855/96 (891%)]\tLoss: 48.509010\n",
      "Train Epoche: 2 [856/96 (892%)]\tLoss: 63.192207\n",
      "Train Epoche: 2 [857/96 (893%)]\tLoss: 35.409981\n",
      "Train Epoche: 2 [858/96 (894%)]\tLoss: 8.693846\n",
      "Train Epoche: 2 [859/96 (895%)]\tLoss: 168.295624\n",
      "Train Epoche: 2 [860/96 (896%)]\tLoss: 24.722246\n",
      "Train Epoche: 2 [861/96 (897%)]\tLoss: 482.920349\n",
      "Train Epoche: 2 [862/96 (898%)]\tLoss: 256.076233\n",
      "Train Epoche: 2 [863/96 (899%)]\tLoss: 325.665192\n",
      "Train Epoche: 2 [864/96 (900%)]\tLoss: 1.063795\n",
      "Train Epoche: 2 [865/96 (901%)]\tLoss: 100.872444\n",
      "Train Epoche: 2 [866/96 (902%)]\tLoss: 288.091003\n",
      "Train Epoche: 2 [867/96 (903%)]\tLoss: 225.152573\n",
      "Train Epoche: 2 [868/96 (904%)]\tLoss: 48.686020\n",
      "Train Epoche: 2 [869/96 (905%)]\tLoss: 3.889108\n",
      "Train Epoche: 2 [870/96 (906%)]\tLoss: 225.469193\n",
      "Train Epoche: 2 [871/96 (907%)]\tLoss: 574.192261\n",
      "Train Epoche: 2 [872/96 (908%)]\tLoss: 120.426079\n",
      "Train Epoche: 2 [873/96 (909%)]\tLoss: 143.289032\n",
      "Train Epoche: 2 [874/96 (910%)]\tLoss: 575.351501\n",
      "Train Epoche: 2 [875/96 (911%)]\tLoss: 64.227020\n",
      "Train Epoche: 2 [876/96 (912%)]\tLoss: 8.943932\n",
      "Train Epoche: 2 [877/96 (914%)]\tLoss: 575.251892\n",
      "Train Epoche: 2 [878/96 (915%)]\tLoss: 80.266838\n",
      "Train Epoche: 2 [879/96 (916%)]\tLoss: 168.372467\n",
      "Train Epoche: 2 [880/96 (917%)]\tLoss: 15.608364\n",
      "Train Epoche: 2 [881/96 (918%)]\tLoss: 0.933714\n",
      "Train Epoche: 2 [882/96 (919%)]\tLoss: 100.069229\n",
      "Train Epoche: 2 [883/96 (920%)]\tLoss: 35.474285\n",
      "Train Epoche: 2 [884/96 (921%)]\tLoss: 24.369335\n",
      "Train Epoche: 2 [885/96 (922%)]\tLoss: 194.464859\n",
      "Train Epoche: 2 [886/96 (923%)]\tLoss: 363.066589\n",
      "Train Epoche: 2 [887/96 (924%)]\tLoss: 575.747681\n",
      "Train Epoche: 2 [888/96 (925%)]\tLoss: 256.253967\n",
      "Train Epoche: 2 [889/96 (926%)]\tLoss: 288.418060\n",
      "Train Epoche: 2 [890/96 (927%)]\tLoss: 324.705231\n",
      "Train Epoche: 2 [891/96 (928%)]\tLoss: 399.292603\n",
      "Train Epoche: 2 [892/96 (929%)]\tLoss: 168.456055\n",
      "Train Epoche: 2 [893/96 (930%)]\tLoss: 35.505924\n",
      "Train Epoche: 2 [894/96 (931%)]\tLoss: 398.983795\n",
      "Train Epoche: 2 [895/96 (932%)]\tLoss: 120.385750\n",
      "Train Epoche: 2 [896/96 (933%)]\tLoss: 398.122009\n",
      "Train Epoche: 2 [897/96 (934%)]\tLoss: 80.449921\n",
      "Train Epoche: 2 [898/96 (935%)]\tLoss: 63.223469\n",
      "Train Epoche: 2 [899/96 (936%)]\tLoss: 15.637057\n",
      "Train Epoche: 2 [900/96 (938%)]\tLoss: 0.927092\n",
      "Train Epoche: 2 [901/96 (939%)]\tLoss: 48.474953\n",
      "Train Epoche: 2 [902/96 (940%)]\tLoss: 99.276901\n",
      "Train Epoche: 2 [903/96 (941%)]\tLoss: 397.832306\n",
      "Train Epoche: 2 [904/96 (942%)]\tLoss: 24.591644\n",
      "Train Epoche: 2 [905/96 (943%)]\tLoss: 398.724548\n",
      "Train Epoche: 2 [906/96 (944%)]\tLoss: 142.926682\n",
      "Train Epoche: 2 [907/96 (945%)]\tLoss: 3.810065\n",
      "Train Epoche: 2 [908/96 (946%)]\tLoss: 8.682082\n",
      "Train Epoche: 2 [909/96 (947%)]\tLoss: 401.798981\n",
      "Train Epoche: 2 [910/96 (948%)]\tLoss: 401.609894\n",
      "Train Epoche: 2 [911/96 (949%)]\tLoss: 24.462536\n",
      "Train Epoche: 2 [912/96 (950%)]\tLoss: 120.079353\n",
      "Train Epoche: 2 [913/96 (951%)]\tLoss: 194.605850\n",
      "Train Epoche: 2 [914/96 (952%)]\tLoss: 168.445877\n",
      "Train Epoche: 2 [915/96 (953%)]\tLoss: 48.611725\n",
      "Train Epoche: 2 [916/96 (954%)]\tLoss: 143.062927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [917/96 (955%)]\tLoss: 0.921222\n",
      "Train Epoche: 2 [918/96 (956%)]\tLoss: 35.497444\n",
      "Train Epoche: 2 [919/96 (957%)]\tLoss: 481.893890\n",
      "Train Epoche: 2 [920/96 (958%)]\tLoss: 63.440895\n",
      "Train Epoche: 2 [921/96 (959%)]\tLoss: 481.528107\n",
      "Train Epoche: 2 [922/96 (960%)]\tLoss: 15.668823\n",
      "Train Epoche: 2 [923/96 (961%)]\tLoss: 99.581604\n",
      "Train Epoche: 2 [924/96 (962%)]\tLoss: 481.438446\n",
      "Train Epoche: 2 [925/96 (964%)]\tLoss: 8.801532\n",
      "Train Epoche: 2 [926/96 (965%)]\tLoss: 482.115753\n",
      "Train Epoche: 2 [927/96 (966%)]\tLoss: 257.110748\n",
      "Train Epoche: 2 [928/96 (967%)]\tLoss: 322.590607\n",
      "Train Epoche: 2 [929/96 (968%)]\tLoss: 4.045044\n",
      "Train Epoche: 2 [930/96 (969%)]\tLoss: 81.075500\n",
      "Train Epoche: 2 [931/96 (970%)]\tLoss: 290.405701\n",
      "Train Epoche: 2 [932/96 (971%)]\tLoss: 224.834747\n",
      "Train Epoche: 2 [933/96 (972%)]\tLoss: 99.447670\n",
      "Train Epoche: 2 [934/96 (973%)]\tLoss: 143.436295\n",
      "Train Epoche: 2 [935/96 (974%)]\tLoss: 120.179512\n",
      "Train Epoche: 2 [936/96 (975%)]\tLoss: 35.745934\n",
      "Train Epoche: 2 [937/96 (976%)]\tLoss: 399.347351\n",
      "Train Epoche: 2 [938/96 (977%)]\tLoss: 398.878052\n",
      "Train Epoche: 2 [939/96 (978%)]\tLoss: 167.912262\n",
      "Train Epoche: 2 [940/96 (979%)]\tLoss: 398.529266\n",
      "Train Epoche: 2 [941/96 (980%)]\tLoss: 8.826881\n",
      "Train Epoche: 2 [942/96 (981%)]\tLoss: 15.750417\n",
      "Train Epoche: 2 [943/96 (982%)]\tLoss: 48.909321\n",
      "Train Epoche: 2 [944/96 (983%)]\tLoss: 24.572409\n",
      "Train Epoche: 2 [945/96 (984%)]\tLoss: 398.700165\n",
      "Train Epoche: 2 [946/96 (985%)]\tLoss: 0.943880\n",
      "Train Epoche: 2 [947/96 (986%)]\tLoss: 398.539398\n",
      "Train Epoche: 2 [948/96 (988%)]\tLoss: 194.679901\n",
      "Train Epoche: 2 [949/96 (989%)]\tLoss: 80.513832\n",
      "Train Epoche: 2 [950/96 (990%)]\tLoss: 4.066038\n",
      "Train Epoche: 2 [951/96 (991%)]\tLoss: 225.103577\n",
      "Train Epoche: 2 [952/96 (992%)]\tLoss: 64.691475\n",
      "Train Epoche: 2 [953/96 (993%)]\tLoss: 398.154663\n",
      "Train Epoche: 2 [954/96 (994%)]\tLoss: 254.942383\n",
      "Train Epoche: 2 [955/96 (995%)]\tLoss: 35.743286\n",
      "Train Epoche: 2 [956/96 (996%)]\tLoss: 15.631907\n",
      "Train Epoche: 2 [957/96 (997%)]\tLoss: 120.166824\n",
      "Train Epoche: 2 [958/96 (998%)]\tLoss: 80.255234\n",
      "Train Epoche: 2 [959/96 (999%)]\tLoss: 24.655848\n",
      "Train Epoche: 2 [960/96 (1000%)]\tLoss: 8.769547\n",
      "Train Epoche: 2 [961/96 (1001%)]\tLoss: 224.098602\n",
      "Train Epoche: 2 [962/96 (1002%)]\tLoss: 168.009018\n",
      "Train Epoche: 2 [963/96 (1003%)]\tLoss: 48.513306\n",
      "Train Epoche: 2 [964/96 (1004%)]\tLoss: 99.006668\n",
      "Train Epoche: 2 [965/96 (1005%)]\tLoss: 194.493530\n",
      "Train Epoche: 2 [966/96 (1006%)]\tLoss: 143.386078\n",
      "Train Epoche: 2 [967/96 (1007%)]\tLoss: 3.909443\n",
      "Train Epoche: 2 [968/96 (1008%)]\tLoss: 0.997358\n",
      "Train Epoche: 2 [969/96 (1009%)]\tLoss: 63.425503\n",
      "Train Epoche: 2 [970/96 (1010%)]\tLoss: 400.934540\n",
      "Train Epoche: 2 [971/96 (1011%)]\tLoss: 290.070099\n",
      "Train Epoche: 2 [972/96 (1012%)]\tLoss: 324.916626\n",
      "Train Epoche: 2 [973/96 (1014%)]\tLoss: 120.036766\n",
      "Train Epoche: 2 [974/96 (1015%)]\tLoss: 482.372925\n",
      "Train Epoche: 2 [975/96 (1016%)]\tLoss: 482.456116\n",
      "Train Epoche: 2 [976/96 (1017%)]\tLoss: 8.729482\n",
      "Train Epoche: 2 [977/96 (1018%)]\tLoss: 254.703308\n",
      "Train Epoche: 2 [978/96 (1019%)]\tLoss: 482.580719\n",
      "Train Epoche: 2 [979/96 (1020%)]\tLoss: 142.848404\n",
      "Train Epoche: 2 [980/96 (1021%)]\tLoss: 80.326637\n",
      "Train Epoche: 2 [981/96 (1022%)]\tLoss: 35.451191\n",
      "Train Epoche: 2 [982/96 (1023%)]\tLoss: 3.830964\n",
      "Train Epoche: 2 [983/96 (1024%)]\tLoss: 63.377506\n",
      "Train Epoche: 2 [984/96 (1025%)]\tLoss: 99.153374\n",
      "Train Epoche: 2 [985/96 (1026%)]\tLoss: 48.353958\n",
      "Train Epoche: 2 [986/96 (1027%)]\tLoss: 15.636403\n",
      "Train Epoche: 2 [987/96 (1028%)]\tLoss: 223.751694\n",
      "Train Epoche: 2 [988/96 (1029%)]\tLoss: 322.804199\n",
      "Train Epoche: 2 [989/96 (1030%)]\tLoss: 0.987412\n",
      "Train Epoche: 2 [990/96 (1031%)]\tLoss: 24.702518\n",
      "Train Epoche: 2 [991/96 (1032%)]\tLoss: 287.653290\n",
      "Train Epoche: 2 [992/96 (1033%)]\tLoss: 362.016510\n",
      "Train Epoche: 2 [993/96 (1034%)]\tLoss: 197.851181\n",
      "Train Epoche: 2 [994/96 (1035%)]\tLoss: 168.882782\n",
      "Train Epoche: 2 [995/96 (1036%)]\tLoss: 3.869729\n",
      "Train Epoche: 2 [996/96 (1038%)]\tLoss: 8.856220\n",
      "Train Epoche: 2 [997/96 (1039%)]\tLoss: 287.850647\n",
      "Train Epoche: 2 [998/96 (1040%)]\tLoss: 224.342728\n",
      "Train Epoche: 2 [999/96 (1041%)]\tLoss: 63.419376\n",
      "Train Epoche: 2 [1000/96 (1042%)]\tLoss: 120.617004\n",
      "Train Epoche: 2 [1001/96 (1043%)]\tLoss: 195.101624\n",
      "Train Epoche: 2 [1002/96 (1044%)]\tLoss: 254.762939\n",
      "Train Epoche: 2 [1003/96 (1045%)]\tLoss: 24.644361\n",
      "Train Epoche: 2 [1004/96 (1046%)]\tLoss: 574.516357\n",
      "Train Epoche: 2 [1005/96 (1047%)]\tLoss: 167.732285\n",
      "Train Epoche: 2 [1006/96 (1048%)]\tLoss: 143.173920\n",
      "Train Epoche: 2 [1007/96 (1049%)]\tLoss: 15.588765\n",
      "Train Epoche: 2 [1008/96 (1050%)]\tLoss: 0.957910\n",
      "Train Epoche: 2 [1009/96 (1051%)]\tLoss: 99.190125\n",
      "Train Epoche: 2 [1010/96 (1052%)]\tLoss: 80.778297\n",
      "Train Epoche: 2 [1011/96 (1053%)]\tLoss: 48.765541\n",
      "Train Epoche: 2 [1012/96 (1054%)]\tLoss: 35.949257\n",
      "Train Epoche: 2 [1013/96 (1055%)]\tLoss: 582.227295\n",
      "Train Epoche: 2 [1014/96 (1056%)]\tLoss: 323.195892\n",
      "Train Epoche: 2 [1015/96 (1057%)]\tLoss: 361.230438\n",
      "Train Epoche: 2 [1016/96 (1058%)]\tLoss: 401.692169\n",
      "Train Epoche: 2 [1017/96 (1059%)]\tLoss: 577.498779\n",
      "Train Epoche: 2 [1018/96 (1060%)]\tLoss: 441.450714\n",
      "Train Epoche: 2 [1019/96 (1061%)]\tLoss: 48.506725\n",
      "Train Epoche: 2 [1020/96 (1062%)]\tLoss: 483.448273\n",
      "Train Epoche: 2 [1021/96 (1064%)]\tLoss: 143.089188\n",
      "Train Epoche: 2 [1022/96 (1065%)]\tLoss: 483.870026\n",
      "Train Epoche: 2 [1023/96 (1066%)]\tLoss: 99.292717\n",
      "Train Epoche: 2 [1024/96 (1067%)]\tLoss: 223.824234\n",
      "Train Epoche: 2 [1025/96 (1068%)]\tLoss: 80.964211\n",
      "Train Epoche: 2 [1026/96 (1069%)]\tLoss: 194.348343\n",
      "Train Epoche: 2 [1027/96 (1070%)]\tLoss: 15.531490\n",
      "Train Epoche: 2 [1028/96 (1071%)]\tLoss: 24.718452\n",
      "Train Epoche: 2 [1029/96 (1072%)]\tLoss: 63.352795\n",
      "Train Epoche: 2 [1030/96 (1073%)]\tLoss: 3.784298\n",
      "Train Epoche: 2 [1031/96 (1074%)]\tLoss: 35.779064\n",
      "Train Epoche: 2 [1032/96 (1075%)]\tLoss: 288.185486\n",
      "Train Epoche: 2 [1033/96 (1076%)]\tLoss: 168.465057\n",
      "Train Epoche: 2 [1034/96 (1077%)]\tLoss: 8.742336\n",
      "Train Epoche: 2 [1035/96 (1078%)]\tLoss: 0.986545\n",
      "Train Epoche: 2 [1036/96 (1079%)]\tLoss: 255.321167\n",
      "Train Epoche: 2 [1037/96 (1080%)]\tLoss: 326.663239\n",
      "Train Epoche: 2 [1038/96 (1081%)]\tLoss: 119.894852\n",
      "Train Epoche: 2 [1039/96 (1082%)]\tLoss: 398.162506\n",
      "Train Epoche: 2 [1040/96 (1083%)]\tLoss: 398.692566\n",
      "Train Epoche: 2 [1041/96 (1084%)]\tLoss: 35.369968\n",
      "Train Epoche: 2 [1042/96 (1085%)]\tLoss: 24.726679\n",
      "Train Epoche: 2 [1043/96 (1086%)]\tLoss: 48.459743\n",
      "Train Epoche: 2 [1044/96 (1088%)]\tLoss: 63.408447\n",
      "Train Epoche: 2 [1045/96 (1089%)]\tLoss: 15.584851\n",
      "Train Epoche: 2 [1046/96 (1090%)]\tLoss: 0.917786\n",
      "Train Epoche: 2 [1047/96 (1091%)]\tLoss: 194.741516\n",
      "Train Epoche: 2 [1048/96 (1092%)]\tLoss: 167.906158\n",
      "Train Epoche: 2 [1049/96 (1093%)]\tLoss: 99.331963\n",
      "Train Epoche: 2 [1050/96 (1094%)]\tLoss: 80.321358\n",
      "Train Epoche: 2 [1051/96 (1095%)]\tLoss: 397.848206\n",
      "Train Epoche: 2 [1052/96 (1096%)]\tLoss: 143.284622\n",
      "Train Epoche: 2 [1053/96 (1097%)]\tLoss: 3.978970\n",
      "Train Epoche: 2 [1054/96 (1098%)]\tLoss: 8.790893\n",
      "Train Epoche: 2 [1055/96 (1099%)]\tLoss: 120.879829\n",
      "Train Epoche: 2 [1056/96 (1100%)]\tLoss: 401.280640\n",
      "Train Epoche: 2 [1057/96 (1101%)]\tLoss: 227.194122\n",
      "Train Epoche: 2 [1058/96 (1102%)]\tLoss: 398.346832\n",
      "Train Epoche: 2 [1059/96 (1103%)]\tLoss: 398.569031\n",
      "Train Epoche: 2 [1060/96 (1104%)]\tLoss: 35.544147\n",
      "Train Epoche: 2 [1061/96 (1105%)]\tLoss: 8.653007\n",
      "Train Epoche: 2 [1062/96 (1106%)]\tLoss: 224.051422\n",
      "Train Epoche: 2 [1063/96 (1107%)]\tLoss: 142.739746\n",
      "Train Epoche: 2 [1064/96 (1108%)]\tLoss: 15.608020\n",
      "Train Epoche: 2 [1065/96 (1109%)]\tLoss: 24.578957\n",
      "Train Epoche: 2 [1066/96 (1110%)]\tLoss: 63.339901\n",
      "Train Epoche: 2 [1067/96 (1111%)]\tLoss: 120.258919\n",
      "Train Epoche: 2 [1068/96 (1112%)]\tLoss: 167.892288\n",
      "Train Epoche: 2 [1069/96 (1114%)]\tLoss: 80.368065\n",
      "Train Epoche: 2 [1070/96 (1115%)]\tLoss: 194.829071\n",
      "Train Epoche: 2 [1071/96 (1116%)]\tLoss: 254.608795\n",
      "Train Epoche: 2 [1072/96 (1117%)]\tLoss: 1.070363\n",
      "Train Epoche: 2 [1073/96 (1118%)]\tLoss: 4.136583\n",
      "Train Epoche: 2 [1074/96 (1119%)]\tLoss: 101.821846\n",
      "Train Epoche: 2 [1075/96 (1120%)]\tLoss: 49.007858\n",
      "Train Epoche: 2 [1076/96 (1121%)]\tLoss: 288.487518\n",
      "Train Epoche: 2 [1077/96 (1122%)]\tLoss: 399.711517\n",
      "Train Epoche: 2 [1078/96 (1123%)]\tLoss: 120.474648\n",
      "Train Epoche: 2 [1079/96 (1124%)]\tLoss: 254.434433\n",
      "Train Epoche: 2 [1080/96 (1125%)]\tLoss: 287.527832\n",
      "Train Epoche: 2 [1081/96 (1126%)]\tLoss: 24.503471\n",
      "Train Epoche: 2 [1082/96 (1127%)]\tLoss: 80.533806\n",
      "Train Epoche: 2 [1083/96 (1128%)]\tLoss: 99.234604\n",
      "Train Epoche: 2 [1084/96 (1129%)]\tLoss: 15.758695\n",
      "Train Epoche: 2 [1085/96 (1130%)]\tLoss: 8.770836\n",
      "Train Epoche: 2 [1086/96 (1131%)]\tLoss: 35.503033\n",
      "Train Epoche: 2 [1087/96 (1132%)]\tLoss: 142.998489\n",
      "Train Epoche: 2 [1088/96 (1133%)]\tLoss: 223.553818\n",
      "Train Epoche: 2 [1089/96 (1134%)]\tLoss: 167.876328\n",
      "Train Epoche: 2 [1090/96 (1135%)]\tLoss: 194.817337\n",
      "Train Epoche: 2 [1091/96 (1136%)]\tLoss: 398.608856\n",
      "Train Epoche: 2 [1092/96 (1138%)]\tLoss: 0.965265\n",
      "Train Epoche: 2 [1093/96 (1139%)]\tLoss: 3.923285\n",
      "Train Epoche: 2 [1094/96 (1140%)]\tLoss: 49.845531\n",
      "Train Epoche: 2 [1095/96 (1141%)]\tLoss: 64.238396\n",
      "Train Epoche: 2 [1096/96 (1142%)]\tLoss: 362.685730\n",
      "Train Epoche: 2 [1097/96 (1143%)]\tLoss: 325.127502\n",
      "Train Epoche: 2 [1098/96 (1144%)]\tLoss: 574.460388\n",
      "Train Epoche: 2 [1099/96 (1145%)]\tLoss: 3.841788\n",
      "Train Epoche: 2 [1100/96 (1146%)]\tLoss: 323.337921\n",
      "Train Epoche: 2 [1101/96 (1147%)]\tLoss: 574.193726\n",
      "Train Epoche: 2 [1102/96 (1148%)]\tLoss: 80.903450\n",
      "Train Epoche: 2 [1103/96 (1149%)]\tLoss: 574.928406\n",
      "Train Epoche: 2 [1104/96 (1150%)]\tLoss: 8.753638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1105/96 (1151%)]\tLoss: 63.325996\n",
      "Train Epoche: 2 [1106/96 (1152%)]\tLoss: 194.804688\n",
      "Train Epoche: 2 [1107/96 (1153%)]\tLoss: 15.683096\n",
      "Train Epoche: 2 [1108/96 (1154%)]\tLoss: 120.104523\n",
      "Train Epoche: 2 [1109/96 (1155%)]\tLoss: 0.932985\n",
      "Train Epoche: 2 [1110/96 (1156%)]\tLoss: 167.740860\n",
      "Train Epoche: 2 [1111/96 (1157%)]\tLoss: 98.969307\n",
      "Train Epoche: 2 [1112/96 (1158%)]\tLoss: 25.011494\n",
      "Train Epoche: 2 [1113/96 (1159%)]\tLoss: 574.324768\n",
      "Train Epoche: 2 [1114/96 (1160%)]\tLoss: 291.102081\n",
      "Train Epoche: 2 [1115/96 (1161%)]\tLoss: 576.881714\n",
      "Train Epoche: 2 [1116/96 (1162%)]\tLoss: 224.152451\n",
      "Train Epoche: 2 [1117/96 (1164%)]\tLoss: 362.804077\n",
      "Train Epoche: 2 [1118/96 (1165%)]\tLoss: 36.087303\n",
      "Train Epoche: 2 [1119/96 (1166%)]\tLoss: 49.448864\n",
      "Train Epoche: 2 [1120/96 (1167%)]\tLoss: 144.442932\n",
      "Train Epoche: 2 [1121/96 (1168%)]\tLoss: 256.786072\n",
      "Train Epoche: 2 [1122/96 (1169%)]\tLoss: 399.426086\n",
      "Train Epoche: 2 [1123/96 (1170%)]\tLoss: 399.093536\n",
      "Train Epoche: 2 [1124/96 (1171%)]\tLoss: 398.479828\n",
      "Train Epoche: 2 [1125/96 (1172%)]\tLoss: 24.953577\n",
      "Train Epoche: 2 [1126/96 (1173%)]\tLoss: 63.675392\n",
      "Train Epoche: 2 [1127/96 (1174%)]\tLoss: 80.271278\n",
      "Train Epoche: 2 [1128/96 (1175%)]\tLoss: 8.684436\n",
      "Train Epoche: 2 [1129/96 (1176%)]\tLoss: 0.961425\n",
      "Train Epoche: 2 [1130/96 (1177%)]\tLoss: 398.212006\n",
      "Train Epoche: 2 [1131/96 (1178%)]\tLoss: 48.240147\n",
      "Train Epoche: 2 [1132/96 (1179%)]\tLoss: 3.836344\n",
      "Train Epoche: 2 [1133/96 (1180%)]\tLoss: 35.697205\n",
      "Train Epoche: 2 [1134/96 (1181%)]\tLoss: 120.252876\n",
      "Train Epoche: 2 [1135/96 (1182%)]\tLoss: 98.928246\n",
      "Train Epoche: 2 [1136/96 (1183%)]\tLoss: 399.360840\n",
      "Train Epoche: 2 [1137/96 (1184%)]\tLoss: 15.671381\n",
      "Train Epoche: 2 [1138/96 (1185%)]\tLoss: 170.079758\n",
      "Train Epoche: 2 [1139/96 (1186%)]\tLoss: 145.641068\n",
      "Train Epoche: 2 [1140/96 (1188%)]\tLoss: 224.469940\n",
      "Train Epoche: 2 [1141/96 (1189%)]\tLoss: 199.991425\n",
      "Train Epoche: 2 [1142/96 (1190%)]\tLoss: 35.580238\n",
      "Train Epoche: 2 [1143/96 (1191%)]\tLoss: 99.253601\n",
      "Train Epoche: 2 [1144/96 (1192%)]\tLoss: 398.611969\n",
      "Train Epoche: 2 [1145/96 (1193%)]\tLoss: 195.440872\n",
      "Train Epoche: 2 [1146/96 (1194%)]\tLoss: 287.877106\n",
      "Train Epoche: 2 [1147/96 (1195%)]\tLoss: 142.971375\n",
      "Train Epoche: 2 [1148/96 (1196%)]\tLoss: 120.488235\n",
      "Train Epoche: 2 [1149/96 (1197%)]\tLoss: 48.431572\n",
      "Train Epoche: 2 [1150/96 (1198%)]\tLoss: 3.830852\n",
      "Train Epoche: 2 [1151/96 (1199%)]\tLoss: 0.914846\n",
      "Train Epoche: 2 [1152/96 (1200%)]\tLoss: 63.242275\n",
      "Train Epoche: 2 [1153/96 (1201%)]\tLoss: 80.558914\n",
      "Train Epoche: 2 [1154/96 (1202%)]\tLoss: 24.551018\n",
      "Train Epoche: 2 [1155/96 (1203%)]\tLoss: 254.801361\n",
      "Train Epoche: 2 [1156/96 (1204%)]\tLoss: 223.960907\n",
      "Train Epoche: 2 [1157/96 (1205%)]\tLoss: 15.696147\n",
      "Train Epoche: 2 [1158/96 (1206%)]\tLoss: 8.859399\n",
      "Train Epoche: 2 [1159/96 (1207%)]\tLoss: 402.469086\n",
      "Train Epoche: 2 [1160/96 (1208%)]\tLoss: 168.318420\n",
      "Train Epoche: 2 [1161/96 (1209%)]\tLoss: 255.317169\n",
      "Train Epoche: 2 [1162/96 (1210%)]\tLoss: 195.557861\n",
      "Train Epoche: 2 [1163/96 (1211%)]\tLoss: 80.347839\n",
      "Train Epoche: 2 [1164/96 (1212%)]\tLoss: 63.482040\n",
      "Train Epoche: 2 [1165/96 (1214%)]\tLoss: 119.946045\n",
      "Train Epoche: 2 [1166/96 (1215%)]\tLoss: 398.395020\n",
      "Train Epoche: 2 [1167/96 (1216%)]\tLoss: 48.429184\n",
      "Train Epoche: 2 [1168/96 (1217%)]\tLoss: 15.522543\n",
      "Train Epoche: 2 [1169/96 (1218%)]\tLoss: 24.714897\n",
      "Train Epoche: 2 [1170/96 (1219%)]\tLoss: 35.533993\n",
      "Train Epoche: 2 [1171/96 (1220%)]\tLoss: 8.805984\n",
      "Train Epoche: 2 [1172/96 (1221%)]\tLoss: 397.874451\n",
      "Train Epoche: 2 [1173/96 (1222%)]\tLoss: 167.950867\n",
      "Train Epoche: 2 [1174/96 (1223%)]\tLoss: 223.478882\n",
      "Train Epoche: 2 [1175/96 (1224%)]\tLoss: 0.962466\n",
      "Train Epoche: 2 [1176/96 (1225%)]\tLoss: 3.778201\n",
      "Train Epoche: 2 [1177/96 (1226%)]\tLoss: 100.798859\n",
      "Train Epoche: 2 [1178/96 (1227%)]\tLoss: 145.652390\n",
      "Train Epoche: 2 [1179/96 (1228%)]\tLoss: 0.890920\n",
      "Train Epoche: 2 [1180/96 (1229%)]\tLoss: 35.450836\n",
      "Train Epoche: 2 [1181/96 (1230%)]\tLoss: 48.449089\n",
      "Train Epoche: 2 [1182/96 (1231%)]\tLoss: 168.258270\n",
      "Train Epoche: 2 [1183/96 (1232%)]\tLoss: 224.034439\n",
      "Train Epoche: 2 [1184/96 (1233%)]\tLoss: 255.368927\n",
      "Train Epoche: 2 [1185/96 (1234%)]\tLoss: 24.591785\n",
      "Train Epoche: 2 [1186/96 (1235%)]\tLoss: 80.079453\n",
      "Train Epoche: 2 [1187/96 (1236%)]\tLoss: 119.971970\n",
      "Train Epoche: 2 [1188/96 (1238%)]\tLoss: 143.158554\n",
      "Train Epoche: 2 [1189/96 (1239%)]\tLoss: 63.356590\n",
      "Train Epoche: 2 [1190/96 (1240%)]\tLoss: 15.631474\n",
      "Train Epoche: 2 [1191/96 (1241%)]\tLoss: 322.872131\n",
      "Train Epoche: 2 [1192/96 (1242%)]\tLoss: 195.029404\n",
      "Train Epoche: 2 [1193/96 (1243%)]\tLoss: 99.914436\n",
      "Train Epoche: 2 [1194/96 (1244%)]\tLoss: 575.492188\n",
      "Train Epoche: 2 [1195/96 (1245%)]\tLoss: 484.559021\n",
      "Train Epoche: 2 [1196/96 (1246%)]\tLoss: 577.522522\n",
      "Train Epoche: 2 [1197/96 (1247%)]\tLoss: 290.046143\n",
      "Train Epoche: 2 [1198/96 (1248%)]\tLoss: 360.201782\n",
      "Train Epoche: 2 [1199/96 (1249%)]\tLoss: 4.200328\n",
      "Train Epoche: 2 [1200/96 (1250%)]\tLoss: 9.153158\n",
      "Train Epoche: 2 [1201/96 (1251%)]\tLoss: 441.740204\n",
      "Train Epoche: 2 [1202/96 (1252%)]\tLoss: 401.115662\n",
      "Train Epoche: 2 [1203/96 (1253%)]\tLoss: 143.134094\n",
      "Train Epoche: 2 [1204/96 (1254%)]\tLoss: 80.593674\n",
      "Train Epoche: 2 [1205/96 (1255%)]\tLoss: 120.364090\n",
      "Train Epoche: 2 [1206/96 (1256%)]\tLoss: 224.246094\n",
      "Train Epoche: 2 [1207/96 (1257%)]\tLoss: 255.132416\n",
      "Train Epoche: 2 [1208/96 (1258%)]\tLoss: 288.074829\n",
      "Train Epoche: 2 [1209/96 (1259%)]\tLoss: 24.648642\n",
      "Train Epoche: 2 [1210/96 (1260%)]\tLoss: 63.438221\n",
      "Train Epoche: 2 [1211/96 (1261%)]\tLoss: 99.528717\n",
      "Train Epoche: 2 [1212/96 (1262%)]\tLoss: 35.761448\n",
      "Train Epoche: 2 [1213/96 (1264%)]\tLoss: 3.823604\n",
      "Train Epoche: 2 [1214/96 (1265%)]\tLoss: 0.977705\n",
      "Train Epoche: 2 [1215/96 (1266%)]\tLoss: 195.366135\n",
      "Train Epoche: 2 [1216/96 (1267%)]\tLoss: 167.947296\n",
      "Train Epoche: 2 [1217/96 (1268%)]\tLoss: 48.471806\n",
      "Train Epoche: 2 [1218/96 (1269%)]\tLoss: 8.923141\n",
      "Train Epoche: 2 [1219/96 (1270%)]\tLoss: 362.520172\n",
      "Train Epoche: 2 [1220/96 (1271%)]\tLoss: 324.210083\n",
      "Train Epoche: 2 [1221/96 (1272%)]\tLoss: 16.100824\n",
      "Train Epoche: 2 [1222/96 (1273%)]\tLoss: 443.848999\n",
      "Train Epoche: 2 [1223/96 (1274%)]\tLoss: 401.627563\n",
      "Train Epoche: 2 [1224/96 (1275%)]\tLoss: 35.664818\n",
      "Train Epoche: 2 [1225/96 (1276%)]\tLoss: 8.697787\n",
      "Train Epoche: 2 [1226/96 (1277%)]\tLoss: 80.173218\n",
      "Train Epoche: 2 [1227/96 (1278%)]\tLoss: 322.645782\n",
      "Train Epoche: 2 [1228/96 (1279%)]\tLoss: 63.585316\n",
      "Train Epoche: 2 [1229/96 (1280%)]\tLoss: 574.231262\n",
      "Train Epoche: 2 [1230/96 (1281%)]\tLoss: 99.545555\n",
      "Train Epoche: 2 [1231/96 (1282%)]\tLoss: 574.552368\n",
      "Train Epoche: 2 [1232/96 (1283%)]\tLoss: 3.838370\n",
      "Train Epoche: 2 [1233/96 (1284%)]\tLoss: 573.913147\n",
      "Train Epoche: 2 [1234/96 (1285%)]\tLoss: 48.465984\n",
      "Train Epoche: 2 [1235/96 (1286%)]\tLoss: 143.131638\n",
      "Train Epoche: 2 [1236/96 (1288%)]\tLoss: 15.686986\n",
      "Train Epoche: 2 [1237/96 (1289%)]\tLoss: 0.937220\n",
      "Train Epoche: 2 [1238/96 (1290%)]\tLoss: 24.386559\n",
      "Train Epoche: 2 [1239/96 (1291%)]\tLoss: 120.748741\n",
      "Train Epoche: 2 [1240/96 (1292%)]\tLoss: 577.926697\n",
      "Train Epoche: 2 [1241/96 (1293%)]\tLoss: 194.781845\n",
      "Train Epoche: 2 [1242/96 (1294%)]\tLoss: 169.241119\n",
      "Train Epoche: 2 [1243/96 (1295%)]\tLoss: 577.199524\n",
      "Train Epoche: 2 [1244/96 (1296%)]\tLoss: 225.592651\n",
      "Train Epoche: 2 [1245/96 (1297%)]\tLoss: 255.695129\n",
      "Train Epoche: 2 [1246/96 (1298%)]\tLoss: 287.919830\n",
      "Train Epoche: 2 [1247/96 (1299%)]\tLoss: 0.921534\n",
      "Train Epoche: 2 [1248/96 (1300%)]\tLoss: 143.224915\n",
      "Train Epoche: 2 [1249/96 (1301%)]\tLoss: 574.004761\n",
      "Train Epoche: 2 [1250/96 (1302%)]\tLoss: 80.242607\n",
      "Train Epoche: 2 [1251/96 (1303%)]\tLoss: 63.648060\n",
      "Train Epoche: 2 [1252/96 (1304%)]\tLoss: 24.608006\n",
      "Train Epoche: 2 [1253/96 (1305%)]\tLoss: 15.588635\n",
      "Train Epoche: 2 [1254/96 (1306%)]\tLoss: 99.273026\n",
      "Train Epoche: 2 [1255/96 (1307%)]\tLoss: 35.645634\n",
      "Train Epoche: 2 [1256/96 (1308%)]\tLoss: 3.790187\n",
      "Train Epoche: 2 [1257/96 (1309%)]\tLoss: 168.255417\n",
      "Train Epoche: 2 [1258/96 (1310%)]\tLoss: 121.120651\n",
      "Train Epoche: 2 [1259/96 (1311%)]\tLoss: 48.648529\n",
      "Train Epoche: 2 [1260/96 (1312%)]\tLoss: 324.846985\n",
      "Train Epoche: 2 [1261/96 (1314%)]\tLoss: 577.036316\n",
      "Train Epoche: 2 [1262/96 (1315%)]\tLoss: 289.566406\n",
      "Train Epoche: 2 [1263/96 (1316%)]\tLoss: 196.541107\n",
      "Train Epoche: 2 [1264/96 (1317%)]\tLoss: 9.128892\n",
      "Train Epoche: 2 [1265/96 (1318%)]\tLoss: 224.355850\n",
      "Train Epoche: 2 [1266/96 (1319%)]\tLoss: 256.776093\n",
      "Train Epoche: 2 [1267/96 (1320%)]\tLoss: 48.718464\n",
      "Train Epoche: 2 [1268/96 (1321%)]\tLoss: 80.209053\n",
      "Train Epoche: 2 [1269/96 (1322%)]\tLoss: 99.086052\n",
      "Train Epoche: 2 [1270/96 (1323%)]\tLoss: 483.501801\n",
      "Train Epoche: 2 [1271/96 (1324%)]\tLoss: 168.434662\n",
      "Train Epoche: 2 [1272/96 (1325%)]\tLoss: 143.494888\n",
      "Train Epoche: 2 [1273/96 (1326%)]\tLoss: 24.617586\n",
      "Train Epoche: 2 [1274/96 (1327%)]\tLoss: 63.408401\n",
      "Train Epoche: 2 [1275/96 (1328%)]\tLoss: 482.228394\n",
      "Train Epoche: 2 [1276/96 (1329%)]\tLoss: 323.105591\n",
      "Train Epoche: 2 [1277/96 (1330%)]\tLoss: 15.686963\n",
      "Train Epoche: 2 [1278/96 (1331%)]\tLoss: 8.773678\n",
      "Train Epoche: 2 [1279/96 (1332%)]\tLoss: 120.209457\n",
      "Train Epoche: 2 [1280/96 (1333%)]\tLoss: 481.730682\n",
      "Train Epoche: 2 [1281/96 (1334%)]\tLoss: 1.060016\n",
      "Train Epoche: 2 [1282/96 (1335%)]\tLoss: 360.481232\n",
      "Train Epoche: 2 [1283/96 (1336%)]\tLoss: 224.853546\n",
      "Train Epoche: 2 [1284/96 (1338%)]\tLoss: 197.747116\n",
      "Train Epoche: 2 [1285/96 (1339%)]\tLoss: 4.115566\n",
      "Train Epoche: 2 [1286/96 (1340%)]\tLoss: 35.390877\n",
      "Train Epoche: 2 [1287/96 (1341%)]\tLoss: 290.749878\n",
      "Train Epoche: 2 [1288/96 (1342%)]\tLoss: 256.186920\n",
      "Train Epoche: 2 [1289/96 (1343%)]\tLoss: 322.594330\n",
      "Train Epoche: 2 [1290/96 (1344%)]\tLoss: 35.859119\n",
      "Train Epoche: 2 [1291/96 (1345%)]\tLoss: 399.387909\n",
      "Train Epoche: 2 [1292/96 (1346%)]\tLoss: 79.861282\n",
      "Train Epoche: 2 [1293/96 (1347%)]\tLoss: 195.518738\n",
      "Train Epoche: 2 [1294/96 (1348%)]\tLoss: 143.761307\n",
      "Train Epoche: 2 [1295/96 (1349%)]\tLoss: 483.401642\n",
      "Train Epoche: 2 [1296/96 (1350%)]\tLoss: 63.931255\n",
      "Train Epoche: 2 [1297/96 (1351%)]\tLoss: 9.069347\n",
      "Train Epoche: 2 [1298/96 (1352%)]\tLoss: 483.578125\n",
      "Train Epoche: 2 [1299/96 (1353%)]\tLoss: 360.197449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1300/96 (1354%)]\tLoss: 287.617828\n",
      "Train Epoche: 2 [1301/96 (1355%)]\tLoss: 25.256376\n",
      "Train Epoche: 2 [1302/96 (1356%)]\tLoss: 3.915358\n",
      "Train Epoche: 2 [1303/96 (1357%)]\tLoss: 224.345642\n",
      "Train Epoche: 2 [1304/96 (1358%)]\tLoss: 167.790222\n",
      "Train Epoche: 2 [1305/96 (1359%)]\tLoss: 0.935785\n",
      "Train Epoche: 2 [1306/96 (1360%)]\tLoss: 15.810918\n",
      "Train Epoche: 2 [1307/96 (1361%)]\tLoss: 99.282471\n",
      "Train Epoche: 2 [1308/96 (1362%)]\tLoss: 255.569763\n",
      "Train Epoche: 2 [1309/96 (1364%)]\tLoss: 50.056831\n",
      "Train Epoche: 2 [1310/96 (1365%)]\tLoss: 122.135429\n",
      "Train Epoche: 2 [1311/96 (1366%)]\tLoss: 143.230713\n",
      "Train Epoche: 2 [1312/96 (1367%)]\tLoss: 399.369232\n",
      "Train Epoche: 2 [1313/96 (1368%)]\tLoss: 167.752029\n",
      "Train Epoche: 2 [1314/96 (1369%)]\tLoss: 255.409851\n",
      "Train Epoche: 2 [1315/96 (1370%)]\tLoss: 35.652557\n",
      "Train Epoche: 2 [1316/96 (1371%)]\tLoss: 223.787567\n",
      "Train Epoche: 2 [1317/96 (1372%)]\tLoss: 80.398216\n",
      "Train Epoche: 2 [1318/96 (1373%)]\tLoss: 48.586681\n",
      "Train Epoche: 2 [1319/96 (1374%)]\tLoss: 3.847583\n",
      "Train Epoche: 2 [1320/96 (1375%)]\tLoss: 15.607309\n",
      "Train Epoche: 2 [1321/96 (1376%)]\tLoss: 24.561409\n",
      "Train Epoche: 2 [1322/96 (1377%)]\tLoss: 8.653836\n",
      "Train Epoche: 2 [1323/96 (1378%)]\tLoss: 398.156433\n",
      "Train Epoche: 2 [1324/96 (1379%)]\tLoss: 120.331154\n",
      "Train Epoche: 2 [1325/96 (1380%)]\tLoss: 63.279537\n",
      "Train Epoche: 2 [1326/96 (1381%)]\tLoss: 1.024749\n",
      "Train Epoche: 2 [1327/96 (1382%)]\tLoss: 400.031067\n",
      "Train Epoche: 2 [1328/96 (1383%)]\tLoss: 99.470421\n",
      "Train Epoche: 2 [1329/96 (1384%)]\tLoss: 196.832672\n",
      "Train Epoche: 2 [1330/96 (1385%)]\tLoss: 398.924683\n",
      "Train Epoche: 2 [1331/96 (1386%)]\tLoss: 48.935188\n",
      "Train Epoche: 2 [1332/96 (1388%)]\tLoss: 120.958542\n",
      "Train Epoche: 2 [1333/96 (1389%)]\tLoss: 63.736568\n",
      "Train Epoche: 2 [1334/96 (1390%)]\tLoss: 398.125519\n",
      "Train Epoche: 2 [1335/96 (1391%)]\tLoss: 35.624851\n",
      "Train Epoche: 2 [1336/96 (1392%)]\tLoss: 398.516632\n",
      "Train Epoche: 2 [1337/96 (1393%)]\tLoss: 15.992478\n",
      "Train Epoche: 2 [1338/96 (1394%)]\tLoss: 24.659702\n",
      "Train Epoche: 2 [1339/96 (1395%)]\tLoss: 99.233635\n",
      "Train Epoche: 2 [1340/96 (1396%)]\tLoss: 3.837057\n",
      "Train Epoche: 2 [1341/96 (1397%)]\tLoss: 398.546539\n",
      "Train Epoche: 2 [1342/96 (1398%)]\tLoss: 143.164841\n",
      "Train Epoche: 2 [1343/96 (1399%)]\tLoss: 0.989287\n",
      "Train Epoche: 2 [1344/96 (1400%)]\tLoss: 8.912546\n",
      "Train Epoche: 2 [1345/96 (1401%)]\tLoss: 81.355782\n",
      "Train Epoche: 2 [1346/96 (1402%)]\tLoss: 400.314392\n",
      "Train Epoche: 2 [1347/96 (1403%)]\tLoss: 120.005341\n",
      "Train Epoche: 2 [1348/96 (1404%)]\tLoss: 142.954269\n",
      "Train Epoche: 2 [1349/96 (1405%)]\tLoss: 168.092422\n",
      "Train Epoche: 2 [1350/96 (1406%)]\tLoss: 24.685596\n",
      "Train Epoche: 2 [1351/96 (1407%)]\tLoss: 482.461823\n",
      "Train Epoche: 2 [1352/96 (1408%)]\tLoss: 195.398880\n",
      "Train Epoche: 2 [1353/96 (1409%)]\tLoss: 35.420364\n",
      "Train Epoche: 2 [1354/96 (1410%)]\tLoss: 48.535263\n",
      "Train Epoche: 2 [1355/96 (1411%)]\tLoss: 98.951866\n",
      "Train Epoche: 2 [1356/96 (1412%)]\tLoss: 80.331100\n",
      "Train Epoche: 2 [1357/96 (1414%)]\tLoss: 15.550592\n",
      "Train Epoche: 2 [1358/96 (1415%)]\tLoss: 8.693122\n",
      "Train Epoche: 2 [1359/96 (1416%)]\tLoss: 288.650696\n",
      "Train Epoche: 2 [1360/96 (1417%)]\tLoss: 254.588684\n",
      "Train Epoche: 2 [1361/96 (1418%)]\tLoss: 0.949208\n",
      "Train Epoche: 2 [1362/96 (1419%)]\tLoss: 3.945851\n",
      "Train Epoche: 2 [1363/96 (1420%)]\tLoss: 485.949158\n",
      "Train Epoche: 2 [1364/96 (1421%)]\tLoss: 400.569214\n",
      "Train Epoche: 2 [1365/96 (1422%)]\tLoss: 63.109329\n",
      "Train Epoche: 2 [1366/96 (1423%)]\tLoss: 224.914948\n",
      "Train Epoche: 2 [1367/96 (1424%)]\tLoss: 362.075012\n",
      "Train Epoche: 2 [1368/96 (1425%)]\tLoss: 326.046265\n",
      "Train Epoche: 2 [1369/96 (1426%)]\tLoss: 24.515911\n",
      "Train Epoche: 2 [1370/96 (1427%)]\tLoss: 80.386429\n",
      "Train Epoche: 2 [1371/96 (1428%)]\tLoss: 143.515091\n",
      "Train Epoche: 2 [1372/96 (1429%)]\tLoss: 168.908325\n",
      "Train Epoche: 2 [1373/96 (1430%)]\tLoss: 15.816442\n",
      "Train Epoche: 2 [1374/96 (1431%)]\tLoss: 399.536957\n",
      "Train Epoche: 2 [1375/96 (1432%)]\tLoss: 398.253784\n",
      "Train Epoche: 2 [1376/96 (1433%)]\tLoss: 0.926359\n",
      "Train Epoche: 2 [1377/96 (1434%)]\tLoss: 398.168976\n",
      "Train Epoche: 2 [1378/96 (1435%)]\tLoss: 398.583405\n",
      "Train Epoche: 2 [1379/96 (1436%)]\tLoss: 8.823168\n",
      "Train Epoche: 2 [1380/96 (1438%)]\tLoss: 3.813377\n",
      "Train Epoche: 2 [1381/96 (1439%)]\tLoss: 99.429054\n",
      "Train Epoche: 2 [1382/96 (1440%)]\tLoss: 119.854904\n",
      "Train Epoche: 2 [1383/96 (1441%)]\tLoss: 35.441963\n",
      "Train Epoche: 2 [1384/96 (1442%)]\tLoss: 64.110916\n",
      "Train Epoche: 2 [1385/96 (1443%)]\tLoss: 49.738903\n",
      "Train Epoche: 2 [1386/96 (1444%)]\tLoss: 197.119461\n",
      "Train Epoche: 2 [1387/96 (1445%)]\tLoss: 255.065582\n",
      "Train Epoche: 2 [1388/96 (1446%)]\tLoss: 224.413818\n",
      "Train Epoche: 2 [1389/96 (1447%)]\tLoss: 15.632904\n",
      "Train Epoche: 2 [1390/96 (1448%)]\tLoss: 35.544884\n",
      "Train Epoche: 2 [1391/96 (1449%)]\tLoss: 223.692963\n",
      "Train Epoche: 2 [1392/96 (1450%)]\tLoss: 287.678192\n",
      "Train Epoche: 2 [1393/96 (1451%)]\tLoss: 48.369358\n",
      "Train Epoche: 2 [1394/96 (1452%)]\tLoss: 63.358608\n",
      "Train Epoche: 2 [1395/96 (1453%)]\tLoss: 80.170746\n",
      "Train Epoche: 2 [1396/96 (1454%)]\tLoss: 255.012283\n",
      "Train Epoche: 2 [1397/96 (1455%)]\tLoss: 8.790123\n",
      "Train Epoche: 2 [1398/96 (1456%)]\tLoss: 120.226082\n",
      "Train Epoche: 2 [1399/96 (1457%)]\tLoss: 168.029419\n",
      "Train Epoche: 2 [1400/96 (1458%)]\tLoss: 574.032227\n",
      "Train Epoche: 2 [1401/96 (1459%)]\tLoss: 3.844683\n",
      "Train Epoche: 2 [1402/96 (1460%)]\tLoss: 0.927339\n",
      "Train Epoche: 2 [1403/96 (1461%)]\tLoss: 99.710960\n",
      "Train Epoche: 2 [1404/96 (1462%)]\tLoss: 194.946701\n",
      "Train Epoche: 2 [1405/96 (1464%)]\tLoss: 24.831589\n",
      "Train Epoche: 2 [1406/96 (1465%)]\tLoss: 143.100830\n",
      "Train Epoche: 2 [1407/96 (1466%)]\tLoss: 360.602417\n",
      "Train Epoche: 2 [1408/96 (1467%)]\tLoss: 324.592926\n",
      "Train Epoche: 2 [1409/96 (1468%)]\tLoss: 399.424194\n",
      "Train Epoche: 2 [1410/96 (1469%)]\tLoss: 486.621796\n",
      "Train Epoche: 2 [1411/96 (1470%)]\tLoss: 441.674774\n",
      "Train Epoche: 2 [1412/96 (1471%)]\tLoss: 120.427376\n",
      "Train Epoche: 2 [1413/96 (1472%)]\tLoss: 80.939812\n",
      "Train Epoche: 2 [1414/96 (1473%)]\tLoss: 16.106327\n",
      "Train Epoche: 2 [1415/96 (1474%)]\tLoss: 143.530167\n",
      "Train Epoche: 2 [1416/96 (1475%)]\tLoss: 100.060379\n",
      "Train Epoche: 2 [1417/96 (1476%)]\tLoss: 398.421356\n",
      "Train Epoche: 2 [1418/96 (1477%)]\tLoss: 63.624218\n",
      "Train Epoche: 2 [1419/96 (1478%)]\tLoss: 3.939213\n",
      "Train Epoche: 2 [1420/96 (1479%)]\tLoss: 8.900622\n",
      "Train Epoche: 2 [1421/96 (1480%)]\tLoss: 223.624161\n",
      "Train Epoche: 2 [1422/96 (1481%)]\tLoss: 25.172424\n",
      "Train Epoche: 2 [1423/96 (1482%)]\tLoss: 35.672577\n",
      "Train Epoche: 2 [1424/96 (1483%)]\tLoss: 0.918310\n",
      "Train Epoche: 2 [1425/96 (1484%)]\tLoss: 398.446564\n",
      "Train Epoche: 2 [1426/96 (1485%)]\tLoss: 403.597992\n",
      "Train Epoche: 2 [1427/96 (1486%)]\tLoss: 50.077114\n",
      "Train Epoche: 2 [1428/96 (1488%)]\tLoss: 196.191193\n",
      "Train Epoche: 2 [1429/96 (1489%)]\tLoss: 170.515167\n",
      "Train Epoche: 2 [1430/96 (1490%)]\tLoss: 398.607941\n",
      "Train Epoche: 2 [1431/96 (1491%)]\tLoss: 195.405624\n",
      "Train Epoche: 2 [1432/96 (1492%)]\tLoss: 35.487350\n",
      "Train Epoche: 2 [1433/96 (1493%)]\tLoss: 8.853960\n",
      "Train Epoche: 2 [1434/96 (1494%)]\tLoss: 80.019142\n",
      "Train Epoche: 2 [1435/96 (1495%)]\tLoss: 168.376816\n",
      "Train Epoche: 2 [1436/96 (1496%)]\tLoss: 398.427155\n",
      "Train Epoche: 2 [1437/96 (1497%)]\tLoss: 398.096741\n",
      "Train Epoche: 2 [1438/96 (1498%)]\tLoss: 48.449638\n",
      "Train Epoche: 2 [1439/96 (1499%)]\tLoss: 63.157787\n",
      "Train Epoche: 2 [1440/96 (1500%)]\tLoss: 24.732584\n",
      "Train Epoche: 2 [1441/96 (1501%)]\tLoss: 15.656075\n",
      "Train Epoche: 2 [1442/96 (1502%)]\tLoss: 143.577881\n",
      "Train Epoche: 2 [1443/96 (1503%)]\tLoss: 398.804291\n",
      "Train Epoche: 2 [1444/96 (1504%)]\tLoss: 3.820679\n",
      "Train Epoche: 2 [1445/96 (1505%)]\tLoss: 0.942189\n",
      "Train Epoche: 2 [1446/96 (1506%)]\tLoss: 100.196365\n",
      "Train Epoche: 2 [1447/96 (1507%)]\tLoss: 121.551628\n",
      "Train Epoche: 2 [1448/96 (1508%)]\tLoss: 255.369278\n",
      "Train Epoche: 2 [1449/96 (1509%)]\tLoss: 226.475555\n",
      "Train Epoche: 2 [1450/96 (1510%)]\tLoss: 24.615068\n",
      "Train Epoche: 2 [1451/96 (1511%)]\tLoss: 255.615677\n",
      "Train Epoche: 2 [1452/96 (1512%)]\tLoss: 99.549934\n",
      "Train Epoche: 2 [1453/96 (1514%)]\tLoss: 573.955811\n",
      "Train Epoche: 2 [1454/96 (1515%)]\tLoss: 143.179672\n",
      "Train Epoche: 2 [1455/96 (1516%)]\tLoss: 8.869558\n",
      "Train Epoche: 2 [1456/96 (1517%)]\tLoss: 35.600708\n",
      "Train Epoche: 2 [1457/96 (1518%)]\tLoss: 63.339626\n",
      "Train Epoche: 2 [1458/96 (1519%)]\tLoss: 48.556812\n",
      "Train Epoche: 2 [1459/96 (1520%)]\tLoss: 0.912332\n",
      "Train Epoche: 2 [1460/96 (1521%)]\tLoss: 15.678032\n",
      "Train Epoche: 2 [1461/96 (1522%)]\tLoss: 573.118713\n",
      "Train Epoche: 2 [1462/96 (1523%)]\tLoss: 119.887978\n",
      "Train Epoche: 2 [1463/96 (1524%)]\tLoss: 3.877095\n",
      "Train Epoche: 2 [1464/96 (1525%)]\tLoss: 577.841858\n",
      "Train Epoche: 2 [1465/96 (1526%)]\tLoss: 224.553619\n",
      "Train Epoche: 2 [1466/96 (1527%)]\tLoss: 170.657303\n",
      "Train Epoche: 2 [1467/96 (1528%)]\tLoss: 574.867065\n",
      "Train Epoche: 2 [1468/96 (1529%)]\tLoss: 80.920097\n",
      "Train Epoche: 2 [1469/96 (1530%)]\tLoss: 198.585190\n",
      "Train Epoche: 2 [1470/96 (1531%)]\tLoss: 579.140381\n",
      "Train Epoche: 2 [1471/96 (1532%)]\tLoss: 8.828041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1472/96 (1533%)]\tLoss: 3.817421\n",
      "Train Epoche: 2 [1473/96 (1534%)]\tLoss: 48.807827\n",
      "Train Epoche: 2 [1474/96 (1535%)]\tLoss: 63.362209\n",
      "Train Epoche: 2 [1475/96 (1536%)]\tLoss: 287.825989\n",
      "Train Epoche: 2 [1476/96 (1538%)]\tLoss: 254.961746\n",
      "Train Epoche: 2 [1477/96 (1539%)]\tLoss: 80.655380\n",
      "Train Epoche: 2 [1478/96 (1540%)]\tLoss: 168.191666\n",
      "Train Epoche: 2 [1479/96 (1541%)]\tLoss: 223.833069\n",
      "Train Epoche: 2 [1480/96 (1542%)]\tLoss: 142.978622\n",
      "Train Epoche: 2 [1481/96 (1543%)]\tLoss: 15.820403\n",
      "Train Epoche: 2 [1482/96 (1544%)]\tLoss: 24.521843\n",
      "Train Epoche: 2 [1483/96 (1545%)]\tLoss: 99.206253\n",
      "Train Epoche: 2 [1484/96 (1546%)]\tLoss: 119.756989\n",
      "Train Epoche: 2 [1485/96 (1547%)]\tLoss: 0.963506\n",
      "Train Epoche: 2 [1486/96 (1548%)]\tLoss: 579.165955\n",
      "Train Epoche: 2 [1487/96 (1549%)]\tLoss: 440.141174\n",
      "Train Epoche: 2 [1488/96 (1550%)]\tLoss: 483.953186\n",
      "Train Epoche: 2 [1489/96 (1551%)]\tLoss: 534.451965\n",
      "Train Epoche: 2 [1490/96 (1552%)]\tLoss: 325.642670\n",
      "Train Epoche: 2 [1491/96 (1553%)]\tLoss: 195.087341\n",
      "Train Epoche: 2 [1492/96 (1554%)]\tLoss: 35.968082\n",
      "Train Epoche: 2 [1493/96 (1555%)]\tLoss: 361.815552\n",
      "Train Epoche: 2 [1494/96 (1556%)]\tLoss: 402.008972\n",
      "Train Epoche: 2 [1495/96 (1557%)]\tLoss: 168.144592\n",
      "Train Epoche: 2 [1496/96 (1558%)]\tLoss: 397.943085\n",
      "Train Epoche: 2 [1497/96 (1559%)]\tLoss: 120.510841\n",
      "Train Epoche: 2 [1498/96 (1560%)]\tLoss: 143.124222\n",
      "Train Epoche: 2 [1499/96 (1561%)]\tLoss: 63.450714\n",
      "Train Epoche: 2 [1500/96 (1562%)]\tLoss: 482.145477\n",
      "Train Epoche: 2 [1501/96 (1564%)]\tLoss: 8.801206\n",
      "Train Epoche: 2 [1502/96 (1565%)]\tLoss: 35.466152\n",
      "Train Epoche: 2 [1503/96 (1566%)]\tLoss: 48.337460\n",
      "Train Epoche: 2 [1504/96 (1567%)]\tLoss: 80.567406\n",
      "Train Epoche: 2 [1505/96 (1568%)]\tLoss: 3.878320\n",
      "Train Epoche: 2 [1506/96 (1569%)]\tLoss: 482.049500\n",
      "Train Epoche: 2 [1507/96 (1570%)]\tLoss: 99.044838\n",
      "Train Epoche: 2 [1508/96 (1571%)]\tLoss: 194.446320\n",
      "Train Epoche: 2 [1509/96 (1572%)]\tLoss: 16.020521\n",
      "Train Epoche: 2 [1510/96 (1573%)]\tLoss: 0.937070\n",
      "Train Epoche: 2 [1511/96 (1574%)]\tLoss: 225.689240\n",
      "Train Epoche: 2 [1512/96 (1575%)]\tLoss: 322.030060\n",
      "Train Epoche: 2 [1513/96 (1576%)]\tLoss: 26.239527\n",
      "Train Epoche: 2 [1514/96 (1577%)]\tLoss: 362.599152\n",
      "Train Epoche: 2 [1515/96 (1578%)]\tLoss: 291.610291\n",
      "Train Epoche: 2 [1516/96 (1579%)]\tLoss: 256.986053\n",
      "Train Epoche: 2 [1517/96 (1580%)]\tLoss: 9.090570\n",
      "Train Epoche: 2 [1518/96 (1581%)]\tLoss: 195.282959\n",
      "Train Epoche: 2 [1519/96 (1582%)]\tLoss: 35.423767\n",
      "Train Epoche: 2 [1520/96 (1583%)]\tLoss: 360.314392\n",
      "Train Epoche: 2 [1521/96 (1584%)]\tLoss: 143.960846\n",
      "Train Epoche: 2 [1522/96 (1585%)]\tLoss: 63.969334\n",
      "Train Epoche: 2 [1523/96 (1586%)]\tLoss: 1.050761\n",
      "Train Epoche: 2 [1524/96 (1588%)]\tLoss: 224.604156\n",
      "Train Epoche: 2 [1525/96 (1589%)]\tLoss: 80.652023\n",
      "Train Epoche: 2 [1526/96 (1590%)]\tLoss: 49.216145\n",
      "Train Epoche: 2 [1527/96 (1591%)]\tLoss: 15.977147\n",
      "Train Epoche: 2 [1528/96 (1592%)]\tLoss: 120.224075\n",
      "Train Epoche: 2 [1529/96 (1593%)]\tLoss: 574.525452\n",
      "Train Epoche: 2 [1530/96 (1594%)]\tLoss: 3.966567\n",
      "Train Epoche: 2 [1531/96 (1595%)]\tLoss: 168.351212\n",
      "Train Epoche: 2 [1532/96 (1596%)]\tLoss: 99.457047\n",
      "Train Epoche: 2 [1533/96 (1597%)]\tLoss: 485.937134\n",
      "Train Epoche: 2 [1534/96 (1598%)]\tLoss: 440.744263\n",
      "Train Epoche: 2 [1535/96 (1599%)]\tLoss: 323.811554\n",
      "Train Epoche: 2 [1536/96 (1600%)]\tLoss: 256.422913\n",
      "Train Epoche: 2 [1537/96 (1601%)]\tLoss: 25.249102\n",
      "Train Epoche: 2 [1538/96 (1602%)]\tLoss: 576.727539\n",
      "Train Epoche: 2 [1539/96 (1603%)]\tLoss: 291.552277\n",
      "Train Epoche: 2 [1540/96 (1604%)]\tLoss: 399.960175\n",
      "Train Epoche: 2 [1541/96 (1605%)]\tLoss: 575.296814\n",
      "Train Epoche: 2 [1542/96 (1606%)]\tLoss: 0.945718\n",
      "Train Epoche: 2 [1543/96 (1607%)]\tLoss: 574.394470\n",
      "Train Epoche: 2 [1544/96 (1608%)]\tLoss: 168.175842\n",
      "Train Epoche: 2 [1545/96 (1609%)]\tLoss: 63.351822\n",
      "Train Epoche: 2 [1546/96 (1610%)]\tLoss: 3.907483\n",
      "Train Epoche: 2 [1547/96 (1611%)]\tLoss: 8.694009\n",
      "Train Epoche: 2 [1548/96 (1612%)]\tLoss: 24.567451\n",
      "Train Epoche: 2 [1549/96 (1614%)]\tLoss: 359.836548\n",
      "Train Epoche: 2 [1550/96 (1615%)]\tLoss: 15.629664\n",
      "Train Epoche: 2 [1551/96 (1616%)]\tLoss: 35.863605\n",
      "Train Epoche: 2 [1552/96 (1617%)]\tLoss: 80.439842\n",
      "Train Epoche: 2 [1553/96 (1618%)]\tLoss: 224.443130\n",
      "Train Epoche: 2 [1554/96 (1619%)]\tLoss: 48.730927\n",
      "Train Epoche: 2 [1555/96 (1620%)]\tLoss: 289.959412\n",
      "Train Epoche: 2 [1556/96 (1621%)]\tLoss: 325.109741\n",
      "Train Epoche: 2 [1557/96 (1622%)]\tLoss: 196.195190\n",
      "Train Epoche: 2 [1558/96 (1623%)]\tLoss: 122.165680\n",
      "Train Epoche: 2 [1559/96 (1624%)]\tLoss: 101.620224\n",
      "Train Epoche: 2 [1560/96 (1625%)]\tLoss: 573.922546\n",
      "Train Epoche: 2 [1561/96 (1626%)]\tLoss: 258.035339\n",
      "Train Epoche: 2 [1562/96 (1627%)]\tLoss: 144.839386\n",
      "Train Epoche: 2 [1563/96 (1628%)]\tLoss: 15.717607\n",
      "Train Epoche: 2 [1564/96 (1629%)]\tLoss: 3.808154\n",
      "Train Epoche: 2 [1565/96 (1630%)]\tLoss: 143.580566\n",
      "Train Epoche: 2 [1566/96 (1631%)]\tLoss: 120.006966\n",
      "Train Epoche: 2 [1567/96 (1632%)]\tLoss: 80.360420\n",
      "Train Epoche: 2 [1568/96 (1633%)]\tLoss: 99.598579\n",
      "Train Epoche: 2 [1569/96 (1634%)]\tLoss: 48.214767\n",
      "Train Epoche: 2 [1570/96 (1635%)]\tLoss: 8.739133\n",
      "Train Epoche: 2 [1571/96 (1636%)]\tLoss: 35.510136\n",
      "Train Epoche: 2 [1572/96 (1638%)]\tLoss: 573.603333\n",
      "Train Epoche: 2 [1573/96 (1639%)]\tLoss: 63.358368\n",
      "Train Epoche: 2 [1574/96 (1640%)]\tLoss: 573.004456\n",
      "Train Epoche: 2 [1575/96 (1641%)]\tLoss: 0.887809\n",
      "Train Epoche: 2 [1576/96 (1642%)]\tLoss: 573.540527\n",
      "Train Epoche: 2 [1577/96 (1643%)]\tLoss: 573.607178\n",
      "Train Epoche: 2 [1578/96 (1644%)]\tLoss: 24.870611\n",
      "Train Epoche: 2 [1579/96 (1645%)]\tLoss: 168.349518\n",
      "Train Epoche: 2 [1580/96 (1646%)]\tLoss: 196.473190\n",
      "Train Epoche: 2 [1581/96 (1647%)]\tLoss: 226.089188\n",
      "Train Epoche: 2 [1582/96 (1648%)]\tLoss: 575.523193\n",
      "Train Epoche: 2 [1583/96 (1649%)]\tLoss: 575.375122\n",
      "Train Epoche: 2 [1584/96 (1650%)]\tLoss: 35.542553\n",
      "Train Epoche: 2 [1585/96 (1651%)]\tLoss: 63.301258\n",
      "Train Epoche: 2 [1586/96 (1652%)]\tLoss: 223.795288\n",
      "Train Epoche: 2 [1587/96 (1653%)]\tLoss: 255.513107\n",
      "Train Epoche: 2 [1588/96 (1654%)]\tLoss: 143.133209\n",
      "Train Epoche: 2 [1589/96 (1655%)]\tLoss: 482.371063\n",
      "Train Epoche: 2 [1590/96 (1656%)]\tLoss: 15.748587\n",
      "Train Epoche: 2 [1591/96 (1657%)]\tLoss: 482.332611\n",
      "Train Epoche: 2 [1592/96 (1658%)]\tLoss: 167.678482\n",
      "Train Epoche: 2 [1593/96 (1659%)]\tLoss: 120.270927\n",
      "Train Epoche: 2 [1594/96 (1660%)]\tLoss: 48.436543\n",
      "Train Epoche: 2 [1595/96 (1661%)]\tLoss: 0.897780\n",
      "Train Epoche: 2 [1596/96 (1662%)]\tLoss: 99.102119\n",
      "Train Epoche: 2 [1597/96 (1664%)]\tLoss: 194.327286\n",
      "Train Epoche: 2 [1598/96 (1665%)]\tLoss: 24.826904\n",
      "Train Epoche: 2 [1599/96 (1666%)]\tLoss: 81.524017\n",
      "Train Epoche: 2 [1600/96 (1667%)]\tLoss: 292.369843\n",
      "Train Epoche: 2 [1601/96 (1668%)]\tLoss: 324.757324\n",
      "Train Epoche: 2 [1602/96 (1669%)]\tLoss: 4.114752\n",
      "Train Epoche: 2 [1603/96 (1670%)]\tLoss: 9.082800\n",
      "Train Epoche: 2 [1604/96 (1671%)]\tLoss: 366.022919\n",
      "Train Epoche: 2 [1605/96 (1672%)]\tLoss: 482.294098\n",
      "Train Epoche: 2 [1606/96 (1673%)]\tLoss: 63.568714\n",
      "Train Epoche: 2 [1607/96 (1674%)]\tLoss: 99.347816\n",
      "Train Epoche: 2 [1608/96 (1675%)]\tLoss: 8.802318\n",
      "Train Epoche: 2 [1609/96 (1676%)]\tLoss: 15.504191\n",
      "Train Epoche: 2 [1610/96 (1677%)]\tLoss: 167.994278\n",
      "Train Epoche: 2 [1611/96 (1678%)]\tLoss: 120.340073\n",
      "Train Epoche: 2 [1612/96 (1679%)]\tLoss: 482.522308\n",
      "Train Epoche: 2 [1613/96 (1680%)]\tLoss: 80.691284\n",
      "Train Epoche: 2 [1614/96 (1681%)]\tLoss: 142.978653\n",
      "Train Epoche: 2 [1615/96 (1682%)]\tLoss: 48.596127\n",
      "Train Epoche: 2 [1616/96 (1683%)]\tLoss: 35.632473\n",
      "Train Epoche: 2 [1617/96 (1684%)]\tLoss: 24.582380\n",
      "Train Epoche: 2 [1618/96 (1685%)]\tLoss: 223.914062\n",
      "Train Epoche: 2 [1619/96 (1686%)]\tLoss: 359.801392\n",
      "Train Epoche: 2 [1620/96 (1688%)]\tLoss: 1.052035\n",
      "Train Epoche: 2 [1621/96 (1689%)]\tLoss: 3.949558\n",
      "Train Epoche: 2 [1622/96 (1690%)]\tLoss: 289.720734\n",
      "Train Epoche: 2 [1623/96 (1691%)]\tLoss: 399.843750\n",
      "Train Epoche: 2 [1624/96 (1692%)]\tLoss: 255.818420\n",
      "Train Epoche: 2 [1625/96 (1693%)]\tLoss: 195.412582\n",
      "Train Epoche: 2 [1626/96 (1694%)]\tLoss: 485.268921\n",
      "Train Epoche: 2 [1627/96 (1695%)]\tLoss: 323.113251\n",
      "Train Epoche: 2 [1628/96 (1696%)]\tLoss: 24.651022\n",
      "Train Epoche: 2 [1629/96 (1697%)]\tLoss: 15.818097\n",
      "Train Epoche: 2 [1630/96 (1698%)]\tLoss: 194.736298\n",
      "Train Epoche: 2 [1631/96 (1699%)]\tLoss: 63.177578\n",
      "Train Epoche: 2 [1632/96 (1700%)]\tLoss: 99.567535\n",
      "Train Epoche: 2 [1633/96 (1701%)]\tLoss: 167.698318\n",
      "Train Epoche: 2 [1634/96 (1702%)]\tLoss: 3.797384\n",
      "Train Epoche: 2 [1635/96 (1703%)]\tLoss: 48.552769\n",
      "Train Epoche: 2 [1636/96 (1704%)]\tLoss: 143.301727\n",
      "Train Epoche: 2 [1637/96 (1705%)]\tLoss: 80.350281\n",
      "Train Epoche: 2 [1638/96 (1706%)]\tLoss: 0.885439\n",
      "Train Epoche: 2 [1639/96 (1707%)]\tLoss: 8.753626\n",
      "Train Epoche: 2 [1640/96 (1708%)]\tLoss: 573.908630\n",
      "Train Epoche: 2 [1641/96 (1709%)]\tLoss: 120.278038\n",
      "Train Epoche: 2 [1642/96 (1710%)]\tLoss: 323.750519\n",
      "Train Epoche: 2 [1643/96 (1711%)]\tLoss: 578.232544\n",
      "Train Epoche: 2 [1644/96 (1712%)]\tLoss: 225.737061\n",
      "Train Epoche: 2 [1645/96 (1714%)]\tLoss: 288.083954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1646/96 (1715%)]\tLoss: 37.065769\n",
      "Train Epoche: 2 [1647/96 (1716%)]\tLoss: 361.182526\n",
      "Train Epoche: 2 [1648/96 (1717%)]\tLoss: 257.925476\n",
      "Train Epoche: 2 [1649/96 (1718%)]\tLoss: 577.424805\n",
      "Train Epoche: 2 [1650/96 (1719%)]\tLoss: 574.202148\n",
      "Train Epoche: 2 [1651/96 (1720%)]\tLoss: 3.872738\n",
      "Train Epoche: 2 [1652/96 (1721%)]\tLoss: 288.242737\n",
      "Train Epoche: 2 [1653/96 (1722%)]\tLoss: 223.992935\n",
      "Train Epoche: 2 [1654/96 (1723%)]\tLoss: 168.424759\n",
      "Train Epoche: 2 [1655/96 (1724%)]\tLoss: 195.167755\n",
      "Train Epoche: 2 [1656/96 (1725%)]\tLoss: 0.943093\n",
      "Train Epoche: 2 [1657/96 (1726%)]\tLoss: 143.102676\n",
      "Train Epoche: 2 [1658/96 (1727%)]\tLoss: 80.096199\n",
      "Train Epoche: 2 [1659/96 (1728%)]\tLoss: 120.068382\n",
      "Train Epoche: 2 [1660/96 (1729%)]\tLoss: 63.411568\n",
      "Train Epoche: 2 [1661/96 (1730%)]\tLoss: 24.527525\n",
      "Train Epoche: 2 [1662/96 (1731%)]\tLoss: 15.771791\n",
      "Train Epoche: 2 [1663/96 (1732%)]\tLoss: 35.414433\n",
      "Train Epoche: 2 [1664/96 (1733%)]\tLoss: 99.539734\n",
      "Train Epoche: 2 [1665/96 (1734%)]\tLoss: 49.125523\n",
      "Train Epoche: 2 [1666/96 (1735%)]\tLoss: 440.051941\n",
      "Train Epoche: 2 [1667/96 (1736%)]\tLoss: 528.896240\n",
      "Train Epoche: 2 [1668/96 (1738%)]\tLoss: 360.440674\n",
      "Train Epoche: 2 [1669/96 (1739%)]\tLoss: 256.020264\n",
      "Train Epoche: 2 [1670/96 (1740%)]\tLoss: 9.516837\n",
      "Train Epoche: 2 [1671/96 (1741%)]\tLoss: 324.269989\n",
      "Train Epoche: 2 [1672/96 (1742%)]\tLoss: 483.200134\n",
      "Train Epoche: 2 [1673/96 (1743%)]\tLoss: 400.655853\n",
      "Train Epoche: 2 [1674/96 (1744%)]\tLoss: 98.883347\n",
      "Train Epoche: 2 [1675/96 (1745%)]\tLoss: 482.172272\n",
      "Train Epoche: 2 [1676/96 (1746%)]\tLoss: 80.961243\n",
      "Train Epoche: 2 [1677/96 (1747%)]\tLoss: 481.885773\n",
      "Train Epoche: 2 [1678/96 (1748%)]\tLoss: 482.098328\n",
      "Train Epoche: 2 [1679/96 (1749%)]\tLoss: 288.857544\n",
      "Train Epoche: 2 [1680/96 (1750%)]\tLoss: 481.983734\n",
      "Train Epoche: 2 [1681/96 (1751%)]\tLoss: 482.659760\n",
      "Train Epoche: 2 [1682/96 (1752%)]\tLoss: 35.562702\n",
      "Train Epoche: 2 [1683/96 (1753%)]\tLoss: 8.813904\n",
      "Train Epoche: 2 [1684/96 (1754%)]\tLoss: 48.445278\n",
      "Train Epoche: 2 [1685/96 (1755%)]\tLoss: 63.702465\n",
      "Train Epoche: 2 [1686/96 (1756%)]\tLoss: 24.609322\n",
      "Train Epoche: 2 [1687/96 (1757%)]\tLoss: 15.588016\n",
      "Train Epoche: 2 [1688/96 (1758%)]\tLoss: 223.592606\n",
      "Train Epoche: 2 [1689/96 (1759%)]\tLoss: 255.142899\n",
      "Train Epoche: 2 [1690/96 (1760%)]\tLoss: 0.926092\n",
      "Train Epoche: 2 [1691/96 (1761%)]\tLoss: 3.834077\n",
      "Train Epoche: 2 [1692/96 (1762%)]\tLoss: 197.109009\n",
      "Train Epoche: 2 [1693/96 (1764%)]\tLoss: 169.678070\n",
      "Train Epoche: 2 [1694/96 (1765%)]\tLoss: 122.193047\n",
      "Train Epoche: 2 [1695/96 (1766%)]\tLoss: 144.864227\n",
      "Train Epoche: 2 [1696/96 (1767%)]\tLoss: 99.267288\n",
      "Train Epoche: 2 [1697/96 (1768%)]\tLoss: 35.541851\n",
      "Train Epoche: 2 [1698/96 (1769%)]\tLoss: 120.093216\n",
      "Train Epoche: 2 [1699/96 (1770%)]\tLoss: 195.304718\n",
      "Train Epoche: 2 [1700/96 (1771%)]\tLoss: 254.838959\n",
      "Train Epoche: 2 [1701/96 (1772%)]\tLoss: 482.383972\n",
      "Train Epoche: 2 [1702/96 (1773%)]\tLoss: 63.580978\n",
      "Train Epoche: 2 [1703/96 (1774%)]\tLoss: 223.529465\n",
      "Train Epoche: 2 [1704/96 (1775%)]\tLoss: 168.158997\n",
      "Train Epoche: 2 [1705/96 (1776%)]\tLoss: 15.722032\n",
      "Train Epoche: 2 [1706/96 (1777%)]\tLoss: 48.592472\n",
      "Train Epoche: 2 [1707/96 (1778%)]\tLoss: 0.932802\n",
      "Train Epoche: 2 [1708/96 (1779%)]\tLoss: 142.817307\n",
      "Train Epoche: 2 [1709/96 (1780%)]\tLoss: 323.126892\n",
      "Train Epoche: 2 [1710/96 (1781%)]\tLoss: 24.896952\n",
      "Train Epoche: 2 [1711/96 (1782%)]\tLoss: 80.740723\n",
      "Train Epoche: 2 [1712/96 (1783%)]\tLoss: 288.864471\n",
      "Train Epoche: 2 [1713/96 (1784%)]\tLoss: 441.631805\n",
      "Train Epoche: 2 [1714/96 (1785%)]\tLoss: 4.065646\n",
      "Train Epoche: 2 [1715/96 (1786%)]\tLoss: 9.137040\n",
      "Train Epoche: 2 [1716/96 (1788%)]\tLoss: 399.221100\n",
      "Train Epoche: 2 [1717/96 (1789%)]\tLoss: 361.169617\n",
      "Train Epoche: 2 [1718/96 (1790%)]\tLoss: 287.946777\n",
      "Train Epoche: 2 [1719/96 (1791%)]\tLoss: 143.264221\n",
      "Train Epoche: 2 [1720/96 (1792%)]\tLoss: 63.475803\n",
      "Train Epoche: 2 [1721/96 (1793%)]\tLoss: 168.318588\n",
      "Train Epoche: 2 [1722/96 (1794%)]\tLoss: 255.043320\n",
      "Train Epoche: 2 [1723/96 (1795%)]\tLoss: 120.796783\n",
      "Train Epoche: 2 [1724/96 (1796%)]\tLoss: 8.772309\n",
      "Train Epoche: 2 [1725/96 (1797%)]\tLoss: 15.863989\n",
      "Train Epoche: 2 [1726/96 (1798%)]\tLoss: 48.687397\n",
      "Train Epoche: 2 [1727/96 (1799%)]\tLoss: 24.602669\n",
      "Train Epoche: 2 [1728/96 (1800%)]\tLoss: 35.649490\n",
      "Train Epoche: 2 [1729/96 (1801%)]\tLoss: 99.247253\n",
      "Train Epoche: 2 [1730/96 (1802%)]\tLoss: 195.063660\n",
      "Train Epoche: 2 [1731/96 (1803%)]\tLoss: 223.520309\n",
      "Train Epoche: 2 [1732/96 (1804%)]\tLoss: 3.774271\n",
      "Train Epoche: 2 [1733/96 (1805%)]\tLoss: 0.981306\n",
      "Train Epoche: 2 [1734/96 (1806%)]\tLoss: 80.197868\n",
      "Train Epoche: 2 [1735/96 (1807%)]\tLoss: 325.266663\n",
      "Train Epoche: 2 [1736/96 (1808%)]\tLoss: 363.099945\n",
      "Train Epoche: 2 [1737/96 (1809%)]\tLoss: 63.420158\n",
      "Train Epoche: 2 [1738/96 (1810%)]\tLoss: 48.387074\n",
      "Train Epoche: 2 [1739/96 (1811%)]\tLoss: 255.047760\n",
      "Train Epoche: 2 [1740/96 (1812%)]\tLoss: 99.561104\n",
      "Train Epoche: 2 [1741/96 (1814%)]\tLoss: 119.872086\n",
      "Train Epoche: 2 [1742/96 (1815%)]\tLoss: 143.117508\n",
      "Train Epoche: 2 [1743/96 (1816%)]\tLoss: 398.996307\n",
      "Train Epoche: 2 [1744/96 (1817%)]\tLoss: 8.861421\n",
      "Train Epoche: 2 [1745/96 (1818%)]\tLoss: 0.920439\n",
      "Train Epoche: 2 [1746/96 (1819%)]\tLoss: 80.519272\n",
      "Train Epoche: 2 [1747/96 (1820%)]\tLoss: 35.491806\n",
      "Train Epoche: 2 [1748/96 (1821%)]\tLoss: 24.552568\n",
      "Train Epoche: 2 [1749/96 (1822%)]\tLoss: 167.963333\n",
      "Train Epoche: 2 [1750/96 (1823%)]\tLoss: 194.885757\n",
      "Train Epoche: 2 [1751/96 (1824%)]\tLoss: 16.125416\n",
      "Train Epoche: 2 [1752/96 (1825%)]\tLoss: 3.931815\n",
      "Train Epoche: 2 [1753/96 (1826%)]\tLoss: 225.807938\n",
      "Train Epoche: 2 [1754/96 (1827%)]\tLoss: 35.511761\n",
      "Train Epoche: 2 [1755/96 (1828%)]\tLoss: 99.320389\n",
      "Train Epoche: 2 [1756/96 (1829%)]\tLoss: 48.393398\n",
      "Train Epoche: 2 [1757/96 (1830%)]\tLoss: 483.180176\n",
      "Train Epoche: 2 [1758/96 (1831%)]\tLoss: 483.445251\n",
      "Train Epoche: 2 [1759/96 (1832%)]\tLoss: 481.915070\n",
      "Train Epoche: 2 [1760/96 (1833%)]\tLoss: 15.714147\n",
      "Train Epoche: 2 [1761/96 (1834%)]\tLoss: 143.045441\n",
      "Train Epoche: 2 [1762/96 (1835%)]\tLoss: 24.445354\n",
      "Train Epoche: 2 [1763/96 (1836%)]\tLoss: 481.593231\n",
      "Train Epoche: 2 [1764/96 (1838%)]\tLoss: 8.679015\n",
      "Train Epoche: 2 [1765/96 (1839%)]\tLoss: 482.283203\n",
      "Train Epoche: 2 [1766/96 (1840%)]\tLoss: 482.931915\n",
      "Train Epoche: 2 [1767/96 (1841%)]\tLoss: 3.942039\n",
      "Train Epoche: 2 [1768/96 (1842%)]\tLoss: 0.902325\n",
      "Train Epoche: 2 [1769/96 (1843%)]\tLoss: 168.949295\n",
      "Train Epoche: 2 [1770/96 (1844%)]\tLoss: 122.148178\n",
      "Train Epoche: 2 [1771/96 (1845%)]\tLoss: 63.081474\n",
      "Train Epoche: 2 [1772/96 (1846%)]\tLoss: 194.906845\n",
      "Train Epoche: 2 [1773/96 (1847%)]\tLoss: 81.270630\n",
      "Train Epoche: 2 [1774/96 (1848%)]\tLoss: 143.420410\n",
      "Train Epoche: 2 [1775/96 (1849%)]\tLoss: 195.673218\n",
      "Train Epoche: 2 [1776/96 (1850%)]\tLoss: 24.658865\n",
      "Train Epoche: 2 [1777/96 (1851%)]\tLoss: 35.543259\n",
      "Train Epoche: 2 [1778/96 (1852%)]\tLoss: 288.024445\n",
      "Train Epoche: 2 [1779/96 (1853%)]\tLoss: 168.302597\n",
      "Train Epoche: 2 [1780/96 (1854%)]\tLoss: 15.771091\n",
      "Train Epoche: 2 [1781/96 (1855%)]\tLoss: 8.755177\n",
      "Train Epoche: 2 [1782/96 (1856%)]\tLoss: 398.493774\n",
      "Train Epoche: 2 [1783/96 (1857%)]\tLoss: 120.803825\n",
      "Train Epoche: 2 [1784/96 (1858%)]\tLoss: 80.297050\n",
      "Train Epoche: 2 [1785/96 (1859%)]\tLoss: 398.313477\n",
      "Train Epoche: 2 [1786/96 (1860%)]\tLoss: 99.158195\n",
      "Train Epoche: 2 [1787/96 (1861%)]\tLoss: 63.278965\n",
      "Train Epoche: 2 [1788/96 (1862%)]\tLoss: 0.937124\n",
      "Train Epoche: 2 [1789/96 (1864%)]\tLoss: 3.818222\n",
      "Train Epoche: 2 [1790/96 (1865%)]\tLoss: 49.043888\n",
      "Train Epoche: 2 [1791/96 (1866%)]\tLoss: 400.858307\n",
      "Train Epoche: 2 [1792/96 (1867%)]\tLoss: 225.692291\n",
      "Train Epoche: 2 [1793/96 (1868%)]\tLoss: 256.108765\n",
      "Train Epoche: 2 [1794/96 (1869%)]\tLoss: 35.470810\n",
      "Train Epoche: 2 [1795/96 (1870%)]\tLoss: 142.914703\n",
      "Train Epoche: 2 [1796/96 (1871%)]\tLoss: 167.985458\n",
      "Train Epoche: 2 [1797/96 (1872%)]\tLoss: 8.736508\n",
      "Train Epoche: 2 [1798/96 (1873%)]\tLoss: 119.974579\n",
      "Train Epoche: 2 [1799/96 (1874%)]\tLoss: 80.405022\n",
      "Train Epoche: 2 [1800/96 (1875%)]\tLoss: 48.301727\n",
      "Train Epoche: 2 [1801/96 (1876%)]\tLoss: 15.657121\n",
      "Train Epoche: 2 [1802/96 (1877%)]\tLoss: 99.438774\n",
      "Train Epoche: 2 [1803/96 (1878%)]\tLoss: 63.411415\n",
      "Train Epoche: 2 [1804/96 (1879%)]\tLoss: 24.536867\n",
      "Train Epoche: 2 [1805/96 (1880%)]\tLoss: 0.953542\n",
      "Train Epoche: 2 [1806/96 (1881%)]\tLoss: 194.842987\n",
      "Train Epoche: 2 [1807/96 (1882%)]\tLoss: 223.557983\n",
      "Train Epoche: 2 [1808/96 (1883%)]\tLoss: 485.871704\n",
      "Train Epoche: 2 [1809/96 (1884%)]\tLoss: 4.078220\n",
      "Train Epoche: 2 [1810/96 (1885%)]\tLoss: 485.911041\n",
      "Train Epoche: 2 [1811/96 (1886%)]\tLoss: 289.183502\n",
      "Train Epoche: 2 [1812/96 (1888%)]\tLoss: 486.099426\n",
      "Train Epoche: 2 [1813/96 (1889%)]\tLoss: 482.869720\n",
      "Train Epoche: 2 [1814/96 (1890%)]\tLoss: 254.898315\n",
      "Train Epoche: 2 [1815/96 (1891%)]\tLoss: 323.512726\n",
      "Train Epoche: 2 [1816/96 (1892%)]\tLoss: 482.052917\n",
      "Train Epoche: 2 [1817/96 (1893%)]\tLoss: 99.209274\n",
      "Train Epoche: 2 [1818/96 (1894%)]\tLoss: 63.491856\n",
      "Train Epoche: 2 [1819/96 (1895%)]\tLoss: 80.354881\n",
      "Train Epoche: 2 [1820/96 (1896%)]\tLoss: 120.492401\n",
      "Train Epoche: 2 [1821/96 (1897%)]\tLoss: 35.631252\n",
      "Train Epoche: 2 [1822/96 (1898%)]\tLoss: 482.678009\n",
      "Train Epoche: 2 [1823/96 (1899%)]\tLoss: 3.852994\n",
      "Train Epoche: 2 [1824/96 (1900%)]\tLoss: 223.956055\n",
      "Train Epoche: 2 [1825/96 (1901%)]\tLoss: 255.157730\n",
      "Train Epoche: 2 [1826/96 (1902%)]\tLoss: 15.746134\n",
      "Train Epoche: 2 [1827/96 (1903%)]\tLoss: 48.272888\n",
      "Train Epoche: 2 [1828/96 (1904%)]\tLoss: 142.978973\n",
      "Train Epoche: 2 [1829/96 (1905%)]\tLoss: 195.087479\n",
      "Train Epoche: 2 [1830/96 (1906%)]\tLoss: 8.860018\n",
      "Train Epoche: 2 [1831/96 (1907%)]\tLoss: 0.955582\n",
      "Train Epoche: 2 [1832/96 (1908%)]\tLoss: 168.592163\n",
      "Train Epoche: 2 [1833/96 (1909%)]\tLoss: 291.163269\n",
      "Train Epoche: 2 [1834/96 (1910%)]\tLoss: 26.093639\n",
      "Train Epoche: 2 [1835/96 (1911%)]\tLoss: 485.895233\n",
      "Train Epoche: 2 [1836/96 (1912%)]\tLoss: 482.326416\n",
      "Train Epoche: 2 [1837/96 (1914%)]\tLoss: 99.163116\n",
      "Train Epoche: 2 [1838/96 (1915%)]\tLoss: 24.587540\n",
      "Train Epoche: 2 [1839/96 (1916%)]\tLoss: 120.193123\n",
      "Train Epoche: 2 [1840/96 (1917%)]\tLoss: 35.669525\n",
      "Train Epoche: 2 [1841/96 (1918%)]\tLoss: 195.178528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [1842/96 (1919%)]\tLoss: 15.672968\n",
      "Train Epoche: 2 [1843/96 (1920%)]\tLoss: 63.322708\n",
      "Train Epoche: 2 [1844/96 (1921%)]\tLoss: 80.393616\n",
      "Train Epoche: 2 [1845/96 (1922%)]\tLoss: 48.679543\n",
      "Train Epoche: 2 [1846/96 (1923%)]\tLoss: 3.896017\n",
      "Train Epoche: 2 [1847/96 (1924%)]\tLoss: 8.774626\n",
      "Train Epoche: 2 [1848/96 (1925%)]\tLoss: 482.570923\n",
      "Train Epoche: 2 [1849/96 (1926%)]\tLoss: 482.189209\n",
      "Train Epoche: 2 [1850/96 (1927%)]\tLoss: 0.954049\n",
      "Train Epoche: 2 [1851/96 (1928%)]\tLoss: 484.204376\n",
      "Train Epoche: 2 [1852/96 (1929%)]\tLoss: 225.783661\n",
      "Train Epoche: 2 [1853/96 (1930%)]\tLoss: 171.416351\n",
      "Train Epoche: 2 [1854/96 (1931%)]\tLoss: 144.237671\n",
      "Train Epoche: 2 [1855/96 (1932%)]\tLoss: 291.673035\n",
      "Train Epoche: 2 [1856/96 (1933%)]\tLoss: 257.506714\n",
      "Train Epoche: 2 [1857/96 (1934%)]\tLoss: 35.511337\n",
      "Train Epoche: 2 [1858/96 (1935%)]\tLoss: 99.534500\n",
      "Train Epoche: 2 [1859/96 (1936%)]\tLoss: 24.688202\n",
      "Train Epoche: 2 [1860/96 (1938%)]\tLoss: 15.692994\n",
      "Train Epoche: 2 [1861/96 (1939%)]\tLoss: 48.830849\n",
      "Train Epoche: 2 [1862/96 (1940%)]\tLoss: 168.502579\n",
      "Train Epoche: 2 [1863/96 (1941%)]\tLoss: 483.474945\n",
      "Train Epoche: 2 [1864/96 (1942%)]\tLoss: 143.198700\n",
      "Train Epoche: 2 [1865/96 (1943%)]\tLoss: 8.784319\n",
      "Train Epoche: 2 [1866/96 (1944%)]\tLoss: 80.394943\n",
      "Train Epoche: 2 [1867/96 (1945%)]\tLoss: 120.723648\n",
      "Train Epoche: 2 [1868/96 (1946%)]\tLoss: 224.505508\n",
      "Train Epoche: 2 [1869/96 (1947%)]\tLoss: 194.767685\n",
      "Train Epoche: 2 [1870/96 (1948%)]\tLoss: 255.174622\n",
      "Train Epoche: 2 [1871/96 (1949%)]\tLoss: 3.921489\n",
      "Train Epoche: 2 [1872/96 (1950%)]\tLoss: 0.972065\n",
      "Train Epoche: 2 [1873/96 (1951%)]\tLoss: 324.659515\n",
      "Train Epoche: 2 [1874/96 (1952%)]\tLoss: 65.279694\n",
      "Train Epoche: 2 [1875/96 (1953%)]\tLoss: 289.313751\n",
      "Train Epoche: 2 [1876/96 (1954%)]\tLoss: 99.406075\n",
      "Train Epoche: 2 [1877/96 (1955%)]\tLoss: 224.916489\n",
      "Train Epoche: 2 [1878/96 (1956%)]\tLoss: 195.427322\n",
      "Train Epoche: 2 [1879/96 (1957%)]\tLoss: 79.945351\n",
      "Train Epoche: 2 [1880/96 (1958%)]\tLoss: 63.402199\n",
      "Train Epoche: 2 [1881/96 (1959%)]\tLoss: 8.831953\n",
      "Train Epoche: 2 [1882/96 (1960%)]\tLoss: 15.674010\n",
      "Train Epoche: 2 [1883/96 (1961%)]\tLoss: 35.596405\n",
      "Train Epoche: 2 [1884/96 (1962%)]\tLoss: 143.524414\n",
      "Train Epoche: 2 [1885/96 (1964%)]\tLoss: 3.887080\n",
      "Train Epoche: 2 [1886/96 (1965%)]\tLoss: 0.927910\n",
      "Train Epoche: 2 [1887/96 (1966%)]\tLoss: 575.225891\n",
      "Train Epoche: 2 [1888/96 (1967%)]\tLoss: 120.436584\n",
      "Train Epoche: 2 [1889/96 (1968%)]\tLoss: 573.925354\n",
      "Train Epoche: 2 [1890/96 (1969%)]\tLoss: 168.327896\n",
      "Train Epoche: 2 [1891/96 (1970%)]\tLoss: 574.182739\n",
      "Train Epoche: 2 [1892/96 (1971%)]\tLoss: 398.762177\n",
      "Train Epoche: 2 [1893/96 (1972%)]\tLoss: 289.650940\n",
      "Train Epoche: 2 [1894/96 (1973%)]\tLoss: 257.223022\n",
      "Train Epoche: 2 [1895/96 (1974%)]\tLoss: 25.810270\n",
      "Train Epoche: 2 [1896/96 (1975%)]\tLoss: 47.895061\n",
      "Train Epoche: 2 [1897/96 (1976%)]\tLoss: 325.515259\n",
      "Train Epoche: 2 [1898/96 (1977%)]\tLoss: 361.794312\n",
      "Train Epoche: 2 [1899/96 (1978%)]\tLoss: 24.773840\n",
      "Train Epoche: 2 [1900/96 (1979%)]\tLoss: 81.031265\n",
      "Train Epoche: 2 [1901/96 (1980%)]\tLoss: 48.937874\n",
      "Train Epoche: 2 [1902/96 (1981%)]\tLoss: 254.770279\n",
      "Train Epoche: 2 [1903/96 (1982%)]\tLoss: 143.480011\n",
      "Train Epoche: 2 [1904/96 (1983%)]\tLoss: 168.709671\n",
      "Train Epoche: 2 [1905/96 (1984%)]\tLoss: 120.907578\n",
      "Train Epoche: 2 [1906/96 (1985%)]\tLoss: 35.712261\n",
      "Train Epoche: 2 [1907/96 (1986%)]\tLoss: 485.091797\n",
      "Train Epoche: 2 [1908/96 (1988%)]\tLoss: 15.865356\n",
      "Train Epoche: 2 [1909/96 (1989%)]\tLoss: 483.099426\n",
      "Train Epoche: 2 [1910/96 (1990%)]\tLoss: 63.601833\n",
      "Train Epoche: 2 [1911/96 (1991%)]\tLoss: 8.860966\n",
      "Train Epoche: 2 [1912/96 (1992%)]\tLoss: 483.300262\n",
      "Train Epoche: 2 [1913/96 (1993%)]\tLoss: 195.234619\n",
      "Train Epoche: 2 [1914/96 (1994%)]\tLoss: 225.333862\n",
      "Train Epoche: 2 [1915/96 (1995%)]\tLoss: 0.941332\n",
      "Train Epoche: 2 [1916/96 (1996%)]\tLoss: 3.879164\n",
      "Train Epoche: 2 [1917/96 (1997%)]\tLoss: 290.502045\n",
      "Train Epoche: 2 [1918/96 (1998%)]\tLoss: 325.702972\n",
      "Train Epoche: 2 [1919/96 (1999%)]\tLoss: 100.652283\n",
      "Train Epoche: 2 [1920/96 (2000%)]\tLoss: 489.357971\n",
      "Train Epoche: 2 [1921/96 (2001%)]\tLoss: 63.857052\n",
      "Train Epoche: 2 [1922/96 (2002%)]\tLoss: 80.320259\n",
      "Train Epoche: 2 [1923/96 (2003%)]\tLoss: 573.620056\n",
      "Train Epoche: 2 [1924/96 (2004%)]\tLoss: 0.926786\n",
      "Train Epoche: 2 [1925/96 (2005%)]\tLoss: 168.283722\n",
      "Train Epoche: 2 [1926/96 (2006%)]\tLoss: 142.806778\n",
      "Train Epoche: 2 [1927/96 (2007%)]\tLoss: 3.821044\n",
      "Train Epoche: 2 [1928/96 (2008%)]\tLoss: 224.313568\n",
      "Train Epoche: 2 [1929/96 (2009%)]\tLoss: 99.259819\n",
      "Train Epoche: 2 [1930/96 (2010%)]\tLoss: 195.384872\n",
      "Train Epoche: 2 [1931/96 (2011%)]\tLoss: 119.982353\n",
      "Train Epoche: 2 [1932/96 (2012%)]\tLoss: 35.558235\n",
      "Train Epoche: 2 [1933/96 (2014%)]\tLoss: 24.812565\n",
      "Train Epoche: 2 [1934/96 (2015%)]\tLoss: 574.423523\n",
      "Train Epoche: 2 [1935/96 (2016%)]\tLoss: 49.008705\n",
      "Train Epoche: 2 [1936/96 (2017%)]\tLoss: 576.697083\n",
      "Train Epoche: 2 [1937/96 (2018%)]\tLoss: 360.797943\n",
      "Train Epoche: 2 [1938/96 (2019%)]\tLoss: 575.588745\n",
      "Train Epoche: 2 [1939/96 (2020%)]\tLoss: 256.387543\n",
      "Train Epoche: 2 [1940/96 (2021%)]\tLoss: 288.988342\n",
      "Train Epoche: 2 [1941/96 (2022%)]\tLoss: 8.906572\n",
      "Train Epoche: 2 [1942/96 (2023%)]\tLoss: 15.447618\n",
      "Train Epoche: 2 [1943/96 (2024%)]\tLoss: 324.490326\n",
      "Train Epoche: 2 [1944/96 (2025%)]\tLoss: 575.431946\n",
      "Train Epoche: 2 [1945/96 (2026%)]\tLoss: 287.895050\n",
      "Train Epoche: 2 [1946/96 (2027%)]\tLoss: 398.246185\n",
      "Train Epoche: 2 [1947/96 (2028%)]\tLoss: 63.450016\n",
      "Train Epoche: 2 [1948/96 (2029%)]\tLoss: 48.305058\n",
      "Train Epoche: 2 [1949/96 (2030%)]\tLoss: 167.804749\n",
      "Train Epoche: 2 [1950/96 (2031%)]\tLoss: 398.578003\n",
      "Train Epoche: 2 [1951/96 (2032%)]\tLoss: 143.128159\n",
      "Train Epoche: 2 [1952/96 (2033%)]\tLoss: 194.729568\n",
      "Train Epoche: 2 [1953/96 (2034%)]\tLoss: 24.500851\n",
      "Train Epoche: 2 [1954/96 (2035%)]\tLoss: 8.821716\n",
      "Train Epoche: 2 [1955/96 (2036%)]\tLoss: 80.227638\n",
      "Train Epoche: 2 [1956/96 (2038%)]\tLoss: 35.517532\n",
      "Train Epoche: 2 [1957/96 (2039%)]\tLoss: 15.661417\n",
      "Train Epoche: 2 [1958/96 (2040%)]\tLoss: 99.323395\n",
      "Train Epoche: 2 [1959/96 (2041%)]\tLoss: 322.235382\n",
      "Train Epoche: 2 [1960/96 (2042%)]\tLoss: 254.588531\n",
      "Train Epoche: 2 [1961/96 (2043%)]\tLoss: 1.046760\n",
      "Train Epoche: 2 [1962/96 (2044%)]\tLoss: 3.732930\n",
      "Train Epoche: 2 [1963/96 (2045%)]\tLoss: 225.110596\n",
      "Train Epoche: 2 [1964/96 (2046%)]\tLoss: 121.060349\n",
      "Train Epoche: 2 [1965/96 (2047%)]\tLoss: 0.931943\n",
      "Train Epoche: 2 [1966/96 (2048%)]\tLoss: 8.902049\n",
      "Train Epoche: 2 [1967/96 (2049%)]\tLoss: 143.281937\n",
      "Train Epoche: 2 [1968/96 (2050%)]\tLoss: 195.775177\n",
      "Train Epoche: 2 [1969/96 (2051%)]\tLoss: 168.288452\n",
      "Train Epoche: 2 [1970/96 (2052%)]\tLoss: 255.210419\n",
      "Train Epoche: 2 [1971/96 (2053%)]\tLoss: 574.268127\n",
      "Train Epoche: 2 [1972/96 (2054%)]\tLoss: 224.058365\n",
      "Train Epoche: 2 [1973/96 (2055%)]\tLoss: 3.808231\n",
      "Train Epoche: 2 [1974/96 (2056%)]\tLoss: 24.660162\n",
      "Train Epoche: 2 [1975/96 (2057%)]\tLoss: 63.520176\n",
      "Train Epoche: 2 [1976/96 (2058%)]\tLoss: 80.399170\n",
      "Train Epoche: 2 [1977/96 (2059%)]\tLoss: 15.743832\n",
      "Train Epoche: 2 [1978/96 (2060%)]\tLoss: 574.093689\n",
      "Train Epoche: 2 [1979/96 (2061%)]\tLoss: 98.941032\n",
      "Train Epoche: 2 [1980/96 (2062%)]\tLoss: 120.313728\n",
      "Train Epoche: 2 [1981/96 (2064%)]\tLoss: 35.989159\n",
      "Train Epoche: 2 [1982/96 (2065%)]\tLoss: 48.262100\n",
      "Train Epoche: 2 [1983/96 (2066%)]\tLoss: 288.458313\n",
      "Train Epoche: 2 [1984/96 (2067%)]\tLoss: 322.316071\n",
      "Train Epoche: 2 [1985/96 (2068%)]\tLoss: 361.153900\n",
      "Train Epoche: 2 [1986/96 (2069%)]\tLoss: 574.966736\n",
      "Train Epoche: 2 [1987/96 (2070%)]\tLoss: 400.715729\n",
      "Train Epoche: 2 [1988/96 (2071%)]\tLoss: 573.742676\n",
      "Train Epoche: 2 [1989/96 (2072%)]\tLoss: 63.449089\n",
      "Train Epoche: 2 [1990/96 (2073%)]\tLoss: 80.346848\n",
      "Train Epoche: 2 [1991/96 (2074%)]\tLoss: 195.476624\n",
      "Train Epoche: 2 [1992/96 (2075%)]\tLoss: 255.185135\n",
      "Train Epoche: 2 [1993/96 (2076%)]\tLoss: 99.229225\n",
      "Train Epoche: 2 [1994/96 (2077%)]\tLoss: 398.325043\n",
      "Train Epoche: 2 [1995/96 (2078%)]\tLoss: 0.959600\n",
      "Train Epoche: 2 [1996/96 (2079%)]\tLoss: 8.809570\n",
      "Train Epoche: 2 [1997/96 (2080%)]\tLoss: 168.743958\n",
      "Train Epoche: 2 [1998/96 (2081%)]\tLoss: 48.657749\n",
      "Train Epoche: 2 [1999/96 (2082%)]\tLoss: 24.672022\n",
      "Train Epoche: 2 [2000/96 (2083%)]\tLoss: 15.697653\n",
      "Train Epoche: 2 [2001/96 (2084%)]\tLoss: 223.920120\n",
      "Train Epoche: 2 [2002/96 (2085%)]\tLoss: 120.419945\n",
      "Train Epoche: 2 [2003/96 (2086%)]\tLoss: 143.954163\n",
      "Train Epoche: 2 [2004/96 (2088%)]\tLoss: 35.892479\n",
      "Train Epoche: 2 [2005/96 (2089%)]\tLoss: 289.948364\n",
      "Train Epoche: 2 [2006/96 (2090%)]\tLoss: 441.570557\n",
      "Train Epoche: 2 [2007/96 (2091%)]\tLoss: 3.900406\n",
      "Train Epoche: 2 [2008/96 (2092%)]\tLoss: 484.572021\n",
      "Train Epoche: 2 [2009/96 (2093%)]\tLoss: 365.938324\n",
      "Train Epoche: 2 [2010/96 (2094%)]\tLoss: 324.858246\n",
      "Train Epoche: 2 [2011/96 (2095%)]\tLoss: 574.190247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoche: 2 [2012/96 (2096%)]\tLoss: 8.717880\n",
      "Train Epoche: 2 [2013/96 (2097%)]\tLoss: 255.301102\n",
      "Train Epoche: 2 [2014/96 (2098%)]\tLoss: 99.241745\n",
      "Train Epoche: 2 [2015/96 (2099%)]\tLoss: 79.984276\n",
      "Train Epoche: 2 [2016/96 (2100%)]\tLoss: 167.752243\n",
      "Train Epoche: 2 [2017/96 (2101%)]\tLoss: 573.900696\n",
      "Train Epoche: 2 [2018/96 (2102%)]\tLoss: 15.572942\n",
      "Train Epoche: 2 [2019/96 (2103%)]\tLoss: 63.484024\n",
      "Train Epoche: 2 [2020/96 (2104%)]\tLoss: 48.618473\n",
      "Train Epoche: 2 [2021/96 (2105%)]\tLoss: 120.083748\n",
      "Train Epoche: 2 [2022/96 (2106%)]\tLoss: 3.806166\n",
      "Train Epoche: 2 [2023/96 (2107%)]\tLoss: 0.947203\n",
      "Train Epoche: 2 [2024/96 (2108%)]\tLoss: 142.874649\n",
      "Train Epoche: 2 [2025/96 (2109%)]\tLoss: 573.958618\n",
      "Train Epoche: 2 [2026/96 (2110%)]\tLoss: 35.366634\n",
      "Train Epoche: 2 [2027/96 (2111%)]\tLoss: 24.264486\n",
      "Train Epoche: 2 [2028/96 (2112%)]\tLoss: 223.982513\n",
      "Train Epoche: 2 [2029/96 (2114%)]\tLoss: 196.146942\n",
      "Train Epoche: 2 [2030/96 (2115%)]\tLoss: 323.290741\n",
      "Train Epoche: 2 [2031/96 (2116%)]\tLoss: 288.925629\n",
      "Train Epoche: 2 [2032/96 (2117%)]\tLoss: 361.012543\n",
      "Train Epoche: 2 [2033/96 (2118%)]\tLoss: 576.119568\n"
     ]
    }
   ],
   "source": [
    "max_epochs = h.opt_combination['epochen']\n",
    "max_epochs_dyn = h_dynamic.opt_combination['epochen']\n",
    "lr = h.opt_combination['lr']\n",
    "lr_dynamic = h_dynamic.opt_combination['lr']\n",
    "\n",
    "#cuda = input('Cuda? [y/n]: ')\n",
    "opt_combination = julian\n",
    "cuda = 'n'\n",
    "model = Netz()\n",
    "opt={}\n",
    "for key in opt_combination.keys():\n",
    "#for key in julian.keys():\n",
    "    if key != 'mae':#mae wird noch in dictionary von HP_Optimizer hinzugefügt und muss entfernt werden\n",
    "        opt[key] = opt_combination[key]\n",
    "#model_dynamic = NetzDynamic(opt)\n",
    "model_dynamic = NetzDynamic(julian)\n",
    "if cuda.lower() == 'y':\n",
    "    model.cuda() \n",
    "    model_dynamic.cuda()\n",
    "\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)     \n",
    "optimizer_d = optim.Adam(model_dynamic.parameters(), lr = lr_dynamic)   \n",
    "\n",
    "def train_cuda(epoch):\n",
    "    model.train()\n",
    "    batch_id = 0\n",
    "    for key in train_T.keys():\n",
    "        \n",
    "        for data, target in train_T[key]:\n",
    "            data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            #print(\"Out: \", out, out.size())\n",
    "            #print(\"Target: \", target, target.size())\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id *len(data), len(train),\n",
    "            100. * batch_id / len(train), loss.item()))\n",
    "            batch_id +=1\n",
    "        \n",
    "    #random.shuffle(train_data)\n",
    "def train_(epoch):\n",
    "    model.train()\n",
    "    batch_id = 0\n",
    "    for key in train_T.keys():\n",
    "        for data, target in train_T[key]:\n",
    "            #data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)#.cuda()\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1)#.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            #print(\"Out: \", out, out.size())\n",
    "            #print(\"Target: \", target, target.size())\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id *len(data), len(train),\n",
    "            100. * batch_id / len(train), loss.item()))\n",
    "            batch_id +=1\n",
    "            \n",
    "def train_cuda_dynamic(epoch):\n",
    "    model_dynamic.train()\n",
    "    batch_id = 0\n",
    "    for key in train_T.keys():\n",
    "        \n",
    "        for data, target in train_T[key]:\n",
    "            data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out = model_dynamic(data)\n",
    "            #print(\"Out: \", out, out.size())\n",
    "            #print(\"Target: \", target, target.size())\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id *len(data), len(train),\n",
    "            100. * batch_id / len(train), loss.item()))\n",
    "            batch_id +=1\n",
    "        \n",
    "    #random.shuffle(train_data)\n",
    "def train_dynamic(epoch):\n",
    "    model.train()\n",
    "    batch_id = 0\n",
    "    for key in train_T.keys():\n",
    "        for data, target in train_T[key]:\n",
    "            #data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)#.cuda()\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1)#.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out = model_dynamic(data)\n",
    "            #print(\"Out: \", out, out.size())\n",
    "            #print(\"Target: \", target, target.size())\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id *len(data), len(train),\n",
    "            100. * batch_id / len(train), loss.item()))\n",
    "            batch_id +=1\n",
    "        \n",
    "    #random.shuffle(train_data)\n",
    "if cuda.lower() == 'y':\n",
    "    for epoch in range(1,max_epochs):\n",
    "        train_cuda(epoch) \n",
    "    for epoch in range(1,max_epochs_dyn):\n",
    "        train_cuda_dynamic(epoch) \n",
    "else:\n",
    "    for epoch in range(1,max_epochs):\n",
    "        train_(epoch)  \n",
    "    for epoch in range(1,max_epochs_dyn):\n",
    "        train_dynamic(epoch)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_times_cuda(test_final):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    help_dict = {}\n",
    "    for key in test_final.keys():\n",
    "        #print(key)\n",
    "        help_dict = {}\n",
    "        for data, target in test_final[key]:\n",
    "            model.eval()\n",
    "            #files.listdir(path)\n",
    "            data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1).cuda()\n",
    "            out = model(data).cpu()\n",
    "            #print(out)\n",
    "            out = out.detach().numpy()\n",
    "            #out = np.round(out)\n",
    "            target = target.cpu()\n",
    "            target = target.detach().numpy()\n",
    "            #print(data)\n",
    "            #print(data[\"driverId\"])\n",
    "            total += abs(out - target[0][0])\n",
    "            #print(\"current_position: \", data[0][0].item())\n",
    "            #print(\"Output: \", out)\n",
    "            #print(\"Target: \", target)\n",
    "            help_dict[target[0][0]] = out\n",
    "            #print(\"Difference: \", out - target)\n",
    "            count+=1\n",
    "        #help_dict = sorted(help_dict.items(), key=operator.itemgetter(1))\n",
    "        result_dict[key] = help_dict\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "def test_times(test_final):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    help_dict = {}\n",
    "    for key in test_final.keys():\n",
    "        #print(key)\n",
    "        help_dict = {}\n",
    "        for data, target in test_final[key]:\n",
    "            model.eval()\n",
    "            #files.listdir(path)\n",
    "            #data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1)#.cuda()\n",
    "            out = model(data)#.cpu()\n",
    "            #print(out)\n",
    "            out = out.detach().numpy()\n",
    "            #out = np.round(out)\n",
    "            #target = target.cpu()\n",
    "            target = target.detach().numpy()\n",
    "            #print(data)\n",
    "            #print(data[\"driverId\"])\n",
    "            total += abs(out - target[0][0])\n",
    "            #print(\"current_position: \", data[0][0].item())\n",
    "            #print(\"Output: \", out)\n",
    "            #print(\"Target: \", target)\n",
    "            help_dict[target[0][0]] = out\n",
    "            #print(\"Difference: \", out - target)\n",
    "            count+=1\n",
    "        #help_dict = sorted(help_dict.items(), key=operator.itemgetter(1))\n",
    "        result_dict[key] = help_dict\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "def test_times_cuda_dynamic(test_final):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    help_dict = {}\n",
    "    for key in test_final.keys():\n",
    "        #print(key)\n",
    "        help_dict = {}\n",
    "        for data, target in test_final[key]:\n",
    "            model_dynamic.eval()\n",
    "            #files.listdir(path)\n",
    "            data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1).cuda()\n",
    "            out = model_dynamic(data).cpu()\n",
    "            #print(out)\n",
    "            out = out.detach().numpy()\n",
    "            #out = np.round(out)\n",
    "            target = target.cpu()\n",
    "            target = target.detach().numpy()\n",
    "            #print(data)\n",
    "            #print(data[\"driverId\"])\n",
    "            total += abs(out - target[0][0])\n",
    "            #print(\"current_position: \", data[0][0].item())\n",
    "            #print(\"Output: \", out)\n",
    "            #print(\"Target: \", target)\n",
    "            help_dict[target[0][0]] = out\n",
    "            #print(\"Difference: \", out - target)\n",
    "            count+=1\n",
    "        #help_dict = sorted(help_dict.items(), key=operator.itemgetter(1))\n",
    "        result_dict[key] = help_dict\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "def test_times_dynamic(test_final):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    result_dict = {}\n",
    "    help_dict = {}\n",
    "    for key in test_final.keys():\n",
    "        #print(key)\n",
    "        help_dict = {}\n",
    "        for data, target in test_final[key]:\n",
    "            model.eval()\n",
    "            #files.listdir(path)\n",
    "            #data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1)#.cuda()\n",
    "            out = model_dynamic(data)#.cpu()\n",
    "            #print(out)\n",
    "            out = out.detach().numpy()\n",
    "            #out = np.round(out)\n",
    "            #target = target.cpu()\n",
    "            target = target.detach().numpy()\n",
    "            #print(data)\n",
    "            #print(data[\"driverId\"])\n",
    "            total += abs(out - target[0][0])\n",
    "            #print(\"current_position: \", data[0][0].item())\n",
    "            #print(\"Output: \", out)\n",
    "            #print(\"Target: \", target)\n",
    "            help_dict[target[0][0]] = out\n",
    "            #print(\"Difference: \", out - target)\n",
    "            count+=1\n",
    "        #help_dict = sorted(help_dict.items(), key=operator.itemgetter(1))\n",
    "        result_dict[key] = help_dict\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "if cuda.lower() =='y':\n",
    "    total_results = test_times_cuda(test_T)\n",
    "    total_results_dynamic = test_times_cuda_dynamic(test_T)\n",
    "else:\n",
    "    total_results = test_times(test_T)\n",
    "    total_results_dynamic = test_times_dynamic(test_T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dfs = {}\n",
    "for raceId, dict in total_results.items():    \n",
    "    #Auswerten der Vorhersagen aus den Outputdaten des Modells\n",
    "    A = [x[0][0] for x in list((dict.values()))]\n",
    "    A_dyn = [x[0][0] for x in list((total_results_dynamic[raceId].values()))]\n",
    "    y = list(dict.keys())\n",
    "    y_dyn = list(total_results_dynamic[raceId].keys())\n",
    "    t = pd.DataFrame(columns = ['target', 'prediction'])\n",
    "    t_dyn = pd.DataFrame(columns = ['target', 'prediction'])\n",
    "    t['target'] = y\n",
    "    t['prediction'] = A\n",
    "    t_dyn['target'] = y_dyn\n",
    "    t_dyn['prediction'] = A_dyn\n",
    "    #print(\"raceId\", raceId)\n",
    "    temp = split_by_race[raceId]\n",
    "    #name = split_by_race[raceId].where()\n",
    "    #sortieren des DataFrames nach den vorhergesagten Positionen, aufsteigend\n",
    "    end = sqldf.sqldf('''select * from t order by prediction ASC''')\n",
    "    end.reset_index(inplace = True)\n",
    "    end_dyn = sqldf.sqldf('''select * from t_dyn order by prediction ASC''')\n",
    "    end_dyn.reset_index(inplace = True)\n",
    "    #Da DF nun nach prediction aufsteigend sortiert ist, kann veränderter Index als predictete Position gesetzt werden\n",
    "    end.rename(columns = {'index': 'predicted_position'},inplace = True)\n",
    "    end_dyn.rename(columns = {'index': 'predicted_position'},inplace = True)\n",
    "    #zur Übersichtlichkeit wird nun der endgültige DF nach den richtigen Positionen (target) sortiert (aufsteigend)\n",
    "    end = sqldf.sqldf('''select * from end order by target ASC''')\n",
    "    end_dyn = sqldf.sqldf('''select * from end_dyn order by target ASC''')\n",
    "    end['predicted_position'] = end['predicted_position']+1\n",
    "    end_dyn['predicted_position'] = end_dyn['predicted_position']+1\n",
    "    #pred_name = sqldf.sqldf(\"\"\"select \n",
    "    #            distinct\n",
    "    #            d.driver_fullname as name,\n",
    "    #            t.predicted_position as pred,\n",
    "    #            d.podium_position as target_position\n",
    "    #            from \n",
    "    #            end t inner join temp d\n",
    "    #            on t.predicted_position = d.podium_position\"\"\")\n",
    "    end[\"driver_pred\"] = 0\n",
    "    end[\"driver_target\"] = 0\n",
    "    end_dyn[\"driver_pred\"] = 0\n",
    "    end_dyn[\"driver_target\"] = 0\n",
    "    for pos in temp.podium_position.unique():\n",
    "        \n",
    "        name = temp.where(temp.podium_position == pos).dropna(how = \"all\")\n",
    "        #print(name)\n",
    "        name_ = name[\"driver_fullname\"][list(name.index)[0]]\n",
    "        idx_target = end.where(end.target == pos).dropna(how = \"all\").index\n",
    "        idx_pred = end.where(end.predicted_position == pos).dropna(how = \"all\").index\n",
    "        end.loc[idx_target,\"driver_target\"] = name_\n",
    "        end.loc[idx_pred,\"driver_pred\"] = name_\n",
    "        \n",
    "        idx_target = end_dyn.where(end_dyn.target == pos).dropna(how = \"all\").index\n",
    "        idx_pred = end_dyn.where(end_dyn.predicted_position == pos).dropna(how = \"all\").index\n",
    "        end_dyn.loc[idx_target,\"driver_target\"] = name_\n",
    "        end_dyn.loc[idx_pred,\"driver_pred\"] = name_\n",
    "        \n",
    "    #umstellen der Spaltenreihenfolge\n",
    "    '''end = end[['target', 'predicted_position', 'prediction',\"driver_target\", \"driver_pred\"]]\n",
    "    end_dyn = end_dyn[['target', 'predicted_position', 'prediction',\"driver_target\", \"driver_pred\"]]'''\n",
    "    end = end[['target', 'predicted_position', 'prediction']]\n",
    "    end_dyn = end_dyn[['target', 'predicted_position', 'prediction']]\n",
    "    result_dfs[raceId] = end\n",
    "    r = str(raceId)+'_dynamic'\n",
    "    result_dfs[r] = end_dyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaceId: 977\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    4.608701\n",
      "1      2.0                   2    6.688302\n",
      "2      3.0                   5    7.273826\n",
      "3      4.0                   3    7.031137\n",
      "4      5.0                   4    7.176562\n",
      "5      6.0                   8   10.772938\n",
      "6      7.0                   7   10.440473\n",
      "7      8.0                   6   10.280284\n",
      "8      9.0                   9   14.760722\n",
      "9     10.0                  10   15.240193\n",
      "10    11.0                  11   16.974724\n",
      "11    12.0                  14   18.260279\n",
      "12    13.0                  12   17.071960\n",
      "13    14.0                  13   18.122776\n",
      "14    15.0                  17   20.587730\n",
      "15    16.0                  15   19.672127\n",
      "16    20.0                  16   20.417309 \n",
      "\n",
      "RaceId: 977_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                  17    0.049432\n",
      "1      2.0                   5    0.009425\n",
      "2      3.0                  11    0.032367\n",
      "3      4.0                   2   -0.028189\n",
      "4      5.0                   3   -0.016876\n",
      "5      6.0                  14    0.039604\n",
      "6      7.0                   7    0.018225\n",
      "7      8.0                  16    0.042908\n",
      "8      9.0                  13    0.035095\n",
      "9     10.0                   6    0.017031\n",
      "10    11.0                  15    0.042220\n",
      "11    12.0                   9    0.018849\n",
      "12    13.0                  12    0.033962\n",
      "13    14.0                   8    0.018378\n",
      "14    15.0                  10    0.031207\n",
      "15    16.0                   4    0.002636\n",
      "16    20.0                   1   -0.102972 \n",
      "\n",
      "RaceId: 916\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    3.757093\n",
      "1      2.0                   2    4.061953\n",
      "2      3.0                   4    7.043853\n",
      "3      4.0                   3    6.894292\n",
      "4      5.0                   5    7.562785\n",
      "5      6.0                   6    8.540191\n",
      "6      7.0                  13   15.127810\n",
      "7      8.0                   7    9.783912\n",
      "8      9.0                  12   13.195449\n",
      "9     10.0                  11   12.890233\n",
      "10    11.0                   9   12.708781\n",
      "11    12.0                   8   11.561585\n",
      "12    13.0                  10   12.859460\n",
      "13    14.0                  15   17.898487\n",
      "14    15.0                  14   16.306726\n",
      "15    22.0                  16   26.265982 \n",
      "\n",
      "RaceId: 916_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   6    0.005291\n",
      "1      2.0                  10    0.014652\n",
      "2      3.0                  15    0.045202\n",
      "3      4.0                   4   -0.008615\n",
      "4      5.0                  16    0.060085\n",
      "5      6.0                   9    0.012033\n",
      "6      7.0                   7    0.011413\n",
      "7      8.0                  13    0.030325\n",
      "8      9.0                   1   -0.047982\n",
      "9     10.0                  11    0.016908\n",
      "10    11.0                   2   -0.038766\n",
      "11    12.0                   8    0.011526\n",
      "12    13.0                   3   -0.019688\n",
      "13    14.0                  12    0.028197\n",
      "14    15.0                   5    0.003084\n",
      "15    22.0                  14    0.036420 \n",
      "\n",
      "RaceId: 850\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   2    4.323573\n",
      "1      2.0                   3    4.423704\n",
      "2      3.0                   1    3.235024\n",
      "3      4.0                   4    5.031270\n",
      "4      5.0                   5    5.732529\n",
      "5      6.0                   8    9.048547\n",
      "6      7.0                   6    5.732950\n",
      "7      8.0                   7    8.124899\n",
      "8      9.0                   9   12.918879\n",
      "9     10.0                  10   14.172988\n",
      "10    11.0                  12   15.728959\n",
      "11    12.0                  13   16.731743\n",
      "12    13.0                  14   16.873363\n",
      "13    14.0                  11   14.293384\n",
      "14    15.0                  18   21.054394\n",
      "15    16.0                  15   19.020330\n",
      "16    17.0                  16   19.180609\n",
      "17    18.0                  20   21.662823\n",
      "18    19.0                  19   21.528818\n",
      "19    20.0                  17   19.888676\n",
      "20    24.0                  21   21.871569 \n",
      "\n",
      "RaceId: 850_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                  20    0.046044\n",
      "1      2.0                  11    0.029442\n",
      "2      3.0                  18    0.044396\n",
      "3      4.0                  10    0.022784\n",
      "4      5.0                  16    0.039979\n",
      "5      6.0                  17    0.042730\n",
      "6      7.0                   5   -0.005304\n",
      "7      8.0                   6    0.004827\n",
      "8      9.0                  19    0.045991\n",
      "9     10.0                  14    0.031760\n",
      "10    11.0                  21    0.048668\n",
      "11    12.0                  13    0.031304\n",
      "12    13.0                   8    0.008418\n",
      "13    14.0                  12    0.031035\n",
      "14    15.0                   9    0.018576\n",
      "15    16.0                  15    0.038817\n",
      "16    17.0                   2   -0.052232\n",
      "17    18.0                   7    0.005020\n",
      "18    19.0                   3   -0.015181\n",
      "19    20.0                   1   -0.096477\n",
      "20    24.0                   4   -0.006504 \n",
      "\n",
      "RaceId: 914\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   2    3.139410\n",
      "1      2.0                   1    2.896689\n",
      "2      3.0                   4    7.022326\n",
      "3      4.0                   3    6.663863\n",
      "4      5.0                   5    7.401717\n",
      "5      6.0                   6    7.544441\n",
      "6      7.0                   7    8.340919\n",
      "7      8.0                  10   11.379123\n",
      "8      9.0                  22   21.902756\n",
      "9     10.0                  11   12.674686\n",
      "10    11.0                  13   12.986519\n",
      "11    12.0                   9   10.762557\n",
      "12    13.0                  19   17.345753\n",
      "13    14.0                  20   18.440342\n",
      "14    15.0                  12   12.925539\n",
      "15    16.0                  14   15.162858\n",
      "16    17.0                  17   17.034622\n",
      "17    18.0                  21   20.032261\n",
      "18    19.0                  18   17.137707\n",
      "19    20.0                  16   16.545820\n",
      "20    21.0                  15   16.531988\n",
      "21    22.0                   8    9.247701 \n",
      "\n",
      "RaceId: 914_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   4   -0.013760\n",
      "1      2.0                  14    0.024303\n",
      "2      3.0                  18    0.036465\n",
      "3      4.0                  21    0.042648\n",
      "4      5.0                  13    0.023145\n",
      "5      6.0                   8    0.006668\n",
      "6      7.0                   9    0.010769\n",
      "7      8.0                  19    0.037841\n",
      "8      9.0                  16    0.034009\n",
      "9     10.0                  11    0.020939\n",
      "10    11.0                  12    0.020953\n",
      "11    12.0                  15    0.030644\n",
      "12    13.0                  10    0.014647\n",
      "13    14.0                  17    0.034653\n",
      "14    15.0                   6   -0.004417\n",
      "15    16.0                   1   -0.113382\n",
      "16    17.0                   3   -0.016582\n",
      "17    18.0                   7   -0.003531\n",
      "18    19.0                  22    0.044152\n",
      "19    20.0                   2   -0.076879\n",
      "20    21.0                   5   -0.010768\n",
      "21    22.0                  20    0.041413 \n",
      "\n",
      "RaceId: 895\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    3.206903\n",
      "1      2.0                   2    4.301144\n",
      "2      3.0                   4    5.982826\n",
      "3      4.0                   5    6.133799\n",
      "4      5.0                   7    9.357502\n",
      "5      6.0                   3    5.726243\n",
      "6      7.0                   9   11.174831\n",
      "7      8.0                   8   10.754193\n",
      "8      9.0                  14   15.932184\n",
      "9     10.0                   6    6.787096\n",
      "10    11.0                  13   15.392959\n",
      "11    12.0                  19   20.564199\n",
      "12    13.0                  15   16.009048\n",
      "13    14.0                  16   18.780502\n",
      "14    15.0                  10   11.223981\n",
      "15    16.0                  12   14.606763\n",
      "16    17.0                  11   13.297225\n",
      "17    18.0                  20   21.647915\n",
      "18    19.0                  18   20.459618\n",
      "19    22.0                  17   20.369358 \n",
      "\n",
      "RaceId: 895_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   5    0.010125\n",
      "1      2.0                  16    0.038249\n",
      "2      3.0                   3   -0.056528\n",
      "3      4.0                   6    0.017057\n",
      "4      5.0                   2   -0.077424\n",
      "5      6.0                   7    0.025124\n",
      "6      7.0                  18    0.048037\n",
      "7      8.0                   8    0.025601\n",
      "8      9.0                  10    0.028351\n",
      "9     10.0                  20    0.054471\n",
      "10    11.0                  19    0.048571\n",
      "11    12.0                   9    0.028244\n",
      "12    13.0                  13    0.032441\n",
      "13    14.0                  17    0.039865\n",
      "14    15.0                  15    0.035266\n",
      "15    16.0                  12    0.032067\n",
      "16    17.0                  11    0.030240\n",
      "17    18.0                   1   -0.129109\n",
      "18    19.0                   4    0.008581\n",
      "19    22.0                  14    0.033633 \n",
      "\n",
      "RaceId: 937\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    3.271915\n",
      "1      2.0                   2    3.704617\n",
      "2      3.0                   6    8.522925\n",
      "3      4.0                   5    8.459086\n",
      "4      5.0                   7   10.304982\n",
      "5      6.0                   4    7.868340\n",
      "6      7.0                   9   11.744938\n",
      "7      8.0                  10   12.605297\n",
      "8      9.0                   8   11.735207\n",
      "9     10.0                  11   13.912089\n",
      "10    11.0                  12   15.509250\n",
      "11    12.0                   3    7.654602\n",
      "12    13.0                  13   17.779270\n",
      "13    14.0                  16   19.645470\n",
      "14    15.0                  14   17.850189\n",
      "15    16.0                  15   18.930355\n",
      "16    20.0                  17   26.101318 \n",
      "\n",
      "RaceId: 937_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                  16    0.043347\n",
      "1      2.0                  10    0.033886\n",
      "2      3.0                   4   -0.029622\n",
      "3      4.0                  11    0.038518\n",
      "4      5.0                   5    0.014493\n",
      "5      6.0                  14    0.041985\n",
      "6      7.0                   8    0.028765\n",
      "7      8.0                   9    0.032083\n",
      "8      9.0                  17    0.043905\n",
      "9     10.0                  13    0.039967\n",
      "10    11.0                  12    0.039231\n",
      "11    12.0                  15    0.042751\n",
      "12    13.0                   7    0.028048\n",
      "13    14.0                   6    0.026365\n",
      "14    15.0                   1   -0.088218\n",
      "15    16.0                   3   -0.049881\n",
      "16    20.0                   2   -0.061695 \n",
      "\n",
      "RaceId: 859\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    3.291831\n",
      "1      2.0                   2    3.403213\n",
      "2      3.0                   4    5.452568\n",
      "3      4.0                   3    4.529085\n",
      "4      5.0                   6    6.185384\n",
      "5      6.0                   7   10.191387\n",
      "6      7.0                   5    6.165661\n",
      "7      8.0                   8   10.506331\n",
      "8      9.0                   9   14.143505\n",
      "9     10.0                  13   15.435987\n",
      "10    11.0                  10   14.315902\n",
      "11    12.0                  11   14.340753\n",
      "12    13.0                  14   16.591236\n",
      "13    14.0                  15   17.121208\n",
      "14    15.0                  12   14.923440\n",
      "15    16.0                  17   18.062359\n",
      "16    17.0                  16   18.002846\n",
      "17    18.0                  18   20.502481\n",
      "18    19.0                  19   22.472073\n",
      "19    20.0                  20   23.972393\n",
      "20    24.0                  21   24.596134 \n",
      "\n",
      "RaceId: 859_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                  21    0.047882\n",
      "1      2.0                  12    0.031409\n",
      "2      3.0                  17    0.037165\n",
      "3      4.0                  18    0.038827\n",
      "4      5.0                   8    0.020851\n",
      "5      6.0                  14    0.032998\n",
      "6      7.0                   6   -0.002904\n",
      "7      8.0                  13    0.032237\n",
      "8      9.0                  20    0.042145\n",
      "9     10.0                   9    0.023008\n",
      "10    11.0                  10    0.025221\n",
      "11    12.0                  16    0.033380\n",
      "12    13.0                  11    0.026867\n",
      "13    14.0                   7    0.016608\n",
      "14    15.0                   5   -0.019795\n",
      "15    16.0                   1   -0.075052\n",
      "16    17.0                  19    0.041419\n",
      "17    18.0                   2   -0.051462\n",
      "18    19.0                   4   -0.020402\n",
      "19    20.0                  15    0.033143\n",
      "20    24.0                   3   -0.045988 \n",
      "\n",
      "RaceId: 933\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   2    4.664098\n",
      "1      2.0                   1    3.999485\n",
      "2      3.0                   4    7.928703\n",
      "3      4.0                   3    6.236262\n",
      "4      5.0                   5    9.302206\n",
      "5      6.0                   7   10.313209\n",
      "6      7.0                   8   10.630432\n",
      "7      8.0                   9   12.619611\n",
      "8      9.0                   6    9.845833\n",
      "9     10.0                  10   12.900940\n",
      "10    11.0                  11   13.374888\n",
      "11    12.0                  12   13.488832\n",
      "12    13.0                  13   15.802909\n",
      "13    14.0                  14   18.539618\n",
      "14    20.0                  15   30.271030 \n",
      "\n",
      "RaceId: 933_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   3   -0.005192\n",
      "1      2.0                  14    0.044179\n",
      "2      3.0                  15    0.052680\n",
      "3      4.0                  12    0.031350\n",
      "4      5.0                  10    0.019677\n",
      "5      6.0                  11    0.023704\n",
      "6      7.0                   5    0.002706\n",
      "7      8.0                   7    0.006388\n",
      "8      9.0                   6    0.005163\n",
      "9     10.0                   8    0.008801\n",
      "10    11.0                   9    0.014819\n",
      "11    12.0                   4    0.002658\n",
      "12    13.0                  13    0.040946\n",
      "13    14.0                   1   -0.049943\n",
      "14    20.0                   2   -0.016348 \n",
      "\n",
      "RaceId: 961\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    3.150578\n",
      "1      2.0                   2    3.848136\n",
      "2      3.0                   3    3.892678\n",
      "3      4.0                   4    4.684368\n",
      "4      5.0                   5    6.172566\n",
      "5      6.0                   6    6.975805\n",
      "6      7.0                   7    8.318064\n",
      "7      8.0                   8    8.396846\n",
      "8      9.0                  10   12.056281\n",
      "9     10.0                   9   10.753736\n",
      "10    11.0                  13   15.694425\n",
      "11    12.0                  12   15.522874\n",
      "12    13.0                  14   16.748249\n",
      "13    14.0                  11   13.970038\n",
      "14    15.0                  15   18.748272\n",
      "15    16.0                  18   25.082346\n",
      "16    17.0                  19   25.936958\n",
      "17    18.0                  17   22.655090\n",
      "18    22.0                  16   19.127527 \n",
      "\n",
      "RaceId: 961_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   3   -0.008821\n",
      "1      2.0                   5    0.010500\n",
      "2      3.0                  16    0.046115\n",
      "3      4.0                  12    0.037707\n",
      "4      5.0                  13    0.039398\n",
      "5      6.0                   9    0.026831\n",
      "6      7.0                   7    0.023135\n",
      "7      8.0                  15    0.045517\n",
      "8      9.0                  17    0.047326\n",
      "9     10.0                  11    0.033210\n",
      "10    11.0                   1   -0.078334\n",
      "11    12.0                  19    0.049202\n",
      "12    13.0                   4    0.000531\n",
      "13    14.0                  10    0.027211\n",
      "14    15.0                   8    0.024170\n",
      "15    16.0                  18    0.048933\n",
      "16    17.0                  14    0.042259\n",
      "17    18.0                   6    0.019013\n",
      "18    22.0                   2   -0.025427 \n",
      "\n",
      "RaceId: 849\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   3    4.438317\n",
      "1      2.0                   1    3.142769\n",
      "2      3.0                   2    3.635562\n",
      "3      4.0                   6    8.245680\n",
      "4      5.0                   4    6.565201\n",
      "5      6.0                   5    7.872661\n",
      "6      7.0                   9   13.073941\n",
      "7      8.0                  10   13.575425\n",
      "8      9.0                   8   11.670512\n",
      "9     10.0                  11   15.584268\n",
      "10    11.0                   7   10.155707\n",
      "11    12.0                  12   16.499142\n",
      "12    13.0                  14   17.755968\n",
      "13    14.0                  15   18.108448\n",
      "14    15.0                  13   16.504070\n",
      "15    16.0                  17   20.256153\n",
      "16    17.0                  18   21.468811\n",
      "17    18.0                  19   22.418009\n",
      "18    19.0                  16   18.768387\n",
      "19    24.0                  20   28.309752 \n",
      "\n",
      "RaceId: 849_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                  15    0.039900\n",
      "1      2.0                  20    0.045626\n",
      "2      3.0                  12    0.033152\n",
      "3      4.0                   6    0.013707\n",
      "4      5.0                  10    0.024187\n",
      "5      6.0                   7    0.019001\n",
      "6      7.0                  18    0.041786\n",
      "7      8.0                  17    0.041317\n",
      "8      9.0                  13    0.035104\n",
      "9     10.0                   8    0.023128\n",
      "10    11.0                  19    0.044078\n",
      "11    12.0                  11    0.025479\n",
      "12    13.0                  16    0.040457\n",
      "13    14.0                   9    0.024117\n",
      "14    15.0                  14    0.035230\n",
      "15    16.0                   1   -0.039645\n",
      "16    17.0                   2   -0.024354\n",
      "17    18.0                   4    0.003504\n",
      "18    19.0                   3   -0.013339\n",
      "19    24.0                   5    0.012748 \n",
      "\n",
      "RaceId: 931\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   2    3.463095\n",
      "1      2.0                   3    4.879926\n",
      "2      3.0                   1    3.158865\n",
      "3      4.0                   5    6.992380\n",
      "4      5.0                   6    7.484138\n",
      "5      6.0                   4    6.655976\n",
      "6      7.0                   7    9.549592\n",
      "7      8.0                   9   12.743148\n",
      "8      9.0                  11   13.361831\n",
      "9     10.0                  10   13.317679\n",
      "10    11.0                  13   17.529655\n",
      "11    12.0                   8   12.189072\n",
      "12    13.0                  15   19.004774\n",
      "13    14.0                  12   14.519334\n",
      "14    15.0                  14   17.897430\n",
      "15    16.0                  16   19.349667\n",
      "16    17.0                  18   19.454403\n",
      "17    20.0                  17   19.371819 \n",
      "\n",
      "RaceId: 931_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                  16    0.054509\n",
      "1      2.0                  11    0.035326\n",
      "2      3.0                   3    0.003650\n",
      "3      4.0                  15    0.047556\n",
      "4      5.0                  13    0.042181\n",
      "5      6.0                  12    0.039688\n",
      "6      7.0                  14    0.044349\n",
      "7      8.0                   8    0.031724\n",
      "8      9.0                  10    0.033067\n",
      "9     10.0                   5    0.021710\n",
      "10    11.0                  17    0.055958\n",
      "11    12.0                   1   -0.063094\n",
      "12    13.0                   4    0.020365\n",
      "13    14.0                   6    0.022123\n",
      "14    15.0                   9    0.032214\n",
      "15    16.0                   2   -0.010734\n",
      "16    17.0                   7    0.029603\n",
      "17    20.0                  18    0.059803 \n",
      "\n",
      "RaceId: 948\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    3.673914\n",
      "1      2.0                   3    4.766872\n",
      "2      3.0                   2    4.642877\n",
      "3      4.0                   4    5.999123\n",
      "4      5.0                   7    8.129961\n",
      "5      6.0                   8    9.634169\n",
      "6      7.0                   9   10.028468\n",
      "7      8.0                  11   12.533306\n",
      "8      9.0                   6    7.037904\n",
      "9     10.0                   5    6.133418\n",
      "10    11.0                  12   12.548656\n",
      "11    12.0                  15   14.983571\n",
      "12    13.0                  10   12.026172\n",
      "13    14.0                  14   13.962089\n",
      "14    15.0                  16   16.944805\n",
      "15    16.0                  13   13.737624\n",
      "16    22.0                  17   18.145264 \n",
      "\n",
      "RaceId: 948_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                  10    0.026573\n",
      "1      2.0                   4   -0.011318\n",
      "2      3.0                  12    0.033571\n",
      "3      4.0                  15    0.039183\n",
      "4      5.0                  13    0.034644\n",
      "5      6.0                   1   -0.051287\n",
      "6      7.0                   9    0.016953\n",
      "7      8.0                   7    0.011699\n",
      "8      9.0                  14    0.035264\n",
      "9     10.0                  16    0.039790\n",
      "10    11.0                   6    0.008566\n",
      "11    12.0                   8    0.014748\n",
      "12    13.0                  11    0.033122\n",
      "13    14.0                   5    0.000026\n",
      "14    15.0                  17    0.043024\n",
      "15    16.0                   3   -0.026739\n",
      "16    22.0                   2   -0.047010 \n",
      "\n",
      "RaceId: 841\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   1    3.070523\n",
      "1      2.0                   2    3.915323\n",
      "2      3.0                   3    4.851281\n",
      "3      4.0                   6    6.278488\n",
      "4      5.0                   4    5.104171\n",
      "5      6.0                   7    6.983726\n",
      "6      7.0                   5    5.718679\n",
      "7      8.0                   8    9.861912\n",
      "8      9.0                   9   11.899505\n",
      "9     10.0                  10   12.191176\n",
      "10    11.0                  11   14.155154\n",
      "11    12.0                  12   14.164865\n",
      "12    13.0                  13   16.105232\n",
      "13    14.0                  15   16.895437\n",
      "14    24.0                  14   16.484848 \n",
      "\n",
      "RaceId: 841_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                  13    0.041737\n",
      "1      2.0                   5    0.024496\n",
      "2      3.0                   8    0.037438\n",
      "3      4.0                   7    0.037086\n",
      "4      5.0                  14    0.043048\n",
      "5      6.0                   9    0.039054\n",
      "6      7.0                   6    0.032738\n",
      "7      8.0                  15    0.043269\n",
      "8      9.0                  11    0.039961\n",
      "9     10.0                  10    0.039954\n",
      "10    11.0                   3    0.017424\n",
      "11    12.0                  12    0.040310\n",
      "12    13.0                   4    0.023907\n",
      "13    14.0                   1   -0.060933\n",
      "14    24.0                   2   -0.022226 \n",
      "\n",
      "RaceId: 878\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                   2    6.363161\n",
      "1      2.0                   1    5.305524\n",
      "2      3.0                   3    7.992776\n",
      "3      4.0                   7    9.224631\n",
      "4      5.0                   5    8.840098\n",
      "5      6.0                   6    9.146325\n",
      "6      7.0                  11   11.695652\n",
      "7      8.0                  10   11.521380\n",
      "8      9.0                  15   14.395468\n",
      "9     10.0                  13   13.366005\n",
      "10    11.0                  16   16.268623\n",
      "11    12.0                   8   11.013916\n",
      "12    13.0                   9   11.322665\n",
      "13    14.0                  17   17.795135\n",
      "14    15.0                  14   14.362619\n",
      "15    16.0                  12   12.661729\n",
      "16    17.0                  18   19.862238\n",
      "17    18.0                  19   20.639860\n",
      "18    19.0                  20   20.851089\n",
      "19    20.0                  21   22.625538\n",
      "20    21.0                  22   23.678503\n",
      "21    22.0                  23   25.412422\n",
      "22    24.0                   4    8.760448 \n",
      "\n",
      "RaceId: 878_dynamic\n",
      "    target  predicted_position  prediction\n",
      "0      1.0                  10   -0.005287\n",
      "1      2.0                   9   -0.006243\n",
      "2      3.0                  23    0.041846\n",
      "3      4.0                  20    0.029286\n",
      "4      5.0                  21    0.031568\n",
      "5      6.0                   3   -0.061401\n",
      "6      7.0                   1   -0.074412\n",
      "7      8.0                  14    0.025588\n",
      "8      9.0                   5   -0.026111\n",
      "9     10.0                  13    0.015838\n",
      "10    11.0                  19    0.028650\n",
      "11    12.0                  12    0.011222\n",
      "12    13.0                   7   -0.020574\n",
      "13    14.0                  15    0.025600\n",
      "14    15.0                  11    0.008088\n",
      "15    16.0                  17    0.026904\n",
      "16    17.0                   8   -0.016255\n",
      "17    18.0                  22    0.038706\n",
      "18    19.0                   6   -0.023641\n",
      "19    20.0                   2   -0.067750\n",
      "20    21.0                   4   -0.041210\n",
      "21    22.0                  18    0.028357\n",
      "22    24.0                  16    0.026595 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in result_dfs.items():\n",
    "    print('RaceId:',key)\n",
    "    print(value,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetzDynamic(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=10, out_features=150, bias=True)\n",
       "    (1): Linear(in_features=150, out_features=180, bias=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=180, out_features=190, bias=True)\n",
       "    (4): Linear(in_features=190, out_features=120, bias=True)\n",
       "    (5): Linear(in_features=120, out_features=100, bias=True)\n",
       "    (6): Linear(in_features=100, out_features=70, bias=True)\n",
       "    (7): Linear(in_features=70, out_features=30, bias=True)\n",
       "    (8): Linear(in_features=30, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Netz(\n",
       "  (fc1): Linear(in_features=10, out_features=150, bias=True)\n",
       "  (fc2): Linear(in_features=150, out_features=180, bias=True)\n",
       "  (fc3): Linear(in_features=180, out_features=190, bias=True)\n",
       "  (fc4): Linear(in_features=190, out_features=120, bias=True)\n",
       "  (fc5): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (fc6): Linear(in_features=100, out_features=70, bias=True)\n",
       "  (fc7): Linear(in_features=70, out_features=30, bias=True)\n",
       "  (fc8): Linear(in_features=30, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nogo_columns_hannah_kacke = [#'grid',\n",
    "                #'race_completion',\n",
    "                'lap_position','circuitId','lap_number',\n",
    "                'podium_position', 'raceId',\n",
    "                'grandprix_name', 'driver_fullname',\n",
    "               'constructor_name', 'total_laps',\n",
    "               'status_clean', 'constructorId',\n",
    "                'total_milliseconds', 'driverId'\n",
    "               'lap_in_milliseconds','year', 'stop_binary','constructorId_1.0',\n",
    "                 'constructorId_3.0',\n",
    "                 'constructorId_4.0',\n",
    "                 'constructorId_5.0',\n",
    "                 'constructorId_6.0',\n",
    "                 'constructorId_9.0',\n",
    "                 'constructorId_10.0',\n",
    "                 'constructorId_15.0',\n",
    "                 'constructorId_131.0',\n",
    "                 'constructorId_164.0',\n",
    "                 'constructorId_166.0',\n",
    "                 'constructorId_205.0',\n",
    "                 'constructorId_206.0',\n",
    "                 'constructorId_207.0',\n",
    "                 'constructorId_208.0',\n",
    "                 'constructorId_209.0',\n",
    "                 'constructorId_210.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_netz = {'relu1':['linear',53,150], 'relu2':['linear',150,180], 'relu3':['dropout', 180,100],'relu4':['linear',180,30],'relu5':['linear',30,1]}\n",
    "\n",
    "model_dynamic = NetzDynamic(opt_combination)\n",
    "\n",
    "optimizer = optim.Adam(model_dynamic.parameters(), lr = 0.001)     \n",
    "\n",
    "def train_cuda(epoch):\n",
    "    model.train()\n",
    "    batch_id = 0\n",
    "    for key in train_T.keys():\n",
    "        \n",
    "        for data, target in train_T[key]:\n",
    "            data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0).cuda()\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            #print(\"Out: \", out, out.size())\n",
    "            #print(\"Target: \", target, target.size())\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id *len(data), len(train),\n",
    "            100. * batch_id / len(train), loss.item()))\n",
    "            batch_id +=1\n",
    "        \n",
    "    #random.shuffle(train_data)\n",
    "def train_(epoch):\n",
    "    model.train()\n",
    "    batch_id = 0\n",
    "    for key in train_T.keys():\n",
    "        for data, target in train_T[key]:\n",
    "            #data = data.cuda()\n",
    "            target = torch.Tensor(target).unsqueeze(0)#.cuda()\n",
    "            shape = target.size()[1]\n",
    "            target = target.resize(shape,1)#.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            #print(\"Out: \", out, out.size())\n",
    "            #print(\"Target: \", target, target.size())\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Train Epoche: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_id *len(data), len(train),\n",
    "            100. * batch_id / len(train), loss.item()))\n",
    "            batch_id +=1\n",
    "        \n",
    "\n",
    "for epoch in range(1,3):\n",
    "    train_(epoch)  \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
